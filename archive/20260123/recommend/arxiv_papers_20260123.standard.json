{
  "mode": "standard",
<<<<<<< HEAD
  "generated_at": "2026-01-23T04:38:08.075278+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 6,
    "deep_divecandidates": 7,
    "deep_cap": 11,
    "deep_selected": 7,
    "quick_candidates": 23,
=======
  "generated_at": "2026-01-23T03:57:04.175059+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 6,
    "deep_divecandidates": 10,
    "deep_cap": 11,
    "deep_selected": 10,
    "quick_candidates": 20,
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
    "quick_skim_target": 16,
    "quick_selected": 16
  },
  "deep_dive": [
    {
<<<<<<< HEAD
      "id": "2601.15812v1",
      "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models",
      "abstract": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.",
      "authors": [
        "Shir Ashury-Tahan",
        "Yifan Mai",
        "Elron Bandel",
        "Michal Shmueli-Scheuer",
        "Leshem Choshen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22 09:52:39+00:00",
      "link": "https://arxiv.org/pdf/2601.15812v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Evaluation framework for failure analysis in LLM benchmarks",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.16206v1",
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "authors": [
        "Daixuan Cheng",
        "Shaohan Huang",
        "Yuxian Gu",
        "Huatong Song",
        "Guoxin Chen",
        "Li Dong",
        "Wayne Xin Zhao",
        "Ji-Rong Wen",
        "Furu Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-22 18:57:09+00:00",
      "link": "https://arxiv.org/pdf/2601.16206v1",
      "tags": [
        "keyword:RL",
=======
      "id": "2601.15628v1",
      "title": "CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models",
      "abstract": "Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.",
      "authors": [
        "Haibo Tong",
        "Zeyang Yue",
        "Feifei Zhao",
        "Erliang Lin",
        "Lu Jia",
        "Ruolin Chen",
        "Yinqian Sun",
        "Qian Zhang",
        "Yi Zeng"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 03:59:19+00:00",
      "link": "https://arxiv.org/pdf/2601.15628v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "comprehensive benchmark for frontier LLMs like GPT and Qwen",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.16199v1",
      "title": "PAL*M: Property Attestation for Large Generative Models",
      "abstract": "Machine learning property attestations allow provers (e.g., model providers or owners) to attest properties of their models/datasets to verifiers (e.g., regulators, customers), enabling accountability towards regulations and policies. But, current approaches do not support generative models or large datasets. We present PAL*M, a property attestation framework for large generative models, illustrated using large language models. PAL*M defines properties across training and inference, leverages confidential virtual machines with security-aware GPUs for coverage of CPU-GPU operations, and proposes using incremental multiset hashing over memory-mapped datasets to efficiently track their integrity. We implement PAL*M on Intel TDX and NVIDIA H100, showing it is efficient, scalable, versatile, and secure.",
      "authors": [
        "Prach Chantasantitam",
        "Adam Ilyas Caulfield",
        "Vasisht Duddu",
        "Lachlan J. Gunn",
        "N. Asokan"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR"
      ],
      "published": "2026-01-22 18:51:13+00:00",
      "link": "https://arxiv.org/pdf/2601.16199v1",
      "tags": [
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
        "keyword:ppo",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
<<<<<<< HEAD
      "llm_evidence": "introduces reinforcement learning for LLM agents aligning with RL and PPO interests and LLM queries",
      "llm_tags": [
        "keyword:RL",
        "keyword:ppo",
=======
      "llm_evidence": "framework for property attestation in large language models aligns with LLM evaluation query",
      "llm_tags": [
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
        "query:大厂llm"
      ]
    },
    {
<<<<<<< HEAD
      "id": "2601.15690v1",
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "abstract": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.",
      "authors": [
        "Jiaxin Zhang",
        "Wendi Cui",
        "Zhuohang Li",
        "Lifu Huang",
        "Bradley Malin",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "published": "2026-01-22 06:21:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15690v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）",
=======
      "id": "2601.15625v1",
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.",
      "authors": [
        "Zhiwei Zhang",
        "Fei Zhao",
        "Rui Wang",
        "Zezhong Wang",
        "Bin Liang",
        "Jiakang Wang",
        "Yao Hu",
        "Shaosheng Cao",
        "Kam-Fai Wong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22 03:57:35+00:00",
      "link": "https://arxiv.org/pdf/2601.15625v1",
      "tags": [
        "keyword:RL",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
<<<<<<< HEAD
      "llm_evidence": "survey on LLM uncertainty in reinforcement learning and autonomous agents",
      "llm_tags": [
        "query:大厂llm",
        "keyword:RL"
=======
      "llm_evidence": "Addresses LLM tool use and error recovery using reinforcement learning",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      ]
    },
    {
      "id": "2601.15706v1",
      "title": "Improving Methodologies for LLM Evaluations Across Global Languages",
      "abstract": "As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.   The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.",
      "authors": [
        "Akriti Vij",
        "Benjamin Chua",
        "Darshini Ramiah",
        "En Qi Ng",
        "Mahran Morsidi",
        "Naga Nikshith Gangarapu",
        "Sharmini Johnson",
        "Vanessa Wilfred",
        "Vikneswaran Kumaran",
        "Wan Sie Lee",
        "Wenzhuo Yang",
        "Yongsen Zheng",
        "Bill Black",
        "Boming Xia",
        "Frank Sun",
        "Hao Zhang",
        "Qinghua Lu",
        "Suyu Ma",
        "Yue Liu",
        "Chi-kiu Lo",
        "Fatemeh Azadi",
        "Isar Nejadgholi",
        "Sowmya Vajjala",
        "Agnes Delaborde",
        "Nicolas Rolin",
        "Tom Seimandi",
        "Akiko Murakami",
        "Haruto Ishi",
        "Satoshi Sekine",
        "Takayuki Semitsu",
        "Tasuku Sasaki",
        "Angela Kinuthia",
        "Jean Wangari",
        "Michael Michie",
        "Stephanie Kasaon",
        "Hankyul Baek",
        "Jaewon Noh",
        "Kihyuk Nam",
        "Sang Seo",
        "Sungpil Shin",
        "Taewhi Lee",
        "Yongsu Kim",
        "Daisy Newbold-Harrop",
        "Jessica Wang",
        "Mahmoud Ghanem",
        "Vy Hong"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 07:18:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15706v1",
      "tags": [
        "keyword:ppo",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
<<<<<<< HEAD
      "llm_evidence": "Describes multilingual evaluation benchmarks for frontier AI models",
=======
      "llm_evidence": "Technical evaluation of frontier AI models across global languages and safeguards",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
<<<<<<< HEAD
=======
      "id": "2601.15708v1",
      "title": "Persona Switch: Mixing Distinct Perspectives in Decoding Time",
      "abstract": "Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.",
      "authors": [
        "Junseok Kim",
        "Nakyeong Yang",
        "Kyomin Jung"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 07:30:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15708v1",
      "tags": [
        "keyword:ppo"
      ],
      "llm_score": 8.0,
      "llm_evidence": "describes decoding methodologies and evaluation of widely-used large language models",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.15761v1",
      "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning",
      "abstract": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.",
      "authors": [
        "Xiefeng Wu",
        "Mingyu Hu",
        "Shu Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 08:51:16+00:00",
      "link": "https://arxiv.org/pdf/2601.15761v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Proposes a new off-policy actor-critic method for reinforcement learning",
      "llm_tags": [
        "keyword:RL"
      ]
    },
    {
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "id": "2601.15953v1",
      "title": "Decoupling Return-to-Go for Efficient Decision Transformer",
      "abstract": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.",
      "authors": [
        "Yongyi Wang",
        "Hanyu Liu",
        "Lingfeng Li",
        "Bozhou Chen",
        "Ang Li",
        "Qirui Zheng",
        "Xionghui Yang",
        "Wenxin Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 13:42:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15953v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 8.0,
<<<<<<< HEAD
      "llm_evidence": "Proposes an efficient architecture for offline reinforcement learning by decoupling return-to-go",
=======
      "llm_evidence": "Improves Decision Transformer efficiency for offline reinforcement learning",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "llm_tags": [
        "keyword:RL"
      ]
    },
    {
<<<<<<< HEAD
      "id": "2601.16018v1",
      "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
      "abstract": "This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.",
      "authors": [
        "Özgür Uğur",
        "Mahmut Göksu",
        "Mahmut Çimen",
        "Musa Yılmaz",
        "Esra Şavirdi",
        "Alp Talha Demir",
        "Rumeysa Güllüce",
        "İclal Çetin",
        "Ömer Can Sağbaş"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 14:41:32+00:00",
      "link": "https://arxiv.org/pdf/2601.16018v1",
      "tags": [
        "keyword:ppo",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Technical report on training large language models from scratch",
=======
      "id": "2601.16007v1",
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.",
      "authors": [
        "Chak-Wing Mak",
        "Guanyu Zhu",
        "Boyi Zhang",
        "Hongji Li",
        "Xiaowei Chi",
        "Kevin Zhang",
        "Yichen Wu",
        "Yangfan He",
        "Chun-Kai Fan",
        "Wentao Lu",
        "Kuangzhi Ge",
        "Xinyu Fang",
        "Hongyang He",
        "Kuan Lu",
        "Tianxiang Xu",
        "Li Zhang",
        "Yongxin Ni",
        "Youhua Li",
        "Shanghang Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-22 14:33:01+00:00",
      "link": "https://arxiv.org/pdf/2601.16007v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Benchmark for physical reasoning in foundational multimodal large language models",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.16172v1",
      "title": "Structured Hints for Sample-Efficient Lean Theorem Proving",
      "abstract": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.",
      "authors": [
        "Zachary Burton"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 18:16:46+00:00",
      "link": "https://arxiv.org/pdf/2601.16172v1",
      "tags": [
        "keyword:RL",
        "keyword:ppo",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
<<<<<<< HEAD
      "llm_evidence": "Discusses DeepSeek-Prover-V1.5 which is an industry LLM using RL and PPO-style training",
      "llm_tags": [
        "keyword:RL",
        "keyword:ppo",
=======
      "llm_evidence": "evaluates DeepSeek-Prover which uses LLMs and reinforcement learning",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.16206v1",
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "authors": [
        "Daixuan Cheng",
        "Shaohan Huang",
        "Yuxian Gu",
        "Huatong Song",
        "Guoxin Chen",
        "Li Dong",
        "Wayne Xin Zhao",
        "Ji-Rong Wen",
        "Furu Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-22 18:57:09+00:00",
      "link": "https://arxiv.org/pdf/2601.16206v1",
      "tags": [
        "keyword:RL",
        "keyword:ppo",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
      "llm_evidence": "combines LLM architectures with reinforcement learning for agentic intelligence",
      "llm_tags": [
        "keyword:RL",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
        "query:大厂llm"
      ]
    }
  ],
  "quick_skim": [
    {
<<<<<<< HEAD
      "id": "2601.15645v1",
      "title": "Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation",
      "abstract": "Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.",
      "authors": [
        "Zhiyao Ren",
        "Yibing Zhan",
        "Siyuan Liang",
        "Guozheng Ma",
        "Baosheng Yu",
        "Dacheng Tao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 04:51:39+00:00",
      "link": "https://arxiv.org/pdf/2601.15645v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Benchmarking large language models in specialized medical domains",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15688v1",
      "title": "Performance-guided Reinforced Active Learning for Object Detection",
      "abstract": "Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.",
      "authors": [
        "Zhixuan Liang",
        "Xingyu Zeng",
        "Rui Zhao",
        "Ping Luo"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-01-22 06:17:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15688v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet"
      ],
      "llm_score": 6.0,
      "llm_evidence": "proposes a reinforced active learning approach for object detection tasks",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "6"
    },
    {
=======
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "id": "2601.15668v1",
      "title": "EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning",
      "abstract": "Emotional information in speech plays a unique role in multimodal perception. However, current Speech Large Language Models (SpeechLLMs), similar to conventional speech emotion recognition (SER) systems, still treat emotion understanding as a simple classification problem. This provides limited interpretability of predictions, while leaving the LLMs' expressive and reasoning capabilities underutilized. In this work, we take the first step to reformulate SER as a deep reasoning problem through reinforcement learning (RL). We propose EmotionThinker, which is designed to generate accurate emotion predictions with interpretable explanations grounded in fine-grained acoustic cues. To achieve this, we first construct EmotionCoT-35K, an emotional reasoning dataset with Chain-of-Thought annotations and detailed captions. Second, we observe that current SpeechLLMs exhibit weak prosody perception, whereas prosodic cues constitute fundamental signals for interpreting emotions. To address this, we develop the prosody-enhanced foundation model EmotionThinker-Base, and demonstrate that prosody enhancement improves emotion understanding. Third, we introduce Group-Relative-Policy-Optimization with Progressive-Trust-aware-Reasoning-Reward (GRPO-PTR) for RL. Different from standard GRPO, which relies only on rule-based outcome rewards, GRPO-PTR progressively introduces reasoning reward, dynamically adjusts it with a trustworthiness weight reflecting the alignment between reasoning and outcome, and evaluates the overall reasoning quality with a reward model based on multi-dimensional criteria. EmotionThinker outperforms previous state-of-the-art evaluation models both in emotion accuracy and explanation quality, advancing SER toward interpretable multimodal reasoning. Project page: https://github.com/dingdongwang/EmotionThinker",
      "authors": [
        "Dingdong Wang",
        "Shujie Liu",
        "Tianhua Zhang",
        "Youjun Chen",
        "Jinyu Li",
        "Helen Meng"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD"
      ],
      "published": "2026-01-22 05:51:53+00:00",
      "link": "https://arxiv.org/pdf/2601.15668v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 7.0,
<<<<<<< HEAD
      "llm_evidence": "Reinforcement learning for reasoning in speech LLMs",
      "llm_tags": [
        "keyword:RL",
        "keyword:ppo",
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15707v1",
      "title": "D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot",
      "abstract": "Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.",
      "authors": [
        "Qifan Hu",
        "Branko Celler",
        "Weidong Mu",
        "Steven W. Su"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-01-22 07:20:55+00:00",
      "link": "https://arxiv.org/pdf/2601.15707v1",
      "tags": [
        "keyword:RL",
        "keyword:ppo",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Reinforcement learning applied to robot calibration problems",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15717v1",
      "title": "Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling",
      "abstract": "Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.",
      "authors": [
        "Luyao Zhu",
        "Fangfang Zhang",
        "Yi Mei",
        "Mengjie Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 07:38:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15717v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Genetic Programming for scheduling is a core method in symbolic regression research",
      "llm_tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15708v1",
      "title": "Persona Switch: Mixing Distinct Perspectives in Decoding Time",
      "abstract": "Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.",
      "authors": [
        "Junseok Kim",
        "Nakyeong Yang",
        "Kyomin Jung"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 07:30:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15708v1",
      "tags": [
        "keyword:ppo"
      ],
      "llm_score": 6.0,
      "llm_evidence": "proposes a novel decoding method for LLMs to improve zero-shot reasoning capabilities",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15761v1",
      "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning",
      "abstract": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.",
      "authors": [
        "Xiefeng Wu",
        "Mingyu Hu",
        "Shu Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 08:51:16+00:00",
      "link": "https://arxiv.org/pdf/2601.15761v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "off-policy actor-critic reinforcement learning for real-world robotics",
=======
      "llm_evidence": "Reformulates emotion reasoning as a reinforcement learning problem",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
<<<<<<< HEAD
      "id": "2601.15724v1",
      "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning",
      "abstract": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.",
      "authors": [
        "Chenglin Li",
        "Qianglong Chen",
        "Feng Han",
        "Yikun Wang",
        "Xingxi Yin",
        "Yan Gong",
        "Ruilin Li",
        "Yin Zhang",
        "Jiaqi Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-22 07:47:29+00:00",
      "link": "https://arxiv.org/pdf/2601.15724v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "describes building agentic VideoLLMs using LLM-guided reasoning and synthetic trajectories",
      "llm_tags": [
=======
      "id": "2601.15624v1",
      "title": "Explainable Deepfake Detection with RL Enhanced Self-Blended Images",
      "abstract": "Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.",
      "authors": [
        "Ning Jiang",
        "Dingheng Zeng",
        "Yanhong Liu",
        "Haiyang Yi",
        "Shijie Yu",
        "Minghe Weng",
        "Haifeng Shen",
        "Ying Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-22 03:55:46+00:00",
      "link": "https://arxiv.org/pdf/2601.15624v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Uses reinforcement learning (RL) to enhance multimodal large language model performance",
      "llm_tags": [
        "keyword:RL",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
<<<<<<< HEAD
      "id": "2601.15879v1",
      "title": "Evaluating and Achieving Controllable Code Completion in Code LLM",
      "abstract": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.",
      "authors": [
        "Jiajun Zhang",
        "Zeyu Cui",
        "Lei Zhang",
        "Jian Yang",
        "Jiaxi Yang",
        "Qiang Liu",
        "Zilei Wang",
        "Binyuan Hui",
        "Liang Wang",
        "Junyang Lin"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "published": "2026-01-22 11:40:04+00:00",
      "link": "https://arxiv.org/pdf/2601.15879v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "comprehensive evaluation of 40 mainstream LLMs on a new code benchmark",
      "llm_tags": [
=======
      "id": "2601.15690v1",
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "abstract": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.",
      "authors": [
        "Jiaxin Zhang",
        "Wendi Cui",
        "Zhuohang Li",
        "Lifu Huang",
        "Bradley Malin",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "published": "2026-01-22 06:21:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15690v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Discusses LLM uncertainty and reinforcement learning for self-improvement",
      "llm_tags": [
        "keyword:RL",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
<<<<<<< HEAD
      "id": "2601.15773v1",
      "title": "Next Generation Active Learning: Mixture of LLMs in the Loop",
      "abstract": "With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.",
      "authors": [
        "Yuanyuan Qi",
        "Xiaohao Yang",
        "Jueqing Lu",
        "Guoxiang Guo",
        "Joanne Enticott",
        "Gang Liu",
        "Lan Du"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 09:01:42+00:00",
      "link": "https://arxiv.org/pdf/2601.15773v1",
      "tags": [
=======
      "id": "2601.15727v1",
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "abstract": "The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.",
      "authors": [
        "Yang Yu",
        "Peiyu Zang",
        "Chi Hsu Tsai",
        "Haiming Wu",
        "Yixin Shen",
        "Jialing Zhang",
        "Haoyu Wang",
        "Zhiyou Xiao",
        "Jingze Shi",
        "Yuyu Luo",
        "Wentao Zhang",
        "Chunlei Men",
        "Guang Liu",
        "Yonghua Lin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-22 07:53:52+00:00",
      "link": "https://arxiv.org/pdf/2601.15727v1",
      "tags": [
        "keyword:RL",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
<<<<<<< HEAD
      "llm_evidence": "framework utilizing a mixture of large language models for annotation",
=======
      "llm_evidence": "Discusses LLM-based agents for automated kernel generation and optimization",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
<<<<<<< HEAD
      "id": "2601.16061v1",
      "title": "Dynamic Tactile Sensing System and Soft Actor Critic Reinforcement Learning for Inclusion Characterization",
      "abstract": "This paper presents the Dynamic Tactile Sensing System that utilizes robotic tactile sensing in conjunction with reinforcement learning to locate and characterize embedded inclusions. A dual arm robot is integrated with an optical Tactile Imaging Sensor that utilizes the Soft Actor Critic Algorithm to acquire tactile data based on a pixel intensity reward. A Dynamic Interrogation procedure for tactile exploration is developed that enables the robot to first localize inclusion and refine their positions for precise imaging. Experimental validation conducted on Polydimethylsiloxane phantoms demonstrates that the robot using the Tactile Soft Actor Critic Model was able to achieve size estimation errors of 2.61% and 5.29% for soft and hard inclusions compared to 7.84% and 6.87% for expert human operators. Results also show that Dynamic Tactile Sensing System was able to locate embedded inclusions and autonomously determine their mechanical properties, useful in applications such as breast tumor characterization.",
      "authors": [
        "John Bannan",
        "Nazia Rahman",
        "Chang-Hee Won"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-01-22 15:57:15+00:00",
      "link": "https://arxiv.org/pdf/2601.16061v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 7.0,
      "llm_evidence": "utilizes reinforcement learning for robotic tactile sensing and inclusion characterization",
=======
      "id": "2601.15812v1",
      "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models",
      "abstract": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.",
      "authors": [
        "Shir Ashury-Tahan",
        "Yifan Mai",
        "Elron Bandel",
        "Michal Shmueli-Scheuer",
        "Leshem Choshen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22 09:52:39+00:00",
      "link": "https://arxiv.org/pdf/2601.15812v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Evaluation framework for analyzing failure modes in LLM benchmarks",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15737v1",
      "title": "PhysProver: Advancing Automatic Theorem Proving for Physics",
      "abstract": "The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\\sim$5K training samples, PhysProver achieves an overall 2.4\\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.",
      "authors": [
        "Hanning Zhang",
        "Ruida Wang",
        "Rui Pan",
        "Wenyuan Wang",
        "Bingxu Meng",
        "Tong Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22 08:05:32+00:00",
      "link": "https://arxiv.org/pdf/2601.15737v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Enhances formal reasoning and theorem proving using LLMs and dedicated datasets",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15995v1",
      "title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour",
      "abstract": "Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.",
      "authors": [
        "Liang Wang",
        "Kanzhong Yao",
        "Yang Liu",
        "Weikai Qin",
        "Jun Wu",
        "Zhe Sun",
        "Qiuguo Zhu"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-22 14:16:12+00:00",
      "link": "https://arxiv.org/pdf/2601.15995v1",
      "tags": [
        "keyword:RL",
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Uses reinforcement learning for quadruped locomotion benchmarks",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15793v1",
      "title": "HumanLLM: Towards Personalized Understanding and Simulation of Human Nature",
      "abstract": "Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.",
      "authors": [
        "Yuxuan Lei",
        "Tianfu Wang",
        "Jianxun Lian",
        "Zhengyu Hu",
        "Defu Lian",
        "Xing Xie"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 09:27:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15793v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
<<<<<<< HEAD
      "llm_evidence": "Discusses LLM pretraining and simulation capabilities relevant to large language model technical reports",
=======
      "llm_evidence": "discusses LLM pretraining and behavioral simulation",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
<<<<<<< HEAD
      "id": "2601.16200v1",
      "title": "Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing",
      "abstract": "Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\\% to about 1\\%.",
      "authors": [
        "Song Xia",
        "Meiwen Ding",
        "Chenqi Kong",
        "Wenhan Yang",
        "Xudong Jiang"
=======
      "id": "2601.16061v1",
      "title": "Dynamic Tactile Sensing System and Soft Actor Critic Reinforcement Learning for Inclusion Characterization",
      "abstract": "This paper presents the Dynamic Tactile Sensing System that utilizes robotic tactile sensing in conjunction with reinforcement learning to locate and characterize embedded inclusions. A dual arm robot is integrated with an optical Tactile Imaging Sensor that utilizes the Soft Actor Critic Algorithm to acquire tactile data based on a pixel intensity reward. A Dynamic Interrogation procedure for tactile exploration is developed that enables the robot to first localize inclusion and refine their positions for precise imaging. Experimental validation conducted on Polydimethylsiloxane phantoms demonstrates that the robot using the Tactile Soft Actor Critic Model was able to achieve size estimation errors of 2.61% and 5.29% for soft and hard inclusions compared to 7.84% and 6.87% for expert human operators. Results also show that Dynamic Tactile Sensing System was able to locate embedded inclusions and autonomously determine their mechanical properties, useful in applications such as breast tumor characterization.",
      "authors": [
        "John Bannan",
        "Nazia Rahman",
        "Chang-Hee Won"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-01-22 15:57:15+00:00",
      "link": "https://arxiv.org/pdf/2601.16061v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 7.0,
      "llm_evidence": "utilizes reinforcement learning for robotic tactile sensing and characterization",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15808v1",
      "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
      "abstract": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.",
      "authors": [
        "Yuxuan Wan",
        "Tianqing Fang",
        "Zaitang Li",
        "Yintong Huo",
        "Wenxuan Wang",
        "Haitao Mi",
        "Dong Yu",
        "Michael R. Lyu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 09:47:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15808v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Focuses on inference-time scaling and verification for research agents",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16093v1",
      "title": "SAMTok: Representing Any Mask with Two Words",
      "abstract": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
      "authors": [
        "Yikang Zhou",
        "Tao Zhang",
        "Dengxian Gong",
        "Yuanzheng Wu",
        "Ye Tian",
        "Haochen Wang",
        "Haobo Yuan",
        "Jiacong Wang",
        "Lu Qi",
        "Hao Fei",
        "Anran Wang",
        "Zhuochen Wang",
        "Yujing Wang",
        "Cheng Chen",
        "Shunping Ji",
        "Xiangtai Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-22 16:44:09+00:00",
      "link": "https://arxiv.org/pdf/2601.16093v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Uses reinforcement learning and QwenVL series LLMs for pixel-wise capabilities",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15871v1",
      "title": "Why Inference in Large Models Becomes Decomposable After Training",
      "abstract": "Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.",
      "authors": [
        "Jidong Jin"
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
<<<<<<< HEAD
        "cs.CV"
      ],
      "published": "2026-01-22 18:52:21+00:00",
      "link": "https://arxiv.org/pdf/2601.16200v1",
=======
        "cs.AI"
      ],
      "published": "2026-01-22 11:20:57+00:00",
      "link": "https://arxiv.org/pdf/2601.15871v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Discusses internal structures and training methodologies of large-scale AI models",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16097v1",
      "title": "Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating",
      "abstract": "Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.",
      "authors": [
        "Makbule Gulcin Ozsoy"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 16:46:57+00:00",
      "link": "https://arxiv.org/pdf/2601.16097v1",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "tags": [
        "keyword:ppo",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
<<<<<<< HEAD
      "llm_evidence": "Robustness and evaluation of multimodal large language models",
=======
      "llm_evidence": "Multilingual Text2Cypher using LoRA adapters and Large Language Models",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15892v1",
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "abstract": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
      "authors": [
        "Chenghao Fan",
        "Wen Heng",
        "Bo Li",
        "Sichen Liu",
        "Yuxuan Song",
        "Jing Su",
        "Xiaoye Qu",
        "Kai Shen",
        "Wei Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 12:13:17+00:00",
      "link": "https://arxiv.org/pdf/2601.15892v1",
      "tags": [
        "keyword:resnet",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
<<<<<<< HEAD
      "llm_evidence": "diffusion based code large language model architecture and benchmark performance",
=======
      "llm_evidence": "Technical report on training methodologies and benchmarks for code LLMs",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
<<<<<<< HEAD
      "id": "2601.15912v1",
      "title": "TeNet: Text-to-Network for Compact Policy Synthesis",
      "abstract": "Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.",
      "authors": [
        "Ariyan Bighashdel",
        "Kevin Sebastian Luck"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2026-01-22 12:42:30+00:00",
      "link": "https://arxiv.org/pdf/2601.15912v1",
      "tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Uses pretrained LLMs to generate robot policies, bridging large language models and control",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15995v1",
      "title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour",
      "abstract": "Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.",
      "authors": [
        "Liang Wang",
        "Kanzhong Yao",
        "Yang Liu",
        "Weikai Qin",
        "Jun Wu",
        "Zhe Sun",
        "Qiuguo Zhu"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-22 14:16:12+00:00",
      "link": "https://arxiv.org/pdf/2601.15995v1",
      "tags": [
        "keyword:RL",
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "end-to-end reinforcement learning framework for quadruped locomotion",
=======
      "id": "2601.16109v1",
      "title": "Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision",
      "abstract": "We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.",
      "authors": [
        "Yashuai Yan",
        "Tobias Egle",
        "Christian Ott",
        "Dongheui Lee"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-01-22 16:56:52+00:00",
      "link": "https://arxiv.org/pdf/2601.16109v1",
      "tags": [
        "keyword:RL",
        "keyword:ppo",
        "keyword:resnet",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Integrates residual reinforcement learning (RL) for robust control and locomotion",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.16035v1",
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.",
      "authors": [
        "Han Xue",
        "Sikai Liang",
        "Zhikai Zhang",
        "Zicheng Zeng",
        "Yun Liu",
        "Yunrui Lian",
        "Jilong Wang",
        "Qingtao Liu",
        "Xuesong Shi",
        "Li Yi"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-01-22 15:08:53+00:00",
      "link": "https://arxiv.org/pdf/2601.16035v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 6.0,
      "llm_evidence": "RL-based traversal skill learning for humanoids",
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "6"
    }
  ]
}