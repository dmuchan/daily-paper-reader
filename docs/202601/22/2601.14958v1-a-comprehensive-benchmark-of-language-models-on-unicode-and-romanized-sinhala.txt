Title: A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala

URL Source: https://arxiv.org/pdf/2601.14958v1

Published Time: Thu, 22 Jan 2026 02:00:24 GMT

Number of Pages: 6

Markdown Content:
# A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala 

Minuri Rajapakse 

School of Computing Informatics Institute of Technology 

Colombo, Sri Lanka minuri.20220646@iit.ac.lk 

Ruvan Weerasinghe 

School of Computing Informatics Institute of Technology 

Colombo, Sri Lanka ruvan.w@iit.ac.lk 

Abstract —The performance of Language Models (LMs) on lower-resource, morphologically rich languages like Sinhala re-mains under-explored, particularly for Romanized Sinhala, which is prevalent in digital communication. This paper presents a comprehensive benchmark of modern LMs on a diverse corpus of Unicode and Romanized Sinhala. We evaluate open-source models using perplexity, a measure of how well a model pre-dicts a text, and leading closed-source models via a qualitative analysis of sentence completion. Our findings reveal that the Mistral-Nemo-Base-2407 model achieves the strongest predictive performance on Unicode text and the Mistral-7B-v0.3 model for Romanized text. The results also highlight the strong all-around performance of the Llama-3.1-8B model for both scripts. Furthermore, a significant performance disparity exists among closed-source models: Gemini-1.5-pro and DeepSeek excel at Unicode generation, whereas Claude-3.5-Sonnet is superior at handling Romanized text. These results provide an essential guide for practitioners selecting models for Sinhala-specific applications and highlight the critical role of training data in handling script variations. 

Index Terms —Unicode Sinhala, Low-Resource Languages, Language Models, Perplexity, Romanized Sinhala 

I. INTRODUCTION Recent advancements in Language Models (LMs) have demonstrated remarkable capabilities, yet their development has been predominantly focused on high-resource languages like English. Their efficacy on languages such as Sinhala, which are not only low-resource but also linguistically com-plex, remains an open area of research. Sinhala presents unique challenges for standard NLP mod-els. As a morphologically rich and agglutinative language, it features a vast number of potential word forms from a single root. Furthermore, its digital use is characterized by a script duality: the Unicode Sinhala script is used in official and literary contexts, while Romanized Sinhala dominates social media and messaging platforms. Recent studies have considerably enhanced Sinhala machine translation through the application of diverse language-specific fine-tuning methodologies. A crucial strategy entails augment-ing training data; for instance, [1] employed synthetic data to address unusual word challenges, resulting in a significant increase in BLEU scores. The emergence of Transformer ar-chitectures has proven notably influential. [2] applied forward and backward translation techniques to Sinhala-Tamil, out-performing statistical baselines by approximately 3.4 BLEU points. Similarly, [3] fine-tuned mBART with Sinhala data, achieving BLEU gains of 2.8-4.4 for both Sinhala-English and Sinhala-Tamil translation. Efforts have increased to create a transliteration model aimed at enhancing the accuracy of subsequent translation tasks [4]. Although these studies offer essential resources and bench-marks for particular NLP tasks, a thorough assessment of con-temporary, large-scale generative models (such as the Llama and Mistral series) continues to be an unexplored domain. The performance of these models on Romanized Sinhala text, as well as a direct comparison of their capabilities across both scripts, has not been thoroughly investigated. To address this, our paper provides the first multifaceted benchmark of foundational language models. Our primary contributions are: 

Diverse parallel Corpus: A curated evaluation set con-taining both Unicode and Romanized Sinhala, sampled from diverse digital sources. 

Foundational perplexity benchmark: Evaluate leading open-source models using perplexity to measure their basic understanding of language, rather than relying on task-specific metrics like BLEU (Bilingual Evaluation Understudy) or ROUGE (Recall-Oriented Understudy for Gisting Evaluation). 

Qualitative analysis of generation: Assess the practical generative capabilities of advanced models on both scripts. 

Analysis of divergent performance: Our work provides the first analysis of how model performance diverges across Unicode and Romanized Sinhala scripts, offering key insights for practitioners. This is also the first comprehensive benchmark for modern LMs on Sinhala, covering both the Unicode and Romanized scripts. II. RELATED WORK Research on low-resource languages has expanded signif-icantly following the rise of multilingual Transformers, yet Sinhala remains underrepresented in mainstream language modeling research. This section reviews prior work in multilin-gual and low-resource language benchmarks, Sinhala-specific NLP developments and identifies the gap this paper addresses. 

> arXiv:2601.14958v1 [cs.CL] 21 Jan 2026

A. Benchmarks for Low-Resource and Multilingual Lan-guages 

Large-scale evaluation suites such as HELM (Holistic Eval-uation of Language Models) [5], BIG-Bench [6], MMLU [7] and GLUE [8] provide extensive benchmarks for English and high-resource languages, but their coverage of low-resource languages is limited. MMLU includes only a small subset of multilingual questions, and benchmarks like HELM focus primarily on English-centric safety, reasoning and knowledge tasks. Similar challenges of script variation have been observed in other South Asian and multilingual contexts, including Hindi-English code-mixing [9], Arabizi [10] and Bengali-English mixed text [11]. For multilingual machine translation and cross-lingual eval-uation, datasets such as FLORES-200 [12], OPUS [13] and IndicGenBench [14] have been influential. FLORES-200 pro-vides sentence-level evaluation across 200 languages, enabling fine-grained comparison of translation performance. However, these benchmarks mainly target translation accuracy, not in-trinsic language modeling ability such as perplexity. More recent benchmarks like IndoLLM [15] and In-dicGLUE [16] provide evaluation frameworks for South Asian languages, including Sinhala. These works highlight persistent challenges in modeling morphology-heavy and syntactically complex languages. However, none address script variation, which is a common phenomenon in South Asian digital communication. Despite the growing number of multilingual benchmarks, no existing evaluation suite examines performance differences between unicode-native script and romanized variants for low-resource languages. 

B. Sinhala-Specific NLP Research 

Research on Sinhala has traditionally focused on machine translation, stemming from its low-resource status and limited availability of parallel corpora. Several notable works have improved Sinhala-English and Sinhala-Tamil translation per-formance. Tennage et al. [1] employed synthetic data generation to address rare-word problems and data sparsity, resulting in significant BLEU score improvements. Pramodya et al. [2] applied forward-backward translation techniques to Sinhala-Tamil translation, outperforming statistical baselines by ap-proximately 3.4 BLEU points. Thillainathan et al. [3] fine-tuned mBART with Sinhala data, achieving BLEU gains of 2.8-4.4 for both Sinhala-English and Sinhala-Tamil translation in extremely low-resource settings. Mel et al. [4] developed transliteration models using both rule-based and Seq2Seq approaches, aimed at normalizing informal Romanized Sinhala to enhance downstream translation accuracy. These studies collectively demonstrate progress in task-specific NLP for Sinhala. However, they focus on specific downstream tasks (translation, transliteration) rather than ana-lyzing the underlying language modeling capability of modern LLMs on Sinhala text. While the transliteration work high-lights the complexity of Sinhala’s phonology and inconsisten-cies in informal Romanized usage, it has not been connected to systematic model evaluation or cross-script performance comparisons. 

C. Contribution Within the Research Landscape 

This paper addresses these gaps by providing the first intrin-sic, script-aware benchmark of modern foundational language models for Sinhala, covering:  

> •

Unicode Sinhala: The official script used in formal con-texts  

> •

Romanized Sinhala: The prevalent script in digital com-munication Using perplexity for open-source models and qualitative sentence completion for closed-source models, this benchmark systematically reveals how script variation influences model understanding. This contribution expands the scope of Sinhala NLP research beyond task-specific evaluations and offers insights into how foundational models process low-resource scripts in multilingual settings. III. METHODOLOGY 

A. Dataset Curation 

The foundation of this study is a custom-built parallel corpus of 1000 Sinhala sentences, designed to reflect the lan-guage’s use in contemporary digital contexts. The corpus was compiled by sourcing text from a diverse range of platforms, including blogs and social media platforms like YouTube. Each Romanized Sinhala sentence was manually transliterated to its common Unicode equivalent, creating a parallel dataset that reflects naturally occurring text. The manual transliteration process followed a recognized, phonetically-based rule set to handle common spelling variations in digital communications. To ensure better accuracy, a native Sinhala speaker underwent a thorough review of the rule set to correct any errors. For example, the Romanized text ’sh’ was consistently mapped to its corresponding Unicode character, distinguishing it from the Romanized letter ’s’, which was mapped to a different Unicode character. Similarly, the Romanized text ’me’ and ’meh’ were both converted to the same Unicode representation to avoid confusion from different spellings of the same word. This standardized approach was crucial for creating an accurate parallel corpus, though we acknowledge that many Romanization variations exist in different contexts. From this 1000-sentence corpus, a 200-sentence evaluation set was curated to serve as a robust and representative bench-mark 1. A programmatic approach was implemented to mitigate selection bias. First, sentence embeddings were generated using the LaBSE sentence transformer model [17], which supports over 110 languages, including Sinhala. Then, the K-Means clustering algorithm was applied to group these embeddings into 200 distinct semantic clusters. The final evaluation set was constructed by selecting the sentence closest to the centroid of each cluster, thus ensuring a diverse and representative sample of the original corpus. Visual analysis  

> 1https://huggingface.co/datasets/Minuri/sinhala-perplexity-test-dataset Fig. 1. PCA plot for the K-Means method

via PCA plots (Fig. 1) confirmed the high-quality, fair cover-age of the K-Means method compared to alternative ranking techniques. 

B. Models Evaluated 

A range of foundational models was selected for this study, grouped as follows: 

Open-Source Models: bloom-560m [18], gemma-7b [19], gemma-3-4b-pt [20], Mistral-7B-v0.3 [21], zephyr-7b-beta [22], Qwen2-7B [23], Seed-X-PPO-7B [24], SmolLM3-3B [25], Hormoz-8B [26], Llama-3.1-8B [27], phi-4 [28], Mistral-Nemo-Base-2407 [29] and Minitron-8B-Base [30] 

Closed-Source Models: OpenAI GPT-4o [31], Anthropic Claude-3.5-Sonnet, Google Gemini 1.5 Pro [32] and DeepSeek [33]. 

C. Evaluation Metrics 

Two different evaluation metrics were used to evaluate open-source models and closed-source models. 

Perplexity (Open-Source Models): To measure the core language modeling capability of the open-source models, we calculated perplexity. A lower perplexity score indicates a better model fit to the language. 

Qualitative Analysis (Closed-Source Models): Due to limitations preventing perplexity calculation for closed-source models, a qualitative evaluation was performed. On a diverse subset of 50 sentences, models were prompted to complete a sentence fragment. The completions were manually scored by a native Sinhala speaker with experience in linguistic research using a 3-point scale for Coherence and Grammar/Readability. The evaluation criteria were defined as follows: Coherence involves both contextual and logical consistency. 1 - (Excellent): The response demonstrates high relevance, logical coherence, and a natural progression from the prompt. 2- (Acceptable): The completion is comprehensible and somewhat relevant to the prompt, though it may exhibit slight awkwardness, or lack the most natural progression. 3 - (Poor): The response is irrelevant, illogical, or does not form a coherent sentence. Grammar and readability incorporate syntactic correctness and fluency. 1 - (Excellent): The sentence demonstrates grammatical accuracy and utilizes the vocabulary of a native speaker. 2 - (Acceptable): The sentence contains minor grammatical errors, such as awkward word choice, which do not signifi-cantly hinder comprehension. 3 - (Poor): The sentence exhibits significant grammatical errors that hinder comprehension, indicating a basic misun-derstanding of Sinhala sentence structure. While this foundation adds structure, we recognize the subjective aspects of the evaluation. Inter-rater reliability was not evaluated because only one rater was involved, which is acknowledged as a limitation. IV. RESULTS 

A. Perplexity of Open-Source Models 

The perplexity scores for the open-source models are pre-sented in Table I. The results indicate that the Mistral-Nemo-Base-2407 achieved the lowest perplexity for Unicode scripts (2.19) and Mistral-7B-v0.3 achieved the lowest perplexity for Romanized scripts (74.76), outperforming significantly larger models. When ranking the models based on lowest to highest perplexity score for both the scripts, Llama-3.1-8B model with 8 billion parameters ranks second, which shows the best all-around performance for both the scripts (2.37 for Unicode texts and 77.18 for Romanized texts) as mentioned in Table I as well.                                             

> TABLE I PERPLEXITY SCORES OF OPEN -S OURCE MODELS
> Model Parameters Sinhala Unicode Sinhala Romanized
> bloom-560m 560M 8.88 915.6 Seed-X-PPO-7B 7B 668.37 121.94 gemma-3-4b-pt 4B 11.06 132.74 gemma-7b 7B 7.78 153.6 zephyr-7b-beta 7B 6.16 133.91 SmolLM3-3B 3B 4.0 170.77 Hormoz-8B 8B 3.63 184.34 Llama-3.1-8B 8B 2.37 77.18 phi-4 14B 3.19 113.97 Mistral-7B-v0.3 7B 3.62 74.76 Mistral-Nemo-Base-2407 12B 2.19 105.35 Minitron-8B-Base 8B 2.53 244.33 Qwen2-7B 7B 4.36 141.78

B. Qualitative Performance of Closed-Source Models 

The qualitative analysis revealed a significant performance divergence based on script type, as shown in Table II. Gemini-1.5-pro and DeepSeek were the top performers on Sinhala Unicode text. In contrast, Claude-3.5-Sonnet demonstrated superior performance on Romanized Sinhala text. Our error analysis revealed that models often struggle with subject-verb agreement in the Sinhala language. For instance, when given the prompt ’monawada meke karanna...’ (What to do...), GPT-4o produced the completion ’...barida kiyala hitene’ (...don’t think so), which demonstrates a failure to correctly connect the matching verb to the subject or to fit standard sentence-ending structures, highlighting a struggle with Sinhala grammar rules. This specific type of grammatical error was common across several models, highlighting a key weakness in their understanding of Sinhala morphology. 

C. Example Model Outputs 

To illustrate the qualitative differences between models, Table III presents completions for the prompt ”mama kalin...” (I was...) in both Unicode and Romanized scripts. Here, the sentence generated by GPT-4o is somewhat understandable, so it is acceptable (scoring 2) in Coherence, but poor in grammar (scoring 3). The generated sentences of Gemini-1.5-pro, Claude-3.5-Sonnet, and DeepSeek are understandable and excellent in Coherence (scoring 1). Those sentences are readable (scoring 2), but they also lack an understanding of Sinhala sentence structure. V. DISCUSSION This study offers valuable insights into the current state of LMs for the Sinhala language. The most significant result from the perplexity benchmark is the enhanced performance of the Mistral-Nemo-Base-2407 model on Unicode text. This indicates that recent, optimized model architectures, along with extensive, high-quality pre-training data, are essential for developing a strong foundational understanding of the complexities of Sinhala grammar. One interesting finding from the perplexity scores is the inconsistent relationship between model size and performance on Sinhala text. The data indicate that an increased parameter count does not necessarily ensure improved language compre-hension. For instance, the Llama-3.1-8B model, with 8 billion parameters, achieved a superior perplexity score of 2.37 on Unicode text compared to the phi-4 model, which, despite having 14 billion parameters, scored 3.19. This highlights that tokenization efficiency and architectural optimizations play a crucial role in a model’s proficiency with a morphologically rich, low-resource language like Sinhala. This finding also positions the Llama-3.1-8B model as an excellent choice for developers who are looking for a computationally efficient model for Unicode Sinhala tasks. Furthermore, the qualitative and quantitative analyses both highlight the critical importance of training data composition. The divergence in performance between models implies that their respective pre-training corpora have various concentra-tions of Unicode versus Romanized Sinhala text. The strength of some models for Romanized Sinhala suggests a greater exposure to unfiltered web content where such scripts are common. The performance analysis of all models reveals that founda-tional models have a basic understanding of Sinhala; however, notable challenges persist, especially in producing text that is consistently coherent and grammatically accurate. This highlights the necessity for fine-tuning that is specific to each language. VI. L IMITATIONS 

This study has several limitations. The evaluation set, while diverse, is limited to 200 sentences; this may not fully capture the diverse spelling variations of Romanized Sinhala found in digital communication. Also, the qualitative analysis of the closed-source models was done by a single evaluator, which introduces the potential for subjective bias when scoring for grammar and coherence. Future work would benefit from validation by multiple native speakers. The evaluation is based on monolingual Sinhala in either Unicode or Romanized scripts and does not assess the cross-scripting, which is also common in digital communication. VII. CONCLUSION This work represents the first comprehensive benchmark of modern LMs on both Unicode and Romanized Sinhala text, providing baseline results and insights for future research on TABLE II AVERAGE QUALITATIVE SCORES OF CLOSED -SOURCE MODELS 

Model Script Avg. Coherence Avg. Grammar/Readability 

GPT-4o Unicode 1.66 1.74 Romanized 1.54 1.72 Gemini-1.5-pro Unicode 1.22 1.46 Romanized 1.76 2.02 Claude-3.5-Sonnet Unicode 1.4 1.52 Romanized 1.32 1.56 

DeepSeek Unicode 1.28 1.44 

Romanized 2.46 2.8 TABLE III QUALITATIVE SCORES (R ATINGS : 1=E XCELLENT , 2=A CCEPTABLE , 3=P OOR )

Model Script Completion Coherence Grammar 

GPT-4o Romanized mama kalin yanna siti. 2 3Gemini-1.5-pro Unicode මම කලින‍් ආවා. 1 2Claude-3.5-Sonnet Romanized mama kalin gedara giya. 1 2DeepSeek Unicode මම කලින‍්කිව‍් වා. 1 2

low-resource languages. We found that Mistral-Nemo-Base-2407 demonstrates the strongest overall performance, particu-larly on Unicode Sinhala, while other models, such as Mistral-7B-v0.3 showed surprising strengths in Romanized Sinhala, likely due to training data differences. These benchmarks serve as a significant resource for researchers and developers engaged in low-resource language technologies, particularly within South Asia. For Unicode Sinhala, models like Mistral-Nemo-Base-2407, achieved the best perplexity scores, whereas DeepSeek achieved good coherence and grammatical fluency. In con-trast, Claude-3.5-Sonnet displayed relative advantages for Ro-manized Sinhala, reflecting exposure to common web texts during training. The findings also suggest that the Llama-3.1-8B model can be helpful for dual-script applications (both Unicode and Romanized Sinhala). Developers targeting such applications may benefit from hybrid pipelines that dynam-ically choose models based on script, or from fine-tuning a single model on a balanced corpus covering both Unicode and Romanized Sinhala. Furthermore, our findings on the performance divergence between Unicode and Romanized Sinhala scripts offer avaluable lesson for the NLP community working on other low-resource languages, such as those with Hindi/Hinglish or Arabic/Arabizi variations, that exhibit similar digital script dualities. Looking ahead, future work will focus on collecting more naturally occurring Sinhala Unicode and Romanized texts from different sources to compute perplexity, analysis on cross-scripting, and incorporating crowd-sourced evaluations with multiple evaluators to ensure reliable and unbiased qual-itative assessment. REFERENCES 

[1] P. Tennage, P. Sandaruwan, M. Thilakarathne, A. Herath, and S. Ranathunga, “Handling rare word problem using synthetic training data for Sinhala and Tamil neural machine translation,” Language Resources and Evaluation , May 2018. [2] A. Pramodya, K. T. Y. Mahima, R. Pushpananda, and R. Weerasinghe, “Enhancing neural machine translation for the Sinhala-Tamil language pair with limited resources,” International Journal on Advances in ICT for Emerging Regions (ICTer) , vol. 17, no. 1, pp. 24-33, May 2024. doi: 10.4038/icter.v17i1.7274. [3] S. Thillainathan, S. Ranathunga, and S. Jayasena, “Fine-tuning self-supervised multilingual sequence-to-sequence models for extremely low-resource NMT,” in Proc. MERCon , vol. 2, pp. 432-437, Jul. 2021. doi: 10.1109/mercon52712.2021.9525720. [4] D. Mel, K. Wickramasinghe, C. de Silva, and S. Ranathunga, “Sinhala transliteration: A comparative analysis between rule-based and Seq2Seq approaches,” arXiv preprint arXiv:2501.00529 , Jan. 2025. [Online]. Available: https://arxiv.org/abs/2501.00529. [5] R. Liang et al., “Holistic Evaluation of Language Models,” arXiv preprint arXiv:2202.08073, Feb. 2022. [Online]. Available: https://arxiv.org/abs/2202.08073 [6] J. Srivastava et al., “Beyond the Imitation Game: Quantify-ing and Extrapolating the Capabilities of Language Models,” arXiv preprint arXiv:2206.04615, Jun. 2022. [Online]. Available: https://arxiv.org/abs/2206.04615 [7] D. Hendrycks et al., “Measuring Massive Multitask Language Un-derstanding,” arXiv preprint arXiv:2009.03300, Sep. 2020. [Online]. Available: https://arxiv.org/abs/2009.03300 [8] A. Wang et al., “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,” in Proc. ICLR, 2019. [9] T. Bhat et al., “IIT Bombay English-Hindi Code-Mixed Dataset,” arXiv preprint arXiv:1707.09057, Jul. 2017. [Online]. Available: https://arxiv.org/abs/1707.09057 [10] F. Darwish, “Arabizi Identification in Twitter Data,” in Proc. EMNLP, 2014, pp. 429-434. [11] T. Mandal, “Bengali-English Code-Mixed Social Media Text: Dataset, Analysis, and Benchmark,” arXiv preprint arXiv:2101.09885, Jan. 2021. [Online]. Available: https://arxiv.org/abs/2101.09885 [12] N. Team et al., “FLORES-200: A Multilingual Benchmark for Low-Resource Machine Translation,” arXiv preprint arXiv:2112.01588, Dec. 2021. [Online]. Available: https://arxiv.org/abs/2112.01588 [13] J. Tiedemann, “Parallel Data, Tools and Interfaces in OPUS,” in Proc. LREC, 2012. [14] H. Singh, N. Gupta, S. Bharadwaj, D. Tewari, and P. Talukdar, ”Indic-GenBench: A Multilingual Benchmark to Evaluate Generation Capabil-ities of LLMs on Indic Languages,” in Proc. 62nd Annu. Meeting of the Association for Computational Linguistics (ACL), Bangkok, Thailand, Aug. 2024, pp. 11047-11073. [15] A. Kalyan et al., “IndoLLM: A Benchmark for Evaluating Large Lan-guage Models on Indic Languages,” arXiv preprint arXiv:2404.05935, Apr. 2024. [Online]. Available: https://arxiv.org/abs/2404.05935 [16] S. Kakwani et al., “IndicGLUE: A Benchmark for Indic Natural Lan-guage Understanding,” arXiv preprint arXiv:2104.07439, Apr. 2021. [Online]. Available: https://arxiv.org/abs/2104.07439 [17] “sentence-transformers/LaBSE,” Huggingface.co . [Online]. Available: https://huggingface.co/sentence-transformers/LaBSE. [18] A. Wicaksana, K. Sorensen, and F. Dinarta, “Enhancing Hate Speech Detection in Mixed-Language Texts: A Comparative Study of BLOOM and XLM-RoBERTa Models,” in 2025 17th International Conference on Computer and Automation Engineering (ICCAE) , Mar. 2025, pp. 419-425, doi: 10.1109/iccae64891.2025.10980554. [19] Gemma Team et al. , “Gemma: Open Models Based on Gemini Research and Technology,” arXiv preprint arXiv:2403.08295 , Mar. 2024. [Online]. Available: https://arxiv.org/abs/2403.08295. [20] G. Team et al., “Gemma 3 Technical Report,” arXiv preprint arXiv:2503.19786 , Mar. 2025. [Online]. Available: https://arxiv.org/abs/2503.19786. [21] A. Q. Jiang et al. , “Mistral 7B,” arXiv preprint arXiv:2310.06825 , Oct. 2023. [Online]. Available: https://arxiv.org/abs/2310.06825. [22] L. Tunstall et al., “Zephyr: Direct Distillation of LM Alignment,” 

arXiv preprint arXiv:2310.16944 , Oct. 2023. [Online]. Available: https://arxiv.org/abs/2310.16944. [23] A. Yang et al., “Qwen2 Technical Report,” arXiv preprint arXiv:2407.10671 , Sep. 2024. [Online]. Available: https://arxiv.org/abs/2407.10671. [24] S. Cheng et al., “Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters,” arXiv preprint arXiv:2507.13618 , Aug. 2025. [Online]. Available: https://arxiv.org/abs/2507.13618. [25] “SmolLM3-3B,” Huggingface.co , Aug. 05, 2025. [Online]. Available: https://huggingface.co/HuggingFaceTB/SmolLM3-3B. [26] “mann-e/Hormoz-8B,” Huggingface.co , Nov. 29, 2024. [Online]. Avail-able: https://huggingface.co/mann-e/Hormoz-8B. [27] A. Dubey et al., “The Llama 3 Herd of Models,” arXiv preprint arXiv:2407.21783 , Nov. 2024. [Online]. Available: https://arxiv.org/abs/2407.21783. [28] M. Abdin et al., “Phi-4 Technical Report,” arXiv preprint arXiv:2412.08905 , Dec. 2024. [Online]. Available: https://arxiv.org/abs/2412.08905. [29] “mistralai/Mistral-Nemo-Base-2407,” Huggingface.co , 2025. [Online]. Available: https://huggingface.co/mistralai/Mistral-Nemo-Base-2407. [30] Sreenivas, Sharath Turuvekere et al., “LLM Pruning and Distillation in Practice: The Minitron Approach,” arXiv preprint arXiv:2408.11796 ,Dec. 2024. [Online]. Available: https://arxiv.org/abs/2408.11796. [31] H. Wang et al., “M4U: Evaluating Multilingual Understanding and Rea-soning for Large Multimodal Models,” arXiv preprint arXiv:2405.15638 ,Apr. 2025. [Online]. Available: https://arxiv.org/abs/2405.15638. [32] M. Reid et al., “Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,” arXiv preprint arXiv:2403.05530v5 , Dec. 2024. [Online]. Available: https://arxiv.org/abs/2403.05530v5. [33] L. Xiong et al., “DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models,” IEEE/CAA Journal of Automatica Sinica, vol. 12, no. 5, pp. 841-858, May 2025, doi: https://doi.org/10.1109/jas.2025.125495.