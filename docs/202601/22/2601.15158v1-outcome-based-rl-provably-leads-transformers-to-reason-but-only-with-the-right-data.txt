Title: Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data

URL Source: https://arxiv.org/pdf/2601.15158v1

Published Time: Thu, 22 Jan 2026 02:04:34 GMT

Number of Pages: 80

Markdown Content:
# Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Yuval Ran-Milo * 1 Yotam Alexander * 1 Shahar Mendel 1 Nadav Cohen 1

## Abstract 

Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can sponta-neously develop the ability to generate intermedi-ate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradi-ent descent to discover such systematic reasoning remains poorly understood. We address this by an-alyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to con-verge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties re-quired for this emergence, identifying the critical role of ”simple examples”: instances requiring fewer reasoning steps. When the training distri-bution places sufficient mass on these simpler in-stances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning be-comes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathemati-cal reasoning tasks, validating that our theoretical findings carry over to practical settings. 

## 1 Introduction 

The Transformer architecture (Vaswani et al., 2017) has emerged as the de facto standard for sequence modeling, revolutionizing diverse fields ranging from natural language processing and code generation to protein structure predic-tion and mathematical problem solving (Chen et al., 2021; Lewkowycz et al., 2022; Kovalevskiy et al., 2024). While the standard pretraining paradigm—next-token prediction   

> *Equal contribution 1Tel Aviv University. Correspondence to: Yuval Ran-Milo <yuvalmilo@mail.tau.ac.il >.

on vast corpora—has proven exceptionally effective, there is growing focus on enhancing the reasoning capabilities of these models through Reinforcement Learning (RL). The integration of Transformers with RL has unlocked dramatic performance gains on complex reasoning tasks, such as mathematical problem solving and code generation (Shao et al., 2024; Dou et al., 2024). Recent empirical break-throughs, such as the DeepSeek-R1 models (DeepSeek-AI et al., 2025), demonstrate that training with simple RL al-gorithms like GRPO (Shao et al., 2024) or PPO (Schulman et al., 2017) to optimize solely for valid final answers can induce models to spontaneously generate detailed reasoning traces (DeepSeek-AI et al., 2025). In these scenarios, despite training solely on final-answer correctness, the model spontaneously learns to generate CoT reasoning steps without any explicit supervision on these intermediate steps. This raises a fundamental question: how does a sparse reward at the end of generation guide gradient descent through the combinatorial space of outputs to dis-cover a systematic reasoning algorithm? Understanding the mechanics of this emergence is critical, especially since it appears to depend on specific yet poorly understood prop-erties of the training data. Empirical studies of RL-trained language models have increasingly highlighted that the com-position of training data (beyond just its size) plays a crucial role in successful learning, with observations suggesting that the presence of examples of varying complexity is crit-ical for models to discover effective reasoning strategies (Narvekar et al., 2020; Parashar et al., 2025). Despite these empirical advancements, it is not well un-derstood how RL drives Transformers to learn to reason effectively or how data composition affects this process. In this work, we take a step towards addressing this issue by theoretically analyzing the gradient flow dynamics of single-layer Transformers trained on a synthetic reasoning task. In line with other works on Transformer reasoning (San-ford et al., 2024; Agrawal et al., 2024; Spies et al., 2025), we focus on a graph traversal task. Specifically, our task is to identify the terminal vertex of a chain in a directed graph. We show that, under standard complexity-theoretic assumptions, this task captures a key structural property of reasoning: it cannot be solved in a single step but admits a 1

> arXiv:2601.15158v1 [cs.LG] 21 Jan 2026 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data

simple iterative solution, making CoT generation both nec-essary and sufficient. Our analysis reveals that despite being trained solely on final-answer correctness, the model con-verges to a structured, interpretable algorithm. Specifically, RL drives the Transformer to learn to explicitly traverse the graph vertex-by-vertex, effectively ”reasoning” its way to the solution. Furthermore, we show the model can imple-ment alternative, less efficient algorithms that solve the task. Our result therefore implies that gradient flow induces an 

implicit bias in the learning dynamics, guiding them towards efficient solutions. We further characterize the distributional properties required for this emergence, identifying the critical role of ”simple examples”—instances requiring fewer reasoning steps—in guiding the optimization. We prove that when the training distribution has sufficient mass on these simpler instances, gradient flow learns the efficient traversal strategy in time polynomial in the chain length; conversely, when this mass vanishes, gradient-based learning becomes infeasible as the chain length grows. Crucially, the learned algorithm enables length generalization: models trained on shorter chains can successfully extrapolate to solve longer chains not seen during training. Perhaps most surprisingly, our theory implies that training on out-of-distribution simple examples may boost performance on harder in-distribution tasks more than training on those hard examples directly. We corroborate our theoretical results with experiments in our synthetic setting, and validate their broader applicability by fine-tuning Qwen-based models (Qwen et al., 2025) on mathematical reasoning tasks. Our experiments confirm that models trained on short chains not only achieve near-perfect accuracy, but do so by implementing an efficient step-by-step traversal algorithm. Moreover, these models successfully generalize to longer chains, validating our theo-retical predictions. Finally, we confirm empirically the crit-ical role of data composition: excluding simple examples prevents the emergence of reasoning, whereas, strikingly, replacing hard in-distribution examples with simple out-of-distribution ones can actually boost performance on the hard examples themselves. Our findings provide a step towards a rigorous theoretical foundation for understanding how sparse RL feedback can drive the emergence of step-by-step reasoning, demonstrat-ing that phenomena observed in large-scale RL training can be rigorously understood. 

## 2 Related Work 

2.1 Expressivity Benefits of Transformers with CoT 

A growing theoretical literature seeks to explain how CoT boosts Transformer performance by expanding its effective expressivity. Several works show that CoT allows constant-depth Transformers to solve tasks such as parity, arithmetic, and regular languages that are inaccessible to single-step decoding (Feng et al., 2023; Merrill & Sabharwal, 2024; Li et al., 2024). We contribute to this literature by establishing a natural graph traversal problem that provably requires CoT under standard complexity-theoretic assumptions. 

2.2 Training Dynamics of Transformers 

A parallel line of work analyzes how gradient-based train-ing shapes Transformer behavior. Several studies track the emergence of algorithmic heads, programmable computa-tion, or compositional rules (Kim & Suzuki, 2025; Giannou et al., 2023; Zhang et al., 2025; Wang et al., 2025a;b), while other results show that gradient descent leads Transform-ers to implement in-context learning algorithms (Guo et al., 2023; Bai et al., 2023; von Oswald et al., 2023; Zhang et al., 2023; Lin et al., 2024; Nichani et al., 2024). Complementary work analyzes when CoT trajectory supervision improves the sample and computational complexity of learning (Joshi et al., 2025). Recent analyses further show that gradient-based training can drive Transformers that employ CoT to implement ex-plicit multi-step algorithms (Yang et al., 2025; Huang et al., 2025a; Chen et al., 2025; Huang et al., 2025b). These works, however, typically rely on training with next-token pre-diction and do not characterize when CoT-style reasoning emerges under RL training. Two results that do analyze RL-trained Transformers are (Lyu et al., 2025; Bu et al., 2025), but both differ from our setting in crucial ways. The first (Lyu et al., 2025) assumes a setting with dense rewards at every step along the CoT, relying on a much richer feedback signal than the sparse, terminal-only rewards we study. The second (Bu et al., 2025), although formally using a terminal reward, defines the curriculum over progressively longer prefixes, so the learner effectively receives feedback for partial progress; in contrast, our setting provides feedback only on the full fi-nal outcome. Furthermore, they analyze a simplified regime with only a single gradient update per curriculum stage, while we consider full multi-step training dynamics. 

2.3 Effect of Training Data Distribution on Learning 

The composition and ordering of training data can signifi-cantly impact learning in neural networks. Empirical and theoretical work across supervised learning, unsupervised learning, and reinforcement learning demonstrates that data distribution affects generalization, sample efficiency, and the emergence of specific learned behaviors (Schaul et al., 2016; Zhang et al., 2017; Belkin et al., 2019; Nakkiran et al., 2019; Arora et al., 2019; Chen et al., 2020; Kumar et al., 2020; Fedus et al., 2020; Levine et al., 2020; Feld-man, 2021; Alexander et al., 2023). Work on curriculum 2Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

learning further suggests that ordering examples by com-plexity—determined by factors such as length, rarity, or comprehensibility—can significantly aid the learning pro-cess (Bengio et al., 2009; Graves et al., 2017; Narvekar et al., 2020; Portelas et al., 2020; Soviany et al., 2022; Abbe et al., 2024; Bu et al., 2025). More specifically, both empirical and theoretical investigations into Transformers emphasize that data diversity and a balanced mixture of easy and hard examples are essential for the effective learning of com-plex compositional tasks (Ranaldi et al., 2023; Wang et al., 2025b; Behnia et al., 2025). Our results sharpen this picture by proving that, under sparse outcome-based terminal re-wards, CoT-style step-by-step reasoning emerges if and only if the training distribution places non-vanishing mass on ”simple” instances; otherwise learning becomes infeasible. 

## 3 Preliminaries 

3.1 Notation 

Let 1 denote the indicator function and [n] := {1, . . . , n }.For a sequence of tokens s = ( s1, . . . , s L), and for any 

1 ≤ j ≤ l ≤ L we use the shorthand sj:l := ( sj , . . . , s l).Let Σ denote a finite vocabulary. Without loss of generality, we identify Σ with [|Σ|], allowing us to index vocabulary tokens by integers. For any matrix M ∈ R|Σ|×| Σ| (or M ∈

R|Σ|× d for some d ∈ N) and tokens s, s ′ ∈ Σ, we write 

Ms,s ′ (or Ms, :) for the entry (or row) of M indexed by the tokens. 

3.2 Transformer Architecture 

Let Σ denote a finite vocabulary. We consider a Trans-former architecture with d layers that maps sequences 

s1: T = ( s1, . . . , s T ) ∈ ΣT of arbitrary length T to a proba-bility distribution over the vocabulary Σ.The Transformer first encodes the input sequence using one-hot embeddings: for each token sl ∈ Σ, we define 

xl = esl ∈ R1×| Σ| to be the one-hot vector corresponding to sl. We denote the resulting embedding matrix by H(0) =[x1, . . . , x T ]⊤ ∈ RT ×| Σ|.The Transformer then applies d attention layers sequentially. We parameterize the model by θ = {(Al, V l)}dl=1 , where each layer l ∈ [d] has an attention matrix Al ∈ R|Σ|×| Σ|

and a value matrix Vl ∈ R|Σ|×| Σ|. In the single-layer case (d = 1 ), we simplify notation by writing A = A1 and 

V = V1.At each layer l ∈ [d], the hidden state, H(l), is computed using an activation function σ and a causal masking oper-ator MASK σ which ensures that position i attends only to positions j ≤ i:

H(l) = σ



MASK σ

 H(l−1) Al(H(l−1) )⊤

H(l−1) Vl .(1) We consider two instantiations. In the softmax Transformer ,

σ = softmax (applied row-wise) and MASK σ (Z)ij = Zij 

if j ≤ i and −∞ otherwise. In the linear Transformer , σ

is the identity function and MASK σ (Z)ij = Zij if j ≤ i

and 0 otherwise. Unless otherwise specified, our statements apply to both variants. Finally, the Transformer applies a softmax to the last posi-tion of the final hidden state to produce a probability distri-bution over the vocabulary: 

TF( s1: T ; θ) = softmax  H(d)

> T, :

 ∈ R1×| Σ| . (2) 

3.3 Autoregressive Generation 

We use the Transformer architecture to generate sequences via autoregressive sampling. Given an initial prompt 

s1: L0 , we define a set of terminal tokens Σterm (s1: L0 ) ⊆

Σ. The model generates tokens sL0+1 , s L0+2 , . . . sequen-tially. At each step l ≥ L0, the model samples the next token sl+1 ∼ TF( s1: l; θ). The model continues sampling autoregressively until reaching a terminal token, 

i.e. , until sl ∈ Σterm (s1: L0 ) for some l > L 0. We de-note by proll (θ) the distribution over complete trajectories 

τ = ( s1: L0 , s L0+1 , . . . , s Lterm ) induced by the model pa-rameters θ, where sLterm ∈ Σterm (s1: L0 ). We denote by 

|τ | = Lterm − L0 the length of the generated trajectory (ex-cluding the prompt). Given two parameter settings θ and 

θ′, we measure the difference between their induced rollout distributions using the total variation distance :

TV  proll (θ), p roll (θ′) := 12

X

> τ

proll (θ)[ τ ] − proll (θ′)[ τ ] ,

(3) where the sum is over all possible rollouts τ .

3.4 Reinforcement Learning 

We train the model using reinforcement learning with a loss function over complete rollouts. Given a trajectory 

τ = ( s1: L0 , s L0+1 , . . . , s Lterm ), we denote a loss function 

ℓ(τ ) which evaluates the quality of the trajectory. For any distribution D over input sequences s1: L0 , the training ob-jective is defined as: 

L(D, θ ) = E  

> s1: L0∼D

E 

> τ∼proll (θ)

ℓ(τ ).

## 4 The Task: Chain Identification 

We study a graph traversal task: given a set of directed edges forming two disjoint chains and a starting vertex, the goal 3Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

a1 a2 · · · ak · · · an

b1 b2 · · · bk · · · bn

Starting vertex Target 

Figure 1: Illustration of the chain identification task. The input contains edges forming two disjoint chains Ca = ( a1, . . . , a n) and 

Cb = ( b1, . . . , b n), and a starting vertex vstart (here ak ). The task is to identify the terminal vertex of the chain containing vstart (here 

an). 

is to identify the terminal vertex of the chain containing the starting vertex (see Figure 1 for an illustration). This task captures the essential structural property of reasoning tasks: the answer cannot be produced in a single step (as we show in Section 5.1), but emerges naturally from a step-by-step iterative procedure. Formally, let n ∈ N denote the chain length. We define the 

vertex set V = [2 n] and the edge vocabulary E = V × V ,where each ordered pair (u, v ) denotes a directed edge from 

u to v. The full vocabulary is Σ = V ∪ E . We define a chain as a sequence of n distinct vertices (v1, . . . , v n)

with vi ∈ V , which we identify with its directed edge set 

{(v1, v 2), (v2, v 3), . . . , (vn−1, v n)}.

4.1 Data Distribution 

We define a general family of distributions DQ over input sequences, parameterized by a distribution Q over the index set [n − 1] . To sample s1: L0 ∼ D Q, we first partition V

uniformly at random into two disjoint subsets of size n

each, and uniformly order the vertices within each subset to form chains Ca = ( a1, . . . , a n) and Cb = ( b1, . . . , b n). We then sample an index k ∼ Q , choose a chain C ∈ {C a, Cb}

uniformly at random, and set the starting vertex vstart to be the k-th vertex of C. The input sequence s1: L0 consists of an arbitrary ordering of all edges in Ca and Cb, followed by 

vstart .1 For an instance of the problem s1: L0 we define the set of terminal tokens to include the final vertices of the two chains ( an and bn). Formally Σterm (s1: L0 ) = {an, b n}.

4.2 Task Loss 

Given an input sequence s1: L0 containing edges forming two chains Ca = ( a1, . . . , a n) and Cb = ( b1, . . . , b n) with starting vertex vstart , let τ = ( s1: L0 , s L0+1 , . . . , s Lterm ) de-note a rollout terminated at the first sLterm ∈ Σterm (s1: L0 ).We define y∗(s1: L0 ) as the terminal vertex of the chain con-

> 1

While the order of edges is arbitrary (as we use no positional encodings), vstart must be the last token so that the model can attend from it to the edges to predict the next vertex. 

taining vstart :

y∗(s1: L0 ) := 

(

an if vstart ∈ C a,bn if vstart ∈ C b.

We use the loss ℓ(τ ) := 1{sLterm ̸ = y∗(s1: L0 )}.

4.3 Step Types and Chain Traversal 

To characterize the model’s behavior during generation, we define three types of atomic operations. We say that the model performs a forward step at time 

t > L 0 if the generated token st is the target of a forward edge from the previous token st−1, i.e., (st−1, s t) ∈ C a ∪C b.A backward step occurs when st is the source of an edge to 

st−1, i.e., (st, s t−1) ∈ C a ∪ C b. Finally, a switch step occurs when st belongs to a different chain than st−1, that is, when 

st−1 ∈ C a and st ∈ C b, or vice versa. A rollout τ = ( s1: L0 , s L0+1 , . . . , s Lterm ) is a chain traversal 

if it consists of a consecutive sequence of forward steps starting from vstart . Formally, we require that for all l ∈{L0 + 1 , . . . , L term }, the token sl is a forward step from 

sl−1 (where sL0 = vstart ). 

## 5 How Could a Transformer Solve the Task? 

In this section we analyze how the chain identification task can be solved by Transformers. First, in Section 5.1, we prove that the task fundamentally requires the use of multi-step reasoning: under standard complexity-theoretic assumptions, we show that no Transformer of fixed depth can solve the task when restricted to outputting the answer in a single step. Second, in Section 5.2, we establish that multi-step reasoning is sufficient to solve the task: we show the existence of explicit constructions of single-layer Trans-formers that achieve arbitrarily low loss via autoregressive generation. Together, these results show that multi-step rea-soning is both necessary and sufficient for solving the chain identification task. Moreover, we prove that low loss can be achieved via multiple reasoning algorithms: from efficient chain traversal (Section 4.3) that sequentially follows edges from the starting vertex to the terminal vertex, to inefficient algorithms that produce arbitrarily long trajectories. Later in the paper, we show both theoretically (Section 6) and empirically (Section 7) that under gradient methods, the model nonetheless learns the efficient chain traversal algo-rithm, suggesting an implicit bias towards learning efficient algorithms. 

5.1 Reasoning is Necessary to Solve the Task 

We first prove that multi-step reasoning is necessary: for any fixed depth Transformer, the task cannot be solved when the model is required to output the answer in a single step. 4Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

To formalize this, we define the single-step loss , which measures performance when the model must output the answer immediately: 

Lsingle (DQ, θ ) := E

> s∼D Q

E

> ˆy∼TF( s;θ)

1{ˆy̸ = y∗(s)}. (4) Theorem 1 shows that for Transformers of any fixed depth, there exists a distribution where single-step generation in-curs a loss bounded from below by a constant. This contrasts sharply with the results in Section 5.2: autoregressive gen-eration can achieve arbitrarily small loss on all distributions. This separation demonstrates the necessity of multi-step reasoning. 

Theorem 1. Under standard complexity-theoretic assump-tions (see Appendix D), for any fixed depth d ∈ N, there exist n ∈ N, a distribution Q over [n − 1] , and c ∈ R>0

such that for all parameters θ = {(Ai, V i)}Di=1 :

Lsingle (DQ, θ ) ≥ c.Proof sketch (full proof in Appendix C). Using results of Li et al. (Li et al., 2024), we first note that constant-depth Transformers without CoT generation can only compute functions in T C 0, the class of constant-depth circuits with unbounded fan-in threshold gates. We then relate our chain identification task to standard problems in circuit complex-ity by giving reductions from the word problem on S5 (the symmetric group on five elements) to the ORD problem (path reachability in directed graphs), and from ORD to the chain identification task. The word problem on S5 is 

N C 1-complete, where N C 1 is the class of Boolean func-tions computable by polynomial-size, logarithmic-depth circuits with bounded fan-in, so these reductions show that solving chain identification in T C 0 would imply that an 

N C 1-complete problem lies in T C 0. Under the widely believed conjecture that T C 0̸ = N C 1, this is impossible, and hence the chain identification task is not in T C 0. The full reductions and formal complexity-theoretic details are deferred to Appendix C. 

5.2 Reasoning is Sufficient to Solve the Task 

Having established that multi-step reasoning is necessary, we now show that it is also sufficient: we prove that there exist explicit constructions of single-layer Transformers that achieve arbitrarily low loss via autoregressive gener-ation. Crucially, we show that low loss can be achieved via multiple reasoning algorithms, from efficient to highly inefficient ones. Proposition 1 establishes that the task can be efficiently solved via chain traversal (Section 4.3): there exists a single-layer Transformer that, with arbitrarily high probability, generates a trajectory sequentially following edges from the starting vertex to the terminal vertex and thus achieves arbitrarily low loss. However, the loss func-tion (Section 4.2) only penalizes the final prediction, not the trajectory length. This allows for inefficient algorithms that produce arbitrarily long trajectories while still achiev-ing low loss. Proposition 2 establishes that such algorithms indeed exist: for any loss, probability δ > 0 and trajectory length k, one can construct a single-layer linear Transformer achieving said loss while producing trajectories longer than 

k with probability greater than 1 − δ. Together, these re-sults demonstrate that low loss does not uniquely determine the reasoning algorithm—multi-step reasoning is sufficient, but the specific algorithm learned is not prescribed by the objective alone. 

Proposition 1. For any n ∈ N, Q a distribution over [n−1] 

and ε, δ ∈ R>0, there exists a single-layer Transformer with parameters θ such that L(DQ, θ ) < ε . Moreover, for every input sequence s1: L0 from DQ, with probability at least 

1 − ε over τ ∼ proll (θ), the rollout τ is a chain traversal (Section 4.3). Proof sketch (full proof in Appendix A). Assume by induc-tion that the model has correctly performed chain traversal up to a non-terminal vertex up to time l producing a se-quence s1: l where l ≥ 2n − 1. The prefix s1:2 n−2 contains the edges, and the last token sl is a vertex. We construct the attention matrix A such that for any vertex u ∈ V ,the attention weight from u to any outgoing edge (u, v )

is strictly larger than the weight to any other token. The value matrix V is constructed such that V(u,v ),v = α for some α > 0 for all edges (u, v ), while all other entries are zero. Since sl is in some chain and is a non-terminal vertex, there exists some edge (sl, k ) ∈ s1:2 n−2. This construction ensures that the model attends to (sl, k ) more strongly than to any other edge, and the output logits sat-isfy TF( s1: l)k/TF( s1: l)v = Ω( α) for any v̸ = k. Conse-quently, the probability of outputting the next vertex in the chain, k, becomes arbitrarily close to one for sufficiently large α, completing the induction. 

Proposition 2. For any n ∈ N with n > 3, Q a distribution over [n − 1] , k ∈ N , and ε, δ ∈ R>0, there exists a single-layer linear-attention Transformer with parameters θ such that L(DQ, θ ) < ε , and for every input sequence s1: L0 from 

DQ, the probability that |τ | > k is greater than 1 − δ.Proof sketch (full proof in Appendix B). We construct pa-rameters similar to those in the proof of Proposition 1. How-ever, instead of configuring the transformer’s parameters to always move forward in the chain, we set the parameters such that from any vertex, the model moves backward to its predecessor with high probability, forward to its suc-cessor with small probability, and to the other chain with negligible probability. This creates a random walk confined (with high probability) to the correct chain that frequently 5Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

backtracks, producing arbitrarily long trajectories before eventually reaching the terminal vertex. 

## 6 Dynamical Analysis 

In this section, we theoretically analyze the learning dynam-ics of Transformers trained via gradient flow on the objec-tive L(DQ, θ ) (Section 4) for different data distributions 

DQ. We establish two main results characterizing whether learning is successful or not based on the data distribution. First, we analyze the regime where the training data is a mixture of ”easy” and ”hard” examples. We show that in this setting, gradient flow efficiently learns the chain traver-sal algorithm: the model learns to perform a forward step 

(Section 4.3) with arbitrarily high probability on any test distribution, within time polynomial in the chain length (Theorem 2). Importantly, this ensures that the learned model generalizes out of distribution, including hard exam-ples, possibly unseen during training. Moreover, this result highlights an implicit bias towards efficiency: while the task can be solved by inefficient algorithms (Proposition 2), gra-dient flow naturally converges to the efficient step-by-step traversal. Second, we show that easy examples are necessary for this efficient learning. Conversely, Theorem 3 establishes that when the model is trained only on hard examples, learning is exponentially slow: unless the training time is exponential in the chain length, the distribution over rollouts remains statistically indistinguishable from initialization, which im-plies that neither the loss nor the probability of performing a forward step can change significantly from their initial values. 

6.1 Training Regime 

Throughout this section, we focus on single-layer Trans-formers. We analyze gradient flow on the objective 

L(DQ, θ ) under a training regime described below. While the assumptions we detail below are necessary for our the-oretical analysis, they are not required for the main result to hold in practice. Indeed, as we demonstrate in Section 7, the model learns the chain traversal algorithm under stan-dard initialization and training schemes when the training distribution includes easy examples, and fails to learn when trained only on hard examples. 

Base model. As is common practice in Reinforcement Learning for Transformers, we assume that Policy Gradient is applied to a base model which has already acquired a minimal level of task proficiency during the pre-training process (DeepSeek-AI et al., 2025; Ouyang et al., 2022). Specifically, for some input sequence s1: L0 , let pfwd , pbwd ,and pswitch denote the probabilities that the model performs a forward, backward, or switch step respectively, at the first step (time t = L0 + 1 ). To capture the model’s initial proficiency, we assume that for any input sequence s1: L0 ,

pfwd > c 0 and pfwd > p bwd + b0 for some constants c0 > 0

and b0 > 0. This encodes a weak bias towards following the chain direction rather than reversing it. Additionally, we assume that the model is not initialized near a solution to the task. Specifically, we require that for any input sequence 

s1: L0 , pswitch > d 0 for some d0 > 0, ensuring separation from the constructions in Section 5.2 (both efficient and inefficient.) 

Training algorithm. Following standard practice in theo-retical analyses of neural-network training dynamics (Chizat & Bach, 2018; Mei et al., 2019; Slutzky et al., 2025), we analyze continuous-time gradient flow, which is known to closely approximate gradient descent with moderately small step sizes (Elkabetz & Cohen, 2021). Moreover, our results extend to discrete-time gradient descent with fixed step size by standard discretization techniques. Following prior theo-retical work on Transformers (Nichani et al., 2024; Wang et al., 2025b), we adopt a simplified regime in which only the attention matrix A is trained, while the value matrix V

is kept fixed at its initialization. Thus the gradient flow dynamics are: 

dA (t)

dt = −∇ AL(D, A (t)) . (5) 

Attention matrix initialization. For the attention matrix 

A, we employ a symmetric initialization in which A is invari-ant under vertex permutations. 2 This captures the idea that the model begins without any bias toward specific vertices, treating all of them symmetrically. This symmetry assump-tion implies that there exist constants α, β, γ, ε, δ ∈ R such that for any edge (u, w ) ∈ E and vertices v, w ′ ∈ V 3:

Av, (u,w ) = α if v = u, (6) 

Av, (u,w ) = β if v = w, (7) 

Av, (u,w ) = γ if v / ∈ { u, w }. (8) 

Av,w ′ = η if v = w′, (9) 

Av,w ′ = δ if v̸ = w′ (10) We also assume that α, γ > 0.

Value matrix. Following (Nichani et al., 2024; Wang et al., 2025b; Huang et al., 2025b) which assumes V to              

> 2More precisely, for any permutation πof the vertex set
> V, which extends naturally to edge tokens via π(( u, w )) =(π(u), π (w)) , the matrix Asatisfies Aπ(s),π (s′)=As,s ′for all tokens s, s ′∈Σ.
> 3Note that we do not specify the attention initialization be-tween pairs of edges; since the attending token is always a vertex, these values do not affect the dynamics.

6Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

be sparse, we set the value vector of each edge (u, w ) to cor-respond to its endpoints: V(u,w ),v = 1{v ∈ { u, w }} . We also set vertex tokens to have zero value vectors: Vv, : = 0 

for all v ∈ V .

Task difficulty. We formalize the difficulty of the training distribution based on the number of reasoning steps required to solve its examples. Recall from Section 4.1 that each example starts at the k-th vertex of a chain for some k ∈

[n − 1] . Examples starting near the end of the chain (large k)require fewer autoregressive steps to traverse (Section 4.3), whereas those starting near the beginning (small k) require more steps. To capture this, we define (k, c )-easy and (k, c )-hard distributions as follows. 

Definition 1. Let Q be a distribution over indices [n − 1] .For any parameters k ∈ [n−1] and c ∈ (0 , 1] , we say that Q

is (k, c )-easy if Pn−1 

> i=n−k

Q(i) ≥ c, i.e., at least probability 

c is assigned to the final k positions of the chain. Conversely, we say that Q is (k, c )-hard if Pki=1 Q(i) ≥ c, i.e., at least probability c is assigned to the first k positions of the chain. We say that a distribution is k-easy if it is (k, 1) -easy, and 

k-hard if it is (k, 1) -hard. 

6.2 Learning With Easy Examples Yields Chain Traversal 

When trained on a mixture of hard and easy examples, 4 the model learns to perform forward steps (Section 4.3) with arbitrarily high probability in time polynomial in the chain length n. This learned capability extends to any test dis-tribution, demonstrating generalization to harder examples unseen during training. 5 This convergence to the efficient chain traversal algorithm reveals an implicit bias towards efficiency: as shown in Section 5.2, arbitrarily low loss can also be achieved via less efficient algorithms, yet gradient flow selects the efficient strategy. 

Theorem 2. Let k > 0 be an integer and 0 < a < 1.Suppose the training distribution Q is (k, c )-easy and 

(an, 1 − c)-hard (Definition 1). Then for any ε > 0, there exists a time t0 > 0 with t0 = O( n2  

> ck+2 0·d0·c·ε2

) such that 

θ(t0) := ( A(t0), V ) satisfies the following: for any test distribution Q′, if we sample an input sequence s1: L0 from 

DQ′

and a rollout τ = ( s1: L0 , s L0+1 , . . . , s Lterm ) accord-ing to θ(t0), then for each l ∈ { L0 + 1 , . . . , L term }, the model performs a forward step (Section 4.3) at time l with probability at least 1 − ε.

> 4We conjecture that this result can be extended to more general distributions that also contain examples of intermediate difficulty. We leave this extension to future work.
> 5While Theorem 2 guarantees high probability of forward steps rather than low loss on general test distributions, Theorem 2 does ensure arbitrarily small loss for distributions which consist solely of easy examples.

Proof sketch (full proof in Appendix F.9). Fix a time t and condition on an input sequence s1: L0 . Under our initial-ization (Section 6.1) , the rollout generated by θ(t) is a time-homogeneous Markov chain on the vertices (stopped upon hitting sLterm ). Our initialization is permutation-invariant over vertex labels (Section 6.1), and DQ is defined by sampling a uniformly random partition and ordering of V. This symmetry is pre-served by gradient flow, so the model’s behavior depends only on permutation-invariant structure (e.g. distance from terminal vertex), not on the specific vertex labeling. Thus we may fix any canonical two-chain graph Ca = ( a1, . . . , a n),

Cb = ( b1, . . . , b n) without loss of generality. Moreover, the symmetry guaranteed by our initialization (Section 6.1) implies that it suffices to track three tran-sition probabilities: the probability of a forward step, a backward step, and a chain switch (equivalently, the pa-rameters pfwd , p bwd , p switch from our initialization assump-tions). In our regime these are parameterized by α, β, γ :

(pfwd , p bwd , p switch ) are given by a softmax over logits de-termined by α, β, γ respectively. To analyze their dynamics, we decompose trajectories according to whether a long jump 

occurs, i.e. a transition that changes the distance to the termi-nal vertex by at least 2. Long jumps are symmetric between the two chains, and conditioned on a long jump occurring before termination, the probability of same-side absorption is 1/2. Let τ := inf {t ≥ 0 : sL0+t ∈ Σterm (s1: L0 )} be the termination time of the rollout. Let S(v) be the probability of same side absorption starting from sL0 = v. We write 

h(v) for the probability that the rollout makes at least one 

long jump before time τ , and write ¯SN L (v) for the same-side absorption probability conditional on no long jump occurring (which we call the NL chain). Equivalently, h(v)

is the absorption probability of the long-jump state in the 

long–jump–absorbing chain ( LJ chain ) obtained by redi-recting every long jump to a single absorbing state. This gives the basic decomposition 

S(v) = (1 − h(v)) ¯SN L (v) + 12 h(v).

Differentiating with respect to a parameter θ ∈ { α, β, γ }

yields 

∂θ S(v) = −(∂θ h(v)) 

 ¯SN L (v)− 12



+(1 −h(v)) ∂θ ¯SN L (v).

(11) where ∂θ denotes the time-t parameter derivative (under gradient flow) of the corresponding quantity. Controlling these derivatives requires a source decompo-sition for absorption probabilities in Markov chains. Con-cretely, let (Xt)t≥0 be a Markov chain with θ-dependent kernel Pθ , let T be an absorbing set, let τ := inf {t ≥ 0 : 

Xt ∈ T }, and let uθ (x) := Pr x[Xτ ∈ T1] be an absorp-tion probability of interest. Define the local θ-source at a 7Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

transient state x / ∈ T by 

sθ (x) := X

> y

 ∂θ Pθ (x, y ) uθ (y),

which is easier to analyze because it depends only on the single-step sensitivity of the transition probabilities out of 

x. The derivative of the same side absorption probability admits the trajectory representation 

∂θ uθ (v) = E

"τ −1X

> t=0

sθ (Xt)

#

.

Applying this representation in the long–jump–absorbing and NL chains allows one to track how the effective for-ward/backward/switch probabilities evolve under gradient flow: pbwd (t) and pswitch (t) decrease with t, while pfwd (t)

increases at a sufficient rate. Choosing t = O( n2  

> ck+2 0·d0·c·ε2

)

then yields that each rollout step is forward with probability at least 1 − ε.

6.3 Easy Examples are Necessary for Efficient Learning 

We now analyze the learning dynamics when the training distribution consists only of hard examples. Theorem 3 shows that in this regime, learning fails: unless the training time is exponential in the chain’s length, the distribution over rollouts remains ε-close to initialization in total vari-ation distance (Equation (3)). Consequently, both the loss and the probability of performing a forward step remain 

ε-close to their initial values. This sharply contrasts with Theorem 2, which shows that if the training distribution con-tains easy examples, gradient flow learns in time polynomial in the chain length. 

Theorem 3. Let 0 < a < 1 and ε > 0. Suppose the training distribution Q is an -hard (Definition 1). Then for all times 

t with t < ε · 2Ω( n(1 −a)) , for any test distribution Q′, the distribution over rollouts induced by θ(0) = ( A(0) , V ) and 

θ(t) = ( A(t), V ) on inputs from DQ′

satisfy 

TV  proll (θ(t)) , p roll (θ(0))  < ε. (12) 

Proof sketch (full proof in Appendix G). We proceed as in the proof sketch of Theorem 2 by viewing the model’s roll-outs as a Markov chain and decomposing the same-side absorption probability via (11) . We show, again using the source decompostion, that under an an -hard training distri-bution these derivatives are exponentially small in n, which implies that the one-step kernels (and hence the induced rollout distributions) stay ε-close in total variation distance up to the stated time scale.               

> Chain size mTest Acc. Chain Trav. 4100% 100% 8100% 99.3% 12 99.3% 94.2% Table 1: Performance of models trained on chains of size m.Test Acc. measures the fraction of rollouts that output the correct termi-nal vertex (task success). Chain Trav. (chain traversal correctness) measures the fraction of rollouts that are chain traversals as defined in Section 4.3, verifying the model implements the chain traversal algorithm.

## 7 Experiments 

In this section, we empirically validate the theoretical predic-tions from Section 6 in two complementary settings: first, using single-layer Transformers trained on the synthetic chain identification task (Section 7.1), closely matching our theoretical setup; and second, using a real world Large Language Model (Qwen 2.5 3B) fine-tuned on a mathe-matical reasoning task (Section 7.2). In both settings, our experiments confirm three key results: (1) when training distributions contain sufficient mass on simpler examples, models learn the efficient chain traversal algorithms (Sec-tions 7.1.1 and 7.2.1); (2) these learned algorithms gen-eralize to longer reasoning chains unseen during training (Sections 7.1.2 and 7.2.2); and (3) exposure to simpler ex-amples during training is necessary for learning to solve complex instances (Sections 7.1.3 and 7.2.3). 6

7.1 Theoretically Inspired Experiments 

We empirically validate our theoretical results from Sec-tions 5 and 6. As in our theory, we focus on single-layer Transformers. To validate the robustness of our theoretical results, we train without the simplifying assumptions from Section 6.1: we initialize the attention matrix randomly with small Gaussian prior, without the symmetry assumption or a minimal task proficiency at init, and we train all the param-eters of the model with gradient descent over batches. We train single-layer Transformers on various distributions D

(Section 4.1) using REINFORCE (Williams, 1992) to opti-mize the objective L(D, θ ) (Section 3.4). Full experimental details are provided in Appendix J. 7.1.1 EMERGENCE OF EFFICIENT REASONING 

We train single-layer Transformers on the chain identifica-tion task (Section 4) for chain sizes m ∈ { 4, 8, 12 }. For each chain size m, we sample instances from graphs with 

2m vertices (as defined in Section 4.1), and sample start-ing vertices uniformly from all positions along the chain except the terminal vertex. Table 1 shows that trained mod-

> 6Code for reproducing our experiments will be made available soon.

8Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data                  

> Test distribution (distance 1to k)Test Acc. Chain Trav.
> k= 4 100% 100%
> k= 8 100% 100%
> k= 11 100% 100% Table 2: Generalization to harder instances. Models trained on easy examples (distance 1–4 from terminal) and evaluated on distributions with increasing difficulty. Test Acc. measures task success (outputting the correct terminal vertex), while Chain Trav.
> measures whether rollouts are chain traversals (Section 4.3).

els achieve near-perfect task accuracy (outputting the cor-rect terminal vertex) and high chain traversal percentage (rollouts that sequentially follow edges in the chain, as for-malized in Section 4.3). This demonstrates that models not only solve the task but implement the chain traversal algorithm (Section 4.3), validating our theoretical predic-tion from Section 6.2 that Transformers learn to perform multi-step reasoning via autoregressive generation. More-over, as we have previously shown in Section 5.2, arbitrarily low loss can also be achieved via inefficient algorithms, the fact that gradient-based training converges to the efficient chain traversal algorithm confirms an implicit bias towards efficient reasoning. 7.1.2 OUT OF DISTRIBUTION GENERALIZATION 

We demonstrate that models trained on simple examples gen-eralize to harder instances requiring more reasoning steps. We use chains of length n = 12 and train on instances where the starting vertex is sampled uniformly from positions n−4

to n − 1 (i.e., distance 1 to 4 from the terminal vertex), cor-responding to instances requiring at most 4 reasoning steps. We then evaluate on harder distributions, where the starting vertex is sampled uniformly from positions n − k to n − 1

for k ∈ { 4, 8, 11 }, with k = 11 representing the maximal difficulty level (starting from the first vertex of the chain). Table 2 shows that models maintain high task accuracy and chain traversal percentage even on harder instances, demonstrating that the learned chain traversal algorithm (Section 4.3) generalizes beyond the training distribution to examples requiring significantly more reasoning steps. 7.1.3 SOLVING COMPLEX EXAMPLES REQUIRES 

TRAINING ON SIMPLE EXAMPLES 

We validate our finding that learning to solve complex ex-amples depends on exposure to simpler examples during training. We train models for chain size m = 5 with 2m

vertices and starting vertices sampled uniformly from only the first i vertices for various i < m . Smaller values of i ex-clude simpler examples (instances starting near the chain’s end, requiring fewer reasoning steps) and focus on complex examples (instances starting near the beginning, requiring      

> Figure 2: Training curves for models with chain size m= 5 , where starting vertices are sampled uniformly from the first ivertices. All models are evaluated on instances starting from the first vertex (requiring full chain traversal). Excluding simpler examples from training (smaller i) drastically impairs learning performance.

longer reasoning chains). Figure 2 plots training curves for various i values, all evaluated on the most complex instances that start from the first vertex and require full chain traversal. All models are trained for the same number of optimization steps. Critically, larger values of i (which include simpler examples) expose the model to more diverse training data, but with reduced coverage of the test distribution. Despite this greater distributional overlap, excluding sim-pler examples drastically impairs learning: models trained with small i exhibit significantly slower convergence and lower final accuracy compared to models exposed to the full distribution. This validates the theoretical analysis in Section 6.3. 

7.2 Real World Setting 

We now validate our theoretical findings in a more realistic setting using a modern Large Language Model, fine-tuned on a mathematical problem-solving task. 

Data distribution. We construct a mathematical task where the input is presented as natural language text con-taining a randomly shuffled list of affine equations of the form xi = xj + hi (where hi is some integer), one constant assignment xstart = c, and a query for a specified target variable xend . The system of equations connects two disjoint sets of variables, forming a dependency structure analo-gous to the chains in our synthetic setting. The model must compute the value of xend , which is located at the end of the dependency chain containing xstart . We measure task complexity by the required reasoning steps : the distance (in terms of equations) between the starting variable and the target variable. For a maximum dependency length L,we denote by L-Uniform the training distribution where required reasoning steps are sampled uniformly from 1 to 

L − 1, and by L-Hard the distribution with only instances requiring L − 1 steps. See Appendix J.2.1 for example inputs and Appendix J.2 for full experimental details. 9Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Training. We fine-tune Qwen 2.5 3B (Qwen et al., 2025) using a method inspired by DeepSeek-R1 (DeepSeek-AI et al., 2025). The model is prompted to output its rea-soning process within <thinking> and </thinking> 

tags, followed by the final answer within <answer> and 

</answer> tags. Crucially, unlike training in DeepSeek-R1, we provide no explicit instructions on how to reason (e.g., no prompts to ”think step-by-step”); the model must discover effective reasoning strategies on its own. We em-ploy GRPO (Shao et al., 2024), where the reward depends on the correctness of the final answer and adherence to the specified output format, but no supervision is provided for the intermediate reasoning steps. 7.2.1 EMERGENCE OF EFFICIENT REASONING 

We train a model on 15 -Uniform and evaluate the model both on task accuracy and on efficient reasoning , defined as generating a solution that mentions only the variables required for the computation (i.e., those on the path from the starting variable to the target variable). The model achieves a task accuracy of 98 .91% and, notably, out of the correct completions, 100% of solutions exhibit efficient reasoning. Qualitative analysis via LLM annotation over 100 correct completions reveals that 100% of the model’s completions follow a consistent step-by-step algorithm: first, identifying the relevant variables by recursively tracing the dependency chain from the target variable to the constant, and then calculating the values sequentially from the constant to the target, one variable at a time. 7 This implies the model learns an efficient step-by-step reasoning algorithm, consistent with our theoretical prediction (Section 6.2), confirming that the implicit bias of gradient flow drives the model towards efficient algorithmic solutions even in realistic settings. See Appendix J.2.1 for examples of model completions. 7.2.2 OUT OF DISTRIBUTION GENERALIZATION 

We evaluate whether models trained on shorter problems generalize to harder problems requiring more reasoning steps. We train models on 5-Uniform, 10 -Uniform, and 

15 -Uniform, 8 and evaluate all on instances from 15 -Hard (instances requiring the maximal amount of reasoning steps) Table 3 shows that the model trained on 10 -Uniform gen-eralizes effectively, approaching the performance of 15 -Uniform. However, the model trained on 5-Uniform fails to generalize, suggesting that generalization emerges only when training on sequences of a sufficient minimal length.                        

> 7Note that calculating values without first identifying the de-pendency path is implausible, as the model cannot know a priori
> which direction along the set of equations leads to the target vari-able.
> 8All models achieve >98% accuracy on their respective tasks. Training Distribution Test Acc. (on 15 -Hard) % Format
> 15 -Uniform 95.70% 97.40%
> 10 -Uniform 80.60% 89.40%
> 5-Uniform 1.20% 7.30% Table 3: Performance of Qwen 2.5 3B models trained on 5-Uniform, 10 -Uniform, and 15 -Uniform, evaluated on 15 -Hard.
> Test Acc. reports the accuracy of the final answer. % Format re-ports the percentage of outputs adhering to the required tag format.
> Figure 3: Test accuracy on 15-Hard, as a function of training steps. The model trained on 15 -Uniform successfully learns the task, while the model trained on 15 -Hard fails to learn, with accuracy remaining near zero. This corroborates the necessity of simple examples for learning complex reasoning tasks.

7.2.3 SOLVING COMPLEX EXAMPLES REQUIRES 

TRAINING ON SIMPLE EXAMPLES 

We compare models trained on 15 -Uniform (which includes simpler instances) versus 15 -Hard (only instances requir-ing the maximal amount of reasoning steps), evaluating both on 15 -Hard. Figure 3 shows that the model trained on 

15 -Uniform successfully learns the task, while the model trained on 15 -Hard fails to improve, with accuracy remain-ing near zero throughout training. This corroborates our theoretical finding (Section 6.3) that exposure to simple ex-amples is necessary for learning complex reasoning tasks, even at the expense of reduced coverage of the target distri-bution. 

## 8 Conclusion 

We provide a rigorous theoretical foundation for the emer-gence of Chain-of-Thought reasoning in Transformers trained via outcome-based reinforcement learning, and cor-roborate our theory with experiments on both synthetic and realistic reasoning tasks. We show that on problems which require reasoning, with suitable data, gradient flow displays an implicit bias towards efficient reasoning algorithms even when inefficient alternatives achieve the same loss. This bias yields reasoning procedures that extrapolate to longer problem lengths than those seen in training. Crucially, the effect is data-dependent: reasoning emerges only when the 10 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

training distribution places sufficient weight on simple ex-amples. Moving to practical implications, our findings carry a per-haps counterintuitive message: post-training on the target distribution is not always optimal. One might expect that to solve complex reasoning problems, the post-training data should consist primarily of complex instances matching the target task. Our results demonstrate the opposite— excluding simple examples from post-training, even when they fall outside the target distribution, prevents the emer-gence of the reasoning algorithm altogether. This suggests that effective data curation for reasoning models should in-clude sufficient easy instances to bootstrap the learning of complex behaviors. While the critical role of data distri-bution in the emergence of reasoning capabilities has been recognized in prior work (see Section 2.3), we rigorously analyze the sparse outcome-based reward setting and iden-tify the specific types of examples required for reasoning to emerge. We hope that our theoretical analysis will serve as a foundation for developing a quantitative understand-ing of these effects. Such advancements could enable the principled design of post-training curricula, with signifi-cant practical impact on the capabilities of large language models. 

## Acknowledgements 

We thank Yonatan Slutzky and Joey Rudoler for illuminat-ing discussions. This work was supported by the European Research Council (ERC) grant and NN4C 101164614, a Google Research Scholar Award, a Google Research Gift, Meta, the Yandex Initiative in Machine Learning, the Is-rael Science Foundation (ISF) grant 1780/21, the Tel Aviv University Center for AI and Data Science, the Adelis Re-search Fund for Artificial Intelligence, Len Blavatnik and the Blavatnik Family Foundation, and Amnon and Anat Shashua. 

## References 

Abbe, E., Bengio, S., Lotfi, A., and Rizk, K. Generalization on the unseen, logic reasoning and degree curriculum, 2024. URL 

https://arxiv.org/abs/2301.13105 .Agrawal, P., Vasania, S., and Tan, C. Can llms perform structured graph reasoning?, 2024. URL https://arxiv.org/abs/ 2402.01805 .Ajtai, M. σ11 -formulae on finite structures. In Annals of Pure and Applied Logic , volume 24, pp. 1–48, 1983. Alexander, Y., De La Vega, N., Razin, N., and Cohen, N. What makes data suitable for a locally connected neural network? a necessary and sufficient condition based on quantum entangle-ment, 2023. Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N. A theoretical analysis of contrastive unsupervised representation learning, 2019. URL https://arxiv.org/ abs/1902.09229 .Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. Transformers as statisticians: Provable in-context learning with in-context al-gorithm selection, 2023. URL https://arxiv.org/abs/ 2306.04637 .Barrington, D. A. M., Immerman, N., and Straubing, H. Bounded-width polynomial-size branching programs recognize exactly those languages in nc 1. Journal of Computer and System Sci-ences , 38(1):150–164, 1989. Behnia, T., Deora, P., and Thrampoulidis, C. Facts in stats: Impacts of pretraining diversity on language model generalization, 2025. URL https://arxiv.org/abs/2510.16096 .Belkin, M., Hsu, D., Ma, S., and Mandal, S. Reconciling modern machine-learning practice and the classical bias–variance trade-off, July 2019. ISSN 1091-6490. URL http://dx.doi. org/10.1073/pnas.1903070116 .Bengio, Y., Louradour, J., Collobert, R., and Weston, J. Curricu-lum learning. In Proceedings of the 26th Annual International Conference on Machine Learning , ICML ’09, pp. 41–48, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605585161. doi: 10.1145/1553374.1553380. URL 

https://doi.org/10.1145/1553374.1553380 .Bu, D., Huang, W., Han, A., Nitanda, A., Wong, H.-S., Zhang, Q., and Suzuki, T. Provable benefit of curriculum in transformer tree-reasoning post-training, 2025. URL https://arxiv. org/abs/2511.07372 .Chen, B., Li, X., Liang, Y., Shi, Z., and Song, Z. Bypassing the exponential dependency: Looped transformers efficiently learn in-context by multi-step gradient descent, 2025. URL 

https://arxiv.org/abs/2410.11268 .Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374 .Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations, 2020. URL https://arxiv.org/abs/2002.05709 .Chizat, L. and Bach, F. On the global convergence of gradient descent for over-parameterized models using optimal transport, 2018. URL https://arxiv.org/abs/1805.09545 .Cook, S. A. A taxonomy of problems with fast parallel algorithms. 

Information and Control , 64(1-3):2–22, 1985. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., 

11 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .Dou, S., Liu, Y., Jia, H., Xiong, L., Zhou, E., Shen, W., Shan, J., Huang, C., Wang, X., Fan, X., Xi, Z., Zhou, Y., Ji, T., Zheng, R., Zhang, Q., Huang, X., and Gui, T. Stepcoder: Improve code generation with reinforcement learning from compiler feedback, 2024. URL https://arxiv.org/abs/2402.01391 .Elkabetz, O. and Cohen, N. Continuous vs. discrete optimization of deep neural networks, 2021. URL https://arxiv.org/ abs/2107.06608 .Etessami, K. Counting quantifiers, successor relations, and log-arithmic space. In Proceedings of the 10th Annual Structure in Complexity Theory Conference (SCT’95) , SCT ’95, pp. 2, USA, 1995. IEEE Computer Society. ISBN 0818670525. Fedus, W., Ramachandran, P., Agarwal, R., Bengio, Y., Larochelle, H., Rowland, M., and Dabney, W. Revisiting fundamentals of experience replay, 2020. URL https://arxiv.org/ abs/2007.06700 .Feldman, V. Does learning require memorization? a short tale about a long tail, 2021. URL https://arxiv.org/abs/ 1906.05271 .Feng, G., Zhang, B., Gu, Y., Ye, H., He, D., and Wang, L. To-wards revealing the mystery behind chain of thought: A theoret-ical perspective, 2023. URL https://arxiv.org/abs/ 2305.15408 .Furst, M., Saxe, J. B., and Sipser, M. Parity, circuits, and the polynomial-time hierarchy. Mathematical Systems Theory , 17 (1):13–27, 1984. Giannou, A., Rajput, S., yong Sohn, J., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers as programmable computers, 2023. URL https://arxiv.org/abs/2301. 13196 .Graves, A., Bellemare, M. G., Menick, J., Munos, R., and Kavukcuoglu, K. Automated curriculum learning for neural networks, 2017. URL https://arxiv.org/abs/1704. 03003 .Guo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S., and Bai, Y. How do transformers learn in-context beyond simple functions? a case study on learning with representations, 2023. URL https://arxiv.org/abs/2310.10616 .Huang, J., Wang, Z., and Lee, J. D. Transformers learn to imple-ment multi-step gradient descent with chain of thought, 2025a. URL https://arxiv.org/abs/2502.21212 .Huang, Y., Wen, Z., Singh, A., Chi, Y., and Chen, Y. Transformers provably learn chain-of-thought reasoning with length gener-alization, 2025b. URL https://arxiv.org/abs/2511. 07378 .Joshi, N., Vardi, G., Block, A., Goel, S., Li, Z., Misiakiewicz, T., and Srebro, N. A theory of learning with autoregressive chain of thought, 2025. URL https://arxiv.org/abs/2503. 07932 .Kim, J. and Suzuki, T. Transformers provably solve parity effi-ciently with chain of thought, 2025. URL https://arxiv. org/abs/2410.08633 .Kovalevskiy, O., Mateos-Garcia, J., and Tunyasuvunakool, K. Alphafold two years on: validation and impact, 2024. URL 

https://arxiv.org/abs/2403.02124 .Kumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative q-learning for offline reinforcement learning, 2020. URL https: //arxiv.org/abs/2006.04779 .Levine, S., Kumar, A., Tucker, G., and Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on open problems, 2020. URL https://arxiv.org/abs/2005.01643 .Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with lan-guage models, 2022. URL https://arxiv.org/abs/ 2206.14858 .Li, Z., Liu, H., Zhou, D., and Ma, T. Chain of thought empowers transformers to solve inherently serial problems, 2024. URL 

https://arxiv.org/abs/2402.12875 .Lin, L., Bai, Y., and Mei, S. Transformers as decision makers: Provable in-context reinforcement learning via supervised pre-training, 2024. URL https://arxiv.org/abs/2310. 08566 .Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Transformers learn shortcuts to automata, 2023. URL https: //arxiv.org/abs/2210.10749 .Lyu, B., Jia, Y., Cai, X., and Zhu, Z. Transformers with rl or sft provably learn sparse boolean functions, but differently, 2025. URL https://arxiv.org/abs/2511.17852 .Mei, S., Misiakiewicz, T., and Montanari, A. Mean-field theory of two-layers neural networks: dimension-free bounds and ker-nel limit, 2019. URL https://arxiv.org/abs/1902. 06015 .

12 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Merrill, W. and Sabharwal, A. The parallelism tradeoff: Lim-itations of log-precision transformers, 2023. URL https: //arxiv.org/abs/2207.00729 .Merrill, W. and Sabharwal, A. The expressive power of trans-formers with chain of thought, 2024. URL https://arxiv. org/abs/2310.07923 .Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I. Deep double descent: Where bigger models and more data hurt, 2019. URL https://arxiv.org/abs/ 1912.02292 .Narvekar, S., Peng, B., Leonetti, M., Sinapov, J., Taylor, M. E., and Stone, P. Curriculum learning for reinforcement learning domains: A framework and survey, 2020. URL https:// arxiv.org/abs/2003.04960 .Nichani, E., Damian, A., and Lee, J. D. How transformers learn causal structure with gradient descent, 2024. URL https: //arxiv.org/abs/2402.14735 .Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schul-man, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155 .Parashar, S., Gui, S., Li, X., Ling, H., Vemuri, S., Olson, B., Li, E., Zhang, Y., Caverlee, J., Kalathil, D., and Ji, S. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning, 2025. URL https://arxiv.org/abs/2506. 06632 .Portelas, R., Colas, C., Weng, L., Hofmann, K., and Oudeyer, P.-Y. Automatic curriculum learning for deep rl: A short survey, 2020. URL https://arxiv.org/abs/2003.04664 .Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL 

https://arxiv.org/abs/2412.15115 .Ranaldi, L., Pucci, G., and Zanzotto, F. M. Modeling easi-ness for training transformers with curriculum learning. In Mitkov, R. and Angelova, G. (eds.), Proceedings of the 14th International Conference on Recent Advances in Natural Lan-guage Processing , pp. 937–948, Varna, Bulgaria, September 2023. INCOMA Ltd., Shoumen, Bulgaria. URL https: //aclanthology.org/2023.ranlp-1.101/ .Sanford, C., Fatemi, B., Hall, E., Tsitsulin, A., Kazemi, M., Hal-crow, J., Perozzi, B., and Mirrokni, V. Understanding trans-former reasoning capabilities via graph algorithms, 2024. URL 

https://arxiv.org/abs/2405.18512 .Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay, 2016. URL https://arxiv.org/abs/ 1511.05952 .Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms, 2017. URL https: //arxiv.org/abs/1707.06347 .Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300 .Slutzky, Y., Alexander, Y., Razin, N., and Cohen, N. The im-plicit bias of structured state space models can be poisoned with clean labels, 2025. URL https://arxiv.org/abs/ 2410.10473 .Soviany, P., Ionescu, R. T., Rota, P., and Sebe, N. Curriculum learn-ing: A survey, 2022. URL https://arxiv.org/abs/ 2101.10382 .Spies, A. F., Edwards, W., Ivanitskiy, M. I., Skapars, A., R ¨auker, T., Inoue, K., Russo, A., and Shanahan, M. Transformers use causal world models in maze-solving tasks, 2025. URL 

https://arxiv.org/abs/2412.11867 .Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017. URL https://arxiv.org/abs/1706. 03762 .Vollmer, H. Introduction to Circuit Complexity: A Uniform Ap-proach . Springer Science & Business Media, 1999. von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. Trans-formers learn in-context by gradient descent, 2023. URL 

https://arxiv.org/abs/2212.07677 .Wang, J., Blaser, E., Daneshmand, H., and Zhang, S. Transformers can learn temporal difference methods for in-context reinforce-ment learning, 2025a. URL https://arxiv.org/abs/ 2405.13861 .Wang, Z., Nichani, E., Bietti, A., Damian, A., Hsu, D., Lee, J. D., and Wu, D. Learning compositional functions with transformers from easy-to-hard data, 2025b. URL https: //arxiv.org/abs/2505.23683 .Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning , 8: 229–256, 1992. URL https://api.semanticscholar. org/CorpusID:2332513 .Yang, T., Huang, Y., Liang, Y., and Chi, Y. Multi-head transform-ers provably learn symbolic multi-step reasoning via gradient descent, 2025. URL https://arxiv.org/abs/2508. 08222 .Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking generalization, 2017. URL https://arxiv.org/abs/1611.03530 .Zhang, R., Frei, S., and Bartlett, P. L. Trained transformers learn linear models in-context, 2023. URL https://arxiv. org/abs/2306.09927 .Zhang, Y., Du, W., Jin, D., Fu, J., and Jin, Z. Finite state automata inside transformers with chain-of-thought: A mechanistic study on state tracking, 2025. URL https://arxiv.org/abs/ 2502.20129 .

13 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

## A Proof of Proposition 1 

Proof. We prove the result simultaneously for the softmax and linear transformer variants by using the unified weight function w(·) from Lemma 67. Fix any n ≥ 3, any distribution Q over [n − 1] , and any ε > 0. We will construct a single-layer transformer θ = ( A, V )

such that for every input s1: L0 ∼ D Q, with probability at least 1 − ε over the rollout τ ∼ proll (θ), the rollout is a chain traversal and terminates at the correct terminal vertex, implying L(DQ, θ ) < ε . (See Sections 3 and 4 for details) Let A satisfy the assumptions of Lemma 67 with 

α = 1 , β = 0 , γ = 0 ,

and (as assumed in Lemma 67) set all vertex–vertex entries of A equal to γ. Let V satisfy the assumptions of Lemma 67 with 

α′ = 0 , γ′ = 0 , β′ > 0,

for some β′ > 0 to be determined later. Fix any time during autoregressive generation before termination, and let v be the last generated vertex (so the current input sequence s ends with v). Let u denote the (unique) successor of v in its chain if it exists. By Lemma 67 and our choice ∆α = α′ − γ′ = 0 and ∆β = β′ − γ′ = β′, the logit differences are controlled by the incoming-edge weights. For every nonterminal v ∈ (Ca ∪ C b) \ { an, b n}, the successor u has an incoming edge (v, u ) ∈ G whose attention score from v is Av, (v,u ) = α = 1 ; every other vertex k̸ = u has either an incoming edge with attention score 0 from v (i.e., 

Av, (pred( k),k ) ∈ { β, γ } = {0}) or has no incoming edge in G (start vertices). Hence there is a logit gap of the form 

o(v, G, Z v )u − o(v, G, Z v )k ≥ β′  w(1) − w(0)  for all k̸ = u. 

Since w(·) is monotone increasing, w(1) > w (0) in both attention variants: for linear attention, w(1) − w(0) = 1 ; for softmax attention, w(1) − w(0) = e−1 

> Zv

(where Zv is defined in Lemma 67). Moreover, under our choice of A, all tokens other than the unique outgoing edge (v, u ) have attention score 0 from v (including all vertex tokens, since we set the vertex–vertex block to γ = 0 ). Therefore, for softmax attention, 

Zv =

> L

X

> j=1

exp( Av,s j ) ≤ e + ( L − 1) · e0 = e + L − 1,

where L is the current prefix length. At this point we condition on the “good” event E that the rollout has followed the correct chain successor at every previous step (equivalently, it is a chain traversal up to the current time). On E, the number of generated vertices before termination is at most n (the chain length), hence L ≤ L0 + n ≤ (2 n − 1) + n = 3 n − 1. Thus, on E,

w(1) − w(0) ≥



1, linear attention ,e − 13n − 2 + e , softmax attention .

Let δn > 0 denote this (deterministic) lower bound. Next, we will bound the conditional failure probability given E, and then show that Pr( E) → 1 by as β′ → ∞ .The next-token distribution is TF( s1: L; θ) = softmax  o(v, G, Z v ). Using the logit gap and bounding the denominator by 

|Σ| terms, we obtain for every nonterminal v:

Pr sL+1 = u sL = v ≥ 11 + P  

> k̸=u

exp   − β′δn

 ≥ 1 − (|Σ| − 1) exp   − β′δn

.

All rollout starts from a vertex which is in one of the chains, at most n such nonterminal-to-successor transitions are required to reach the chain terminal. Therefore, by a union bound over these steps, the probability that the rollout ever deviates from the correct successor at some step is at most 

n(|Σ| − 1) exp   − β′δn

.

Choose β′ large enough so that this quantity is at most ε; then with probability at least 1 − ε the rollout follows the chain edges until it reaches the correct terminal vertex, at which point the generation stops by definition of Σterm (s1: L0 ). This implies that with probability at least 1 − ε the rollout is a chain traversal (and hence incurs zero loss), so L(DQ, θ ) < ε .14 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Remark (relation to Section 6). In the linear-attention variant, if we wish to fit the value-matrix parameterization discussed in Section 6 and we mask-out the current vertex, we can instead take α′ = β′ = 1 and γ′ = 0 and take 

β = α → ∞ (keeping β = γ = 0 ). Similar analysis yields that the probability of advancing to the correct successor tends to 1 as α → ∞ .

## B Proof of Proposition 2 

Proof. Fix n ≥ 3, a distribution Q over [n − 1] , a target length threshold k ∈ N, and a loss tolerance ε > 0. We’ll assume w.l.o.g. that δ > ε (otherwise we can lower ε). We use the notation and symmetry setup of Lemma 67, in particular the parameters α, β, γ in the attention matrix and 

α′, β ′, γ ′ in the value matrix. We also reuse the notation ∆α := α′ − γ′ and ∆β := β′ − γ′. The formulas for the per-vertex logit contributions Ψk as a function of (α, β, γ, ∆α, ∆β ) are exactly those in Lemma 67. As the transformer is assumed to be linear, we have w(μ) = μ.We construct a 1-layer transformer with attention parameters 

γ = 0 , β = −α − c

for some free parameters α ∈ R and c > 0 to be chosen later, and with value parameters 

γ′ = 0 , α′ = 1 , β′ = −1.

Thus ∆α = 1 and ∆β = −1. We keep the vertex–vertex block of A equal to γ = 0 , and set Vv, : = 0 for all vertex tokens 

v ∈ V , exactly as in Lemma 67. Let v be any non-terminal vertex in one of the chains, and consider the logits at the last position when the current prefix ends with v. Applying Lemma 67 and substituting γ = 0 , w(γ) = 0 , ∆α = 1 , ∆β = −1, we obtain for an interior vertex (Case 1 in Lemma 67) that the only nonzero logit contributions on the two incident vertices are 

Ψforw = ∆ β w(α) = β′ α = −α, Ψback = ∆ αw(β) = α′ β = β = −α − c. 

The current vertex v itself has 

Ψv = ∆ αw(α) + ∆ β w(β) = α − β = 2 α + c, 

and every other vertex in either chain (and all vertices outside the chains) has Ψk = 0 (since all those terms are proportional to w(γ) = 0 in Lemma 67). Thus, up to some offset C, for any vertex v which is ”in the middle of the chain” (not the start, end, second or penultimate vertex of some chain) has a local logit pattern of the form 

ℓv = 2 α + c, ℓback = −α − c, ℓforw = −α, ℓother = 0 .

The same calculation applied to the vertex v which are not the start of a chain Lemma 67 (Cases 2,3,5) gives the same qualitative picture: for any non-terminal v on either chain the logits on the two incident chain vertices are linear functions of 

α and c of the form 

ℓforw (c, α ) = β′ α = −α, ℓback (c, α ) = α′ β = −α − c, 

while all vertices in the other chain and all vertices outside Ca ∪ Cb have logit 0, and the current vertex v has logit 2α + c.In particular the only dependence on c in the differences between the forward and backward logits is 

ℓforw − ℓback = c. 

Finally, when the current vertex v is the start of a chain (Case 4 in Lemma 67), there is no backward neighbor within the chain. In this case, the forward logit is still ℓforw = −α, but there is no ℓback , so as α → −∞ , the forward probability to move forward limits to one. We now use this to tune the random-walk behaviour in two stages. 

1. Controlling the conditional walk along the chain (choosing c). Fix α for the moment. Condition on the “good event” that at each step the model only chooses the forward or backward neighbour in the chain (i.e., we ignore the possibility of 15 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

staying at v or jumping to any other vertex). Under this conditioning, from every interior vertex v the next-token distribution over {forw , back } has the form 

P(forw | { forw , back }) = eℓforw 

eℓforw + eℓback 

= 11 + e−(ℓforw −ℓback ) = 11 + e−c , P(back | { forw , back }) = 11 + ec .

Thus the forward/backward bias of the induced nearest-neighbour walk along the chain depends only on c, via a smooth, strictly monotone function. As c → 0 the walk becomes approximately unbiased ( P(forw) → 1/2); as c → ∞ the walk becomes almost deterministic in the forward direction; as c → −∞ it becomes almost deterministic in the backward direction. Let us denote by G the event that the rollout consists only of forward/backward moves along the chain. Standard random-walk estimates on a finite chain imply that, for any fixed chain length n, and starting index k ∈ [n − 1] , there exists a choice of c and some Lmax (dependent on c) such that, conditional on G, the hitting time T of the terminal vertex satisfies 

P Lmax > T > L G ≥ 1 − ε/ 2

for any prescribed L. This implies that conditioned on G, the rollout will take more than L and less than Lmax steps to reach the terminal vertex with probability at least 1 − ε/ 2. (which assures loss smaller than 1 − ε/ 2.) 

2. Making P(Gc) ≤ ε/ 2 (choosing α). We now choose α to ensure that the model stays on the chain. Taking α → −∞ , the logits for the forward and backward neighbors ( ℓforw = −α, ℓback = −α − c) tend to +∞, while the logit for the current vertex ( ℓv = 2 α + c) tends to −∞ and other vertices remain at 0. Consequently, the probability mass concentrates on the forward and backward neighbors, with the ratio pforw /p back = ec preserved. The probability of outputting a token outside the chain (or the wrong terminal) vanishes relative to the move probabilities. Together, these two points imply the theorem. 

Remark (relation to Section 6). The construction above directly uses the symmetric parameterization of Lemma 67. If we wish to fit the slightly different value-matrix parameterization used in Section 6.1 and additionally mask out the current vertex v (so that “stay” moves are disallowed), we can instead take α = β = 1 , γ = 0 and choose α′ = β′ + c, γ′ = 0 . The same two-step tuning (first choosing the bias parameter c to control the conditional walk along the chain, then scaling β′ to suppress jumps to other chains) yields the same conclusion. 

## C Proof of Theorem 1 D Complexity-Theoretic Assumptions 

Our hardness result relies on standard assumptions from circuit complexity theory regarding the computational power of constant-depth versus logarithmic-depth circuits. Circuit complexity characterizes computational problems by the resources required by boolean circuits to solve them. A circuit family consists of a circuit for each input length n, where each circuit is a directed acyclic graph with input nodes, internal gates (computing boolean functions), and output nodes. The key resources are the size (number of gates) and depth (longest path from input to output). 

Definition 2 (AC 0). A language L ⊆ { 0, 1}∗ is in AC 0 if there exists a constant d ∈ N, a polynomial p(·), and a family of boolean circuits {Cn}n∈N such that for all n ∈ N:• Cn has depth at most d and size at most p(n),• Cn uses unbounded fan-in AND, OR, and NOT gates, • For all x ∈ { 0, 1}n, Cn(x) = 1[x ∈ L].It is known that simple problems like PARITY (computing the XOR of all input bits) provably lie outside AC 0 (Furst et al., 1984; Ajtai, 1983). 

Definition 3 (TC 0). A language L ⊆ { 0, 1}∗ is in TC 0 if there exists a constant d ∈ N, a polynomial p(·), and a family of boolean circuits {Cn}n∈N such that for all n ∈ N:16 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

• Cn has depth at most d and size at most p(n),• Cn uses unbounded fan-in AND, OR, NOT, and MAJORITY gates, where a MAJORITY gate outputs 1 if and only if the majority of its inputs are 1, • For all x ∈ { 0, 1}n, Cn(x) = 1[x ∈ L].The additional power of MAJORITY gates allows TC 0 to compute functions like MAJORITY and modular arithmetic that are impossible for AC 0. It is known that AC 0 ⊊ TC 0 (Barrington et al., 1989). 

Definition 4 (NC 1). A language L ⊆ { 0, 1}∗ is in NC 1 if there exists a polynomial p(·) and a family of boolean circuits 

{Cn}n∈N such that for all n ∈ N:• Cn has depth O(log n) and size at most p(n),• Cn uses bounded fan-in (typically fan-in 2) AND, OR, and NOT gates, • For all x ∈ { 0, 1}n, Cn(x) = 1[x ∈ L].This class corresponds to problems efficiently parallelizable with a logarithmic number of parallel steps. Examples include evaluating boolean formulas and computing certain regular languages (Cook, 1985). 

Conjecture 1 (TC 0 ⊊ NC 1). There exist problems in NC 1 that are not in TC 0.

This separation remains unproven and is considered a fundamental open problem in complexity theory (Vollmer, 1999). Several natural problems are candidates for witnessing this separation, including certain composition operations on non-solvable groups and the iterated multiplication problem (Li et al., 2024). For our graph traversal task, the assumption that it lies outside TC 0 (under Conjecture 1) implies it cannot be solved by transformers in a single forward pass (constant depth) regardless of model width or precision, thus necessitating the iterative reasoning enabled by Chain-of-Thought generation. This connects the expressiveness limitations of constant-depth transformers (Liu et al., 2023; Merrill & Sabharwal, 2024) to the fundamental question of whether our task requires inherently sequential computation. 

## E A Complexity–Theoretic Obstruction for No–CoT Transformers 

In this section we formalize a conditional lower bound showing that, under the standard conjecture TC 0̸ = NC 1, a constant–depth transformer without chain–of–thought (CoT) steps cannot compute a simple “two–chain endpoint” function. The argument proceeds via a chain of reductions starting from an NC 1–complete word problem, passing through the logspace–complete problem ORD , and ending at a decision version of the two–chain endpoint problem. Throughout, all circuit classes ( AC 0, TC 0, NC 1) are non–uniform unless explicitly marked as uniform. Uniform variants will be denoted AC 0

> u

, TC 0

> u

, NC 1

> u

, etc. 

E.1 Problems and Classes 

We begin by fixing the problems we will use. 

Definition 5 (Word problem for a finite group) . Let G be a fixed finite group given by a finite generating set Σ and multiplication table. The word problem for G, denoted WP G, is the language 

WP G = { w ∈ Σ∗ : the product of the letters of w equals the identity of G }.

It is classical (Barrington’s theorem and refinements) that for a suitable finite group G (e.g. A5) the problem WP G is 

NC 1–complete under AC 0 many–one reductions. 

Definition 6 (Ordering / path reachability problem ORD ). Fix n ∈ N. An ORD instance on [n] = {1, . . . , n } consists of: • a successor function succ : [ n] → [n] ∪ {⊥} , where succ (i) = j encodes a directed edge i → j and succ (i) = ⊥

means i has no outgoing edge; 17 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

• two distinguished vertices s, t ∈ [n].We restrict attention to instances satisfying the promise that the directed graph P = ([ n], E ) with edges E = {(i, succ (i)) : 

succ (i)̸ = ⊥} is a single directed path : every vertex has in-degree and out-degree at most one, and the graph is connected. The language ORD is: 

ORD = {(succ , s, t ) : t is reachable from s along the unique directed path on [n]} .

Definition 7 (Two–chain endpoint function and decision problem) . A directed graph G = ([ n], E ) is a union of at most two directed chains if every vertex has in-degree and out-degree at most one, and G has at most two connected components. • The two–chain endpoint function 

FTwoChain : { G, v } −→ [n]

is defined on pairs (G, v ) where G is a directed graph on [n] that is a union of at most two directed chains, and v ∈ [n].It outputs the unique terminal vertex of the (unique) directed chain of G that contains v. (If v is itself terminal, then 

FTwoChain (G, v ) = v.) • The associated decision problem is 

TwoChainEndEq = { (G, v, q ) : G is a union of at most two chains and FTwoChain (G, v ) = q }.

Definition 8 (No–CoT transformer class) . Let T0 denote the class of functions computed by transformers with: • depth O(1) (no chain–of–thought / recurrent unrolling); • width / embedding dimension and number of heads at most poly( n) on length-n inputs; • fixed-point arithmetic with O(log n) bits of precision in each scalar. We view each such transformer family as a non–uniform circuit family (one transformer per input length). 

E.2 Complexity–theoretic background 

We collect the complexity–theoretic ingredients we need. 

Lemma 1 (Word problems complete for NC 1). There exists a finite group G such that WP G ∈ NC 1 and WP G is NC 1–complete under (non–uniform) AC 0 many–one reductions. That is, for every language L ∈ NC 1 there is a family of AC 0

circuits (fn) such that for all x,

x ∈ L ⇐⇒ f|x|(x) ∈ WP G.

(Li et al., 2024) 

Lemma 2 (Uniform NC 1 is contained in logspace) . Let NC 1 

> u

denote DLOGTIME–uniform NC 1, and let L denote determin-istic logspace. Then 

NC 1 

> u

⊆ L.

Lemma 3 (ORD is L–complete under AC 0

> u

). There is a formalization of ORD such that ORD ∈ L and ORD is L–complete under DLOGTIME–uniform AC 0 reductions (equivalently, first–order interpretations). That is, for every A ∈ L there is a DLOGTIME–uniform AC 0 function f with 

x ∈ A ⇐⇒ f (x) ∈ ORD .

(Etessami, 1995) Combining the previous lemmas, we obtain an AC 0 reduction from the word problem to ORD .

Lemma 4 (WP G reduces to ORD in AC 0). Fix a group G as in Lemma 1. Then there exists a DLOGTIME–uniform AC 0

many–one reduction f such that 

w ∈ WP G ⇐⇒ f (w) ∈ ORD .

Equivalently, WP G ≤AC 0 

> m

ORD .

18 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. By Lemma 1 and Lemma 2, the (uniform) language WP G is in L. By Lemma 3, there is a DLOGTIME–uniform 

AC 0 reduction f such that w ∈ WP G ⇐⇒ f (w) ∈ ORD . This is precisely an AC 0 

> u

(hence AC 0) many–one reduction as claimed. We next state the transformer-to-TC 0 upper bound and a basic closure property. We start with some helper lemmas from Merrill–Sabharwal (Merrill & Sabharwal, 2023) 

Lemma 5 (Any O(log N )-input function has depth-3 implementation) . Let c > 0 and f : {0, 1}c log N → { 0, 1}m. There is a depth-3 {∧ , ∨, ¬} -circuit of size O(N c + log N + m) computing f 9.

Lemma 6 (Iterated p-precision addition) . Let p = c log N . There exists a constant d⊕ such that for each n ≤ N the 

p-precision sum of x1, . . . , x n ∈ { 0, 1}p is computed by a depth-d⊕, polynomial-size threshold circuit. 10 .

Lemma 7 (p-bit multiplication) . Let p = c log N . The truncated/rounded product (u, v ) ∈ { 0, 1}p × { 0, 1}p 7 → u · v ∈{0, 1}p depends on 2p = O(log N ) input bits; hence by Lemma 5 it has a depth-d× implementation with d× = 3 and polynomial size. 

Lemma 8 (No–CoT transformers are contained in TC 0). For every function family F computed by a family of no–CoT transformers in T0, there is a non–uniform TC 0 circuit family computing F . In other words, 

T0 ⊆ TC 0.

Proof. We define a family of models {Tn}, where a model Tn accepts input from vocabulary Σn of size |Σn| = n2.Fix n ∈ N. The input is a sequence of 1-hot vectors of length l = n − 2, (h(0) 1 , . . . , h (0)  

> l

) with h(0)  

> i

∈ { e1, ..., e |Σn|, 0}.Hence the total input bit-length is 

N = l · | Σn| = n2 ∗ (n − 2) . (13) Let L ≥ 1 be a fixed constant. For each layer ℓ ∈ { 0, . . . , L − 1} there are shared matrices 

Q(ℓ), K (ℓ), V (ℓ) ∈ R|Σn|×| Σn| (14) with p-bit entries, where 

p = c log N (15) for a fixed constant c > 0. Hidden vectors at layer ℓ are h(ℓ) 

> i

∈ R|Σn| (with p-bit coordinates); for ℓ = 0 , h(0)  

> i

is the 1-hot input. For all positions i, j the layer computes 

q(ℓ) 

> i

= Q(ℓ) h(ℓ) 

> i

, k(ℓ) 

> j

= K(ℓ) h(ℓ) 

> j

, v(ℓ) 

> j

= V (ℓ) h(ℓ) 

> j

, (16) 

r(ℓ) 

> i,j

=

> |Σn|

X

> t=1

q(ℓ) 

> i

[t] k(ℓ) 

> j

[t], (17) 

s(ℓ) 

> i,j

=



r(ℓ) 

> i,j

(Linear) ,

exp  r(ℓ)

> i,j

Plj′=1 exp  r(ℓ)

> i,j ′

 (Softmax) , (18) 

a(ℓ) 

> i

=

> l

X

> j=1

s(ℓ) 

> i,j

v(ℓ) 

> j

, h(ℓ+1)  

> i

:= a(ℓ) 

> i

. (19) Every scalar multiplication returns a p-bit result under a fixed truncation/rounding rule; all sums use the p-precision addition operator from Appendix A of Merrill–Sabharwal(Merrill & Sabharwal, 2024). We will now show that the Forward Pass of the described model is in TC .

> 9

Lemma 1 in Merrill–Sabharwa(Merrill & Sabharwal, 2023) 

> 10

Appendix A in Merrill–Sabharwa(Merrill & Sabharwal, 2023) 

19 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

All steps below are performed in parallel over the relevant indices and coordinates. 

(A) Matrix–vector products (depth d× + d⊕). For each coordinate, 

q(ℓ) 

> i

[t] = 

> |Σn|

X

> r=1

Q(ℓ)[t, r ] h(ℓ) 

> i

[r], (20) 

k(ℓ) 

> j

[t] = 

> |Σn|

X

> r=1

K(ℓ)[t, r ] h(ℓ) 

> j

[r], (21) 

v(ℓ) 

> j

[t] = 

> |Σn|

X

> r=1

V (ℓ)[t, r ] h(ℓ) 

> j

[r]. (22) Each product is a p-bit multiply (Lemma 7, depth d×); each sum is an iterated p-precision sum on |Σn| = O(N ) elements (Lemma 6, depth d⊕). All three MV products are computed in parallel, so this stage costs d× + d⊕ depth. 

(B 1) Similarities (Linear, depth d× + d⊕). 

s(ℓ) 

> i,j

=

> |Σn|

X

> t=1

q(ℓ) 

> i

[t] k(ℓ) 

> j

[t], (23) computed by p-bit multiplies (depth d×) followed by a p-precision sum (depth d⊕). 

(B 2) Similarities (Softmax, depth 3+ d⊕ +3 ). Given the scores r(ℓ) 

> i,j

from the model definition, compute u(ℓ) 

> i,j

:= exp p(r(ℓ) 

> i,j

).By Lemma 5, exp p : {0, 1}p → { 0, 1}p has a depth-3, polynomial-size implementation. Next compute Z(ℓ) 

> i

:= Plj′=1 u(ℓ)

> i,j ′

using Lemma 6 (depth d⊕). Finally output s(ℓ) 

> i,j

:= ÷p(u(ℓ) 

> i,j

, Z (ℓ) 

> i

); since ÷p depends on 2p = O(log N ) input bits, Lemma 5 gives depth 3 and polynomial size. 

(C) Scalar–vector products (depth d×). For each (i, j ) and coordinate k,

(s(ℓ) 

> i,j

v(ℓ) 

> j

)[ k] = s(ℓ) 

> i,j

v(ℓ) 

> j

[k], (24) a p-bit multiply (Lemma 7). 

(D) Vector sums for head output (depth d⊕). For each i and coordinate k,

a(ℓ) 

> i

[k] = 

> l

X

> j=1

 s(ℓ) 

> i,j

v(ℓ) 

> j

[k], (25) an iterated p-precision sum of l p -bit values (Lemma 6). 

Per-layer depth. The stages (A)–(D) are sequential; their depths add: 

(d× + d⊕) + ( d× + d⊕) + d× + d⊕ = 3 d× + 3 d⊕. (26) With d× = 3 , each layer has depth at most 9 + 3 d⊕, a constant independent of n and N .

Overall depth. We have shown that each layer has constant depth 3d× + 3 d⊕ and polynomial size. Stacking 

L = O(1) layers gives overall constant depth L(3 d× + 3 d⊕) and polynomial size. Parameters are hard-wired, so the family is non-uniform. Hence Tn ∈ TC .

Remarks: This model represents the model described in the main text by setting L = 1 and |Σ|n to be the vocabulary of the model for graphs with n nodes. The input size to the model is consistent as any 2 chain graph on n nodes can be represente with exactly n − 2 edges. 

Lemma 9 (TC 0 is closed under AC 0 many–one reductions) . Let L1, L 2 be languages with L1 ≤AC 0 

> m

L2. If L2 ∈ TC 0,then L1 ∈ TC 0 as well. 

20 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof sketch. Let (fn) be the AC 0 reduction and (Cn) a TC 0 circuit family for L2. For each input length n, define a circuit Dn(x) = C|fn(x)|(fn(x)) . The Dn have constant depth, polynomial size, and threshold gates: they are obtained by composing an AC 0 circuit with a TC 0 circuit. Thus (Dn) is a TC 0 family deciding L1.

E.3 From ORD to Two–Chain Endpoint 

We now formalize the reduction from ORD to the decision version of the two–chain endpoint problem. We fix the following encodings. 

Definition 9 (Encoding graphs for ORD and TwoChainEndEq ). An ORD instance (succ , s, t ) on [n] is encoded as: • For each i ∈ [n], the value succ (i) ∈ [n + 1] , where we identify ⊥ with 0, written in binary using ⌈log( n + 1) ⌉ bits; • the vertices s, t ∈ [n], written in binary using ⌈log n⌉ bits each. The encoding of a TwoChainEndEq instance (G, v, q ) uses the same representation for G via a successor function succ G

on [n], together with v, q ∈ [n] in binary. 

Lemma 10 (ORD reduces to TwoChainEndEq in AC 0). There is a DLOGTIME–uniform AC 0 many–one reduction 

(succ , s, t ) 7 → ( gsucc , ˜v, ˜q)

such that 

(succ , s, t ) ∈ ORD ⇐⇒ ( gsucc , ˜v, ˜q) ∈ TwoChainEndEq ,

and moreover the directed graph induced by gsucc is a disjoint union of exactly two directed chains of the same length. Proof. Let (succ , s, t ) be an ORD instance on [n] satisfying the promise that P = ([ n], E ) is a single directed path. 

Encoding convention. We will build an output instance on 2n vertices, which we identify with the set [n] × { 0, 1}. We encode a vertex (i, b ) by writing the copy bit b (one bit) followed by the binary encoding of i ∈ [n] using ⌈log n⌉ bits. The special value ⊥ is encoded as all zeros. This is just a fixed representation choice for this reduction. 

(1) Construction of ( gsucc , ˜v, ˜q). Define gsucc : [ n] × { 0, 1} → ([ n] × { 0, 1}) ∪ {⊥} as follows. For every i ∈ [n] and 

b ∈ { 0, 1},

gsucc (i, b ) = 



⊥, if succ (i) = ⊥,

(succ (i), b ), if succ (i)̸ = ⊥ and i̸ = t, 

(succ (t), 1 − b), if succ (t)̸ = ⊥ and i = t. 

Set ˜v := ( s, 0) .Let end ∈ [n] be the (unique) terminal vertex of the input path, i.e. the unique vertex with succ (end) = ⊥. Define the target vertex ˜q by 

˜q := 

(

(end , 1) , if succ (t)̸ = ⊥,

(end , 0) , if succ (t) = ⊥.

(2) Structure: two equal–length chains. Consider the input path P . For each copy bit b, the edges (i, b ) → (succ (i), b )

follow P within copy b except at the single vertex (t, b ), where (when succ (t)̸ = ⊥) the edge is redirected to the other copy. Therefore the graph on [n] × { 0, 1} induced by gsucc has out-degree at most one everywhere. If succ (t) = ⊥, then gsucc (t, b ) = ⊥ and the output graph is simply two disjoint copies of the same directed path, hence exactly two directed chains, each on n vertices. If succ (t)̸ = ⊥, let u := succ (t). Cutting the input path at the edge t → u decomposes it into a prefix ending at t and a suffix starting at u and ending at end . The redirections (t, 0) → (u, 1) and (t, 1) → (u, 0) “swap” the suffixes between the two copies. Hence the output graph is the disjoint union of exactly the two chains: 

 prefix in copy 0 ·  suffix in copy 1,  prefix in copy 1 ·  suffix in copy 0.

21 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Each chain contains exactly one copy of every original vertex, so each has exactly n vertices. Thus the output is a union of exactly two directed chains of the same length. 

(3) Correctness. We show that (succ , s, t ) ∈ ORD ⇐⇒ ( gsucc , ˜v, ˜q) ∈ TwoChainEndEq .• If succ (t) = ⊥, then t = end is the terminal of the unique directed path on [n]. Since the input graph is a single directed path, t is reachable from every s, so (succ , s, t ) ∈ ORD . In the output graph, the chain containing ˜v = ( s, 0) 

is exactly copy 0 of the path, whose terminal is (end , 0) = ˜ q. Hence ( gsucc , ˜v, ˜q) ∈ TwoChainEndEq .• Suppose succ (t)̸ = ⊥. Consider the unique directed walk from s in the input path. If t is reachable from s, then the walk reaches t, so in the output the walk from (s, 0) reaches (t, 0) and then crosses to copy 1, after which it follows the suffix in copy 1 until terminating at (end , 1) = ˜ q. Thus FTwoChain ( eG, ˜v) = ˜ q. If t is not reachable from s, then the walk from (s, 0) never reaches (t, 0) , hence never crosses copies, and it terminates at (end , 0) ̸ = (end , 1) = ˜ q.Therefore FTwoChain ( eG, ˜v)̸ = ˜ q.This proves the desired equivalence. 

(4) AC 0 and uniformity. We argue that the mapping is computable by a DLOGTIME–uniform AC 0 circuit family. First, ˜v = ( s, 0) is obtained by copying the bits of s and setting the leading copy bit to 0.Next, the bits of end are selectable in AC 0 under the promise that there is a unique terminal vertex. For each i ∈ [n] let 

IsEnd( i) be the predicate “ succ (i) = ⊥”, which is a depth–2 AC 0 check that all bits of the encoding of succ (i) are 0. Then each bit of end can be written as an unbounded OR over i ∈ [n]:

end r =

> n

_

> i=1



IsEnd( i) ∧ bit r (i)



,

where bit r (i) is the r-th bit of the (fixed) binary encoding of the constant index i. The copy bit of ˜q is 1 iff succ (t)̸ = ⊥,which is again a depth–2 AC 0 predicate on the bits of succ (t). Thus ˜q is computable in constant depth. Finally, each output entry gsucc (i, b ) is obtained from the input bits of succ (i) and the predicate EQ( i, t ) (computed as in Lemma 10 by a depth–2 AC 0 equality test between the constant index i and the binary input t), together with the predicate 

succ (t)̸ = ⊥. The rule above is a constant-depth case distinction implemented by AND/OR/NOT gates. DLOGTIME uniformity follows since the circuit only uses fixed wiring patterns plus the standard uniform equality gadgets. This completes the proof. 

E.4 From the Function Problem to the Decision Problem 

We now relate the function FTwoChain to the decision problem TwoChainEndEq .

Lemma 11 (From endpoint function to endpoint equality) . Let FTwoChain be the two–chain endpoint function. Define the associated decision language 

LFTwoChain = { (x, q ) : FTwoChain (x) = q },

where x encodes (G, v ) and q encodes a vertex in [n]. Then TwoChainEndEq = LFTwoChain , and the mapping (G, v, q ) 7 →

(( G, v ), q ) is computable in DLOGTIME–uniform AC 0.Proof. By definition, TwoChainEndEq is precisely the set of triples (G, v, q ) such that FTwoChain (G, v ) = q. The encoding of instances in LFTwoChain is just the concatenation of the encoding of (G, v ) and the encoding of q, which matches the encoding of (G, v, q ) used above, up to a trivial re-ordering of fields. The re-ordering is implemented by wiring and does not require any gates, so it is computable in depth 1. Thus TwoChainEndEq = LFTwoChain and the identity mapping is a DLOGTIME–uniform AC 0 transformation between the two encodings. 

Lemma 12 (Computing FTwoChain implies deciding TwoChainEndEq in TC 0). Suppose FTwoChain is computable by a non–uniform TC 0 circuit family. Then TwoChainEndEq ∈ TC 0.

22 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. Let (Cn) be TC 0 circuits computing FTwoChain on inputs of length n. To decide whether FTwoChain (x) = q, we can: • feed x into C|x| to obtain an output y encoding a vertex in [n];• compare y and q bitwise using an AC 0 equality circuit: y = q iff each bit matches. Equality on O(log n) bits is in AC 0, and the composition of a TC 0 circuit with an AC 0 post–processing is still a TC 0

circuit by the closure Lemma 9. Thus TwoChainEndEq = LFTwoChain ∈ TC 0.

E.5 Main Theorem: No–CoT Transformers Cannot Solve Two–Chain Endpoint 

We are now ready to state and prove the main conditional lower bound. 

Theorem 4 (No–CoT transformers cannot compute the two–chain endpoint function, unless TC 0 = NC 1). Assume 

TC 0̸ = NC 1. Then there is no family of no–CoT transformers in T0 that computes FTwoChain on all valid inputs. Equivalently, FTwoChain /∈ T 0.Proof. Suppose, for contradiction, that there is a family of no–CoT transformers computing FTwoChain exactly on all valid inputs (graphs that are unions of at most two chains). By Lemma 8, we have T0 ⊆ TC 0, so FTwoChain ∈ TC 0. By Lemma 12, this implies TwoChainEndEq ∈ TC 0.By Lemma 10, we have ORD ≤AC 0 

> m

TwoChainEndEq . By the closure Lemma 9, it follows that ORD ∈ TC 0.By Lemma 4, we also have WP G ≤AC 0 

> m

ORD for some fixed finite group G. Combining this with ORD ∈ TC 0 and applying Lemma 9 again, we obtain WP G ∈ TC 0.Finally, by Lemma 1, WP G is NC 1–complete under AC 0 reductions. Hence for every language L ∈ NC 1, there is an AC 0

reduction L ≤AC 0 

> m

WP G. By Lemma 9, this implies L ∈ TC 0. Therefore NC 1 ⊆ TC 0.Since we always have TC 0 ⊆ NC 1, we conclude TC 0 = NC 1, contradicting the assumption that these classes are different. Thus our initial assumption that a no–CoT transformer can compute FTwoChain must be false. 

## F Proof of Theorem 2 

This section proves Theorem 2 and is organized as follows. In Appendix F.1, we show that the loss can be expressed as the same-side absorption probability of a Markov chain, which we call the canonical chain. Appendix F.2 introduces the key definitions (depth, long jumps) and two surrogate chains—the long-jump-absorbing chain and the no-long-jump chain—along with the source function representation that relates parameter derivatives to expected sources along trajectories. Appendix F.3 and Appendix F.4 compute the source functions and derivative estimates for these two chains respectively. Appendix F.6 combines these estimates to establish that ∂αS ≥ 0, ∂β S ≤ 0, and ∂γ S ≤ 0 for easy examples, implying monotonic parameter evolution under gradient flow. Appendix F.7 shows that hard examples contribute negligibly to the gradient. Finally, Appendix F.9 assembles these components to complete the proof. We establish the derivative estimates at initialization, but they in fact hold throughout the learning process as long as the forward transition probability remains non-decreasing and the long-jump probability stays bounded away from zero. Since our analysis shows that both conditions are maintained for all t ≤ t0, these estimates in fact hold for all t ≤ t0.

F.1 Loss can be written as absorption probability of a Markov chain 

Lemma 67 implies that the loss from Section 4, under the assumptions of Section 6.1, is equivalent to the probability of same-side absorption in a Markov chain on the state space of two identical directed chains A and B of equal length n, with absorbing endpoints an, b n, and nonterminal states a1, . . . , a n−1, b 1, . . . , b n−1. We will therefore analyze this absorption probability throughout the subsequent sections, which we term the canonical Markov chain .The transition probabilities of the canonical chain are determined by a logit structure with parameters α, β, γ ∈ R and a fixed constant d > 0, where we specialize to the case α′ = β′ = d. While our paper is interested in the case d = 1 , we keep 23 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

d as a parameter to show that our analysis generalizes to other cases. Without loss of generality, we subtract dγ (t) from all logits (which does not affect the softmax probabilities). If v is in the middle of the chain, we have that: 

o(v, C )k =



dα (t), k is one ahead of v in the chain 

dβ (t), k is one back of v in the chain 

0, k start of some chain (not necessary that with v)

0, k end of some chain (not necessary that with v)

dγ (t), otherwise If v is at the penultimate vertex of the chain, we have that: 

o(v, C )k =



d(α(t) − γ(t)) , k is one ahead of v in the chain 

dβ (t), k is one back of v in the chain 

0, k start of some chain (not necessary that with v)

0, k is the end of chain without vdγ (t), otherwise If v is at the second vertex of the chain, we have that: 

o(v, C )k =



dα (t), k is one ahead of v in the chain 

d(β(t) − γ(t)) , k is one back of v in the chain 

0, k start of some chain without v

0, k end of some chain (not necessary that with v)

dγ (t), otherwise If v is at the first vertex of the chain, we have that: 

o(v, C )k =



dα (t), k is one ahead of v in the chain 

0, k start of some chain without v

0, k is the end of some chain (not necessary that with v)

dγ (t), otherwise The transition probabilities are then given by softmax: 

π(v → k) = exp( o(v, C )k)

P 

> k′

exp( o(v, C )k′ ) .

F.2 Preliminaries 

This subsection establishes the foundational definitions and tools for analyzing the canonical Markov chain. We begin by defining key quantities such as hitting probabilities, depth, and long jumps (Definitions 11 and 12). We then introduce two surrogate chains (Definitions 13 and 14) that decompose the canonical chain’s behavior, enabling separate analysis of long-jump and local-step dynamics. Finally, we present the source function representation (Lemma 14) that expresses parameter derivatives in terms of expected sources along trajectories, which forms the basis for our derivative analysis in subsequent sections. F.2.1 USEFUL DEFINITIONS 

Definition 10 (Hitting probabilities and same–side value) . For any state v (including absorbing states), let 

hA(v) := Pr  

> v

[Xτ = an], hB (v) := Pr  

> v

[Xτ = bn],

24 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

where τ is the (almost surely finite) hitting time of {an, b n}.For v ∈ { a1, . . . , a n−1} on chain A, the “same–side absorption probability” is hA(v); for v ∈ { b1, . . . , b n−1} on chain B,it is hB (v). Denote these by 

S(v) := 

(

hA(v), v ∈ { a1, . . . , a n−1},hB (v), v ∈ { b1, . . . , b n−1}.

Definition 11 (Depth / distance to terminal) . Fix a chain length n ≥ 2 and consider the state space 

V := {a1, . . . , a n, b 1, . . . , b n}.

Define the depth (distance to the terminal on the same chain) by the function 

d : V → { 0, 1, . . . , n − 1}, d(ai) := n − i, d(bi) := n − i. 

Definition 12 (Long jump) . A transition x → y is a long jump if the depth changes by 2 or more, i.e. 

|d(y) − d(x)| ≥ 2.

F.2.2 SURROGATE CHAINS 

To analyze the canonical chain, we introduce two auxiliary Markov chains: the long-jump-absorbing chain and the no-long-jump chain. Lemma 13 below shows that the derivative of the same-side absorption probability can be decomposed into contributions from these two surrogate chains. This decomposition is fundamental to our analysis, as it separates the effect of long-range transitions from local transitions. 

Definition 13 (Long–jump–absorbing chain) . The long–jump–absorbing chain is the Markov chain obtained from the two–chain model by redirecting every long jump x → y to a single new absorbing state e: all non–long–jump transition probabilities are unchanged, and the total probability mass of long jumps out of x is assigned to e. The absorbing set is 

T := {an, b n, e }.

Definition 14 (No–long–jump (NL) chain) . The no–long–jump (NL) chain is the Markov chain obtained from the two–chain model by disallowing all long jumps: from any nonterminal state x, the only allowed transitions are to states y̸ = x with 

|d(y) − d(x)| ≤ 1, and the transition probabilities are obtained from the original chain by conditioning on this. (As in the original chain, the terminal states an, b n are absorbing). 

Proposition 3 (Long-jump conditional absorption is 12 ). Fix parameters (hence a fixed transition kernel) and a start state v.Let 

τ := inf {t ≥ 0 : Xt ∈ { an, b n}} , J := inf {t ≥ 0 : ( Xt → Xt+1 ) is a long jump },

and write LJ := {J ≤ τ } for the event that a long jump occurs before absorption. Then 

Pr  

> v

[Xτ = an | LJ ] = 12 .

Proof. Let φ be the chain-swap map on states: φ(ai) = bi and φ(bi) = ai. For any state y, by chain-swap symmetry of the model we have 

Pr  

> y

[Xτ = an] + Pr 

> φ(y)

[Xτ = an] = 1 ,

since swapping chains interchanges the absorbing events {Xτ = an} and {Xτ = bn}.Now condition on the time and landing state of the first long jump. On LJ , let Y := XJ+1 be the landing state of the first long jump. By definition of a long jump and the logit structure, the distribution of Y is symmetric under φ (targets come in 

φ-paired states with equal one-step probability from any fixed source), so Pr v [Y = y | LJ ] = Pr v [Y = φ(y) | LJ ] for every y. Therefore, 

Pr  

> v

[Xτ = an | LJ ] = X

> y

Pr  

> v

[Y = y | LJ ] Pr  

> y

[Xτ = an]= 12

X

> y

Pr  

> v

[Y = y | LJ ]



Pr  

> y

[Xτ = an] + Pr 

> φ(y)

[Xτ = an]



= 12

X

> y

Pr  

> v

[Y = y | LJ ] · 1 = 12 ,

25 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

as claimed. 

Lemma 13 (Derivative decomposition) . Fix θ ∈ { α, β, γ } and a start state v. Let 

τ := inf {t ≥ 0 : Xt ∈ { an, b n}} , J := inf {t ≥ 0 : ( Xt → Xt+1 ) is a long jump },

and define N L := {J > τ } and LJ := {J ≤ τ }. Set 

Sθ (v) := θ

Pr  

> v

[Xτ = an], hθ (v) := θ

Pr  

> v

(LJ ), ¯SN L θ (v) := θ

Pr  

> v

(Xτ = an | N L ), ¯SLJ θ (v) := θ

Pr  

> v

(Xτ = an | LJ ).

Then 

Sθ (v) = (1 − hθ (v)) ¯SN L θ (v) + hθ (v) ¯SLJ θ (v),

and 

∂θ Sθ (v) = ( ∂θ hθ (v))   ¯SLJ θ (v) − ¯SN L θ (v) + (1 − hθ (v)) ∂θ ¯SN L θ (v) + hθ (v) ∂θ ¯SLJ θ (v).

By Proposition 3, ¯SLJ θ (v) = 12 , hence 

Sθ (v) = 12 + (1 − hθ (v)) 

 ¯SN L θ (v) − 12



, ∂θ Sθ (v) = −(∂θ hθ (v)) 

 ¯SN L θ (v) − 12



+ (1 − hθ (v)) ∂θ ¯SN L θ (v).

Proof. The argument is identical for θ ∈ { α, β, γ }, so we fix an arbitrary θ throughout. Since {N L, LJ } is a partition of the sample space, 

Sθ (v) = θ

Pr  

> v

(Xτ = an, N L ) + θ

Pr  

> v

(Xτ = an, LJ ).

Writing Pr θv (Xτ = an, N L ) = Pr θv (N L ) Pr θv (Xτ = an | N L ) = (1 − hθ (v)) ¯SN L θ (v) and Pr θv (Xτ = an, LJ ) = Pr θv (LJ ) Pr θv (Xτ = an | LJ ) = hθ (v) ¯SLJ θ (v) yields 

Sθ (v) = (1 − hθ (v)) ¯SN L θ (v) + hθ (v) ¯SLJ θ (v).

Differentiate the preceding identity with respect to θ and apply the product rule: 

∂θ Sθ (v) = −(∂θ hθ (v)) ¯SN L θ (v) + (1 − hθ (v)) ∂θ ¯SN L θ (v) + ( ∂θ hθ (v)) ¯SLJ θ (v) + hθ (v)∂θ ¯SLJ θ (v)= ( ∂θ hθ (v))   ¯SLJ θ (v) − ¯SN L θ (v) + (1 − hθ (v)) ∂θ ¯SN L θ (v) + hθ (v) ∂θ ¯SLJ θ (v).

By Proposition 3, ¯SLJ θ (v) = 12 ; substituting this constant into the value identity and its derivative identity yields the displayed simplifications. F.2.3 SOURCE FUNCTION OF SURROGATE CHAINS AND DERIVATIVE FORM 

To analyze how the absorption probabilities change under gradient flow, we require a tractable representation of their parameter derivatives ∂θ u(v) where θ ∈ { α, β, γ } and u(v) is an absorption probability of interest. Direct computation of these derivatives is challenging due to the complex dependency of multi-step trajectories on the parameters. The source function presented bellow is advantageous because sources depend only on single-step transition sensitivities and can be computed explicitly from our logit structure, enabling the quantitative bounds developed in Appendices F.3 and F.4. 

Lemma 14 (Source representation) . Let (Xt)t≥0 be a finite Markov chain with transition matrix Pθ smoothly parameterized by θ ∈ R. Let T be a nonempty absorbing set and τ := inf {t ≥ 0 : Xt ∈ T }. Fix a target state t⋆ ∈ T and define the absorption probability 

uθ (x) := θ

Pr  

> x

[Xτ = t⋆], x ∈ S ,

so uθ (t⋆) = 1 and uθ (t) = 0 for t ∈ T \ { t⋆}. Then for every nonterminal state x / ∈ T , uθ is Pθ –harmonic: 

uθ (x) = X

> y∈S

Pθ (x, y ) uθ (y).

26 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Define the source for ∂θ uθ by 

sθ (x) := X

> y∈S

 ∂θ Pθ (x, y ) uθ (y), x / ∈ T. 

Then gθ := ∂θ uθ satisfies, for all x / ∈ T ,

gθ (x) = X

> y

Pθ (x, y )gθ (y) + sθ (x), gθ (t) = 0 , t ∈ T, 

and admits the trajectory representation 

∂θ uθ (x) = Eθx

"τ −1X

> t=0

sθ (Xt)

#

, x ∈ S .

Proof. Let S be the finite state space and let T ⊂ S be the absorbing set. Write U := S \ T for the transient states. Reorder states so that transient states come first, and write the transition matrix in block form 

Pθ =

Qθ Rθ

0 I



,

where Qθ is the |U | × | U | transient-to-transient submatrix. Fix a target t⋆ ∈ T and define the absorption probability 

uθ (x) := θ

Pr  

> x

[Xτ = t⋆], τ := inf {t ≥ 0 : Xt ∈ T }.

Then uθ (t⋆) = 1 and uθ (t) = 0 for t ∈ T \ { t⋆}, and for x ∈ U , the harmonic equations read 

uθ (x) = X

> y∈U

Qθ (x, y )uθ (y) + X

> t∈T

Rθ (x, t )uθ (t).

Equivalently, in vector form on U ,

uθ = Qθ uθ + rθ , rθ (x) := X

> t∈T

Rθ (x, t )uθ (t) = Rθ (x, t ⋆).

Hence 

(I − Qθ )uθ = rθ . (27) Differentiate (27) with respect to θ:

−(∂θ Qθ )uθ + ( I − Qθ ) ∂θ uθ = ∂θ rθ .

Let gθ := ∂θ uθ . Rearranging gives the linear system on U ,

(I − Qθ )gθ = sθ , sθ := ∂θ rθ + ( ∂θ Qθ )uθ . (28) Noting that uθ is constant on T , we also have the boundary condition gθ |T ≡ 0.Because the chain is absorbing, the spectral radius satisfies ρ(Qθ ) < 1, hence (I − Qθ ) is invertible and 

(I − Qθ )−1 =

> ∞

X

> k=0

Qkθ (absolutely convergent). Therefore from (28), 

gθ = ( I − Qθ )−1sθ =

> ∞

X

> k=0

Qkθ sθ . (29) For x ∈ U , the vector (Qkθ sθ )( x) equals the expected source at time k before absorption: 

(Qkθ sθ )( x) = X

> y∈U

Qkθ (x, y ) sθ (y) = Eθx

sθ (Xk) 1{k < τ }.

27 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Summing over k and justifying the exchange of sum and expectation by dominated convergence (since |sθ | is bounded on the finite state space and Eθx [τ ] < ∞ for every transient x in a finite absorbing chain) gives 

gθ (x) = 

> ∞

X

> k=0

Eθx

sθ (Xk) 1{k < τ } = Eθx

"τ −1X

> k=0

sθ (Xk)

#

.

Since gθ = ∂θ uθ and gθ (t) = 0 for t ∈ T , this holds for all x ∈ S (trivially on T ). This is the desired representation. 

F.3 Analysis of long-jump-absorbing chain 

In this subsection, we analyze the long-jump-absorbing Markov chain defined in Definition 13. Our goal is to compute the source functions at each transient state x, which by Lemma 14 determine the derivatives of the long-jump absorption probability. We establish in Lemma 15 that these sources exhibit symmetry between the two chains, and then derive explicit formulas and bounds for the α, β, and γ sources in subsequent lemmas. These results feed into Appendix F.3.1 where we obtain quantitative derivative estimates, and ultimately into Appendix F.6 where we show monotonicity of the loss as a function of the parameters. 

Lemma 15 (Source symmetry) . Work in the two–chain model and in the long–jump–absorbing chain of Definition 13. Let 

T := {an, b n, e }, τ := inf {t ≥ 0 : Xt ∈ T }, uθ (x) := θ

Pr  

> x

[Xτ = e].

For θ ∈ { α, β, γ } define the local source on transient states 

sθ (x) := X

> y

 ∂θ Pθ (x, y ) uθ (y), x / ∈ T, 

where Pθ is the transition matrix of the long–jump–absorbing chain. Then for every i ∈ { 1, . . . , n − 1} and every θ ∈ { α, β, γ },

sθ (ai) = sθ (bi).

Proof. Fix θ and let φ be the chain-swap involution: φ(ai) = bi, φ(bi) = ai for i ≤ n, and φ(e) = e. By symmetry of the logits, the long–jump redirection, and the softmax, the kernel Pθ is φ-invariant: 

Pθ (φ(x), φ (y)) = Pθ (x, y ) (∀x, y ).

Hence ∂θ Pθ (φ(x), φ (y)) = ∂θ Pθ (x, y ), and since the definition of e is symmetric with respect to the two chains, also 

uθ (φ(x)) = uθ (x). Therefore, for any transient x / ∈ T ,

sθ (φ(x)) = X

> y

∂θ Pθ (φ(x), y ) uθ (y) = X

> y

∂θ Pθ (x, y ) uθ (y) = sθ (x),

where we reindexed y 7 → φ(y) and used φ-invariance of ∂θ Pθ and uθ .

Lemma 16 (Monotonicity of long-jump absorption) . Work in the long–jump–absorbing chain (Definition 13) with terminal index n (so T = {an, b n, e } is absorbing) and depth function d(·) as in Definition 11. Let τ := inf {t ≥ 0 : Xt ∈ T } and define u(v) := Pr v [Xτ = e]. By chain–swap symmetry u(ai) = u(bi); write ui := u(ai) = u(bi), so un = 0 .For i = 1 , . . . , n − 1 define the depth-grouped probabilities (aggregating by the effect on depth): 

qi := Pr( Xt+1 = e | Xt ∈ { ai, b i}), fi := Pr( d(Xt+1 ) = d(Xt) − 1 | Xt ∈ { ai, b i}),bi := Pr( d(Xt+1 ) = d(Xt) + 1 | Xt ∈ { ai, b i}), ri := Pr( d(Xt+1 ) = d(Xt) | Xt ∈ { ai, b i}),

where the probabilities aggregate all allowed transitions by their effect on depth (ignoring which chain the next state lies in). These satisfy qi + fi + bi + ri = 1 , with the boundary convention b1 := 0 , and fi > 0.Define forward differences di := ui − ui+1 for i = 1 , . . . , n − 1, and set d0 := 0 . Then di ≥ 0 for all i, hence 

u1 ≥ u2 ≥ · · · ≥ un = 0 .

28 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. Fix i ∈ { 1, . . . , n − 1}. Since u is harmonic on transient states and depends only on depth by symmetry, the harmonic equation at depth i can be written using the depth–grouped probabilities: 

ui = qi · u(e) + fiui+1 + biui−1 + riui = qi + fiui+1 + biui−1 + riui, (30) with un = 0 and b1 = 0 . Rearranging gives 

(1 − ri)ui = qi + fiui+1 + biui−1.

Substitute ui+1 = ui − di and ui−1 = ui + di−1 to obtain 

(1 − ri)ui = qi + fi(ui − di) + bi(ui + di−1) = qi + ( fi + bi)ui − fidi + bidi−1.

Using 1 − ri = qi + fi + bi, cancel (fi + bi)ui from both sides, yielding 

qiui = qi − fidi + bidi−1,

and hence 

fidi = bidi−1 + qi(1 − ui). (31) Since fi > 0, bi ≥ 0, qi ≥ 0, and 1 − ui ≥ 0, the identity (31) implies di ≥ 0 whenever di−1 ≥ 0. For i = 1 , we have 

b1 = 0 and d0 = 0 , so 

f1d1 = q1(1 − u1) ≥ 0 ⇒ d1 ≥ 0.

Inductively, di ≥ 0 for all i, hence ui ≥ ui+1 for all i, proving monotonicity. 

Corollary 1 (Lower bounds on di via forward–step events) . Under the assumptions of Lemma 16, for every i = 1 , . . . , n − 1,

di ≥ qi

fi

(1 − ui).

Moreover, since the depth at state ai is n − i, reaching the terminal requires n − i consecutive forward steps (each decreasing depth by 1), and 

1 − ui ≥ Pr(( n − i) consecutive forward steps starting from state ai),

hence 

di ≥ qi

fi

Pr(( n − i) consecutive forward steps starting from state ai).

In particular, if fk ≥ fmin > 0 for all k ∈ { i, i + 1 , . . . , n − 1}, then 

Pr(( n − i) consecutive forward steps starting from state ai) ≥

> n−i−1

Y

> j=0

fi+j ≥ (fmin )n−i,

and therefore 

di ≥ qi

fin−i−1Y

> j=0

fi+j ≥ qi

fi

(fmin )n−i.

Proof. From (31) and bidi−1 ≥ 0 we get fidi ≥ qi(1 − ui), proving di ≥ qi 

> fi

(1 − ui).Next, 1 − ui = Pr( τ̸ = τe) = Pr( τe > τ ) is the probability of not hitting e before absorption. Since the depth at state ai is 

n − i, reaching the terminal an (depth 0) requires exactly n − i consecutive forward steps. The event that the chain makes 

n − i consecutive forward steps starting from ai implies that e is not hit during those steps, hence is contained in {τe > τ },giving 1 − ui ≥ Pr(( n − i) consecutive forward steps from ai). The remaining inequalities follow by multiplying the corresponding one–step lower bounds and using fk ≥ fmin on the indicated range k ∈ { i, . . . , n − 1}.

Lemma 17 (Long-jump absorption bound) . For every start state x / ∈ T ,

u(x) ≤ p(e)max Ex[τ ].

In particular, whenever Ex[τ ] ≤ D(x)/b 0 (e.g. by applying Lemma 63 to the depth process under a drift gap b0 > 0), 

u(x) ≤ D(x)

b0

p(e)max .

29 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. Let Ft := σ(X0, . . . , X t) and write 

Ne :=  

> τ−1

X

> t=0

1{Xt+1 = e}.

Since e is absorbing and e ∈ T , the event {Xτ = e} occurs iff the chain makes a transition to e before absorption, so 

u(x) = Pr x[Xτ = e] = Ex[Ne]. By the tower property and Markov property, 

Ex[Ne] = Ex

"τ −1X

> t=0

Ex[1{Xt+1 = e} | F t]

#

= Ex

"τ −1X

> t=0

eP (Xt, e )

#

= Ex

"τ −1X

> t=0

p(e; Xt)

#

≤ p(e)max Ex[τ ],

which proves the first display. The final inequality follows by substituting any upper bound on Ex[τ ], such as Ex[τ ] ≤

D(x)/b 0.

Lemma 18 (α-source in LJ chain) . Let eP be the transition kernel of the long–jump–absorbing chain and let u(x) := Pr x[Xτ = e] denote the absorption probability at e (so u(e) = 1 and u(an) = u(bn) = 0 ). Let F (x) ∈ S(x) denote the forward neighbor when it exists and define 

pf (x) := eP (x, F (x)) (set pf (x) := 0 if F (x) does not exist ).

The α–source in the long–jump–absorbing chain satisfies for every transient x / ∈ T ,

sα(x) = d p f (x) u(F (x)) − u(x),

with the convention that sα(x) = 0 if F (x) does not exist. Proof. Fix a transient state x / ∈ T . Let L(x) be the set of long–jump targets in the original chain. Since eP (x, y ) = P (x, y )

for y ∈ S(x) and eP (x, e ) = P 

> z∈L(x)

P (x, z ), we have 

sα(x) = X

> y∈S(x)

(∂αP (x, y )) u(y) + 

 X

> z∈L(x)

∂αP (x, z )

 u(e),

and u(e) = 1 . Because P (x, ·) is a softmax over the logits {o(x)y } in the original chain and only the forward logit depends on α with derivative d, Lemma 61 gives, for each original target y,

∂αP (x, y ) = d P (x, y ) 1{y = F (x)} − P (x, F (x))  = d P (x, y ) 1{y = F (x)} − pf (x).

Introduce the auxiliary successor value 

wx(y) := 

(

u(y), y ∈ S(x),

1, y ∈ L(x).

Then the definition of sα(x) can be rewritten as 

sα(x) = X

> y

(∂αP (x, y )) wx(y),

where the sum ranges over all original targets y from x. Substituting the softmax derivative yields 

sα(x) = d X

> y

P (x, y ) 1{y = F (x)} − pf (x)wx(y) = d



P (x, F (x)) wx(F (x)) − pf (x) X

> y

P (x, y )wx(y)



.

Since wx(F (x)) = u(F (x)) and P (x, F (x)) = pf (x), this becomes 

sα(x) = d p f (x)



u(F (x)) − X

> y

P (x, y )wx(y)



.

30 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Finally, harmonicity of u for the long–jump–absorbing chain gives the aggregated identity 

u(x) = X

> y∈S(x)

eP (x, y )u(y) + eP (x, e )u(e) = X

> y∈S(x)

P (x, y )u(y) + X

> z∈L(x)

P (x, z ) · 1 = X

> y

P (x, y )wx(y),

so P 

> y

P (x, y )wx(y) = u(x), and hence 

sα(x) = d p f (x) u(F (x)) − u(x).

If F (x) does not exist then ∂αo(x)y ≡ 0, hence ∂αP (x, y ) = 0 for all y, which implies ∂α eP (x, y ) = 0 for all y ∈ S(x)∪{ e}

and thus sα(x) = 0 , consistent with the stated convention. 

Lemma 19 (β-source in LJ chain) . Let eP be the transition kernel of the long–jump–absorbing chain and let u(x) := Pr x[Xτ = e] (so u(e) = 1 and u(an) = u(bn) = 0 ). Let B(x) ∈ S(x) denote the backward neighbor when it exists and define 

pb(x) := eP (x, B (x)) (set pb(x) := 0 if B(x) does not exist ).

The β–source in the long–jump–absorbing chain satisfies for every transient x / ∈ T ,

sβ (x) = d p b(x) u(B(x)) − u(x),

with the convention that sβ (x) = 0 if B(x) does not exist. Proof. Fix a transient state x / ∈ T . Let L(x) be the set of long–jump targets in the original chain. Since eP (x, y ) = P (x, y )

for y ∈ S(x) and eP (x, e ) = P 

> z∈L(x)

P (x, z ), we have 

sβ (x) = X

> y∈S(x)

(∂β P (x, y )) u(y) + 

 X

> z∈L(x)

∂β P (x, z )

 u(e),

and u(e) = 1 . Because P (x, ·) is a softmax over the logits {o(x)y } in the original chain and only the backward logit depends on β with derivative d, Lemma 61 gives, for each original target y,

∂β P (x, y ) = d P (x, y ) 1{y = B(x)} − P (x, B (x))  = d P (x, y ) 1{y = B(x)} − pb(x).

Introduce the auxiliary successor value 

wx(y) := 

(

u(y), y ∈ S(x),

1, y ∈ L(x).

Then the definition of sβ (x) can be rewritten as 

sβ (x) = X

> y

(∂β P (x, y )) wx(y),

where the sum ranges over all original targets y from x. Substituting the softmax derivative yields 

sβ (x) = d X

> y

P (x, y ) 1{y = B(x)} − pb(x)wx(y) = d



P (x, B (x)) wx(B(x)) − pb(x) X

> y

P (x, y )wx(y)



.

Since wx(B(x)) = u(B(x)) and P (x, B (x)) = pb(x), this becomes 

sβ (x) = d p b(x)



u(B(x)) − X

> y

P (x, y )wx(y)



.

Finally, harmonicity of u for the long–jump–absorbing chain gives the aggregated identity 

u(x) = X

> y∈S(x)

eP (x, y )u(y) + eP (x, e )u(e) = X

> y∈S(x)

P (x, y )u(y) + X

> z∈L(x)

P (x, z ) · 1 = X

> y

P (x, y )wx(y),

31 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

so P 

> y

P (x, y )wx(y) = u(x), and hence 

sβ (x) = d p b(x) u(B(x)) − u(x).

If B(x) does not exist then ∂β o(x)y ≡ 0, hence ∂β P (x, y ) = 0 for all y, which implies ∂β eP (x, y ) = 0 for all y ∈ S(x)∪{ e}

and thus sβ (x) = 0 , consistent with the stated convention. 

Lemma 20 (γ-source in LJ chain) . Work in the long–jump–absorbing chain (Definition 13) with terminal index n (so 

T := {an, b n, e } is absorbing) and assume self–transitions are disallowed. Fix d > 0 and parameters α, β, γ . Let 

u(v) := Pr  

> v

[Xτ = e], τ := inf {t ≥ 0 : Xt ∈ T },

so u(e) = 1 and u(an) = u(bn) = 0 . By chain–swap symmetry, u(ai) = u(bi); write ui := u(ai) = u(bi) and un = 0 .For a transient state x / ∈ T , define the local γ–source 

sγ (x) := X

> z

(∂γ eP (x, z )) u(z),

where eP is the transition kernel of the long–jump–absorbing chain. Then the following explicit formulas hold. 

(A) Deep middle x = ai, 2 < i < n − 1. Let 

Zmid := edα + edβ + 4 + (2 n − 7) edγ .

Then 

sγ (ai) = d edγ 

Zmid 



(2 n − 10)(1 − ui) + ui−1 + ui+1 − 2ui



.

(B) First vertex x = a1. Let 

Z1 := edα + 3 + (2 n − 5) edγ .

Then 

sγ (a1) = d edγ 

Z1



(2 n − 6)(1 − u1) + ( u2 − u1)



.

(C) Second vertex x = a2. Let 

Z2 := edα + ed(β−γ) + 3 + (2 n − 6) edγ .

Then 

sγ (a2) = d

 (2 n − 8) edγ 

Z2

(1 − u2) + edγ 

Z2

(u3 − u2) + ed(β−γ)

Z2

(u2 − u1)



.

(D) Penultimate vertex x = an−1. Let 

Zn−1 := ed(α−γ) + edβ + 3 + (2 n − 6) edγ .

Then 

sγ (an−1) = d

 (2 n − 8) edγ 

Zn−1

(1 − un−1) + edγ 

Zn−1

(un−2 − un−1) + ed(α−γ)

Zn−1

un−1



.

Proof. We argue as in Lemmas 18 and 19. Fix a transient state x / ∈ T . Let L(x) be the set of long–jump targets in the original chain. Since eP (x, y ) = P (x, y ) for 

y ∈ S(x) and eP (x, e ) = P 

> z∈L(x)

P (x, z ), we have 

sγ (x) = X

> y∈S(x)

(∂γ P (x, y )) u(y) + 

 X

> z∈L(x)

∂γ P (x, z )

 u(e),

32 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

and u(e) = 1 . Introduce the auxiliary successor value 

wx(y) := 

(

u(y), y ∈ S(x),

1, y ∈ L(x).

Then the definition of sγ (x) can be rewritten as 

sγ (x) = X

> y

(∂γ P (x, y )) wx(y), (32) where the sum ranges over all original targets y from x. Harmonicity of u for the long–jump–absorbing chain gives 

u(x) = P 

> y

P (x, y ) wx(y).Because P (x, ·) is a softmax over the logits {o(x)y } in the original chain, Lemma 61 gives, for each original target y,

∂γ P (x, y ) = P (x, y ) ∂γ o(x, y ) − X

> z

P (x, z )∂γ o(x, z ).

In each of the four cases below, ∂γ o(x, y ) is constant on the γ–dependent classes of targets, so (32) reduces to a finite sum over those classes. We now enumerate the γ–dependent targets and whether they are short/long in each case; all remaining targets have ∂γ o(x, y ) = 0 and contribute nothing to sγ (x).

(A) Deep middle x = ai, 2 < i < n − 1. The γ–dependent logits are exactly the “otherwise” targets, which have logit dγ ,hence ∂γ o = d. There are 2n − 7 such targets: the three short switches bi−1, b i, b i+1 and 2n − 10 others. Among these, bi−1

and bi+1 are short with values ui−1, u i+1 , while bi is short with value ui (hence contributes zero after centering), and the remaining 2n−10 are long jumps (value 1). All these have probability edγ /Z mid , where Zmid = edα +edβ +4+(2 n−7) edγ .Substituting into (32) yields the stated formula for sγ (ai).

(B) First vertex x = a1. The unique α–target is a2 (logit dα ); the special zero–logit targets are b1, a n, b n; all remaining 

2n − 5 targets have logit dγ and thus ∂γ o = d. Among the γ–targets, the only short one is b2, with value u2; the other 2n − 6

γ–targets are long jumps (value 1). All γ–targets have probability edγ /Z 1 with Z1 = edα + 3 + (2 n − 5) edγ . Plugging into (32) gives the stated expression for sγ (a1).

(C) Second vertex x = a2. Here a3 has logit dα , a1 has logit d(β − γ) so ∂γ o(a2, a 1) = −d, the special zero–logit targets are b1, a n, b n, and all remaining 2n − 6 targets have logit dγ and ∂γ o = d. Among these γ–targets, the short ones are 

b2 (value u2, hence cancels) and b3 (value u3); the remaining 2n − 8 γ–targets are long jumps (value 1). The softmax denominator is Z2 = edα + ed(β−γ) + 3 + (2 n − 6) edγ , so P (a2, a 1) = ed(β−γ)/Z 2 and each γ–target has probability 

edγ /Z 2. Substituting these contributions into (32) yields the stated formula for sγ (a2).

(D) Penultimate vertex x = an−1. Here an has logit d(α − γ) so ∂γ o(an−1, a n) = −d, an−2 has logit dβ , the special zero–logit targets are a1, b 1, b n, and all remaining 2n − 6 targets have logit dγ and ∂γ o = d. Among these γ–targets, the short ones are bn−2 (value un−2) and bn−1 (value un−1, hence cancels); the remaining 2n − 8 γ–targets are long jumps (value 1). The denominator is Zn−1 = ed(α−γ) + edβ + 3 + (2 n − 6) edγ , so P (an−1, a n) = ed(α−γ)/Z n−1 and each 

γ–target has probability edγ /Z n−1. Plugging these into (32) gives the stated formula for sγ (an−1).

Lemma 21 (γ-source sign bounds) . Assume the setting and notation of Lemma 20 and Lemma 16. Let τ be the absorption time into T = {an, b n, e }. For a deep-middle index i with 2 < i < n − 1, define the deep-middle one-step absorption probability 

qmid := eP (ai, e ) = (2 n − 10) edγ 

Zmid 

.

Assume a forward-drift condition at the depths under consideration: 

fj ≥ bj for all relevant depths j. (33) 

Then there exist constants n0 ∈ N and C⋆ ≥ 1, depending only on (α, β, γ, d ), such that for all n ≥ n0 the following bounds hold. 

33 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

(A) Deep middle ( x = aj , 2 < j < n − 1). 

sγ (aj ) ≥ d

2 qj (1 − uj ) > 0, qj := eP (aj , e ).

In particular, since qj ≥ C−1 

> ⋆

qmid for n ≥ n0,

sγ (aj ) ≥ d

2C⋆

qmid (1 − uj ) > 0.

(B) First vertex ( x = a1). 

sγ (a1) ≥ d

2 q1 (1 − u1) > 0, q1 := eP (a1, e ).

(C) Penultimate vertex ( x = an−1). 

sγ (an−1) ≥ d q n−1 (1 − un−1) ≥ 0, qn−1 := eP (an−1, e ).

(D) Second vertex ( x = a2). Let q2 := eP (a2, e ). Then 

sγ (a2) ≥ − d (d1 + d2), d1 := u1 − u2, d2 := u2 − u3,

and moreover, for all n ≥ n0,

d1 + d2 ≤ 3C⋆ qmid Ea2 [τ ], hence sγ (a2) ≥ − 3C⋆ d q mid Ea2 [τ ].

Moreover, for all n ≥ n0,

|sγ (a2)| ≤ d q 2 + d (d1 + d2) ≤ 4C⋆ d q mid Ea2 [τ ],

using q2 ≤ C⋆qmid from (34) and Ea2 [τ ] ≥ 1. By symmetry, the same bounds hold with a2 replaced by b2.Finally, under (33) one has the crude polynomial bound 

Ea2 [τ ] ≤ poly( n),

and under the stronger drift condition inf j (fj − bj ) ≥ ∆ > 0 one has 

Ea2 [τ ] ≤ C

∆ n

for a constant C depending only on the (logit-determined) depth-grouped probabilities. Proof. We use the explicit formulas of Lemma 20 and the difference recurrence of Lemma 16. 

Step 1: Comparability of one-step e-absorption probabilities. For each transient depth j, the quantity qj = eP (aj , e )

equals the total probability of the long-jump class, whose numerator is (2 n + O(1)) edγ and whose denominator equals 

(2 n + O(1)) edγ + O(1) , where the O(1) terms depend only on (α, β, γ, d ) and come from the finitely many non-γ logits and the finitely many special zero-logit targets. Consequently, there exist n0 and C⋆ ≥ 1 depending only on (α, β, γ, d )

such that for all n ≥ n0,

C−1 

> ⋆

qmid ≤ qj ≤ C⋆qmid for all transient j. (34) 

Step 2: Positive lower bound in the deep middle. Fix j with 2 < j < n − 1. Lemma 20 gives 

sγ (aj ) = d edγ 

Zmid 



(2 n − 10)(1 − uj ) + uj−1 + uj+1 − 2uj



= d edγ 

Zmid 



(2 n − 10)(1 − uj ) + dj−1 − dj



.

By the recurrence (31) at depth j,

dj = bj

fj

dj−1 + qj

fj

(1 − uj ),

34 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

hence 

dj−1 − dj = dj−1



1 − bj

fj



− qj

fj

(1 − uj ) ≥ − qj

fj

(1 − uj ),

where we used fj ≥ bj so that 1 − bj 

> fj

≥ 0. Therefore 

(2 n − 10)(1 − uj ) + dj−1 − dj ≥



(2 n − 10) − qj

fj



(1 − uj ).

Using edγ  

> Zmid

= qj 

> 2n−10

yields 

sγ (aj ) ≥ d q j (1 − uj )



1 − qj

fj (2 n − 10) 



.

For all n large enough (depending only on fixed parameters through fj and the bound qj ≤ 1), the parenthesis is at least 

1/2, giving sγ (aj ) ≥ d 

> 2

qj (1 − uj ) > 0. The refinement in terms of qmid follows from (34). 

Step 3: Positive lower bound at the first vertex. Lemma 20 gives 

sγ (a1) = d edγ 

Z1



(2 n − 6)(1 − u1) + ( u2 − u1)



= d edγ 

Z1



(2 n − 6)(1 − u1) − d1



.

At j = 1 , the recurrence (31) with b1 = 0 gives f1d1 = q1(1 − u1), hence d1 = q1 

> f1

(1 − u1). Thus 

sγ (a1) = d edγ 

Z1



(2 n − 6) − q1

f1



(1 − u1) = d q 1(1 − u1)



1 − q1

f1(2 n − 6) 



,

where we used edγ  

> Z1

= q1 

> 2n−6

. For n large enough the parenthesis is at least 1/2, yielding sγ (a1) ≥ d 

> 2

q1(1 − u1) > 0.

Step 4: Nonnegativity at the penultimate vertex. Lemma 20 gives 

sγ (an−1) = d

 (2 n − 8) edγ 

Zn−1

(1 − un−1) + edγ 

Zn−1

(un−2 − un−1) + ed(α−γ)

Zn−1

un−1



.

Each term in brackets is nonnegative ( 1 − un−1 ≥ 0, un−2 − un−1 = dn−2 ≥ 0, and un−1 ≥ 0), so dropping the last two terms yields sγ (an−1) ≥ d q n−1(1 − un−1) ≥ 0.

Step 5: Second-vertex negativity and control by qmid · E [τ ]. From Lemma 20, 

sγ (a2) = d [q2(1 − u2) − P (a2, b 3)d2 − P (a2, a 1)d1] .

Since q2(1 − u2) ≥ 0 and the probabilities are at most 1, we obtain the crude lower bound 

sγ (a2) ≥ − d(d1 + d2).

Also, using |q2(1 − u2) − P (a2, b 3)d2 − P (a2, a 1)d1| ≤ q2(1 − u2) + d1 + d2 ≤ q2 + d1 + d2, we have the crude absolute bound 

|sγ (a2)| ≤ d q 2 + d (d1 + d2).

Next, d2 = u2 − u3 ≤ u2 and d1 = u1 − u2 ≤ u1, hence d1 + d2 ≤ u1 + u2.We now use the hazard identity for absorption at e in the long-jump-absorbing chain. Let q(Xt) := eP (Xt, e ). Since the chain can enter e at most once, 

u(x) = Pr  

> x

(Xτ = e) = Ex

h τ −1X

> t=0

1{Xt+1 = e}

i

= Ex

h τ −1X

> t=0

q(Xt)

i

.

In particular, 

u(x) ≤ qmax Ex[τ ], qmax := sup 

> y / ∈T

eP (y, e ).

35 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

By the comparability (34), qmax ≤ C⋆qmid for n ≥ n0, hence 

u(x) ≤ C⋆qmid Ex[τ ] (n ≥ n0).

Applying this to x = a2 gives u2 ≤ C⋆qmid Ea2 [τ ]. For u1, observe that from a1 in one step one either absorbs or moves to depth 2, so 

Ea1 [τ ] ≤ 1 + sup 

> y∈{ a2,b 2}

Ey [τ ] = 1 + Ea2 [τ ] ≤ 2Ea2 [τ ],

using Ea2 [τ ] ≥ 1. Therefore 

u1 ≤ C⋆qmid Ea1 [τ ] ≤ 2C⋆qmid Ea2 [τ ].

Combining yields 

d1 + d2 ≤ u1 + u2 ≤ 3C⋆qmid Ea2 [τ ],

and hence 

sγ (a2) ≥ − 3C⋆d q mid Ea2 [τ ].

Finally, the stated polynomial (or linear under strict drift) upper bounds on Ea2 [τ ] follow from standard biased (or nonnegatively drifted) one-dimensional random-walk estimates applied to the depth process under (33) (and are used only as coarse growth controls). F.3.1 DERIVATIVE ESTIMATES 

In this subsubsection we derive quantitative bounds on the derivatives of the absorption probability at the long–jump state e

in the long–jump–absorbing chain (Definition 13). We leverage the source computations from the preceding subsubsections with the source representation of the derivative (Lemma 14). Throughout, hθ (v) denotes this absorption probability, i.e., 

hθ (v) := Pr v [Xτ = e].

Lemma 22 (Derivative estimate for α). In the long–jump–absorbing chain , let 

τ := inf {t ≥ 0 : Xt ∈ { an, b n, e }} , vpen := 

(

an−1, v ∈ { a1, . . . , a n−1},bn−1, v ∈ { b1, . . . , b n−1}, σ := inf {t ≥ 0 : Xt = vpen }.

Then the derivative ∂αhα(v) satisfies 

∂αhα(v) = Ev

"τ −1X

> t=0

sα(Xt)

#

≤ sα(v) + sα(vpen ) Pr  

> v

(0 < σ < τ ),

where 

sα(x) = d p f (x) hα(F (x)) − hα(x) ≤ 0,

with the convention sα(x) = 0 if the (same–chain) forward neighbor F (x) does not exist. In particular, if v is k steps from the terminal on its chain, i.e. v ∈ { an−k, b n−k} with 2 ≤ k ≤ n − 1, then 

Pr  

> v

(0 < σ < τ ) ≥ Pr 

> v



Xi = F i(v) ∀i = 1 , . . . , k − 1



,

and hence 

∂αhα(v) ≤ sα(v) + sα(vpen ) Pr 

> v



Xi = F i(v) ∀i = 1 , . . . , k − 1



.

Proof. Let u(·) := hα(·) be the absorption probability at e in the long–jump–absorbing chain, so u(x) = Pr x[Xτ = e]

with τ := inf {t ≥ 0 : Xt ∈ { an, b n, e }} . Apply the source/trajectory representation (Lemma 14) with θ = α:

∂αhα(v) = ∂αu(v) = Ev

"τ −1X

> t=0

sα(Xt)

#

,

where sα(x) := P 

> y

(∂αP (x, y )) u(y) is the local α–source. 36 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

By the explicit α–source identity in the long–jump–absorbing chain (Lemma 18), 

sα(x) = d p f (x) u(F (x)) − u(x),

with the convention sα(x) = 0 if F (x) does not exist. By monotonicity of u along depth (Lemma 16), we have u(F (x)) ≤

u(x) whenever F (x) exists, hence sα(x) ≤ 0 for all transient x. Therefore 

∂αhα(v) = Ev

"τ −1X

> t=0

sα(Xt)

#

≤ E v [sα(X0) + sα(Xσ ) 1{0 < σ < τ }] = sα(v) + sα(vpen ) Pr  

> v

(0 < σ < τ ),

where σ := inf {t ≥ 0 : Xt = vpen } and vpen is the penultimate state on the same chain as v. Indeed, on {0 < σ < τ } we have Xσ = vpen , and for all other times t ∈ { 0, . . . , τ − 1} \ { 0, σ } we have sα(Xt) ≤ 0, so discarding those terms only increases the sum. 

Lemma 23 (Derivative estimate for β). In the long–jump–absorbing chain , with τ, σ, v pen as in Lemma 22, the derivative 

∂β hβ (v) satisfies 

∂β hβ (v) = Ev

"τ −1X

> t=0

sβ (Xt)

#

≥ sβ (v) + sβ (vpen ) Pr  

> v

(0 < σ < τ ),

where 

sβ (x) = d p b(x) hβ (B(x)) − hβ (x) ≥ 0,

with the convention sβ (x) = 0 if the (same–chain) backward neighbor B(x) does not exist. In particular, if v is k steps from the terminal on its chain, i.e. v ∈ { an−k, b n−k} with 2 ≤ k ≤ n − 1, then 

Pr  

> v

(0 < σ < τ ) ≥ Pr 

> v



Xi = F i(v) ∀i = 1 , . . . , k − 1



,

and hence 

∂β hβ (v) ≥ sβ (v) + sβ (vpen ) Pr 

> v



Xi = F i(v) ∀i = 1 , . . . , k − 1



.

Proof. The argument is analogous to that of Lemma 22; we include it here for completeness. Let u(·) := hβ (·) be the absorption probability at e in the long–jump–absorbing chain, so u(x) = Pr x[Xτ = e] with 

τ := inf {t ≥ 0 : Xt ∈ { an, b n, e }} . Apply the source/trajectory representation (Lemma 14) with θ = β:

∂β hβ (v) = ∂β u(v) = Ev

"τ −1X

> t=0

sβ (Xt)

#

,

where sβ (x) := P 

> y

(∂β P (x, y )) u(y) is the local β–source. By the explicit β–source identity in the long–jump–absorbing chain (Lemma 19), 

sβ (x) = d p b(x) u(B(x)) − u(x),

with the convention sβ (x) = 0 if B(x) does not exist. By monotonicity of u along depth (Lemma 16), we have u(B(x)) ≥

u(x) whenever B(x) exists, hence sβ (x) ≥ 0 for all transient x. Therefore the random sum in the trajectory representation is almost surely nonnegative, and so for σ := inf {t ≥ 0 : Xt = vpen },

∂β hβ (v) = Ev

"τ −1X

> t=0

sβ (Xt)

#

≥ E v [sβ (X0) + sβ (Xσ ) 1{0 < σ < τ }] = sβ (v) + sβ (vpen ) Pr  

> v

(0 < σ < τ ),

since X0 = v and on {0 < σ < τ } we have Xσ = vpen .37 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Lemma 24 (Derivative estimate for γ). Assume the forward-drift condition (33) and the setting of Lemma 21. Fix a start state v0 = an−K at distance K ≥ 1 from the terminal, where K is a fixed constant independent of n. Let 

vpen := an−1, τ := inf {t ≥ 0 : Xt ∈ { an, b n, e }} , σ := inf {t ≥ 0 : Xt = vpen }.

Then for all sufficiently large n (depending on K, α, β, γ, d ), 

∂γ hγ (v0) ≥



sγ (vpen )



Pr 

> v0

(0 < σ < τ ) + 12 sγ (v0) > 0,

and 

Pr 

> v0

(0 < σ < τ ) ≥ Pr 

> v0



Xi = F i(v0) ∀i = 1 , . . . , K − 1



.

where sγ (v0) is the local γ–source at the initial state (as given explicitly in Lemma 20 and lower-bounded in Lemma 21). Proof. Let u(v) := Pr v [Xτ = e] denote the absorption probability at e in the long–jump–absorbing chain, with τ := inf {t ≥ 0 : Xt ∈ T } and T := {an, b n, e }. By the source/trajectory representation (Lemma 14), 

∂γ u(v0) = Ev0

"τ −1X

> t=0

sγ (Xt)

#

. (35) Since v0 = an−K with K ≥ 1 fixed, Lemma 21 applies (using case (C) when K = 1 , and case (A) when K ≥ 2) and gives 

sγ (v0) ≥ d

2 qn−K (1 − u(v0)) > 0,

where qn−K = eP (an−K , e ) is the one-step absorption probability. The factor 1 − u(v0) ≥ η > 0 is bounded below by a positive constant depending only on K and the fixed parameters (since the event of K consecutive forward moves has probability at least (fmin )K > 0). Let S2 := {a2, b 2}. By Lemma 21(A)–(C), for all states x / ∈ S2 with x / ∈ T , we have sγ (x) ≥ 0. Thus 

Ev0

"τ −1X

> t=0

sγ (Xt)

#

≥ sγ (v0) + Ev0 [sγ (Xσ ) 1{0 < σ < τ }] + Ev0

" X 

> t:Xt∈S2

sγ (Xt)

#

.

On the event {0 < σ < τ } we have Xσ = vpen = an−1, hence 

Ev0 [sγ (Xσ ) 1{0 < σ < τ }] = sγ (vpen ) Pr 

> v0

(0 < σ < τ ).

Moreover, since v0 = an−K , the event of K − 1 consecutive forward steps implies {0 < σ < τ }, so 

Pr 

> v0

(0 < σ < τ ) ≥ Pr 

> v0



Xi = F i(v0) ∀i = 1 , . . . , K − 1



=

> K−2

Y

> j=0

pf

 F j (v0).

Under the forward-drift condition α > β , there exists ρ ∈ (0 , 1) (depending only on α, β, γ, d ) such that pb(x)/p f (x) ≤ ρ

for all deep–middle states. By the gambler’s–ruin estimate (Lemma 62), the probability of reaching the second vertex 

S2 = {a2, b 2} from a start state at depth K before absorption satisfies 

Pr 

> v0

(H2) ≤ ρ n−K−2,

where H2 := {∃ t < τ : Xt ∈ S2}.38 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

To control the (potentially negative) contribution from visits to S2, we use the absolute bound from Lemma 21(D): for each 

x ∈ S2,

|sγ (x)| ≤ 4C⋆ d q mid Ex[τ ].

Let N2 := P 

> t<τ

1{Xt ∈ S2}. Then 

Ev0

" X 

> t:Xt∈S2

sγ (Xt)

#

≤ sup 

> x∈S2

|sγ (x)| · E v0 [N2] ≤ 4C⋆d q mid 



sup 

> x∈S2

Ex[τ ]



Ev0 [N2],

where we used Ev0 [N2] ≤ Pr v0 (H2)·sup x∈S2 Ex[τ ] and the polynomial bound on hitting times from Lemma 21. Combining with Pr v0 (H2) ≤ ρ n−K−2, we obtain 

Ev0

" X 

> t:Xt∈S2

sγ (Xt)

#

≤ C′ qmid poly( n) · ρ n−K−2,

for a constant C′ > 0 depending only on (α, β, γ, d, K ).Combining the above bounds, we have 

∂γ u(v0) ≥ sγ (v0) + sγ (vpen ) Pr 

> v0

(0 < σ < τ ) − C′ qmid poly( n) · ρ n−K−2.

Finally, note that the main term is itself proportional to qmid : by the lower bound above together with (34), 

sγ (v0) ≥ d

2 qn−K η ≥ d η 

2C⋆

qmid .

Thus both the positive contribution sγ (v0) and the worst-case negative correction from S2 carry a factor qmid , but the latter is suppressed by the exponentially small prefactor poly( n)ρ n−K−2. Since ρ < 1, for all sufficiently large n we have 

C′ qmid poly( n) · ρ n−K−2 ≤ 12 sγ (v0),

and hence 

∂γ hγ (v0) ≥ sγ (vpen ) Pr 

> v0

(0 < σ < τ ) + 12 sγ (v0) > 0.

F.4 Analysis of no-long-jump chain 

In this subsection, we analyze the no-long-jump (NL) chain defined in Definition 14, which captures the canonical chain’s behavior conditioned on never making a long jump before absorption. The analysis mirrors that of Appendix F.3: we compute the source functions for the NL chain, establish symmetry properties, and derive derivative estimates in Appendix F.5.1. 

Assumption 1 (Base model (from the main paper)) . For any input sequence s1: L0 , let pfwd , pbwd , and pswitch denote the probabilities that the model performs a forward, backward, or switch step respectively at the first step (time t = L0 + 1 ). We assume that for any s1: L0 , pfwd > c 0 and pfwd > p bwd + b0 for some constants c0 > 0 and b0 > 0, and additionally 

pswitch > d 0 for some d0 > 0.

Setup (NL chain). Throughout this section we work with the no–long–jump (NL) chain on the two–chain graph of length n,with absorbing set {an, b n}. We write pNL (v → k) for its one–step transition probabilities. Let D(·) denote depth (distance from the absorbing endpoint on its chain) and let 

τ := inf {t ≥ 0 : Xt ∈ { an, b n}} , h(v) := N L 

Pr  

> v

[Xτ = an].

For any parameter θ ∈ { α, β, γ } define the local NL source 

sθ, NL (v) := X 

> k∈N (v)

 ∂θ pNL (v → k) h(k), N (v) := {k ∈ V \ { v} : |D(k) − D(v)| ≤ 1}.

39 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

We also write psw (v) := P 

> k∈S (v)

pNL (v → k) for the single–step switch probability (where S(v) ⊆ N (v) are the cross–chain neighbors) and pmax sw := sup v / ∈{ an,b n} psw (v). Finally let 

pf (v) := Pr  D(X1) = D(v) − 1 | X0 = v,pb(v) := Pr  D(X1) = D(v) + 1 | X0 = v,δ := min 

> v / ∈{ an,b n}

 pf (v) − pb(v) > 0,

where probabilities are under the NL chain. 

Lemma 25 (Switch probability is O(1 /n ) in the NL chain) . Consider the no–long–jump (NL) chain on the two–chain graph of length n (Definition 14), and write psw (v) for the one-step switch probability in the NL chain (as in paragraph F.4 below). Under Assumption 1 and the logit structure of Appendix F.1, there exists a constant C > 0 (depending only on fixed parameters) such that for all nonterminal states v,

psw (v) ≤ Cn .

Proof. Fix a nonterminal state v, and let f be the logit of the forward probability. Because the foward probability is bounded below by a constant c0 > 0, we have that ef ≥ Θ( n)c0 · eγ . Becuase the logits of the switch targets are are all at most γ

(recall that γ > 0), and in the No Long jump chain, there are at most 3 such targets, it follows that the switch probability is bounded above 3 · eγ 

> ef

≤ 3 · eγ 

> Θ( n)c0·eγ

≤ Cn .

Lemma 26 (NL transition scaling) . Let p(v → k) denote the one-step transition probability in the original (unconditioned) chain, and let pNL (v → k) denote the one-step transition probability in the no–long–jump (NL) chain . Fix any nonterminal state v and any state k̸ = v such that the transition v → k is not a long jump (equivalently, |D(k) − D(v)| ≤ 1). Then 

pNL (v → k) ≤ 1

c0

p(v → k),

where c0 is the constant from Assumption 1. Proof. Let N (v) := {y ∈ V \ { v} : |D(y) − D(v)| ≤ 1} be the set of non–long–jump targets from v. By Definition 14, the NL kernel is obtained by conditioning on the event {X1 ∈ N (v)}, hence 

pNL (v → k) = Pr( X1 = k | X0 = v, X 1 ∈ N (v)) = p(v → k)Pr( X1 ∈ N (v) | X0 = v) .

Moreover, the forward move v → F (v) is never a long jump, so Pr( X1 ∈ N (v) | X0 = v) ≥ p(v → F (v)) . By Assumption 1, p(v → F (v)) ≥ c0, hence Pr( X1 ∈ N (v) | X0 = v) ≥ c0, which yields the claimed bound. 

Lemma 27 (Same-side absorption in NL chain) . Fix k ≥ 1. In the notation of the NL setup above (paragraph F.4), for 

v = an−k we have 

1 − h(v) ≤ pmax sw · kb0

.

Consequently, for fixed k, the same-side absorption probability tends to 1 as n tends to infinity in the NL chain. Proof. Let D(·) be the depth–distance to the end and define, for each transient state x ∈ V \ { an, b n},

pf (x) := Pr  D(X1) = D(x) − 1 | X0 = x,pb(x) := Pr  D(X1) = D(x) + 1 | X0 = x,δ := min  

> x∈V\{ an,b n}

 pf (x) − pb(x) > 0.

If Xτ = bn starting from an−k, then the trajectory must switch from chain A to chain B at least once before absorption. Let 

E be the event that at least one switch occurs before τ , and let Nsw be the number of switches before τ . Then 

1 − SN L n (an−k) = Pr( Xτ = bn) ≤ Pr( E) ≤ Pr( Nsw ≥ 1) ≤ E [Nsw ].

40 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Moreover, 

E[Nsw ] = E

h τ −1X

> t=0

1{switch at t}

i

≤ E 

h τ −1X

> t=0

psw (Xt)

i

≤ pmax sw E[τ ].

By Lemma 63 applied to the depth process (using that pf (x) − pb(x) ≥ δ for all transient x), E[τ ] ≤ k/δ . Combining yields the stated bound. The consequence now follows from lemma 25. 

Lemma 28 (Same-side value differences) . Fix k ≥ 2. In the notation of the NL setup above (paragraph F.4), for every 

j ∈ { 1, . . . , k − 1},

|h(an−j ) − h(an−(j+1) )| ≤ max 



1 − SN L n (an−j ), 1 − SN L n (an−(j+1) )



≤ pmax sw max 



EN L an−j [τ ], EN L an−(j+1) [τ ]



≤ pmax sw max 

 D(an−j ) 

> δ

, D(an−(j+1) )

> δ



= pmax sw 

j + 1 

δ ,

where D(an−j ) = j.Proof. Let uj := SN L n (an−j ) ∈ [0 , 1] . Then 

|uj − uj+1 | = |(1 − uj ) − (1 − uj+1 )| ≤ max(1 − uj , 1 − uj+1 ).

Moreover, for any start state x ∈ { a1, . . . , a n−1}, the event {Xτ = bn} implies that the trajectory switches chains at least once before absorption, hence 

1 − N L 

Pr  

> x

(Xτ = an) = N L 

Pr  

> x

(Xτ = bn) ≤ E N L x [Nsw ] ≤ pmax sw EN L x [τ ],

where the last inequality follows from EN L x [Nsw ] ≤ E N L x

 P τ −1 

> t=0

psw (Xt) ≤ pmax sw EN L x [τ ]. Applying this with x = an−j

and x = an−(j+1) and combining with the first display gives the stated bound. For the final inequality, apply Lemma 63 to the depth process (with drift gap δ) to obtain EN L an−j [τ ] ≤ D(an−j )/δ = j/δ and EN L an−(j+1) [τ ] ≤ D(an−(j+1) )/δ =(j + 1) /δ .

F.5 Source computations in the no–long–jump (NL) chain Lemma 29 (γ-source in NL chain) . In the notation of the NL setup above (paragraph F.4), consider the local γ–source 

sγ, NL (v) and the switch probability psw (v). Define osc( h) := sup x,y ∈V |h(x) − h(y)| ≤ 1. Under the logit structure of Appendix F.1 (after the global shift by −dγ ), the only γ–dependence among allowed NL moves is: 

• If v / ∈ { a2, b 2, a n−1, b n−1}, then ∂γ o(v, k ) = + d for k ∈ S (v) and ∂γ o(v, k ) = 0 for k ∈ N (v) \ S (v).

• If v ∈ { a2, b 2}, then in addition ∂γ o(v, v −) = −d for the unique same–chain backward neighbor v− ∈ N (v).

• If v ∈ { an−1, b n−1}, then in addition ∂γ o(v, v +) = −d for the unique same–chain forward neighbor v+ ∈ N (v).Then: 

(A) Generic vertices v / ∈ { a2, b 2, a n−1, b n−1}.

sγ, NL (v) = d X

> k∈S (v)

pNL (v → k) h(k) − h(v), hence |sγ, NL (v)| ≤ d p sw (v) osc( h) ≤ d p sw (v).

(B) Second vertex v ∈ { a2, b 2}.

sγ, NL (v) = d X

> k∈S (v)

pNL (v → k) h(k) − h(v) − d p NL (v → v−) h(v−) − h(v).

41 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Consequently, 

|sγ, NL (v)| ≤ d p sw (v) osc( h) + d p NL (v → v−) |h(v−) − h(v)| ≤ d p sw (v) + d p NL (v → v−) |h(v−) − h(v)|.

(C) Penultimate vertex v ∈ { an−1, b n−1}.

sγ, NL (v) = d X

> k∈S (v)

pNL (v → k) h(k) − h(v) − d p NL (v → v+) h(v+) − h(v).

Consequently, 

|sγ, NL (v)| ≤ d p sw (v) osc( h) + d p NL (v → v+) |h(v+) − h(v)| ≤ d p sw (v) + d p NL (v → v+) |h(v+) − h(v)|.

Proof. Fix v ∈ U . Abbreviate pk := pNL (v → k) and ok := o(v, k ) for k ∈ N (v). By the (restricted) softmax derivative identity (Lemma 61), 

∂γ pk = pk



∂γ ok − X 

> k′∈N (v)

pk′ ∂γ ok′



.

Therefore 

sγ, NL (v) = X 

> k∈N (v)

 ∂γ pk

 h(k)= X 

> k∈N (v)

pk h(k)



∂γ ok − X 

> k′∈N (v)

pk′ ∂γ ok′



= X 

> k∈N (v)

pk h(k) ∂γ ok −

 X 

> k∈N (v)

pk h(k)

 X 

> k′∈N (v)

pk′ ∂γ ok′



.

Since h is harmonic at transient states for the NL chain, h(v) = Ev [h(X1)] , we can rewrite this in centered form as 

sγ, NL (v) = X 

> k∈N (v)

pk

 ∂γ ok

   h(k) − h(v).

Now apply the case split on ∂γ o(v, ·) listed in the statement: • If v / ∈ { a2, b 2, a n−1, b n−1}, then ∂γ o(v, k ) = d on k ∈ S (v) and zero otherwise, giving (A). • If v ∈ { a2, b 2}, then in addition ∂γ o(v, v −) = −d, giving (B). • If v ∈ { an−1, b n−1}, then in addition ∂γ o(v, v +) = −d, giving (C). The displayed upper bounds follow from the triangle inequality together with |h(k) − h(v)| ≤ osc( h) ≤ 1.

Lemma 30 (α-source in NL chain) . In the notation of the NL setup above (paragraph F.4), let v+ be the same–chain forward neighbor of v when it exists. Let v+ denote the unique same–chain forward neighbor of v (so v+ = ai+1 if v = ai

with i ≤ n − 1, and similarly on chain B; in particular a+ 

> n−1

= an and b+ 

> n−1

= bn). Then 

sα, NL (v) = d p NL (v → v+)



h(v+) − h(v)



. (36) 

Moreover, 

|sα, NL (v)| ≤ d p NL (v → v+) osc v (h) ≤ d p NL (v → v+) ≤ d, osc v (h) := max   

> k∈N (v)

h(k) − min   

> k∈N (v)

h(k) ≤ 1. (37) 42 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. Fix v ∈ U . Abbreviate pk := pNL (v → k) and ok := o(v, k ) for k ∈ N (v). For restricted softmax, 

∂αpk = pk



∂αok − X 

> k′∈N (v)

pk′ ∂αok′



.

By the specified logits, α appears only on the forward edge k = v+ with coefficient +d, and nowhere else among allowed neighbors. Hence 

∂αok =

(

d, k = v+,

0, k ∈ N (v) \ { v+}.

Therefore, X 

> k′∈N (v)

pk′ ∂αok′ = d p NL (v → v+), X 

> k∈N (v)

pk h(k) ∂αok = d p NL (v → v+) h(v+).

Now expand 

sα, NL (v) = X 

> k∈N (v)

(∂αpk) h(k) = X 

> k∈N (v)

pkh(k)∂αok −

 X 

> k∈N (v)

pkh(k)

 X 

> k′∈N (v)

pk′ ∂αok′



,

and substitute the two identities above to obtain (36). For (37) , note that h is harmonic for the NL chain at transient v, so h(v) = Ev [h(X1)] = P  

> k∈N (v)

pk h(k) is a convex combination of {h(k) : k ∈ N (v)}. Hence |h(v+) − h(v)| ≤ osc v (h) ≤ 1. Multiply by d p NL (v → v+).

Lemma 31 (β-source in NL chain) . In the notation of the NL setup above (paragraph F.4), let v− be the same–chain backward neighbor of v when it exists. Let v− denote the unique same–chain backward neighbor of v when it exists (so 

v− = ai−1 if v = ai with i ≥ 2, and similarly on chain B). Then for all v ∈ U ,

sβ, NL (v) = 

(d p NL (v → v−)



h(v−) − h(v)



, if v− exists ,

0, if v ∈ { a1, b 1}. (38) 

Moreover, whenever v− exists, 

|sβ, NL (v)| ≤ d p NL (v → v−) osc v (h) ≤ d p NL (v → v−) ≤ d, osc v (h) ≤ 1. (39) 

Proof. Fix v ∈ U . If v ∈ { a1, b 1}, the logits do not contain β on any allowed edge, so ∂β o(v, k ) = 0 for all k ∈ N (v) and hence sβ, NL (v) = 0 .Otherwise, the specified logits place β only on the single backward edge k = v−, with coefficient +d (including the boundary case v ∈ { a2, b 2} where the back logit is d(β − γ)). Thus 

∂β ok =

(

d, k = v−,

0, k ∈ N (v) \ { v−}.

The rest of the argument is identical to the proof of Lemma 30: use the restricted softmax derivative, expand sβ, NL (v), and substitute 

X 

> k′∈N (v)

pNL (v → k′) ∂β ok′ = d p NL (v → v−), X 

> k∈N (v)

pNL (v → k) h(k) ∂β ok = d p NL (v → v−) h(v−),

to get (38). The bound (39) follows from |h(v−) − h(v)| ≤ osc v (h) ≤ 1 (and Ev [h(X1)] = h(v) by harmonicity). F.5.1 DERIVATIVE ESTIMATES 

Having computed the source functions for the NL chain in the preceding lemmas, we now derive quantitative bounds on the derivatives of the same-side absorption probability. 43 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Lemma 32 (Truncation bound for ∂β u). In the notation of the NL setup above (paragraph F.4), let 

u(x) := N L 

Pr  

> x

[Xτ = an], uβ (x) := ∂β u(x),

and fix v = an−k with k independent of n. Assume the base model assumptions of Assumption 1 (in particular, a uniform drift gap b0 > 0). Then there exist constants C, c > 0 (depending only on fixed parameters and b0) such that for all n large enough, 

|uβ (v)| ≤ C

√nb0

p(2 ⌊n1/4⌋) 

> b

pmax sw + C n exp 



− c2

8 n1/4



· sup 

> x / ∈{ an,b n}

|sβ, NL (x)|.

Proof. Fix K := ⌊n1/4⌋. By the trajectory/source representation (Lemma 14) for the NL chain, 

uβ (v) = Ev

"τ −1X

> t=0

sβ, NL (Xt)

#

.

Decompose on {τ ≤ K} and {τ > K }. On {τ ≤ K}, the sum has at most K terms and the depth can increase by at most one per step, so every visited state satisfies D(Xt) ≤ D(v) + t ≤ k + K ≤ 2K for all large n. Therefore, 

|uβ (v)| ≤ K · sup  

> x:D(x)≤2K

|sβ, NL (x)|.

By Lemma 31, |sβ, NL (x)| ≤ d p b(x) |u(B(x)) − u(x)|. By Lemma 28, |u(B(x)) − u(x)| ≤ pmax sw Ex[τ ]. Under the base model forward-bias gap b0 > 0, the depth-to-terminal process has Ex[τ ] ≤ D(x)/b 0 ≤ 2K/b 0 on the band D(x) ≤ 2K.Combining yields 

sup  

> x:D(x)≤2K

|sβ, NL (x)| ≤ C p (2 K) 

> b

pmax sw · 2Kb0

,

for a constant C > 0, hence |uβ (v)| ≲ K2 

> b0

p(2 K) 

> b

pmax sw . For {τ > K }, write Pτ −1 

> t=0

= PK−1 

> t=0

+ Pτ −1 

> t=K

. The first part is bounded identically by the same K-step band argument above. For the tail, since sup x Ex[τ ] ≤ n/b 0 under the drift gap and 

|sβ, NL (x)| ≤ sup y |sβ, NL (y)|, the contribution after time K is at most on the order of n · sup y |sβ, NL (y)|. Multiplying by 

Pr v (τ > K ) and using Lemma 64 (with t = ⌊n1/4⌋) gives 

Pr  

> v

(τ > K ) · n · sup 

> y

|sβ, NL (y)| ≤ n · exp 



− c2

8 n1/4



· sup 

> y

|sβ, NL (y)|,

The next lemma is completely analogous to Lemma 32 and is included for completeness. 

Lemma 33 (Truncation bound for ∂αu). In the notation of the NL setup above (paragraph F.4), let 

u(x) := N L 

Pr  

> x

[Xτ = an], uα(x) := ∂αu(x),

and fix v = an−k with k independent of n. Assume the base model assumptions of Assumption 1 (in particular, a uniform drift gap b0 > 0). Then there exist constants C, c > 0 (depending only on fixed parameters and b0) such that for all n large enough, 

|uα(v)| ≤ C

√nb0

p(2 ⌊n1/4⌋) 

> f

pmax sw + C n exp 



− c2

8 n1/4



· sup 

> x / ∈{ an,b n}

|sα, NL (x)|,

where p(m) 

> f

:= sup {pNL (x → x+) : x / ∈ { an, b n}, D (x) ≤ m}.Proof. Fix K := ⌊n1/4⌋. By the trajectory/source representation (Lemma 14) for the NL chain, 

uα(v) = Ev

"τ −1X

> t=0

sα, NL (Xt)

#

.

44 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Decompose on {τ ≤ K} and {τ > K }. On {τ ≤ K}, the sum has at most K terms and the depth can increase by at most one per step, so every visited state satisfies D(Xt) ≤ D(v) + t ≤ k + K ≤ 2K for all large n. Therefore, 

|uα(v)| ≤ K · sup  

> x:D(x)≤2K

|sα, NL (x)|.

By Lemma 30, |sα, NL (x)| ≤ d p NL (x → x+) |u(x+) − u(x)|. By Lemma 28 (applied to the appropriate chain), |u(x+) −

u(x)| ≤ pmax sw Ex[τ ]. Under the base model forward-bias gap b0 > 0, the depth-to-terminal process has Ex[τ ] ≤ D(x)/b 0 ≤

2K/b 0 on the band D(x) ≤ 2K. Combining yields 

sup  

> x:D(x)≤2K

|sα, NL (x)| ≤ C p (2 K) 

> f

pmax sw · 2Kb0

,

for a constant C > 0, hence |uα(v)| ≲ K2 

> b0

p(2 K) 

> f

pmax sw . For {τ > K }, write Pτ −1 

> t=0

= PK−1 

> t=0

+ Pτ −1 

> t=K

. The first part is bounded identically by the same K-step band argument above. For the tail, since sup x Ex[τ ] ≤ n/b 0 under the drift gap and 

|sα, NL (x)| ≤ sup y |sα, NL (y)|, the contribution after time K is at most on the order of n · sup y |sα, NL (y)|. Multiplying by 

Pr v (τ > K ) and using Lemma 64 (with t = ⌊n1/4⌋) gives 

Pr  

> v

(τ > K ) · n · sup 

> y

|sα, NL (y)| ≤ n · exp 



− c2

8 n1/4



· sup 

> y

|sα, NL (y)|,

as needed. 

The next lemma is completely analogous to Lemmas 32 and 33 and is included for completeness. 

Lemma 34 (Truncation bound for ∂γ u). In the notation of the NL setup above (paragraph F.4), let 

u(x) := N L 

Pr  

> x

[Xτ = an], uγ (x) := ∂γ u(x),

and fix v = an−k with k independent of n. Assume the base model assumptions of Assumption 1 (in particular, a uniform drift gap b0 > 0). Then there exist constants C, c > 0 (depending only on fixed parameters and b0) such that for all n large enough, 

|uγ (v)| ≤ C n 1/4 pmax sw + C

√nb0

p(2 ⌊n1/4⌋) 

> f

pmax sw + C n exp 



− c2

8 n1/4



· sup 

> x / ∈{ an,b n}

|sγ, NL (x)|,

where p(m) 

> f

:= sup {pNL (x → x+) : x / ∈ { an, b n}, D (x) ≤ m}.Proof. Fix K := ⌊n1/4⌋. By the trajectory/source representation (Lemma 14) for the NL chain, 

uγ (v) = Ev

"τ −1X

> t=0

sγ, NL (Xt)

#

.

Decompose on {τ ≤ K} and {τ > K }. On {τ ≤ K}, the sum has at most K terms and the depth can increase by at most one per step, so every visited state satisfies D(Xt) ≤ D(v) + t ≤ k + K ≤ 2K for all large n. Therefore, 

|uγ (v)| ≤ K · sup  

> x:D(x)≤2K

|sγ, NL (x)|.

For x with D(x) ≤ 2K, Lemma 29 gives a uniform upper bound: by part (A) we have |sγ, NL (x)| ≤ d p sw (x), while at the boundary x ∈ { an−1, b n−1} part (C) yields 

|sγ, NL (x)| ≤ d p sw (x) + d p NL (x → x+) |u(x+) − u(x)|.

By Lemma 28 (applied to the appropriate chain), |u(x+) − u(x)| ≤ pmax sw Ex[τ ]. Under the base model forward-bias gap 

b0 > 0, the depth-to-terminal process has Ex[τ ] ≤ D(x)/b 0 ≤ 2K/b 0 on the band D(x) ≤ 2K. Combining yields 

sup  

> x:D(x)≤2K

|sγ, NL (x)| ≤ C p max sw + C p (2 K) 

> f

pmax sw · 2Kb0

,

45 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

and hence |uγ (v)| ≲ K p max sw + K2 

> b0

p(2 K) 

> f

pmax sw . For {τ > K }, write Pτ −1 

> t=0

= PK−1 

> t=0

+ Pτ −1 

> t=K

. The first part is bounded identically by the same K-step band argument above. For the tail, since sup x Ex[τ ] ≤ n/b 0 under the drift gap and 

|sγ, NL (x)| ≤ sup y |sγ, NL (y)|, the contribution after time K is at most on the order of n · sup y |sγ, NL (y)|. Multiplying by 

Pr v (τ > K ) and using Lemma 64 (with t = ⌊n1/4⌋) gives 

Pr  

> v

(τ > K ) · n · sup 

> y

|sγ, NL (y)| ≤ n · exp 



− c2

8 n1/4



· sup 

> y

|sγ, NL (y)|,

as needed. 

F.6 Monotonicity of α, β, γ and derivative estimates 

We establish monotonicity and estimates for the derivativesof α, β, γ at initialization, but they in fact hold throughout the learning process as long as the forward transition probability remains non-decreasing and the long-jump probability stays bounded away from zero. Since our analysis shows that both conditions are maintained for all t ≤ t0, the estimates below in fact hold for all t ≤ t0.

Lemma 35 (Transition mass at penultimate vertex) . Assume γ > 0 and the logit structure of Appendix F.1 (after the global shift by −dγ ), and consider the original chain (equivalently, the long–jump–absorbing chain before redirection of long jumps). For a transient state x / ∈ { an, b n}, let 

pb(x) := Pr( X1 = B(x) | X0 = x) (set pb(x) := 0 if B(x) does not exist ),p(e; x) := Pr( a long jump at time 1 | X0 = x),

where B(x) is the same–chain backward neighbor when it exists. Then there exists n0 ∈ N (depending only on fixed parameters α, β, γ, d ) such that for all n ≥ n0,

max  

> x / ∈{ an,b n}

pb(x) ≤ 2 pb(an−1), max  

> x / ∈{ an,b n}

p(e; x) ≤ 2 p(e; an−1),

and by chain-swap symmetry the same bounds hold with an−1 replaced by bn−1.Proof. Fix a chain (say A); the statement for chain B follows by chain-swap symmetry. Write the one-step kernel as a softmax over logits: for each transient x and each allowed target y,

P (x, y ) = eo(x,y )

Z(x) , Z(x) := X

> z

eo(x,z ),

where the sum ranges over all original one-step targets z from x (including long-jump targets). Therefore 

pb(x) = eo(x,B (x)) 

Z(x) , p(e; x) = X

> z∈L(x)

eo(x,z )

Z(x) =

P 

> z∈L(x)

eo(x,z )

Z(x) .

Under the logit specification of Appendix F.1 (after the harmless global shift by −dγ ), the logits take only finitely many values, and both the numerator P 

> z∈L(x)

eo(x,z ) and the normalizer Z(x) can be written explicitly in each of the four boundary regimes. In particular (see the enumeration underlying Lemma 20 for the corresponding target counts and denominators): • Deep middle x = ai, 2 < i < n − 1:

Zmid = edα + edβ + 4 + (2 n − 7) edγ , pb(ai) = edβ 

Zmid 

, p(e; ai) = (2 n − 10) edγ 

Zmid 

.

• Second vertex x = a2:

Z2 = edα + ed(β−γ) + 3 + (2 n − 6) edγ , pb(a2) = ed(β−γ)

Z2

, p(e; a2) = (2 n − 8) edγ 

Z2

.

46 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

• First vertex x = a1:

Z1 = edα + 3 + (2 n − 5) edγ , pb(a1) = 0 , p(e; a1) = (2 n − 6) edγ 

Z1

.

• Penultimate vertex x = an−1:

Zn−1 = ed(α−γ) + edβ + 3 + (2 n − 6) edγ , pb(an−1) = edβ 

Zn−1

, p(e; an−1) = (2 n − 8) edγ 

Zn−1

.

Set Cmax := edα + edβ + 4 . For every transient x on chain A, the normalizer has the form 

Z(x) = Cx + cxedγ , 0 ≤ Cx ≤ Cmax , cx ∈ { 2n − 7, 2n − 6, 2n − 5},

and at the penultimate vertex Zn−1 = Cn−1 + (2 n − 6) edγ with 0 ≤ Cn−1 ≤ Cmax . Choose n0 so that (2 n0 − 6) edγ ≥

2Cmax . Then for all n ≥ n0,

Zn−1 ≤ (2 n − 6) edγ + Cmax ≤ 32 (2 n − 6) edγ , Z(x) ≥ (2 n − 7) edγ .

We first bound pb. The only vertices with a backward move are ai for i ≥ 2. Among these, the backward weight is edβ 

except at a2, where it is ed(β−γ) ≤ edβ . Hence for all x on chain A,

pb(x) ≤ edβ 

(2 n − 7) edγ .

On the other hand, 

pb(an−1) = edβ 

Zn−1

≥ edβ  

> 32

(2 n − 6) edγ .

Therefore, for all n ≥ n0,

pb(x)

pb(an−1) ≤ 

> 32

(2 n − 6) (2 n − 7) ≤ 2,

which proves max x pb(x) ≤ 2pb(an−1) on chain A.Next we bound the long-jump mass. In each of the cases above we have p(e; x) = Nxedγ /Z (x) with Nx ∈ { 2n − 6, 2n −

8, 2n − 10 }. Thus for all x on chain A and all n ≥ n0,

p(e; x) ≤ (2 n − 6) edγ 

(2 n − 7) edγ = 2n − 62n − 7 .

Meanwhile, 

p(e; an−1) = (2 n − 8) edγ 

Zn−1

≥ (2 n − 8) edγ  

> 32

(2 n − 6) edγ = 23 · 2n − 82n − 6 .

For n large enough (in particular, for n ≥ n0 after possibly increasing n0 by a fixed constant), the ratio 2n−62n−7

. 23 · 2n−82n−6



is at most 2, and therefore max x p(e; x) ≤ 2p(e; an−1) on chain A. The same argument applies on chain B.Recall the derivative decomposition from Lemma 13. For a parameter θ ∈ { α, β, γ } and a start state v, we have 

∂θ Sθ (v) = −(∂θ hθ (v)) 

 ¯SN L θ (v) − 12



+ (1 − hθ (v)) ∂θ ¯SN L θ (v),

where: • Sθ (v) := Pr θv [Xτ = an] is the same–side absorption probability in the original chain. • hθ (v) is the probability of a long jump before absorption, which by definition equals the absorption probability at SLJ 

in the long–jump–absorbing chain where long jumps are redirected to the absorbing state SLJ .47 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

• ¯SN L θ (v) := Pr θv (Xτ = an | N L ) is the same–side absorption probability conditional on no long jump occurring (i.e. in the no–long–jump chain). • By symmetry, ¯SLJ θ (v) = 12 , so the long–jump conditional absorption probability equals 1/2.We will prove monotonicity results for each of the three parameters θ ∈ { α, β, γ } by showing that the term involving the long–jump–absorbing chain, −(∂θ hθ (v)) 

 ¯SN L θ (v) − 12



, dominates the remaining term (1 − hθ (v)) ∂θ ¯SN L θ (v) in the derivative decomposition. 

Lemma 36 (Monotonicity of α). Fix a constant K ≥ 1 independent of n and initialize at v0 := an−K . For all n large enough, the long–jump term in the decomposition of Lemma 13 dominates the NL term, in the sense that 

∂αSα(v0) ≥ − 18



− (∂αhα(v0)) 



≥ 0.

Moreover, ∂αhα(v0) ≤ 0, so the right-hand side is nonnegative. In particular, ∂αSα(v0) ≥ 0, so the same–side absorption probability is (near-terminal) monotonically increasing in α.Proof. Recall the derivative decomposition (Lemma 13): 

∂αSα(v0) = −(∂αhα(v0)) 

 ¯SN L α (v0) − 12



(1 − hα(v0)) ∂α ¯SN L α (v0).

We first lower bound the long–jump contribution. By Lemma 22, together with Corollary 1 and the base model assumption 

pfwd ≥ c0 (Assumption 1), there exists a constant cLJ > 0 (depending only on fixed parameters and K) such that 

|∂αhα(v0)| ≥ cLJ · p(e), p(e) := P LJ -abs (v0, e ).

Moreover, by Lemma 27 applied in the NL chain at depth K, for n large enough we have ¯SN L α (v0) ≥ 34 , hence ¯SN L α (v0) − 

> 12

≥ 14 .We next upper bound the NL contribution. By the truncation bound in Lemma 33 (completely analogous to Lemma 32), we have 

∂α ¯SN L α (v0) ≤ C

√nb0

p(2 ⌊n1/4⌋) 

> f

pmax sw + C n exp 



− c2

8 n1/4



· sup 

> x / ∈{ an,b n}

|sα, NL (x)|.

Using Lemma 28, sup x |sα, NL (x)| ≲ pmax sw sup x EN L x [τ ] ≲ (n/b 0) pmax sw and the exponential tail exp( −Ω( n1/4)) , the last term is a negligible fraction of pmax sw . Since p(2 ⌊n1/4⌋) 

> f

≤ 1, we obtain ∂α ¯SN L α (v0) ≤ C√n p max sw for all n large enough. Finally, note that the relevant comparison is with the one-step long-jump probability at the initialization, p(e) := 

P LJ -abs (v0, e ), which depends on v0 through the local softmax denominator. Nevertheless, the same 1/n ratio holds uniformly. Indeed, from any state x, the NL switch set has only O(1) available targets (at most three), each with logit at most dγ , while the long-jump class consists of Θ( n) targets at the same γ-logit scale. One can check this separately in the deep middle (where there are 2n + O(1) long-jump targets) and at the penultimate vertex (where there are again 2n + O(1) 

long-jump targets); in both cases the ratio of the switch numerator to the long-jump numerator is O(1 /n ), and conditioning on N L only rescales by a factor bounded away from 0 (since the forward move has probability at least c0 by Assumption 1). Consequently, for all n large enough we have the domination pmax sw ≤ C p (e)/n . Therefore, 

(1 − hα(v0)) ∂α ¯SN L α (v0) ≤ 12



− (∂αhα(v0)) 

 ¯SN L α (v0) − 12

 

,

and substituting into the decomposition yields the claimed factor- 12 dominance and ∂αSα(v0) ≥ 0.

Lemma 37 (Monotonicity of β). Fix a constant K ≥ 1 independent of n and initialize at v0 := an−K . For all n large enough, the long–jump term in the decomposition of Lemma 13 dominates the NL term, in the sense that 

∂β Sβ (v0) ≤ − 18



(∂β hβ (v0)) 



≤ 0.

Moreover, ∂β hβ (v0) ≥ 0, so the right-hand side is nonpositive. In particular, ∂β Sβ (v0) ≤ 0, so the same–side absorption probability is (near-terminal) monotonically decreasing in β.

48 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. Recall the derivative decomposition (Lemma 13): 

∂β Sβ (v0) = −(∂β hβ (v0)) 

 ¯SN L β (v0) − 12



+ (1 − hβ (v0)) ∂β ¯SN L β (v0).

We first lower bound the long–jump contribution. By Lemma 23, 

∂β hβ (v0) ≥ sβ (v0) + sβ (vpen ) Pr 

> v0

(0 < σ < τ ),

where vpen := an−1 and σ := inf {t ≥ 0 : Xt = vpen }. We may drop the one of the two terms and get that regardless of whther n = 1 or n > 1,

∂β hβ (v0) ≥ sβ (vpen ) Pr 

> v0

(0 < σ < τ ).

Define the one-step long-jump probability at a state x in the long–jump–absorbing chain by 

p(e; x) := P LJ -abs (x, e ).

By the β-source identity sβ (x) = d p b(x) u(B(x)) − u(x) (Lemma 19), Corollary 1 (applied at B(vpen ) = an−1), and the base model lower bound pfwd ≥ c0 (Assumption 1), we obtain that 

sβ (vpen ) ≥ cK 

> 0

· pb(vpen ) p(e; vpen ).

Moreover, if v0 = an−K then the event of K − 1 consecutive forward steps implies {0 < σ < τ }, hence 

Pr 

> v0

(0 < σ < τ ) ≥ cK−10 .

Combining, 

∂β hβ (v0) ≥ c2K−10 · pb(vpen ) p(e; vpen ).

Moreover, by Lemma 27 applied in the NL chain at depth K, for n large enough we have ¯SN L β (v0) ≥ 34 , hence ¯SN L β (v0) − 

> 12

≥ 14 .We next upper bound the NL contribution. By Lemma 32, 

∂β ¯SN L β (v0) ≤ C

√nb0

p(2 ⌊n1/4⌋) 

> b

pmax sw + C n exp 



− c2

8 n1/4



· sup 

> x / ∈{ an,b n}

|sβ, NL (x)|.

We now control the NL source term. By the NL β–source identity (Lemma 31) and the consecutive–value bound (Lemma 28), for every nonterminal x with backward neighbor B(x),

|sβ, NL (x)| ≤ d p NL (x → B(x)) |h(B(x)) − h(x)| ≤ d p NL (x → B(x)) pmax sw EN L x [τ ].

Therefore, 

sup 

> x

|sβ, NL (x)| ≤ d



sup 

> x

pNL (x → B(x)) 



pmax sw 



sup 

> x

EN L x [τ ]



≤ dn b0



sup 

> x

pNL (x → B(x)) 



pmax sw ,

using sup x EN L x [τ ] ≤ n/b 0 under the drift gap. Next we bound both sup x pNL (x → B(x)) and pmax sw in terms of penultimate-vertex probabilities in the original / LJ-absorbing chain. By Lemma 26, NL transition probabilities are inflated by at most 1/c 0 relative to the original chain, so in particular 

sup 

> x

pNL (x → B(x)) ≤ 1

c0

max  

> x

pb(x), pmax sw ≤ 1

c0

max  

> x

porig sw (x).

Moreover, in the original chain the switch class consists of O(1) γ-logit targets while the long-jump class has Θ( n) γ-logit targets, hence porig sw (x) ≤ C p (e; x)/n for all x. Combining with Lemma 35 gives, for all n large enough, 

sup 

> x

pNL (x → B(x)) ≤ C p b(vpen ), pmax sw ≤ C p(e; vpen )

n .

49 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Also, p(2 ⌊n1/4⌋) 

> b

≤ sup x pNL (x → B(x)) ≤ C p b(vpen ). Substituting these bounds into the two-term upper bound from Lemma 32 yields 

∂β ¯SN L β (v0) ≤ C

√nb0

pb(vpen ) · p(e; vpen )

n ≤ C′

b0

√n pb(vpen ) p(e; vpen )

for all n large enough. Therefore, for n large enough we have 

(1 − hβ (v0)) ∂β ¯SN L β (v0) ≤ 12



(∂β hβ (v0)) 

 ¯SN L β (v0) − 12

 

,

and substituting into the decomposition yields ∂β Sβ (v0) ≤ 12

  − (∂β hβ (v0))( ¯SN L β (v0) − 12 ) ≤ 0.

Lemma 38 (Monotonicity of γ). Fix a constant K ≥ 1 independent of n and initialize at v0 := an−K . For all n large enough, the long–jump term in the decomposition of Lemma 13 dominates the NL term, in the sense that 

∂γ Sγ (v0) ≤ − 18



(∂γ hγ (v0)) 



≤ 0.

Moreover, ∂γ hγ (v0) ≥ 0, so the right-hand side is nonpositive. In particular, ∂γ Sγ (v0) ≤ 0, so the same–side absorption probability is (near-terminal) monotonically decreasing in γ.Proof. We analyze the derivative decomposition (Lemma 13): 

∂γ Sγ (v0) = −(∂γ hγ (v0)) 

 ¯SN L γ (v0) − 12



+ (1 − hγ (v0)) ∂γ ¯SN L γ (v0).

We first lower bound the long–jump contribution in the same way as in the β case. Let vpen := an−1 and σ := inf {t ≥ 0 : 

Xt = vpen } in the long–jump–absorbing chain. By Lemma 24, 

∂γ hγ (v0) ≥ sγ (vpen ) Pr 

> v0

(0 < σ < τ ) + 12 sγ (v0) ≥ sγ (vpen ) Pr 

> v0

(0 < σ < τ ),

since sγ (v0) ≥ 0. Moreover, if v0 = an−K then the event of K − 1 consecutive forward steps implies {0 < σ < τ }, hence 

Pr 

> v0

(0 < σ < τ ) ≥ cK−10

by the base model lower bound pfwd ≥ c0 (Assumption 1). Define the one-step long-jump probability at a state x in the long–jump–absorbing chain by p(e; x) := P LJ -abs (x, e ). At the penultimate vertex, Lemma 21(C) gives 

sγ (vpen ) ≥ d p (e; vpen )  1 − u(vpen ) ≥ c0 · p(e; vpen ),

where u(x) = Pr x[Xτ = e] and the last inequality uses 1. Combining these and considering cases where n = 1 or n > 1

yields ∂γ hγ (v0) ≥ 0.5 · · p(e; vpen ) cK 

> 0

. Moreover, by Lemma 27 applied in the NL chain at depth K, for n large enough we have ¯SN L γ (v0) ≥ 34 , hence ¯SN L γ (v0) − 12 ≥ 14 .We next upper bound the NL contribution. By Lemma 34, 

∂γ ¯SN L γ (v0) ≤ C n 1/4 pmax sw + C

√nb0

p(2 ⌊n1/4⌋) 

> f

pmax sw + C n exp 



− c2

8 n1/4



· sup 

> x / ∈{ an,b n}

|sγ, NL (x)|.

We now control the three terms on the right-hand side using the same comparison-to-penultimate-vertex strategy as in the β

case. First, by Lemma 26, NL probabilities are inflated by at most 1/c 0 relative to the original chain. In the original chain, the switch class consists of O(1) γ-logit targets while the long-jump class has Θ( n) γ-logit targets, so porig sw (x) ≤ C p (e; x)/n 

for all x. Combining with Lemma 35 gives, for all n large enough, 

pmax sw ≤ 1

c0

max  

> x

porig sw (x) ≤ C max x p(e; x)

n ≤ C p(e; vpen )

n .

50 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Next, we bound the NL γ-source. By the NL γ–source bounds (Lemma 29), each sγ, NL (x) is a linear combination of psw (x)

and (at the boundary) one additional term involving a same–chain neighbor difference |h(x±) − h(x)|. By Lemma 28 and sup x EN L x [τ ] ≤ n/b 0 under the drift gap, these neighbor differences are at most pmax sw sup x EN L x [τ ] ≤ (n/b 0) pmax sw .Therefore, 

sup 

> x

|sγ, NL (x)| ≤ C nb0

pmax sw .

Substituting the bound on pmax sw into the first two terms (using p(2 ⌊n1/4⌋) 

> f

≤ 1), and the source bound into the exponentially-suppressed third term, yields 

∂γ ¯SN L γ (v0) ≤ C n1/4

n p(e; vpen ) + C

√nb0

· 1

n p(e; vpen ) + C n exp 



− c2

8 n1/4



· nb0

· 1

n p(e; vpen ).

In particular, for all n large enough, 

∂γ ¯SN L γ (v0) ≤ C′

b0

√n p(e; vpen ).

Therefore, for n large enough we have 

(1 − hγ (v0)) ∂γ ¯SN L γ (v0) ≤ 12



(∂γ hγ (v0)) 

 ¯SN L γ (v0) − 12

 

,

and substituting into the decomposition yields ∂γ Sγ (v0) ≤ 12

  − (∂γ hγ (v0))( ¯SN L γ (v0) − 12 ) ≤ 0.

Lemma 39 (Near–terminal same–side derivative for γ). Fix a constant K ≥ 1 independent of n, and initialize at 

v0 := an−K .

Write p(e) for the one–step probability of a long jump out of v0 in the original chain (equivalently, p(e) = P LJ -abs (v0, e ) in the long–jump–absorbing chain). Write p(e; x) := P LJ -abs (x, e ) and p(e)max := sup x / ∈{ an,b n} p(e; x). Then 

∂γ Sγ (v0) ≤ C1,γ 

√nb0

p(e)max .

Proof (upper bound). Write the derivative decomposition (Lemma 13) at v0:

∂γ Sγ (v0) = −(∂γ hγ (v0)) 

 ¯SN L γ (v0) − 12



+ (1 − hγ (v0)) ∂γ ¯SN L γ (v0).

In the proof of Lemma 38 (the domination step), for all n large enough we showed that 

(1 − hγ (v0)) ∂γ ¯SN L γ (v0) ≤ 12 (∂γ hγ (v0)) · ¯SN L γ (v0) − 12 ≤ 12 ∂γ hγ (v0) .

Therefore, 

∂γ Sγ (v0) ≤ 32 ∂γ hγ (v0) . (40) It remains to bound |∂γ hγ (v0)| in the long–jump–absorbing chain. Let u(x) := Pr x[Xτ = e] denote the absorption probability at e (so hγ (·) = u(·)), with τ := inf {t ≥ 0 : Xt ∈ { an, b n, e }} . By the trajectory/source representation (Lemma 14), 

∂γ hγ (v0) = ∂γ u(v0) = Ev0

"τ −1X

> t=0

sγ (Xt)

#

,

where sγ is the local γ–source in the long–jump–absorbing chain. We use the standard band decomposition with M := 

⌊n1/4⌋.On {τ ≤ M }, all visited states satisfy D(Xt) ≤ D(v0) + t ≤ 2M for n large enough (since D(v0) = K is fixed), hence  

> τ−1

X

> t=0

sγ (Xt) 1{τ ≤ M } ≤ M · sup  

> x:D(x)≤2M

|sγ (x)|.

51 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

To bound the band supremum, use the explicit γ–source formulas in the long–jump–absorbing chain (Lemma 20). In each of the finitely many cases (deep middle / first / second / penultimate), sγ (x) is a finite linear combination of terms of the form 

eP (x, y ) · wx(y),

where either y = e (a long jump in the original chain, redirected to e) so eP (x, y ) = p(e; x) ≤ p(e)max , or else y is one of the O(1) non–long–jump γ–dependent neighbors and then eP (x, y ) ≤ 1 and the coefficient multiplies a value term wx(y)

which is a difference of absorption probabilities (or a centered version thereof). On the band D(·) ≤ 2M , Lemma 17 together with the drift-gap bound E[τ ] ≲ D/b 0 gives 

u(y) = Pr  

> y

[Xτ = e] ≲ Mb0

p(e)max ,

and hence every such value term wx(y) appearing in the case formulas is O (M/b 0) p(e)max 

. Since each case has only 

O(1) such terms, we conclude that 

sup 

> D(x)≤2M

|sγ (x)| ≤ C Mb0

p(e)max 

for a constant C. Therefore the contribution on {τ ≤ M } is at most C M 2 

> b0

p(e)max .On {τ > M }, use the crude bound P 

> t<τ

sγ (Xt) ≤ τ sup x |sγ (x)| together with the tail bound from Lemma 64 (with 

t = M ). The same reasoning as above, with D(·) ≤ n instead of 2M , gives the global bound 

sup 

> x

|sγ (x)| ≤ C nb0

p(e)max .

Multiplying by Pr v0 (τ > M ) = exp( −Ω( M )) (with M = ⌊n1/4⌋) shows this tail contribution is negligible. Thus, 

|∂γ hγ (v0)| ≤ C M 2

b0

p(e)max = C

√nb0

p(e)max .

Combining with (40) yields the claimed upper bound. 

F.7 Hard example contributions to derivative are negligible 

Recall the derivative decomposition from Lemma 13. For a parameter θ ∈ { α, β, γ } and a start state v, we have 

∂θ Sθ (v) = −(∂θ hθ (v)) 

 ¯SN L θ (v) − 12



+ (1 − hθ (v)) ∂θ ¯SN L θ (v),

where: • Sθ (v) := Pr θv [Xτ = an] is the same–side absorption probability in the original chain. • hθ (v) is the probability of a long jump before absorption, which by definition equals the absorption probability at SLJ 

in the long–jump–absorbing chain where long jumps are redirected to the absorbing state SLJ .• ¯SN L θ (v) := Pr θv (Xτ = an | N L ) is the same–side absorption probability conditional on no long jump occurring (i.e. in the no–long–jump chain). • By symmetry, ¯SLJ θ (v) = 12 , so the long–jump conditional absorption probability equals 1/2.We will now argue using this decomposition that the addition of hard examples preserves the properties of the derivatives argued above. Specifically, we will show that the monotonicity and sign properties established for the derivatives ∂θ Sθ (v)

continue to hold when hard examples are included in the training distribution. 52 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

F.7.1 UPPER BOUND ON DERIVATIVE FOR HARD EXAMPLES 

We will now bound the derivatives of the same-side absorption probability for hard examples. 

Lemma 40. Let S be finite and let T ⊆ S be absorbing. Fix A ⊆ T and a starting state v ∈ S. For θ ∈ R, let Pθ be a time-homogeneous Markov kernel on S with absorbing set T . Let 

τ := inf {t ≥ 0 : Xt ∈ T }, R := 1{Xτ ∈ A}, uθ (v) := Eθv [R].

For x / ∈ T and Pθ (x, y ) > 0, define the one-step score 

ϕθ (x, y ) := ∂θ log Pθ (x, y ),

and the stopped cumulative score 

L :=  

> τ−1

X

> t=0

ϕθ (Xt, X t+1 ).

Assume that all differentiations and exchanges of derivative and expectation below are valid, and that Eθv [L2] < ∞. Then 

uθ (v) is differentiable and 

∂θ uθ (v) = Eθv [R L ] = Eθv

(R − uθ (v)) L = θ

Cov  

> v

(R, L ).

Moreover, by Cauchy–Schwarz, 

∂θ uθ (v) ≤

q

uθ (v) 1 − uθ (v) q

Eθv [L2].

Proof. By definition uθ (v) = Eθv [R] where R = 1{Xτ ∈ A} depends on the trajectory only through the absorbed terminal state. Using the log-derivative trick (score-function identity), 

∂θ uθ (v) = ∂θ Eθv [R] = Eθv

R ∂ θ log θ

Pr  

> v

(X0, . . . , X τ ).

Since the chain is time-homogeneous and T is absorbing, the (stopped) path likelihood factors as 

> θ

Pr  

> v

(X0, . . . , X τ ) =  

> τ−1

Y

> t=0

Pθ (Xt, X t+1 ).

Therefore 

∂θ log θ

Pr  

> v

(X0, . . . , X τ ) =  

> τ−1

X

> t=0

∂θ log Pθ (Xt, X t+1 ) =  

> τ−1

X

> t=0

ϕθ (Xt, X t+1 ) = L. 

Plugging this back gives the REINFORCE form 

∂θ uθ (v) = Eθv [R L ].

Next we show that L has mean zero (baseline fact). Condition on Xt = x (with x / ∈ T ). Then 

Eθv [ϕθ (Xt, X t+1 ) | Xt = x] = X

> y∈S

Pθ (x, y ) ∂θ log Pθ (x, y ) = X

> y∈S

∂θ Pθ (x, y ) = ∂θ

 X 

> y∈S

Pθ (x, y )



= 0 .

Summing over t = 0 , . . . , τ − 1 and using linearity of expectation yields 

Eθv [L] = Eθv

"τ −1X

> t=0

ϕθ (Xt, X t+1 )

#

= 0 .

Hence, for the constant baseline b := uθ (v) = Eθv [R],

Eθv [( R − b)L] = Eθv [R L ] − b Eθv [L] = Eθv [R L ].

53 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Therefore 

∂θ uθ (v) = Eθv [R L ] = Eθv [( R − uθ (v)) L].

Since Eθv [L] = 0 , this is exactly the covariance: 

Eθv [( R − uθ (v)) L] = Eθv [R L ] − E θv [R]Eθv [L] = θ

Cov  

> v

(R, L ).

Finally, apply Cauchy–Schwarz: 

∂θ uθ (v) = Eθv [( R − uθ (v)) L] ≤

q

Eθv [( R − uθ (v)) 2]

q

Eθv [L2].

Because R is Bernoulli with mean uθ (v),

Eθv [( R − uθ (v)) 2] = θ

Var  

> v

(R) = uθ (v) 1 − uθ (v).

Substituting yields 

∂θ uθ (v) ≤

q

uθ (v) 1 − uθ (v) q

Eθv [L2].

We have the following decomposition for Eθ [L2]:

Lemma 41. Let S be finite and let T ⊆ S be absorbing. Fix θ ∈ R and a starting state v ∈ S. Let (Xt)t≥0 be the time-homogeneous Markov chain with kernel Pθ and absorption time 

τ := inf {t ≥ 0 : Xt ∈ T }.

For x / ∈ T and Pθ (x, y ) > 0 define the one-step score 

ϕθ (x, y ) := ∂θ log Pθ (x, y ),

and define the stopped cumulative score 

L :=  

> τ−1

X

> t=0

ϕθ (Xt, X t+1 ).

Define the one-step Fisher-type quantity 

Iθ (x) := X

> y∈S

Pθ (x, y ) ϕθ (x, y )2 (x ∈ S \ T ),

and set Iθ (x) := 0 for x ∈ T . Assume Eθv [L2] < ∞ and Eθv [L] = 0 . Then 

> θ

Var  

> v

(L) = Eθv [L2] = Eθv

"τ −1X

> t=0

Iθ (Xt)

#

.

In particular, if I⋆θ := max x∈S\T Iθ (x), then 

> θ

Var  

> v

(L) ≤ I⋆θ Eθv [τ ].

Proof. Let Ft := σ(X0, . . . , X t) and define the stopped increments 

Dt := ϕθ (Xt, X t+1 ) 1{t < τ }, t ≥ 0,

so that L = P 

> t≥0

Dt.54 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

For each t ≥ 0 we have Eθv [Dt | F t] = 0 . Indeed, on the event {t ≥ τ } we have Dt = 0 . On the event {t < τ } we have 

Xt /∈ T , and conditioning on Xt = x gives 

Eθv [Dt | F t] = Eθv [ϕθ (Xt, X t+1 ) | Xt = x] = X

> y∈S

Pθ (x, y ) ∂θ log Pθ (x, y ) = X

> y∈S

∂θ Pθ (x, y ) = 0 .

Hence (Dt)t≥0 is a martingale-difference sequence. Expanding L2 gives 

L2 =

X

> t≥0

Dt



> 2

= X

> t≥0

D2 

> t

+ 2 X

> 0≤s<t

DsDt.

We claim Eθv [DsDt] = 0 for all 0 ≤ s < t . Since Ds is Ft-measurable when s < t , and using the tower property, 

Eθv [DsDt] = Eθv

Eθv [DsDt | F t] = Eθv

Ds Eθv [Dt | F t] = 0 ,

by the martingale-difference property. Therefore 

Eθv [L2] = X

> t≥0

Eθv [D2 

> t

].

For each t,

Eθv [D2 

> t

] = Eθv

Eθv [D2 

> t

| F t] = Eθv

1{t < τ } E θv

ϕθ (Xt, X t+1 )2 | Xt

 .

On {t < τ }, Xt /∈ T , and conditioning on Xt = x yields 

Eθv

ϕθ (Xt, X t+1 )2 | Xt = x = X

> y∈S

Pθ (x, y ) ϕθ (x, y )2 = Iθ (x).

Hence Eθv [D2 

> t

] = Eθv [1{t < τ }Iθ (Xt)] , and summing over t gives 

Eθv [L2] = X

> t≥0

Eθv [1{t < τ }Iθ (Xt)] = Eθv

"τ −1X

> t=0

Iθ (Xt)

#

.

Since Eθv [L] = 0 , we have Var θv (L) = Eθv [L2], proving the main identity. Finally, Iθ (Xt) ≤ I⋆θ for t < τ , so  

> τ−1

X

> t=0

Iθ (Xt) ≤ I⋆θ τ, 

and taking expectations yields Var θv (L) ≤ I⋆θ Eθv [τ ].We now bound these single step terms for the LJ absorbing chain: 

Lemma 42. Work in the long-jump absorbing (LJ) chain with the softmax logit structure. For each θ ∈ { α, β, γ } and each nonterminal state x / ∈ T , let 

Iθ (x) := X

> a∈A (x)

Pα,β,γ (x, a )



∂θ log Pα,β,γ (x, a )

2

denote the one-step Fisher-type term. Then 

Iθ (x) ≤ c2 for all θ ∈ { α, β, γ } and all x / ∈ T. 

Consequently, I⋆θ := max x / ∈T Iθ (x) ≤ c2 for each θ ∈ { α, β, γ }.

55 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. Fix θ ∈ { α, β, γ } and a nonterminal state x / ∈ T . Write the allowed moves out of x as A(x) and write the outgoing probabilities in softmax form 

pa(θ) := Pα,β,γ (x, a ) = eℓa(θ)

Z(θ) , Z(θ) := X

> b∈A (x)

eℓb(θ).

In the LJ logit structure, after the usual normalization for γ (subtracting a common cγ from all logits, which does not change the softmax probabilities), the dependence on θ is affine with coefficient bounded by 1:

ℓa(θ) = ℓ(0)  

> a

+ c k a θ with |ka| ≤ 1.

Define ¯k(θ) := P 

> b∈A (x)

pb(θ) kb. Differentiating log pa(θ) = ℓa(θ) − log Z(θ) gives 

∂θ log pa(θ) = ∂θ ℓa(θ) − Z′(θ)

Z(θ) = c k a − c ¯k(θ) = c (ka − ¯k(θ)) ,

where we used 

Z′(θ)

Z(θ) =

P 

> b

eℓb(θ)∂θ ℓb(θ)

P 

> b

eℓb(θ) = c X

> b

eℓb(θ)

Z(θ) kb = c ¯k(θ).

Therefore 

Iθ (x) = X

> a∈A (x)

pa(θ) ∂θ log pa(θ)2 = c2 X

> a∈A (x)

pa(θ) ( ka − ¯k(θ)) 2 = c2 Var 

> a∼p(θ)

(ka).

Finally, Var( k) ≤ E [k2] and |ka| ≤ 1 imply 

Var 

> a∼p(θ)

(ka) ≤ E a∼p(θ)[k2

> a

] ≤ 1,

hence Iθ (x) ≤ c2, as claimed. Taking the maximum over x / ∈ T yields I⋆θ ≤ c2.

Lemma 43 (Fisher bound for LJ chain derivative) . Work in the long–jump–absorbing (LJ) chain (Definition 13) with absorbing set T = {an, b n, e }. Fix θ ∈ { α, β, γ } and define the long-jump absorption probability 

uθ (v) := θ

Pr  

> v

(Xτ = e), τ := inf {t ≥ 0 : Xt ∈ T }.

Assume that for all starting states v / ∈ T ,

Eθv [τ ] ≤ nb0

,

for some constant b0 > 0 independent of n. Then, with the constant c from Lemma 42, for every v / ∈ T ,

∂θ uθ (v) ≤ c

q

uθ (v) 1 − uθ (v) ·

r nb0

Proof. Apply Lemma 40 with A = {e}. This gives 

|∂θ uθ (v)| ≤ 

q

uθ (v) 1 − uθ (v) q

Eθv [L2].

By Lemma 41, 

Eθv [L2] = Eθv

"τ −1X

> t=0

Iθ (Xt)

#

≤ I⋆θ Eθv [τ ].

Finally, Lemma 42 gives I⋆θ ≤ c2, and by assumption Eθv [τ ] ≤ n/b 0. Combining yields 

|∂θ uθ (v)| ≤ 

q

uθ (v) 1 − uθ (v) r

c2 nb0

= c

q

uθ (v) 1 − uθ (v) ·

r nb0

.

56 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Lemma 44 (Exponential decay of derivatives for hard examples) . Fix a constant a ∈ (0 , 1) and consider the original two–chain model of length n, together with its long–jump–absorbing chain (Definition 13). Assume there exists a constant 

qmin ∈ (0 , 1) , independent of n, such that the one-step long-jump probability satisfies 

p(e; x) := P LJ -abs (x, e ) ≥ qmin for all nonterminal x / ∈ { an, b n}.

Let v be any start state with depth D(v) ≥ an . Then for each parameter θ ∈ { α, β, γ } there exist constants C, c > 0

(depending only on fixed parameters, a, and qmin , but not on n) such that 

∂αSα(v) ≤ C nb0

pmax  

> f

e−cn , ∂β Sβ (v) ≤ C nb0

pmax  

> b

e−cn , ∂γ Sγ (v) ≤ C nb0

p(e)max e−cn ,

where b0 > 0 is the drift-gap constant from Assumption 1, 

pmax  

> f

:= sup 

> x / ∈{ an,b n}

Pr( X1 = F (x) | X0 = x),pmax  

> b

:= sup 

> x / ∈{ an,b n}

Pr( X1 = B(x) | X0 = x),p(e)max := sup 

> x / ∈{ an,b n}

p(e; x),

and F (x), B (x) denote the same–chain forward/backward neighbors when they exist (with the convention that the corre-sponding probability is 0 if the neighbor does not exist). Proof. Let τ := inf {t ≥ 0 : Xt ∈ { an, b n}} be the absorption time in the original chain and let N L := {J > τ } be the event of no long jump before absorption (with J as in Lemma 13). In the long–jump–absorbing chain (Definition 13) with absorption time 

τLJ := inf {t ≥ 0 : Xt ∈ { an, b n, e }} ,

one has N L = {XτLJ ∈ { an, b n}} and LJ = {XτLJ = e}.We use the long–jump / no–long–jump derivative decomposition (Lemma 13): for each θ ∈ { α, β, γ },

∂θ Sθ (v) = −(∂θ hθ (v)) 

 ¯SN L θ (v) − 12



+ (1 − hθ (v)) ∂θ ¯SN L θ (v),

where hθ (v) = Pr θv (LJ ) and 1 − hθ (v) = Pr θv (N L ).Since D(v) ≥ an , on N L the depth can decrease by at most 1 per step, so τ ≥ an . Under the assumption p(e; x) ≥ qmin 

for every nonterminal x, this gives 

> θ

Pr  

> v

(N L ) ≤ θ

Pr  

> v

(no hit of e in the first an steps ) ≤ (1 − qmin )an ≤ e−cn 

for some c > 0 depending only on a, q min .Using | ¯SN L θ (v) − 12 | ≤ 12 , we have 

∂θ Sθ (v) ≤ 12 ∂θ hθ (v) + θ

Pr  

> v

(N L ) · ∂θ ¯SN L θ (v) .

Now ∂θ hθ (v) = ∂θ Pr θv (LJ ) is the derivative of the long-jump absorption probability in the long–jump–absorbing chain (with absorbing set T = {an, b n, e } and target A = {e}). Therefore, applying Lemma 43 yields 

|∂θ hθ (v)| ≤ c

q

hθ (v) 1 − hθ (v) ·

r nb0

= c

r θ

Pr  

> v

(LJ ) θ

Pr  

> v

(N L ) ·

r nb0

≤ C

r nb0

e−c′n,

where we used Pr θv (N L ) ≤ e−cn and absorbed constants by possibly decreasing c.It remains to control |∂θ ¯SN L θ (v)| by the relevant one-step probability scale. By the source identities in the NL chain (Lemmas 30, 31, and the corresponding γ-source bound Lemma 29), the NL sources satisfy |sα,N L (x)| ≤ C p f (x),57 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

|sβ,N L (x)| ≤ C p b(x), and |sγ,N L (x)| ≤ C p sw (x) ≤ C p (e)max . Therefore, by the trajectory/source representation (Lemma 14) and the drift-gap absorption time bound EN L v [τ ] ≤ n/b 0, we have 

|∂α ¯SN L α (v)| ≤ C nb0

pmax  

> f

, |∂β ¯SN L β (v)| ≤ C nb0

pmax  

> b

, |∂γ ¯SN L γ (v)| ≤ C nb0

p(e)max .

Multiplying by Pr θv (N L ) ≤ e−cn and combining with the bound on |∂θ hθ (v)| yields the stated result. 

F.8 Easy+Hard examples derivative estimates 

We now apply the results of the previous subsections to derive our three main derivative bounds that will be used under the training-distribution assumptions of Theorem 2. 

Lemma 45 (Easy+hard expected derivative bound for γ (Theorem 2)) . Assume the training distribution Q satisfies the assumptions of Theorem 2, and that there exists a constant qmin ∈ (0 , 1) , independent of n, such that P LJ -abs (x, e ) ≥ qmin 

for all nonterminal x. Let vi := ai (equivalently, by symmetry one may take vi := bi). Define 

p(e)max := max   

> xnonterminal

P LJ -abs (x, e ).

Then for all n large enough, 

Ei∼Q 

∂γ Sγ (vi) ≤ 0,

and there exist constants C1,γ , C 2,γ > 0 (independent of n and c) such that 

c · C2,γ p(e)max ≤ Ei∼Q 

∂γ Sγ (vi) ≤ c · C1,γ 

√n p (e)max .

Proof. Let vi := ai. Define the index sets 

E := {n − k, . . . , n − 1} (easy) , H := {1, . . . , ⌊(1 − a)n⌋} (hard) .

Since Q is (k, c )-easy we have Q(E) ≥ c, and since Q is (an, 1 − c)-hard we have can decompose the expected derivative as 

Ei∼Q [∂γ Sγ (vi)] = Q(E) · E i∼Q [∂γ Sγ (vi) | i ∈ E ] + Q(H) · E i∼Q [∂γ Sγ (vi) | i ∈ H ]. (41) For each i ∈ E , the depth K := n − i satisfies 1 ≤ K ≤ k. By Lemma 38, ∂γ Sγ (vi) ≤ 0 for all n large enough, and moreover 

C2,γ p(e)max ≤ | ∂γ Sγ (vi)| ≤ C1,γ 

√n p (e)max ,

where the constants C1,γ , C 2,γ > 0 depend only on k (not on n or c). Therefore, the easy-example conditional expectation satisfies 

C2,γ p(e)max ≤ Ei∼Q [∂γ Sγ (vi) | i ∈ E ] ≤ C1,γ 

√n p (e)max . (42) For each i ∈ H , the depth satisfies D(vi) = n − i ≥ an . By Lemma 44, for all n large enough, 

Q(H) · E i∼Q [∂γ Sγ (vi) | i ∈ H ] ≤ C nb0

p(e)max e−cn . (43) Combining (42) and (43) via the triangle inequality, and redefining the constants C1,γ , C 2,γ , we obtain the stated bounds. 

Lemma 46 (Easy+hard expected derivative lower bound for β (Theorem 2)) . Assume the training distribution Q satisfies the assumptions of Theorem 2, and that there exists a constant qmin ∈ (0 , 1) , independent of n, such that P LJ -abs (x, e ) ≥ qmin 

for all nonterminal x. Let vi := ai (equivalently, by symmetry one may take vi := bi). Define 

p(e)max := max   

> xnonterminal

P LJ -abs (x, e ), p(b)max := max   

> xnonterminal

pb(x),

where pb(x) denotes the one–step same–chain backward probability from x in the original chain. Then for all n large enough, 

Ei∼Q 

∂β Sβ (vi) ≤ 0,

and there exists a constant C2,β > 0 (independent of n) such that 

Ei∼Q 

∂β Sβ (vi) ≥ C2,β p(e)max p(b)max .

58 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. As in Lemma 45, we decompose 

Ei∼Q [∂β Sβ (vi)] = Q(E) · E [∂β Sβ (vi) | i ∈ E ] + Q(H) · E [∂β Sβ (vi) | i ∈ H ].

For easy examples, Lemmas 37 and 23 give ∂β Sβ (vi) ≤ 0 and |∂β Sβ (vi)| ≥ C2,β p(e)max p(b)max . For hard examples, Lemma 44 gives |∂β Sβ (vi)| ≤ C nb0 pmax  

> b

e−cn . Since this is exponentially small, combining via the triangle inequality and redefining C2,β yields the stated bounds. 

Lemma 47 (Easy+hard expected derivative lower bound for α (Theorem 2)) . Assume the training distribution Q satisfies the assumptions of Theorem 2, and that there exists a constant qmin ∈ (0 , 1) , independent of n, such that P LJ -abs (x, e ) ≥ qmin 

for all nonterminal x. Let vi := ai (equivalently, by symmetry one may take vi := bi). Define 

p(e)max := max   

> xnonterminal

P LJ -abs (x, e ), p(f )max := max   

> xnonterminal

pf (x),

where pf (x) denotes the one–step same–chain forward probability from x in the original chain. Then for all n large enough, 

Ei∼Q 

∂αSα(vi) ≥ 0,

and there exists a constant C2,α > 0 (independent of n) such that 

Ei∼Q 

∂αSα(vi) ≥ C2,α p(e)max p(f )max .

Proof. As in Lemma 45, we decompose 

Ei∼Q [∂αSα(vi)] = Q(E) · E [∂αSα(vi) | i ∈ E ] + Q(H) · E [∂αSα(vi) | i ∈ H ].

For easy examples, Lemmas 36 and 22 give ∂αSα(vi) ≥ 0 and |∂αSα(vi)| ≥ C2,α p(e)max p(f )max . For hard examples, Lemma 44 gives |∂αSα(vi)| ≤ C nb0 pmax  

> f

e−cn . Since this is exponentially small, combining via the triangle inequality and redefining C2,α yields the stated bounds. 

F.9 Convergence rate 

Main proof We now present the proof of Theorem 2, bringing together the components developed in the previous subsections. 

Proof of Theorem 2. By Lemma 60, the dynamics of the attention parameters are governed by the gradients of the reduced loss S := ˜L(α, β, γ ). Since the loss S is an expectation over the starting vertex location v (sampled from Q), we write 

S = Ev∼Q[Sv (α, β, γ )] , where Sv is the loss term for a specific v. Substituting this into the dynamics from Lemma 60 and taking absolute values, we have with |V | = 2 n:

| ˙α(t)| = 12n(2 n − 1) |E v [∂αSv ]| ,

| ˙β(t)| = 12n(2 n − 1) |E v [∂β Sv ]| ,

| ˙γ(t)| = 1(2 n − 2) · 2n(2 n − 1) |E v [∂γ Sv ]| .

We apply Lemmas 45 to 47 to bound the derivatives. These lemmas require the assumption that p(e)min (t) ≥ qmin for some constant qmin > 0 independent of n. Under this assumption, they imply Ev∼Q[∂αSv ] ≥ 0, Ev∼Q[∂β Sv ] ≤ 0, and 

Ev∼Q[∂γ Sv ] ≤ 0. Since Lemma 60 gives ˙θ = −cθ ∂θ S for positive constants cθ , we have ˙α(t) ≥ 0, ˙β(t) ≤ 0, and ˙γ(t) ≤ 0.We will from now assume that p(e)min (t) ≥ qmin while analyzing the dynamics of the parameters α, β, γ until some time 

t0 (to be determined later). Later, we will show that for all t ≤ t0 indeed p(e)min (t) ≥ qmin .We define the extremal probabilities at time t:

p(e)min (t) := min   

> xnonterminal

P LJ -abs (x, e ; t), p(f )max (t) := max   

> xnonterminal

pf (x; t),

where the dependence on t comes through the parameters α(t), β (t), γ (t).From Lemmas 45 and 47, the expected derivatives satisfy: 59 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

• Ev∼Q[∂αSv ] ≥ C2,α p(e)min (t) p(f )max (t),• c · C2,γ p(e)min (t) ≤ Ev∼Q[∂γ Sv ] ≤ c · C1,γ 

√n p (e)min (t).Next, we derive a lower bound on | ˙α(t)|. From the dynamics and the lower bound on the expected α-derivative: 

| ˙α(t)| ≥ C2,α 

2n(2 n − 1) · p(e)min (t) p(f )max (t).

Under the assumptions of Section 6.1, we have d = 1 in the logit structure. By the softmax structure of the attention logits (Lemma 67), there exist constants D > 0 and C > 0 (independent of n) such that 

p(f )max (0) = eα(0) −D

eα(0) −D + C .

Moreover, since ˙β, ˙γ ≤ 0, we have that 

p(f )max (t) > eα(t)−D

eα(t)−D + C =: g(t).

By assumption (Section 6.1), at initialization we have p(f )max (0) ≥ c0.Define 

qmin := 18 · min 



1, ε c 0

(1 − ε)(1 − c0)



· d0,

where d0 > 0 is the lower bound on the initial switch probability from Section 6.1. Under the assumption that p(e)min (t) ≥ qmin , we have 

| ˙α(t)| ≥ C2,α qmin 

2n(2 n − 1) · p(f )max (t).

Since p(f )max (t) ≥ g(t), we may apply Lemma 66 to conclude that there exists a time 

t0 < 16 n(2 n − 1) 

C2,α d0

· max 



1, (1 − ε)(1 − c0)

ε c 0



· log 

 1 − εε · 1 − c0

c0



such that p(f )max (t0) ≥ 1 − ε and 

α(t0) − α(0) < log 

 1 − εε · 1 − c0

c0



. (44) All that’s left to show is that indeed for any t ≤ t0, p(e)min (t) ≥ qmin for some constant qmin > 0 independent of n.Assume the contrary, then there exists a minimal time 0 ≤ t1 < t 0 such that p(e)min (t1) < q min . First, we now show that for any constant 1 > c 1 > 0 independent of n, to be determined later, for sufficiently large n, for any t ∈ [0 , t 1],

| ˙γ(t)| < c 1| ˙α(t)|.Using the bounds from the lemmas: 

| ˙α(t)| ≥ C2,α 

2n(2 n − 1) · p(e)min (t) p(f )max (t),

and 

| ˙γ(t)| ≤ c · C1,γ 

√n

(2 n − 2) · 2n(2 n − 1) · p(e)min (t).

Taking the ratio and using that the p(e)min (t) terms cancel: 

| ˙γ(t)|| ˙α(t)| ≤ c · C1,γ 

C2,α (2 n − 2) ·√np(f )max (t) .

60 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

By assumption (Section 6.1), the model is initialized such that p(f )max (0) ≥ c0, together with the logit structure (Lemma 67) and the fact that ˙α ≥ 0 and ˙β, ˙γ ≤ 0 implies that p(f )max (t) ≥ c0 for all t if n is sufficiently large. Consequently, for sufficiently large n, we have | ˙γ(t)| < c 1| ˙α(t)|.This inequality allows us to apply Lemma 65 with x(t) = α(t), y (t) = γ(t) and c = 1 /c 1. Note that ˙α ≥ 0, ˙γ ≤ 0, and 

˙α ≥ − ˙γ/c 1. This yields 

α(t) − α(0) ≥ 1

c1

(γ(0) − γ(t)) .

Rearranging and exponentiating gives 

eγ(t) ≥ eγ(0)+ c1(α(0) −α(t)) . (45) By Equation (44), for all t ≤ t1 we have that 

α(t) − α(0) < log 

 1 − εε · 1 − c0

c0



,

which is a constant independent of n. Therefore, we can choose c1 > 0 sufficiently small (independent of n) such that for all t ≤ t0,

eγ(t) ≥ eγ(0)+ c1(α(0) −α(t)) ≥ eγ(0) −c1 log    

> 1−εε·1−c0
> c0
> 

≥ 12 eγ(0) .

By the logit structure (Lemma 67), and the fact that α(t) > β (t), γ (t), 0 at init and throughout the run (by signs of the derivatives), we have that for any t ≤ t1:

p(e)min (t) ≥ 12 · ne γ(t)

2ne γ(t) + eβ(t) + eα(t)

≥ 14 · ne γ(0) 

2ne γ(0) + eβ(0) + eα(0)+( α(t)−α(0)) 

By Equation (44), we have α(t) − α(0) < log 

 1−εε · 1−c0

> c0



, thus: 

p(e)min (t) ≥ 14 · min 



1, ε c 0

(1 − ε)(1 − c0)



· ne γ(0) 

2ne γ(0) + eβ(0) + eα(0) 

≥ 18 · min 



1, ε c 0

(1 − ε)(1 − c0)



· p(e)min (0) 

≥ 18 · min 



1, ε c 0

(1 − ε)(1 − c0)



· d0 = qmin .

Thus p(e)min (t) ≥ qmin for all t ≤ t1, which contradicts the assumption that p(e)min (t1) < q min , as required. 

## G Proof of Theorem 3 

The first lemma records a data processing (pushforward contraction) property of total variation on finite spaces. The next lemma is a general decomposition of the total variation distance between two measures on a product space into a sum of the total variation distance between the marginal measures and the total variation distance between the conditional distributions. 

Lemma 48 (Data processing for total variation on finite spaces) . Let Ω, S be finite sets. Let P, Q be probability measures on Ω, and let f : Ω → S be any map. Denote the pushforward (image) measures by 

(P ◦ f −1)( B) := P  f −1(B), (Q ◦ f −1)( B) := Q f −1(B), B ⊆ S .

Then 

∥P ◦ f −1 − Q ◦ f −1∥TV ≤ ∥ P − Q∥TV .

61 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. By definition of total variation on a finite space, 

∥P ◦ f −1 − Q ◦ f −1∥TV = sup 

> B⊆S

(P ◦ f −1)( B) − (Q ◦ f −1)( B) = sup 

> B⊆S

P (f −1(B)) − Q(f −1(B)) .

Since f −1(B) ⊆ Ω for every B ⊆ S , we have 

sup 

> B⊆S

P (f −1(B)) − Q(f −1(B)) ≤ sup 

> A⊆Ω

P (A) − Q(A) = ∥P − Q∥TV ,

which proves the claim. 

Lemma 49. Let X , Y be finite sets. Let Π, Π′ be probability measures on X × Y with the same first marginal μ on X , i.e. 

Π( x, Y) = Π ′(x, Y) = μ(x) ∀ x ∈ X .

For each x with μ(x) > 0, let 

Kx(·) := Π( · | X = x), Lx(·) := Π ′(· | X = x)

denote the conditional distributions on Y. (For x with μ(x) = 0 , define Kx = Lx arbitrarily.) Then 

∥Π − Π′∥TV ≤ X

> x∈X

μ(x) ∥Kx − Lx∥TV .

Proof. For any event E ⊆ X × Y , write the x-section 

Ex := {y ∈ Y : ( x, y ) ∈ E} ⊆ Y .

We have that 

Π( E) = X

> x∈X

Π( {x} × Ex) = X

> x∈X

Π( X = x) Π( Ex | X = x) = X

> x∈X

μ(x) Kx(Ex),

and similarly 

Π′(E) = X

> x∈X

μ(x) Lx(Ex),

since the first marginal is the same μ for both measures. Therefore 

Π( E) − Π′(E) = X

> x∈X

μ(x)  Kx(Ex) − Lx(Ex).

Taking absolute values and using | P ax| ≤ P |ax| gives 

|Π( E) − Π′(E)| ≤ X

> x∈X

μ(x) Kx(Ex) − Lx(Ex) .

For each fixed x, the set Ex ⊆ Y is an admissible event in the definition of total variation on Y, hence 

Kx(Ex) − Lx(Ex) ≤ sup 

> B⊆Y

Kx(B) − Lx(B) = ∥Kx − Lx∥TV .

Substituting yields 

|Π( E) − Π′(E)| ≤ X

> x∈X

μ(x) ∥Kx − Lx∥TV .

Finally, take a supremum over all events E ⊆ X × Y to obtain 

∥Π − Π′∥TV = sup 

> E⊆X ×Y

|Π( E) − Π′(E)| ≤ X

> x∈X

μ(x) ∥Kx − Lx∥TV ,

as claimed. 62 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Lemma 50 (TV bound for trajectories) . Let X be a finite state space and let P, Q be Markov kernels on X . Fix an initial distribution μ on X . For T ≥ 1, let P(T ) 

> μ,P

and P(T ) 

> μ,Q

denote the laws on X T +1 of the length-(T + 1) trajectories 

(X0, . . . , X T ) with X0 ∼ μ and one-step transitions given by P and Q, respectively. Then 

P(T ) 

> μ,P

− P(T )  

> μ,Q TV

≤ 

> T−1

X

> t=0

sup 

> x∈X

P (x, ·) − Q(x, ·) TV = T · sup 

> x∈X

P (x, ·) − Q(x, ·) TV .

Proof. For t = 0 , 1, . . . , T , let R(t) be the law on X T +1 of the trajectory (X0, . . . , X T ) obtained by starting from X0 ∼ μ,using P for the first t transitions, and using Q for the remaining transitions. Thus R(0) = P(T ) 

> μ,Q

and R(T ) = P(T ) 

> μ,P

. By the triangle inequality, 

P(T ) 

> μ,P

− P(T )  

> μ,Q TV

≤ 

> T−1

X

> t=0

∥R(t+1) − R(t)∥TV .

Fix t ∈ { 0, . . . , T − 1}. Define Y := ( Xt+1 , . . . , X T ) ∈ X T −t. Under both R(t) and R(t+1) , the first t transitions are governed by P , hence the marginal law of Xt is the same. Therefore Lemma 49 applies to the pair (Xt, Y ) and yields 

∥R(t+1) − R(t)∥TV ≤ X

> x∈X

Pr 

> R(t)

(Xt = x) R(t+1) (· | Xt = x) − R(t)(· | Xt = x) TV 

.

Define a Markov kernel M from X to X T −t as follows: for each y ∈ X , M (y, ·) is the trajectory distribution on X T −t of length (T − t) for the Q-chain (Z0, . . . , Z T −t−1) started from Z0 = y. Then, conditional on Xt = x, under R(t) we have 

Xt+1 ∼ Q(x, ·) and Y has distribution Q(x, ·)M , whereas under R(t+1) we have Xt+1 ∼ P (x, ·) and Y has distribution 

P (x, ·)M . Hence 

R(t+1) (· | Xt = x) − R(t)(· | Xt = x) TV 

= ∥P (x, ·)M − Q(x, ·)M ∥TV .

Consider the map f : X T −t → X given by f (z0, . . . , z T −t−1) = z0. By construction, (P (x, ·)M ) ◦ f −1 = P (x, ·) and 

(Q(x, ·)M ) ◦ f −1 = Q(x, ·). Applying Lemma 48 with Ω = X T −t, S = X , and this f gives 

∥P (x, ·) − Q(x, ·)∥TV = ∥(P (x, ·)M ) ◦ f −1 − (Q(x, ·)M ) ◦ f −1∥TV ≤ ∥ P (x, ·)M − Q(x, ·)M ∥TV .

Equivalently, 

∥P (x, ·)M − Q(x, ·)M ∥TV ≤ ∥ P (x, ·) − Q(x, ·)∥TV .

Substituting into the bound on ∥R(t+1) − R(t)∥TV yields 

∥R(t+1) − R(t)∥TV ≤ X

> x∈X

Pr 

> R(t)

(Xt = x) ∥P (x, ·) − Q(x, ·)∥TV ≤ sup 

> x∈X

∥P (x, ·) − Q(x, ·)∥TV .

Summing over t = 0 , . . . , T − 1 completes the proof: 

P(T ) 

> μ,P

− P(T )  

> μ,Q TV

≤ T · sup 

> x∈X

∥P (x, ·) − Q(x, ·)∥TV .

Lemma 51. Let X be finite. Let P, Q be Markov kernels on X and let T ⊆ X be absorbing for both P and Q. Let 

τ := inf {t ≥ 0 : Xt ∈ T }. For an integer M ≥ 0, define the truncated stopping time τ ∧M and consider the corresponding stopped paths X0: τ and X0: τ ∧M . Let LP (X0: τ ) and LQ(X0: τ ) denote the laws of X0: τ under P and Q (respectively), for the same initial law μ. Then 

LP (X0: τ ) − L Q(X0: τ ) TV ≤ M · sup 

> x∈X

∥P (x, ·) − Q(x, ·)∥TV + Pr  

> P

(τ > M ) + Pr  

> Q

(τ > M ).

63 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. Let P(M ) 

> μ,P

and P(M ) 

> μ,Q

be the laws of (X0, . . . , X M ) under kernels P and Q, respectively. The truncated stopped path X0: τ ∧M is a deterministic function of (X0, . . . , X M ), hence by the data processing inequality for total variation (Lemma 48), 

LP (X0: τ ∧M ) − L Q(X0: τ ∧M ) TV ≤ P(M ) 

> μ,P

− P(M ) 

> μ,Q TV

.

By Lemma 50, 

P(M ) 

> μ,P

− P(M )  

> μ,Q TV

≤ M · sup 

> x∈X

∥P (x, ·) − Q(x, ·)∥TV ,

and therefore 

LP (X0: τ ∧M ) − L Q(X0: τ ∧M ) TV ≤ M · sup 

> x∈X

∥P (x, ·) − Q(x, ·)∥TV .

It remains to compare the true stopped path X0: τ with the truncated one X0: τ ∧M . On the event {τ ≤ M } we have the identity X0: τ = X0: τ ∧M . Hence for any event A in the stopped-path sigma-field, 

Pr  

> P

(X0: τ ∈ A) ≤ Pr  

> P

(X0: τ ∧M ∈ A) + Pr  

> P

(τ > M ), (46) and similarly 

Pr  

> Q

(X0: τ ∈ A) ≤ Pr  

> Q

(X0: τ ∧M ∈ A) + Pr  

> Q

(τ > M ). (47) Applying (46) to the complement Ac and using Pr( ·) = 1 − Pr( ·c) yields the corresponding reverse inequality: 

Pr  

> P

(X0: τ ∈ A) ≥ Pr  

> P

(X0: τ ∧M ∈ A) − Pr  

> P

(τ > M ),

and likewise 

Pr  

> Q

(X0: τ ∈ A) ≥ Pr  

> Q

(X0: τ ∧M ∈ A) − Pr  

> Q

(τ > M ).

Combining upper and lower bounds, we obtain for every A,

Pr  

> P

(X0: τ ∈ A) − Pr  

> Q

(X0: τ ∈ A) ≤ Pr  

> P

(X0: τ ∧M ∈ A) − Pr  

> Q

(X0: τ ∧M ∈ A) + Pr  

> P

(τ > M ) + Pr  

> Q

(τ > M ).

Taking a supremum over A gives 

LP (X0: τ ) − L Q(X0: τ ) TV ≤ LP (X0: τ ∧M ) − L Q(X0: τ ∧M ) TV + Pr  

> P

(τ > M ) + Pr  

> Q

(τ > M ).

Substituting the earlier bound on the truncated stopped-path TV completes the proof. 

Lemma 52 (Softmax Lipschitz bound) . Let Y be finite. For logits o, o ′ ∈ RY , define 

σ(o)( y) := eo(y)

P 

> z∈Y

eo(z) .

Then 

∥σ(o) − σ(o′)∥TV ≤ ∥ o − o′∥∞.

Proof. Let ot := (1 − t)o + to ′ and pt := σ(ot). Then 

σ(o′) − σ(o) = 

Z 10

ddt pt dt. 

By the softmax derivative identity (Lemma 61) with parameter t, each coordinate satisfies 

ddt pt(y) = pt(y) (o′(y) − o(y)) − X

> z

pt(z)( o′(z) − o(z)) ≤ 2 pt(y) ∥o′ − o∥∞.

Summing over y gives 

ddt pt 1

≤ 2∥o′ − o∥∞,

hence 

∥σ(o′) − σ(o)∥TV = 12 ∥σ(o′) − σ(o)∥1 ≤ 12

Z 10

ddt pt 1

dt ≤ ∥ o′ − o∥∞.

64 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Lemma 53 (Kernel TV bound) . Let Pα,β,γ and Pα′,β ′,γ ′ be the original-chain kernels induced by the logit structure with parameters (α, β, γ ) and (α′, β ′, γ ′) (and fixed d > 0) (see Appendix F.1). Then 

sup 

> x

Pα,β,γ (x, ·) − Pα′,β ′,γ ′ (x, ·) TV ≤ d |α − α′| + |β − β′| + |γ − γ′|.

Let ePα,β,γ and ePα′,β ′,γ ′ be the corresponding long–jump–absorbing kernels (see Appendix F.2.2) obtained by aggregating all long–jump targets into a single state e. Then 

sup 

> x

ePα,β,γ (x, ·) − ePα′,β ′,γ ′ (x, ·) TV ≤ d |α − α′| + |β − β′| + |γ − γ′|.

Proof. Fix x. In the original chain, Pα,β,γ (x, ·) is a softmax over logits oα,β,γ (x, ·). By inspection of the logit structure (including boundary cases), each coordinate of o(x, ·) changes by at most d(|α − α′| + |β − β′| + |γ − γ′|) when the parameters are changed, hence 

∥oα,β,γ (x, ·) − oα′,β ′,γ ′ (x, ·)∥∞ ≤ d |α − α′| + |β − β′| + |γ − γ′|.

Lemma 52 implies the stated bound for the original kernels after taking a supremum over x.

Corollary 2 (Trajectory TV bound in the long–jump–absorbing chain from parameter changes) . Let ePα,β,γ and ePα′,β ′,γ ′ be the long–jump–absorbing kernels under parameters (α, β, γ ) and (α′, β ′, γ ′), with the same initial distribution μ. Then for every T ≥ 1,

P(T ) 

> μ, ePα,β,γ

− P(T )   

> μ, ePα′,β ′,γ ′TV

≤ T d  |α − α′| + |β − β′| + |γ − γ′|.

If T ⊆ X is absorbing (e.g. T = {an, b n, e }) and τ is the corresponding hitting time, then for every M ≥ 0,

L ePα,β,γ (X0: τ ) − L ePα′,β ′,γ ′ (X0: τ ) TV ≤ M d  |α − α′| + |β − β′| + |γ − γ′| + Pr 

> α,β,γ

(τ > M ) + Pr 

> α′,β ′,γ ′

(τ > M ).

Proof. Combine Lemma 50 with Lemma 53 for the fixed-horizon bound, and combine Lemma 51 with Lemma 53 for the stopped-path bound. 

Lemma 54 (Long-jump lower bound stability) . Let θ(t) = ( α(t), β (t), γ (t)) be a differentiable parameter path and let ePθ

denote the corresponding long–jump–absorbing kernel (See Appendix F.2.2). Define 

pmin (t) := inf 

> x∈X

ePθ(t)(x, {e}).

Assume: 1. (Initial lower bound) pmin (0) ≥ p0 for some p0 > 0.2. (Conditional speed bound) There exists G(L) ≥ 0 such that whenever pmin (s) ≥ p0/2 for all s ∈ [0 , t ], one has 

∥ ˙θ(s)∥1 ≤ G(L) ∀ s ∈ [0 , t ], ∥ ˙θ(s)∥1 := | ˙α(s)| + | ˙β(s)| + | ˙γ(s)|.

3. (Parameter-to-kernel TV) For all θ, θ ′,

sup 

> x∈X

ePθ (x, ·) − ePθ′ (x, ·) TV ≤ d |α − α′| + |β − β′| + |γ − γ′|.

Then for every t with 

t ≤ p0

2d G (L) ,

we have 

pmin (s) ≥ p0

2 ∀ s ∈ [0 , t ].

65 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. Let 

t⋆ := inf {t ≥ 0 : pmin (t) < p 0/2},

with the convention t⋆ = ∞ if the set is empty. By definition, for all s < t ⋆ we have pmin (s) ≥ p0/2, hence by assumption (2), 

∥θ(s) − θ(0) ∥1 ≤

Z s

> 0

∥ ˙θ(u)∥1 du ≤ s G (L) (s < t ⋆).

By assumption (3), for s < t ⋆,

sup 

> x∈X

ePθ(s)(x, ·) − ePθ(0) (x, ·) TV ≤ d ∥θ(s) − θ(0) ∥1 ≤ d s G (L).

Since | ePθ(s)(x, {e}) − ePθ(0) (x, {e})| ≤ ∥ ePθ(s)(x, ·) − ePθ(0) (x, ·)∥TV by definition of total variation, taking inf x on the left and sup x on the right gives, for s < t ⋆,

pmin (s) ≥ pmin (0) − d s G (L) ≥ p0 − d s G (L).

In particular, if s ≤ p0/(2 dG (L)) , then pmin (s) ≥ p0/2. This implies t⋆ > p 0/(2 dG (L)) , and therefore for every 

t ≤ p0/(2 dG (L)) we have t < t ⋆, i.e. pmin (s) ≥ p0/2 for all s ∈ [0 , t ].

Lemma 55 (Geometric absorption tail) . Let eP be a Markov kernel on X such that e ∈ X is absorbing for eP . Let 

τe := inf {t ≥ 0 : Xt = e}. If inf x∈X eP (x, {e}) ≥ p for some p ∈ (0 , 1] , then for every integer m ≥ 0,

Pr( τe > m ) ≤ (1 − p)m, and hence E[τe] ≤ 1

p .

Proof. For each t ≥ 0, on the event {τe > t } we have Xt̸ = e and therefore 

Pr( τe > t + 1 | τe > t ) = Pr( Xt+1 ̸ = e | τe > t ) ≤ 1 − p. 

Thus Pr( τe > t + 1) ≤ (1 − p) Pr( τe > t ), and iterating gives Pr( τe > m ) ≤ (1 − p)m. The expectation bound follows from E[τe] = P 

> m≥0

Pr( τe > m ).

Lemma 56 (TV stability under parameter drift) . Let θ0 = ( α0, β 0, γ 0) and θ1 = ( α1, β 1, γ 1) be two parameter values and let ePθ0 , ePθ1 be the corresponding long–jump–absorbing kernels Appendix F.2.2. Let T ⊆ X be absorbing for both kernels and let τ := inf {t ≥ 0 : Xt ∈ T }. Then for every integer M ≥ 0,

L ePθ0

(X0: τ ) − L ePθ1

(X0: τ ) TV ≤ M d  |α0 − α1| + |β0 − β1| + |γ0 − γ1| + Pr 

> θ0

(τ > M ) + Pr 

> θ1

(τ > M ).

Proof. Apply Lemma 51 with P = ePθ0 and Q = ePθ1 , and then use Lemma 53 to bound sup x ∥P (x, ·) − Q(x, ·)∥TV by 

d(|α0 − α1| + |β0 − β1| + |γ0 − γ1|).

Corollary 3. In the setting of Lemma 54, assume additionally that G(L) ≤ e−cL for some c > 0. Fix any t satisfying 

t ≤ p0/(2 dG (L)) and set θt := θ(t). Then pmin (s) ≥ p0/2 for all s ∈ [0 , t ], and therefore under both ePθ(0) and ePθt ,

Pr( τe > M ) ≤ (1 − p0/2) M ∀ M ≥ 0,

where τe := inf {t ≥ 0 : Xt = e}. Moreover, for any absorbing set T containing e and the corresponding τ ,

L ePθ(0) (X0: τ ) − L ePθt

(X0: τ ) TV ≤ M d ∥θ(t) − θ(0) ∥1 + 2(1 − p0/2) M ∀ M ≥ 0.

In particular, if ∥ ˙θ(s)∥1 ≤ G(L) holds on [0 , t ], then ∥θ(t) − θ(0) ∥1 ≤ tG (L) and hence 

L ePθ(0) (X0: τ ) − L ePθt

(X0: τ ) TV ≤ M d t G (L) + 2(1 − p0/2) M ∀ M ≥ 0.

Proof. The claim pmin (s) ≥ p0/2 on [0 , t ] is Lemma 54. Given this, Lemma 55 with p = p0/2 yields the tail bound for 

τe under both kernels. If T is absorbing and contains e, then {τ > M } ⊆ { τe > M }, so Pr( τ > M ) ≤ Pr( τe > M ) ≤

(1 − p0/2) M under both kernels. The stopped-path TV bound is then Lemma 56 combined with this tail estimate. Finally, if 

∥ ˙θ(s)∥1 ≤ G(L) on [0 , t ], then ∥θ(t) − θ(0) ∥1 ≤ R t 

> 0

G(L) ds = tG (L).66 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Lemma 57 (Gradient bound for hard starts) . Fix a finite index set of start states H ⊆ X and nonnegative weights (wv )v∈H 

with P 

> v∈H

wv = 1 . Let 

L(θ) := X

> v∈H

wv S(v; θ),

where S(v; θ) is the same-side absorption probability Appendix F.1 from v under parameters θ = ( α, β, γ ). Assume S(v; θ)

is differentiable in each coordinate of θ. Then for each ϑ ∈ { α, β, γ },

∂ϑL(θ) ≤ sup 

> v∈H

∂ϑS(v; θ) , and hence ∥∇L (θ)∥1 ≤ X

> ϑ∈{ α,β,γ }

sup 

> v∈H

∂ϑS(v; θ) .

Proof. Fix ϑ ∈ { α, β, γ }. By linearity of differentiation and the triangle inequality, 

∂ϑL(θ) = X

> v∈H

wv ∂ϑS(v; θ) ≤ X

> v∈H

wv ∂ϑS(v; θ) ≤

 X 

> v∈H

wv



· sup 

> v∈H

∂ϑS(v; θ) = sup 

> v∈H

∂ϑS(v; θ) .

Summing over ϑ gives the ℓ1 gradient bound. 

Lemma 58. Let ePθ be the long–jump–absorbing kernel (See Appendix F.2.2) at parameters θ = ( α, β, γ ), and define 

pmin (θ) := min 

> x / ∈{ an,b n}

ePθ (x, e ).

Then for all θ, θ ′,

pmin (θ) ≥ pmin (θ′) − d ∥θ − θ′∥1.

Proof. Fix x / ∈ { an, b n}. By definition of total variation, 

ePθ (x, e ) − ePθ′ (x, e ) ≤ ePθ (x, ·) − ePθ′ (x, ·) TV .

Taking a supremum over x and using Lemma 53 (for eP ) yields 

sup 

> x / ∈{ an,b n}

ePθ (x, e ) − ePθ′ (x, e ) ≤ sup 

> x

ePθ (x, ·) − ePθ′ (x, ·) TV ≤ d ∥θ − θ′∥1.

Therefore, 

pmin (θ) = min 

> x

ePθ (x, e ) ≥ min 

> x

ePθ′ (x, e ) − sup 

> x

ePθ (x, e ) − ePθ′ (x, e ) ≥ pmin (θ′) − d ∥θ − θ′∥1.

Lemma 59. Fix constants a ∈ (0 , 1) and q0 ∈ (0 , 1) . Let H ⊆ X be a set of start states such that D(v) ≥ aL for all 

v ∈ H , and let L(θ) = P 

> v∈H

wv S(v; θ) be as in Lemma 57. Consider the parameter region 

Uq0 := 

n

θ : pmin (θ) ≥ q0

o

.

Then there exist constants C, c > 0 (depending only on fixed model parameters, a, and q0, but not on L) such that for every 

θ ∈ U q0 ,

∥∇L (θ)∥1 ≤ C Lb0

e−cL .

Proof. Fix θ ∈ U q0 . By definition, pmin (θ) ≥ q0, i.e. for every nonterminal x / ∈ { an, b n} we have ePθ (x, e ) ≥ q0. Thus the assumption of Lemma 44 holds with qmin = q0. Applying Lemma 44 to each v ∈ H and using the trivial bounds pmax  

> f

≤ 1,

pmax  

> b

≤ 1, and p(e)max ≤ 1 gives, for each ϑ ∈ { α, β, γ },

sup 

> v∈H

∂ϑS(v; θ) ≤ C Lb0

e−cL ,

after possibly enlarging C and decreasing c (absorbing constants across parameters). Lemma 57 then yields the stated bound on ∥∇L (θ)∥1.67 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Theorem 5 (Exponential-time stability of the stopped-path law under hard-example gradient flow) . Fix a ∈ (0 , 1) and let H ⊆ X be a set of start states with D(v) ≥ aL for all v ∈ H . Let L(θ) = P 

> v∈H

wv S(v; θ) with P 

> v

wv = 1 , and consider the gradient flow ˙θ(t) = −∇L (θ(t)) , θ(0) = θ0.

Let q0 := pmin (θ0) > 0. Then there exist constants C, c > 0 (independent of n) such that the following hold. For every 

0 ≤ t ≤ tboot (L) := q0

4d · b0

CL ecL ,

we have 

pmin (θ(s)) ≥ q0

2 ∀ s ∈ [0 , t ].

Let T = {an, b n, e } and τ := inf {t ≥ 0 : Xt ∈ T } be the absorption time in the long–jump–absorbing chain. Then for every t ∈ [0 , t boot (L)] and every integer M ≥ 0,

Lθ(t)(X0: τ ) − L θ(0) (X0: τ ) TV ≤ M d t ·



C Lb0

e−cL 

+ 2 (1 − q0/2) M .

Proof. Define U := {θ : pmin (θ) ≥ q0/2} and the exit time 

t⋆ := inf 

n

t ≥ 0 : θ(t) /∈ U 

o

,

with the convention t⋆ = ∞ if the set is empty. For s < t ⋆ we have θ(s) ∈ U , so Lemma 59 (applied with q0/2 in place of 

q0) gives 

∥ ˙θ(s)∥1 = ∥∇L (θ(s)) ∥1 ≤ C Lb0

e−cL (s < t ⋆).

Integrating yields, for s < t ⋆,

∥θ(s) − θ(0) ∥1 ≤

Z s

> 0

∥ ˙θ(u)∥1 du ≤ s ·



C Lb0

e−cL 

.

Lemma 58 implies, for s < t ⋆,

pmin (θ(s)) ≥ pmin (θ(0)) − d ∥θ(s) − θ(0) ∥1 ≥ q0 − d s ·



C Lb0

e−cL 

.

If 

s ≤ q0

2d · b0

CL ecL ,

then pmin (θ(s)) ≥ q0/2, contradicting the definition of t⋆ if t⋆ were at most this value. Hence t⋆ exceeds that value. Therefore, for every 

0 ≤ t ≤ tboot (L) := 12 · q0

2d · b0

CL ecL = q0

4d · b0

CL ecL ,

we have t < t ⋆, i.e. pmin (θ(s)) ≥ q0/2 for all s ∈ [0 , t ].Fix such a t and apply Lemma 51 with P = ePθ(0) and Q = ePθ(t) and absorbing set T = {an, b n, e }:

Lθ(t)(X0: τ ) − L θ(0) (X0: τ ) TV ≤ M · sup 

> x

ePθ(t)(x, ·) − ePθ(0) (x, ·) TV + Pr 

> θ(t)

(τ > M ) + Pr 

> θ(0)

(τ > M ).

Lemma 53 yields 

sup 

> x

ePθ(t)(x, ·) − ePθ(0) (x, ·) TV ≤ d ∥θ(t) − θ(0) ∥1.

Moreover, since e ∈ T , {τ > M } ⊆ { τe > M } where τe := inf {t ≥ 0 : Xt = e}. Because pmin (θ(0)) ≥ q0 and 

pmin (θ(t)) ≥ q0/2, Lemma 55 gives 

Pr 

> θ(0)

(τ > M ) ≤ (1 − q0/2) M , Pr 

> θ(t)

(τ > M ) ≤ (1 − q0/2) M .

68 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Combining these bounds yields 

Lθ(t)(X0: τ ) − L θ(0) (X0: τ ) TV ≤ M d ∥θ(t) − θ(0) ∥1 + 2(1 − q0/2) M .

Finally, since θ(s) ∈ U for all s ∈ [0 , t ], the gradient bound above holds on [0 , t ] and integration yields 

∥θ(t) − θ(0) ∥1 ≤ t ·



C Lb0

e−cL 

,

which substituted into the previous display gives the stated inequality. 

## H Supporting Lemmas 

Lemma 60. Under the assumptions and notation of Sections 4 and 6. In particular, the distribution DQm is invariant under vertex permutations, and the value matrix A(t) evolves by gradient flow 

ddt A(t) = −∇ AL(DQm, A (t)) ,

For α, β, γ ∈ R, define A(α, β, γ ) on the vertex–edge block by 

A(α, β, γ )v, (u,w ) =



α if v = u, β if v = w, γ if v / ∈ { u, w },v, u, w ∈ V, u ̸ = w, 

with all other entries defined as in Section 6. Let A(t) solve gradient flow with A(0) = A(α0, β 0, γ 0) for some α0, β 0, γ 0. Then for all t ≥ 0 there exist scalars 

α(t), β (t), γ (t) such that 

Av, (u,w )(t) = 



α(t) if v = u, β(t) if v = w, γ(t) if v / ∈ { u, w },

∀ v ∈ V, (u, w ) ∈ E, u ̸ = w. 

Define the reduced loss 

˜L(α, β, γ ) := L DQm, A (α, β, γ ).

The scalars α(t), β (t), γ (t) satisfy 

˙α(t) = − 12n(2 n − 1) ∂α ˜L α(t), β (t), γ (t),

˙β(t) = − 12n(2 n − 1) ∂β ˜L α(t), β (t), γ (t),

˙γ(t) = − 1(2 n − 2) · 2n(2 n − 1) ∂γ ˜L α(t), β (t), γ (t).

Proof. For a permutation π of the vertex set V , define an operator Tπ on matrices A by 

 Tπ A 

> v, (u,w )

:= Aπ−1(v), (π−1(u),π −1(w)) , v, u, w ∈ V. 

Equivalently, (Tπ A)π(v),(π(u),π (w)) = Av, (u,w ). This corresponds to consistently relabelling all vertices by π.By construction of DQm and the RL loss, relabelling vertices does not change the distribution over inputs or the loss, hence 

L(DQm, T π A) = L(DQm, A ) for all A, π. 

69 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Differentiating gives gradient equivariance 

∇AL(DQm, T π A) = Tπ ∇AL(DQm, A ).

Now let A(t) solve gradient flow. Then 

ddt 

 Tπ A(t) = Tπ

ddt A(t) = −Tπ ∇AL(DQm, A (t)) = −∇ AL(DQm, T π A(t)) .

So Tπ A(t) is also a solution of the same ODE. At t = 0 , the balanced initialization Equations (6) to (8) gives 

Av, (u,w )(0) = 



α0 v = u, β0 v = w, γ0 v / ∈ { u, w },

which is clearly invariant under any permutation of vertex labels, hence Tπ A(0) = A(0) for all π. By uniqueness of solutions to the ODE, we must have 

Tπ A(t) = A(t) for all t ≥ 0, π. 

Thus A(t) always lies in the fixed-point subspace 

S := {A : Tπ A = A for all π}.

To describe S, consider triples (v, u, w ) ∈ V 3 with u̸ = w. Two such triples lie in the same orbit under the action of permutations if and only if they have the same equality pattern among {v, u, w }. There are exactly three possibilities: 1. v = u, v̸ = w;2. v = w, v̸ = u;3. v, u, w are all distinct. Given any two triples with the same pattern, we can construct a permutation π mapping the first triple to the second (e.g., map u 7 → u′, w 7 → w′, and extend arbitrarily), so these are indeed the three orbits. If A ∈ S , then for any two entries 

Av, (u,w ) and Av′,(u′,w ′) from the same orbit we must have 

Av, (u,w ) = Av′,(u′,w ′),

because some π sends one triple to the other and A = Tπ A. Therefore there exist scalars α, β, γ such that 

Av, (u,w ) =



α v = u, β v = w, γ v / ∈ { u, w },

which is exactly the form A(α, β, γ ). This gives the representation of A(t) in terms of α(t), β (t), γ (t).For the dynamics, write A(t) = A(α(t), β (t), γ (t)) and define ˜L as above. By the chain rule, 

∂α ˜L(α, β, γ ) = X

> v=u, u ̸=w

∂∂A v, (u,w )

L(DQm, A (α, β, γ )) ,

and similarly for β, γ . Gradient flow gives 

ddt Av, (u,w )(t) = − ∂∂A v, (u,w )

L(DQm, A (t)) .

70 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

By symmetry, all entries with v = u share the same value α(t) and the same time derivative ˙α(t). Summing the previous identity over all such entries and comparing with the expression for ∂α ˜L yields 

Nα ˙α(t) = −∂α ˜L α(t), β (t), γ (t),

where Nα is the number of pairs (v, (u, w )) with v = u and u̸ = w. For each ordered pair (u, w ) with u̸ = w, there is exactly one v with v = u, so 

Nα = |Edir | = |V |(|V | − 1) ,

where Edir = {(u, w ) ∈ V 2 : u̸ = w}. The same reasoning gives Nβ = |V |(|V | − 1) , and for γ, for each (u, w ) there are 

|V | − 2 vertices v / ∈ { u, w }, so 

Nγ = ( |V | − 2) |V |(|V | − 1) .

Dividing by these constants gives the claimed ODEs for α, β, γ .

## I Auxiliary lemmas 

Lemma 61 (Softmax derivative) . Let Y be a finite set. For each θ ∈ R, let oθ : Y → R be differentiable in θ, and define the softmax distribution Pθ on Y by 

Pθ (y) := exp( oθ (y)) 

P 

> z∈Y

exp( oθ (z)) .

Write dθ (y) := ∂θ oθ (y). Then for every y ∈ Y ,

∂θ Pθ (y) := Pθ (y)



dθ (y) − X

> z∈Y

Pθ (z) dθ (z)



.

Equivalently, for any function w : Y → R,

X

> y∈Y

 ∂θ Pθ (y) w(y) := X

> y∈Y

Pθ (y) dθ (y) w(y) −

 X 

> y∈Y

Pθ (y) dθ (y)

 X 

> y∈Y

Pθ (y) w(y)



,

i.e. X

> y∈Y

 ∂θ Pθ (y) w(y) := Cov Y ∼Pθ

 dθ (Y ), w (Y ).

Proof. Fix θ and abbreviate o(y) := oθ (y), P (y) := Pθ (y), and d(y) := ∂θ oθ (y). Let 

Z := X

> z∈Y

eo(z), so P (y) = eo(y)

Z .

Differentiate using the quotient rule: 

∂θ P (y) := ∂θ eo(y)

Z − eo(y)

Z2 ∂θ Z. 

Since ∂θ eo(y) = eo(y)d(y) and 

∂θ Z = X

> z∈Y

∂θ eo(z) := X

> z∈Y

eo(z)d(z),

we obtain 

∂θ P (y) := eo(y)

Z d(y) − eo(y)

Z · 1

Z

X

> z∈Y

eo(z)d(z).

Recognizing eo(y)/Z = P (y) and eo(z)/Z = P (z) gives 

∂θ P (y) := P (y)



d(y) − X

> z∈Y

P (z)d(z)



,

71 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

which proves the first identity. Multiplying this by w(y) and summing over y ∈ Y yields 

X

> y∈Y

(∂θ P (y)) w(y) := X

> y∈Y

P (y)d(y)w(y) −

 X 

> z∈Y

P (z)d(z)

 X 

> y∈Y

P (y)w(y)



,

which is the stated centered form. The covariance reformulation follows from Cov( A, B ) = E[AB ] − E [A]E[B] with 

Y ∼ Pθ .

Lemma 62 (Elementary gambler’s–ruin bound (no martingales)) . Let (Dt)t≥0 be a Markov chain on {0, 1, . . . , K } with absorbing boundaries {0, K }. For each i ∈ { 1, . . . , K − 1} assume 

Pr( Dt+1 = i − 1 | Dt = i) = pf (i), Pr( Dt+1 = i + 1 | Dt = i) = pb(i),

and Pr( Dt+1 = i | Dt = i) = 1 − pf (i) − pb(i) (holds allowed). Assume pf (i), p b(i) > 0 and that there exists ρ ∈ (0 , 1) 

such that for all i ∈ { 1, . . . , K − 1},

pb(i)

pf (i) ≤ ρ. 

Let τ0 := inf {t ≥ 0 : Dt = 0 } and τK := inf {t ≥ 0 : Dt = K}. Then for every k ∈ { 0, 1, . . . , K },

Pr  

> k

(τK < τ 0) ≤ ρ K−k.

Proof. Let h(i) := Pr i(τK < τ 0). Then h(0) = 0 , h(K) = 1 , and for 1 ≤ i ≤ K − 1, conditioning on one step gives 

h(i) = pf (i)h(i − 1) + pb(i)h(i + 1) +  1 − pf (i) − pb(i)h(i),

hence 

pf (i) h(i) − h(i − 1)  = pb(i) h(i + 1) − h(i). (48) With ∆i := h(i) − h(i − 1) , (48) implies ∆i+1 = pf (i) 

> pb(i)

∆i, hence 

∆i = ∆ 1

> i−1

Y

> m=1

pf (m)

pb(m) .

Since h(k) = Pki=1 ∆i and h(K) = PKi=1 ∆i = 1 , we get 

h(k) = 

Pki=1 

Qi−1 

> m=1
> pf(m)
> pb(m)

PKi=1 

Qi−1 

> m=1
> pf(m)
> pb(m)

.

Using pf (m)/p b(m) ≥ ρ−1 in both numerator and denominator yields 

h(k) ≤

Pki=1 ρ−(i−1) 

PKi=1 ρ−(i−1) := ρK−k ·

Pk−1 

> j=0

ρj

PK−1 

> j=0

ρj ≤ ρK−k.

Lemma 63 (Expected absorption time) . Let (Xt)t≥0 be a Markov chain on {0, 1, . . . , N } such that N is absorbing, from each i ∈ { 1, . . . , N − 1} the chain moves to i + 1 with probability p and to i − 1 with probability q, where p, q ≥ 0,

p + q = 1 and p > q , and from 0 the chain moves to 1 with probability 1. Let TN := inf {t ≥ 0 : Xt = N }. Then for every 

k ∈ { 0, 1, . . . , N },

Ek[TN ] := N − kp − q + 2pq 

(p − q)2

" qp

N

−

 qp

k#

,

and in particular, 

Ek[TN ] ≤ N − kp − q .

72 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. For i ∈ { 0, 1, . . . , N } define mi := Ei[TN ]. If 1 ≤ i ≤ N − 1, conditioning on the first step and using the Markov property gives 

mi = 1 + p m i+1 + q m i−1. (49) At N we have mN = 0 since N is absorbing, and at 0 we have 

m0 = 1 + m1 (50) since the chain moves deterministically from 0 to 1 in one step. Set δ := p − q > 0 and rewrite (49) as 

p m i+1 − mi + q m i−1 = −1, 1 ≤ i ≤ N − 1. (51) A particular solution is given by mpart  

> i

= −i/δ , because 

p



− i + 1 

δ



−



− iδ



+ q



− i − 1

δ



:= −p + qδ = −1

using p + q = 1 . For the associated homogeneous equation 

p u i+1 − ui + q u i−1 = 0 ,

a trial solution ui = ri leads to the characteristic equation pr 2 − r + q = 0 , whose two roots are r = 1 and r = q/p .Writing ρ := q/p ∈ (0 , 1) , the general homogeneous solution is ui = A + Bρ i, hence every solution to (51) has the form 

mi = A + Bρ i − iδ . (52) Imposing mN = 0 in (52) yields 

0 = A + Bρ N − Nδ =⇒ A = Nδ − Bρ N .

Next, (50) gives m0 = 1 + m1. From (52), 

m0 = A + B, m1 = A + Bρ − 1

δ ,

so 

A + B = 1 + 



A + Bρ − 1

δ



=⇒ B(1 − ρ) = 1 − 1

δ .

Now 1 − ρ = 1 − q/p = ( p − q)/p = δ/p and, since p + q = 1 , we have 1 − 1 

> δ

= δ−1 

> δ

= (p−q)−(p+q) 

> δ

= − 2qδ . Therefore 

B · δp = − 2qδ =⇒ B = − 2pq δ2 .

Substituting back gives 

A = Nδ + 2pq δ2 ρN ,

and hence for k ∈ { 0, 1, . . . , N },

mk := N − kδ + 2pq δ2

 ρN − ρk.

This is the stated closed form after recalling δ = p − q and ρ = q/p .To see that this solution is the unique solution to (49) – (50) , suppose ˜m is another solution and let di := mi − ˜mi. Then 

dN = 0 , d0 = d1, and d solves the homogeneous recurrence pd i+1 − di + qd i−1 = 0 , so di = A′ + B′ρi. The condition 

d0 = d1 implies A′ + B′ = A′ + B′ρ, hence B′(1 − ρ) = 0 and therefore B′ = 0 , and then dN = 0 forces A′ = 0 , so 

d ≡ 0.73 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Finally, since ρ ∈ (0 , 1) and k ≤ N , we have ρN ≤ ρk and thus ρN − ρk ≤ 0. Because 2pq/δ 2 ≥ 0, the correction term  

> 2pq δ2

(ρN − ρk) is nonpositive, and therefore 

Ek[TN ] = mk ≤ N − kδ = N − kp − q .

Lemma 64 (Absorption time tail bound) . Let (Xt)t≥0 be a nearest–neighbor random walk with 

Pr( Xt+1 = Xt + 1) = p, Pr( Xt+1 = Xt − 1) = q = 1 − p, p − q ≥ c > 0,

run on a chain of length L with the right endpoint absorbing . Assume the walk starts at distance k from the absorbing endpoint (i.e. X0 = L − k), where k is a fixed constant independent of L. Let 

τ := inf {t ≥ 0 : Xt = L}

be the absorption time at the right edge. Then for any t ≥ 1,

Pr( τ > t ) ≤ Pr 

> t

X

> i=1

ξi ≤ k − 1

!

, ξi ∈ { +1 , −1}, E[ξi] = p − q =: μ (≥ c),

and by Hoeffding’s inequality, 

Pr( τ > t ) ≤ exp 



− (μt − (k − 1)) 2

2t



≤ exp 



− (ct − (k − 1)) 2

2t



.

In particular, taking t = ⌊L1/4⌋ and using that k is constant, for all L large enough (e.g. L1/4 ≥ 2( k − 1) /c ), 

Pr  τ > ⌊L1/4⌋ ≤ exp 



− c2

8 L1/4



.

Proof. Write Xt = X0 + Pti=1 ξi with i.i.d. increments ξi ∈ { +1 , −1} of mean μ = p − q ≥ c. If τ > t then Xt ≤ L − 1,hence Pti=1 ξi = Xt − X0 ≤ (L − 1) − (L − k) = k − 1, proving the first inequality. Hoeffding’s inequality for bounded i.i.d. variables yields 

Pr 

> t

X

> i=1

ξi − μt ≤ − (μt − (k − 1)) 

!

≤ exp 



− (μt − (k − 1)) 2

2t



,

and μ ≥ c gives the stated simplification. For t = ⌊L1/4⌋ and L1/4 ≥ 2( k − 1) /c , we have μt − (k − 1) ≥ ct/ 2, giving the final display. 

Lemma 65 (Dominated change under one–sided derivative domination) . Let x, y : [0 , T ] → R be differentiable functions with initial values x(0) = a and y(0) = b. Assume that for every t ∈ [0 , T ],

x′(t) ≥ 0, y′(t) ≤ 0,

and there exists a constant c > 0 such that 

x′(t) ≥ c −y′(t).

Then for every t ∈ [0 , T ],

x(t) − a ≥ c b − y(t),

equivalently, 

x(t) + c y (t) ≥ a + c b. 

74 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Proof. Define F (t) := x(t) + c y (t). Then F is differentiable and 

F ′(t) = x′(t) + c y ′(t) ≥ 0 for all t ∈ [0 , T ],

by the assumed inequality x′(t) ≥ c(−y′(t)) . Hence F is nondecreasing on [0 , T ], so 

x(t) + c y (t) = F (t) ≥ F (0) = x(0) + c y (0) = a + c b. 

Rearranging gives x(t) − a ≥ c(b − y(t)) , as claimed. 

Lemma 66. Fix C > 0, K > 0, ε ∈ (0 , 1) , d > 0, and D ∈ R. Let α ∈ C1([0 , ∞)) satisfy 

˙α(t) > K for all t ≥ 0,

and define 

g(t) := edα (t)−D

edα (t)−D + C ∈ (0 , 1) .

Let f : [0 , ∞) → (0 , 1) be continuous and assume 

f (0) = g(0) and f (t) > g (t) for all t > 0.

If f (0) < 1 − ε, then there exists Tε ∈ (0 , ¯Tε) such that 

f (Tε) = 1 − ε, 

where 

¯Tε := 1

dK log 

 1 − εε · 1 − f (0) 

f (0) 



.

Moreover, 

α(Tε) − α(0) < 1

d log 

 1 − εε · 1 − f (0) 

f (0) 



.

Proof. Set 

∆ε := log 

 1 − εε · 1 − f (0) 

f (0) 



, ¯Tε := ∆ε

dK .

From f (0) = g(0) = edα (0) −D  

> edα (0) −D+C

we obtain 

edα (0) −D = C f (0) 1 − f (0) , hence dα (0) − D = log 



C f (0) 1 − f (0) 



.

Consequently, 

(dα (0) − D) + ∆ ε = log 



C f (0) 1 − f (0) 



+ log 

 1 − εε · 1 − f (0) 

f (0) 



= log 



C 1 − εε



.

Since ˙α(t) > K for all t ≥ 0, integration yields 

α(t) − α(0) = 

Z t

> 0

˙α(s) ds > Kt (t > 0) .

Multiplying by d and subtracting D gives 

dα (t) − D > (dα (0) − D) + dKt (t > 0) .

In particular, 

dα ( ¯Tε) − D > (dα (0) − D) + dK ¯Tε = ( dα (0) − D) + ∆ ε = log 



C 1 − εε



.

75 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

The map x 7 → ex 

> ex+C

is strictly increasing, hence 

g( ¯Tε) > elog( C(1 −ε)/ε )

elog( C(1 −ε)/ε ) + C = 1 − ε. 

By assumption f (t) > g (t) for t > 0, thus f ( ¯Tε) > g ( ¯Tε) > 1 − ε, while f (0) < 1 − ε. By continuity of f , the intermediate value theorem implies the existence of Tε ∈ (0 , ¯Tε) such that f (Tε) = 1 − ε.Finally, since Tε > 0 and f (Tε) > g (Tε), we have g(Tε) < 1 − ε. By monotonicity of x 7 → ex 

> ex+C

it follows that 

dα (Tε) − D < log 



C 1 − εε



.

Subtracting dα (0) − D = log 



C f (0) 1−f (0) 



yields 

d α(Tε) − α(0)  < log 



C 1 − εε



− log 



C f (0) 1 − f (0) 



= log 

 1 − εε · 1 − f (0) 

f (0) 



.

Lemma 67 (Logit computation) . Fix n ≥ 3. Consider an input sequence s1: L sampled autoregressively as described in Section 4, such that the prefix s1:2 n−2 contains the multiset of edges G of two disjoint directed chains Ca = ( a1, . . . , a n)

and Cb = ( b1, . . . , b n), and the last token is a vertex v = sL where v ∈ (Ca ∪ C b).Assume the transformer’s parameters satisfy: 1. The attention matrix has the symmetric form A = A(α, β, γ ) on the vertex–edge block as in Lemma 60 and γ on all vertex–vertex blocks. 2. The value matrix has the following form: there exist α′, β ′, γ ′ ∈ R such that for every edge token (u, w ) ∈ E and vertex k ∈ V ,

V(u,w ),k =



α′ if k = u, β′ if k = w, γ′ if k / ∈ { u, w },

and Vk, : = 0 for all vertex tokens k ∈ V .

For Softmax Attention, let Zv = PLj=1 exp( AsL,s j ) be the partition function. We define the weight function w(μ) as: 

w(μ) = 

(

μ for Linear Attention ,eμ/Z v for Softmax Attention .

Note that for any token x, the attention weight from v to x is given by w(Av,x ).Then the logits H(1)  

> L, :

depend on v, G, and in the softmax case, the partition function Zv (we denote them by o(v, G, Z v )). In the case of Linear Attention, the dependence on Zv is absent, and we simply write o(v, G ). Furthermore, o(v, G, Z v )k =Ψk + C, where C is a constant independent of k, we define ∆α := α′ − γ′, ∆β := β′ − γ′, and Ψk is given by the following cases (and symmetrically when swapping Ca and Cb). 

Case 1: v is a middle vertex. If v = ai for some i ∈ { 3, . . . , n − 2} (and symmetrically if v = bi), then: 

Ψk =



∆αw(γ) + ∆ β w(α), k = ai+1 ,

∆αw(β) + ∆ β w(γ), k = ai−1,

∆αw(α) + ∆ β w(β), k = ai,

∆αw(γ), k ∈ { a1, b 1},

∆β w(γ), k ∈ { an, b n},

(∆ α + ∆ β )w(γ), k ∈ (Ca ∪ C b) \ { ai−1, a i, a i+1 , a 1, a n, b 1, b n}.

76 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

Case 2: v is the penultimate vertex. If v = an−1 (and symmetrically if v = bn−1), then: 

Ψk =



∆β w(α), k = an,

∆αw(β) + ∆ β w(γ), k = an−2,

∆αw(α) + ∆ β w(β), k = an−1,

∆αw(γ), k ∈ { a1, b 1},

∆β w(γ), k ∈ { bn},

(∆ α + ∆ β )w(γ), k ∈ (Ca ∪ C b) \ { an−2, a n−1, a n, a 1, b 1, b n}.

Case 3: v is the second vertex (and not penultimate). If m ≥ 4 and v = a2 (and symmetrically if v = b2), then: 

Ψk =



∆αw(γ) + ∆ β w(α), k = a3,

∆αw(β), k = a1,

∆αw(α) + ∆ β w(β), k = a2,

∆β w(γ), k ∈ { an, b n},

∆αw(γ), k ∈ { b1},

(∆ α + ∆ β )w(γ), k ∈ (Ca ∪ C b) \ { a1, a 2, a 3, a n, b 1, b n}.

Case 4: v is the first vertex (and not penultimate). If m ≥ 3 and v = a1 (and symmetrically if v = b1), then: 

Ψk =



∆αw(γ) + ∆ β w(α), k = a2,

∆αw(α), k = a1,

∆β w(γ), k ∈ { an, b n},

∆αw(γ), k ∈ { b1},

(∆ α + ∆ β )w(γ), k ∈ (Ca ∪ C b) \ { a1, a 2, a n, b 1, b n}.

Case 5: v is a terminal vertex. If v = an (and symmetrically if v = bn), then: 

Ψk =



∆β w(β), k = an,

∆αw(β) + ∆ β w(γ), k = an−1,

∆αw(γ), k ∈ { a1, b 1},

∆β w(γ), k ∈ { bn},

(∆ α + ∆ β )w(γ), k ∈ (Ca ∪ C b) \ { an−1, a n, a 1, b 1, b n}.

Proof. Let s1: L denote the input sequence with sL = v. From the transformer architecture (Section 3.2), the output logits at position L are given by the attention-weighted sum of value vectors. Specifically: 

H(1)  

> L, :

=

> L

X

> j=1

weight (sL, s j )Vsj ,:,

where weight (sL, s j ) is the attention weight of token sj when attending from sL. Using our definition of w(μ) and the attention matrix entries AsL,s j , this weight is exactly w(AsL,s j ).By our assumption on the value matrix, Vsj ,: = 0 whenever sj is a vertex token. Since the sequence s1: L consists of edge tokens (the set G) and vertex tokens, the sum over j = 1 , . . . , L restricts to the indices corresponding to edges in G:

H(1)  

> L, :

= X

> e∈G

w(Av,e )Ve, :.

This expression confirms that the logits depend on v, G, and implicitly on the context via Zv (in the Softmax case), justifying the notation o(v, G, Z v ).77 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

For a specific target vertex k ∈ V , we have: 

o(v, G, Z v )k = X

> e∈G

w(Av,e )Ve,k .

Using the updated symmetric structure of V where Ve,k = α′ if k is the source of e, β′ if k is the target of e, and γ′

otherwise, we substitute: 

o(v, G, Z v )k = X

> e=( u,w )∈G

w(Av,e ) ( α′1{k = u} + β′1{k = w} + γ′1{k / ∈ { u, w }} )= α′ X

> e∈G, src (e)= k

w(Av,e ) + β′ X

> e∈G, tgt (e)= k

w(Av,e ) + γ′ X

> e∈G,k / ∈e

w(Av,e ).

Let Wout (v, k ) = P 

> e∈G, src (e)= k

w(Av,e ) be the total attention weight on edges outgoing from k, and Win (v, k ) = 

P 

> e∈G, tgt (e)= k

w(Av,e ) be the total weight on edges incoming to k. The total weight is Wtotal (v) = P 

> e∈G

w(Av,e ). We can rewrite the third term using the identity 1{k / ∈ e} = 1 − 1{k = src (e)} − 1{k = tgt (e)}:

γ′ X

> e∈G,k / ∈e

w(Av,e ) = γ′ (Wtotal (v) − Wout (v, k ) − Win (v, k )) .

Substituting this back: 

o(v, G, Z v )k = α′Wout (v, k ) + β′Win (v, k ) + γ′ (Wtotal (v) − Wout (v, k ) − Win (v, k )) = ( α′ − γ′)Wout (v, k ) + ( β′ − γ′)Win (v, k ) + γ′Wtotal (v).

Defining ∆α = α′ − γ′, ∆β = β′ − γ′, and C = γ′Wtotal (v) (which is independent of k), we get: 

o(v, G, Z v )k = ∆ αWout (v, k ) + ∆ β Win (v, k ) + C. 

We define Ψk = ∆ αWout (v, k ) + ∆ β Win (v, k ). It remains to compute Wout (v, k ) and Win (v, k ) in each of the configura-tions listed in the lemma statement. Since G is the disjoint union of two directed chains, each k ∈ (Ca ∪ C b) has at most one outgoing edge (k, succ( k)) (absent iff k ∈ { an, b n}) and at most one incoming edge (pred( k), k ) (absent iff k ∈ { a1, b 1}). For such edges, by the assumed form of A(α, β, γ ) on the vertex–edge block, 

Av, (k, succ( k)) =



α, v = k, β, v = succ( k),γ, otherwise, 

Av, (pred( k),k ) =



α, v = pred( k),β, v = k, γ, otherwise. Applying w(·) and substituting into Ψk = ∆ αWout (v, k ) + ∆ β Win (v, k ) gives exactly the case-by-case expressions stated in the lemma (with the Cb cases obtained by symmetry). 

## J Experimental Details 

J.1 Theoretically Inspired Experiments 

We provide full details for the experiments described in Section 7.1. We use a single-layer transformer with attention matrix 

A ∈ R|Σ|×| Σ| and value matrix V ∈ R|Σ|×| Σ|, as defined in Section 3.2. We initialize both matrices with small random values sampled from a normal distribution with standard deviation 10 −8. We optimize L(D, θ ) using the REINFORCE algorithm (Williams, 1992) with the advantage function computed as the difference between the reward and the average reward across the batch. We use the Adam optimizer with learning rate 0.01 . For each chain size, we train on 1,000 ,000 

samples over 300 epochs with batch size 50 ,000 , and validate on 1,000 samples. During evaluation, we measure test accuracy and chain traversal correctness using greedy decoding (temperature 0). For experiments with a single chain size (see Sections 7.1.1 and 7.1.3), we set max new tokens to n + 1 where n is the chain size; for generalization experiments across multiple chain sizes (see Section 7.1.2), we use max( test chain sizes ) + 1 .78 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data 

J.2 Real World Experiments 

We provide full details for the experiments described in Section 7.2. We fine-tune Qwen 2.5 3B (Qwen et al., 2025) (https://huggingface.co/Qwen/Qwen2.5-3B-Instruct ) using GRPO (Shao et al., 2024). We use a KL coefficient β = 0 .0001 , generate 8 completions per prompt with sampling temperature 0.9 during training, and set max completion length to 1024 tokens. We optimize with AdamW using learning rate 5 × 10 −7 with a cosine learning rate scheduler and warmup ratio of 0.03 . We use a batch size of 1 per device with 8 gradient accumulation steps, training for 600 

steps. Evaluation is performed over 1000 samples for the 15-Hard evaluations, and 1008 for the 15-Uniform evaluation (this amounts to 72 samples per reasoning-length). All test evaluations use greedy decoding (temperature 0). The prompt used for fine-tuning and evaluation is the following: 

[{ "role": "system", "content": "You are a helpful assistant that can solve arithmetic equations systems." }, {"role": "user", "content": f"Given this equation system: {input_problem}\n\n Find the value of the target variable. Show your reasoning in <thinking> </thinking> tags, and provide your final answer in <answer> </answer> tags, for example <answer>12</answer>." }, {"role": "assistant", "content": "I’ll think about the equation system and solve it.\n <think>" }] 

The code for the real-world experiments is based on https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/mini-deepseek-r1-aha-grpo.ipynb J.2.1 EXAMPLE INPUTS AND COMPLETIONS 

We provide two randomly generated examples from the 15 -Uniform distribution (see Section 7.2) to illustrate the model’s reasoning process. Each input consists of a shuffled list of affine equations with one constant assignment (shown in blue) and a query for a target variable (shown in red). The model generates its reasoning within <thinking> tags, where it identifies the dependency chain by backtracking from the target to the constant, then calculates values forward step-by-step, and finally outputs the answer within <answer> tags. See Figure 4 for complete examples. 79 Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data Example 1: Input: x 15 = x 5 + 3. x 5 = x 27 + 2. x 1 = x 24 + 2. x 20 = x 2 + 3. x 2 = x 22 + 3. x 30 = x 3 + 3. x 11 = x 13 + 1. x 26 = x 18 - 3. x 14 = x 20 + 2. x 28 = x 30 - 2. x 16 = x 6 - 3. x 10 = x 23 + 1. x 8 = 3. x 4 = x 8 - 2. x 25 = x 7 - 3. x 7 = x 28 + 3. x 23 = x 14 + 2. x 8 = x 17 - 1. x 3 = x 15 - 1. x 21 = x 25 - 2. x 6 = x 4 - 1. x 13 = x 19 + 3. x 29 = x 12 + 3. x 18 = x 16 - 2. x 17 = x 1 + 2. x 24 = x 29 + 1. x 9 = x 11 + 1. x 12 = x 9 - 3. x 27 = x 26 + 1. x 19 = x 10 + 1. Find x 21. 

Completion: <thinking> We need to find x 21. From equation x 21 = x 25 - 2, we need x 25. From x 25 = x 7 - 3, we need x 7. From x 7 = x 28 + 3, we need x 28. From x 28 = x 30 - 2, we need x 30. From x 30 = x 3 + 3, we need x 3. From x 3 = x 15 - 1, we need x 15. From x 15 = x 5 + 3, we need x 5. From x 5 = x 27 + 2, we need x 27. From x 27 = x 26 + 1, we need x 26. From x 26 = x 18 - 3, we need x 18. From x 18 = x 16 - 2, we need x 16. From x 16 = x 6 - 3, we need x 6. From x 6 = x 4 - 1, we need x 4. From x 4 =x 8 - 2, and we know x 8 = 3. Now calculating forward: x 4 = 3 - 2 = 1, x 6 = 1 - 1= 0, x 16 = 0 - 3 = -3, x 18 = -3 - 2 = -5, x 26 = -5 - 3 = -8, x 27 = -8 + 1 = -7, . . . 

x 25 = 1 - 3 = -2, x 21 = -2 - 2 = -4. </thinking> <answer> -4 </answer> 

Example 2: Input: x 18 = x 8 - 1. x 27 = x 19 + 3. x 7 = x 27 + 2. x 21 = x 24 + 1. x 19 = x 5 -1. x 13 = x 26 - 3. x 22 = x 9 - 1. x 23 = x 2 + 2. x 5 = x 10 - 1. x 12 = x 20 +3. x 15 = x 12 - 3. x 10 = x 21 - 1. x 14 = x 7 + 3. x 8 = x 30 - 1. x 20 = x 13 +3. x 3 = x 28 - 1. x 26 = x 3 - 1. x 1 = x 23 + 1. x 17 = x 25 - 1. x 29 = x 11 - 3. x 30 = 8. x 9 = x 4 - 3. x 24 = x 22 + 2. x 16 = x 1 - 3. x 4 = x 29 + 3. x 6 = x 16 - 2. x 28 = x 6 + 2. x 2 = x 18 + 2. x 11 = x 17 + 2. Find x 15. 

Completion: <thinking> First, let’s identify the dependency chain. We need to find x 15, which depends on x 12. From the equations: x 15 = x 12 - 3, x 12 = x 20 + 3, x 20 =x 13 + 3, x 13 = x 26 - 3, x 26 = x 3 - 1, x 3 = x 28 - 1, x 28 = x 6 + 2, x 6 = x 16 - 2, x 16 = x 1 - 3, x 1 = x 23 + 1, x 23 = x 2 + 2, x 2 = x 18 + 2, x 18 = x 8 - 1, x 8 = x 30 - 1. We know x 30 = 8, so we can calculate: x 8 = 8 - 1 = 7, x 18 = 7 - 1 = 6, x 2 = 6 + 2 = 8, x 23 = 8 + 2 = 10, . . . x 12 = 18 + 3 = 21, x 15 = 21 - 3 = 18. </thinking> <answer> 18 </answer> 

Figure 4: Randomly generated examples from the test set showing input-completion pairs for the mathematical reasoning task. The input contains a shuffled list of affine equations with one constant assignment (shown in blue) and asks for the value of a target variable (shown in red). The model generates its reasoning process within <thinking> tags, identifying the dependency chain and performing step-by-step calculations, then outputs the final answer within <answer> tags. 

80