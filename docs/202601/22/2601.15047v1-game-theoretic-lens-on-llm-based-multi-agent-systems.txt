Title: Game-Theoretic Lens on LLM-based Multi-Agent Systems

URL Source: https://arxiv.org/pdf/2601.15047v1

Published Time: Thu, 22 Jan 2026 01:56:58 GMT

Number of Pages: 9

Markdown Content:
# Game-Theoretic Lens on LLM-based Multi-Agent Systems 

# Jianing Hao 1 , Han Ding 2 , Yuanjian Xu 1 , Tianze Sun 3 , Ran Chen 4 , Wanbo Zhang 5 , Guang Zhang 1,∗ , Siguang Li 1,∗

> 1

# The Hong Kong University of Science and Technology (Guangzhou), 2Beihang University 

> 3

# Harbin Institute of Technology, 4OpenCSG, 5Fudan University, jhao768@connect.hkust-gz.edu.cn, handing@buaa.edu.cn, yxu085@connect.hkust-gz.edu.cn, suntianze@stu.hit.edu.cn, schen@opencsg.com, 23302010062@m.fudan.edu.cn, guangzhang@hkust-gz.edu.cn, siguangli@hkust-gz.edu.cn 

# Abstract 

Large language models (LLMs) have demonstrated strong reasoning, planning, and communication abilities, enabling them to operate as autonomous agents in open environments. While single-agent systems remain limited in adaptability and coor-dination, recent progress has shifted attention to-ward multi-agent systems (MAS) composed of in-teracting LLMs that pursue cooperative, competi-tive, or mixed objectives. This emerging paradigm provides a powerful testbed for studying social dy-namics and strategic behaviors among intelligent agents. However, current research remains frag-mented and lacks a unifying theoretical foundation. To address this gap, we present a comprehensive survey of LLM-based multi-agent systems through a game-theoretic lens. By organizing existing stud-ies around the four key elements of game theory: players, strategies, payoffs, and information, we es-tablish a systematic framework for understanding, comparing, and guiding future research on the de-sign and analysis of LLM-based MAS. 

# 1 Introduction 

Multi-agent systems (MAS) have long served as a foun-dational framework for investigating interaction, decision-making, and strategic behavior among autonomous enti-ties. Classical research has examined these systems through game-theoretic models [Nash, 1950], distributed optimiza-tion [Sandholm, 1999], reinforcement learning [Littman, 1994], and agent-based modeling [Epstein, 1999], providing formal tools for understanding coordination, competition, ne-gotiation, and emergent collective behaviors. The recent advances in large language models (LLMs) have given rise to a new paradigm of multi-agent systems, in which agents can reason [Wei et al. , 2022], plan [Huang et al. ,2022], and communicate [Li et al. , 2023] using natural lan-guage. In such systems, multiple LLM-based agents interact in complex and open-ended environments, exhibiting a wide spectrum of behaviors that may include cooperation [Chen et al. , 2024], competition [Zhu et al. , 2025], negotiation [Hua 

> ∗Corresponding author.

et al. , 2024], social learning, and other emergent phenom-ena [Ashery et al. , 2025]. Continuing a line of research in strategic decision-making systems [Silver and others, 2016], prior studies have explored LLM-based multi-agent systems in diverse contexts, includ-ing task decomposition and planning [Hong et al. , 2024], decision-making under uncertainty [Agashe et al. , 2025], multi-agent coordination [Wu et al. , 2024], social simulation and behavioral modeling [Park et al. , 2023], multi-party de-bate or dialogue [Qian et al. , 2024], and competitive strategy games [Zhu et al. , 2025]. These works highlight the poten-tial of LM-MAS to provide insights into agent interaction, strategic reasoning, communication protocols, and emergent collective behaviors, yet a unified theoretical perspective that systematically organizes these developments is still lacking. Behind every interaction in a MAS lies a strategic decision-making problem, which is precisely the domain that game theory is designed to model. As LLM-powered agents be-come more capable and autonomous, understanding their be-havior through a game-theoretic lens becomes essential. This survey adopts a game-theoretic lens to systematically syn-thesize existing research on LLM-driven multi-agent systems. This paper makes the following three contributions: • We introduce a novel game-theoretic framework for cat-egorizing LLM-based MAS through four core elements: players, strategies, payoffs, and information. This uni-fied perspective facilitates the integration of classical game theory with modern LLM-driven research for sys-temic analysis of agent interactions. • We provide a systematic review of the current body of work on LLM-based multi-agent systems, revealing the insights and limitations of current research. • We identify key research gaps and propose forward-looking research directions, focusing on optimiz-ing equilibrium coordination, designing incentive-compatible communication protocols, and information structure modeling under partial observability. 

# 2 Background and Taxonomy 

2.1 Foundations of Game Theory 

Game theory is the mathematical study of strategic decision-making, where multiple agents interact, and each agent’s out-come depends on the choices made by others. It studies the 

> arXiv:2601.15047v1 [cs.MA] 21 Jan 2026 Figure 1: A game-theoretic framework for LLM-based multi-agent systems illustrates the dynamic interplay between the four core ele-ments of a game: Players, Strategies, Payoffs, and Information.

strategic interactions of rational decision-makers, where each player seeks to maximize their own payoff. A normal-form game is defined by the tuple Γ = ( N, (Si)i∈N , (ui)i∈N ),where N = {1, . . . , n } is the finite set of players, Si is the finite set of pure strategies available to player i, and 

ui:Q 

> j∈N

Sj → R is player i’s payoff function assigning a real-valued reward to every joint strategy profile. A strategy profile is a vector s = ( s1, . . . , s n) ∈ S1 × · · · Sj · · · × Sn,representing the strategy choices of all players. In this con-text, we assume that players are rational and aim to maxi-mize their expected utility by selecting strategies that opti-mize their payoff, based on the information available to them. Strategies may be pure or mixed. A pure strategy for player i is an element si ∈ Si, while a mixed strategy is a probability distribution σi ∈ ∆( Si) over Si. Here 

∆( Si) denotes the simplex of probability measures on Si.A mixed-strategy profile σ = ( σ1, . . . , σ n) specifies a dis-tribution over each player’s actions. Under profile σ, each pure profile s = (s1, . . . , s n) is chosen with probability Qnj=1 σj (sj ), and the expected payoff to player i is Ui(σ) = 

P 

> s∈S

ui(s) Qnj=1 σj (sj ). A pure strategy si can be viewed as a special case of a mixed strategy where the probability is concentrated on a single action. Strategy selection is influenced by both payoffs and infor-mation. Under complete information, all payoff functions and strategy spaces Si are common knowledge, allowing players to perfectly anticipate others’ behavior. In contrast, under incomplete information, each player may hold private knowl-edge or beliefs, modeled as θi. Regardless of the informa-tion structure, players choose strategies that maximize their expected utility based on the information available to them. The attractiveness of strategies is determined by their asso-ciated payoffs, while the structure of available information dictates how strategies are conditioned and how beliefs are updated. A Nash Equilibrium occurs when no player can uni-laterally improve their outcome by changing strategies — as-suming others keep theirs fixed. This concept is critical in MAS, where agents seek stable strategies in competitive or cooperative settings. 

2.2 Multi-Agent Systems as Game-Theoretic Environments 

A MAS consists of multiple autonomous agents interact-ing within a shared environment. Unlike single-agent sys-tems, MAS leverage distributed intelligence, allowing agents to divide work, respond to local signals, and solve complex problems through either collaboration or competition. Tra-ditional MAS relied on rule-based logic and local policies, though challenges remain where local metrics may misrep-resent global behavior, as similar concerns arise in learning-in-games dynamics [Fudenberg and Levine, 1998]. However, with the advent of LLMs, agents can engage in natural lan-guage reasoning, flexible task negotiation, and strategic co-ordination. In the game-theoretic perspective, each agent is modeled as a player i ∈ N with its own strategy space and utility function. Shoham and Leyton-Brown [Shoham and Leyton-Brown, 2008] describe a MAS as a system of au-tonomous entities with potentially divergent interests or in-formation, which mirrors the game-theoretic notion of play-ers having different payoffs or private knowledge. Thus, any MAS interaction can be captured as a strategic-form game: an agent’s possible actions correspond to the player’s strategies, and the agent’s objectives are encoded by its payoff function over joint action profiles. Communication and coordination are key features in MAS. Communication can be modeled as pre-play signaling: agents exchange messages or share observations before choosing their actions. Such signals allow agents to share private information or align expectations. Coordination problems arise when agents share common goals: if their payoffs are aligned, the MAS essentially operates as a cooperative game. More generally, coordinating self-interested agents involves finding equilibria in games where multiple consistent strat-egy profiles exist, or addressing mechanism-design problems that align individual incentives with collective objectives, par-ticularly in settings where communication may carry strate-gic intent or misrepresentation [Kamenica and Gentzkow, 2011]. Autonomy in MAS refers to the absence of a cen-tral controller. Each agent decides independently based on its own payoff and available information. This reflects the non-cooperative game assumption of decentralized decision-making. Agents may have private or incomplete information about the environment or other players’ preferences, analo-gous to players’ types in Bayesian games. In summary, the features of MAS directly map onto game-theoretic concepts: communication equates to signaling or messaging in games, coordination mirrors signaling in games, coordination corre-sponds to equilibrium selection or coalition formation, and autonomy reflects independent, utility-maximizing players. LLM-based MAS introduces unique characteristics. In these systems, agents are driven by large language models that interact through natural language. The effective action space consists of potential utterances or text sequences, which is combinatorial and open-ended. Communication is inher-ently language-driven, enabling rich negotiation and explana-tion. Recent studies have shown that LLM agents can develop communication protocols and roles spontaneously. Through dialogue, these agents can negotiate resources, delegate tasks, or balance cooperation and competition, demonstrating emer-gent behaviors such as cooperation, competition, negotiation, and social interaction. This capacity for strategic communi-cation under incentives explicitly links LLM interactions to classical signaling theory. These characteristics suggest that LLM-based agents operate as game-theoretic players with highly expressive, language-driven strategies and dynamic in-formation exchange, leading to more complex and dynamic interactions than traditional MAS models. 

2.3 Taxonomy based on Game-Theoretic Elements 

In MAS, agents don’t operate in isolation; they interact, in-fluence, and respond to one another. Game theory provides a formal framework for modeling these interactions by treating agents as strategic players whose choices are interdependent. We organize our analysis around four core game-theoretic elements: Player, Strategy, Payoff, and Information. These elements form the foundational components of any strategic model. We focus on these four dimensions because together they define who the agents are, what actions they can take, what they value, and what they know. Notably, we do not treat Action as separate from Strategy, as an action is simply a specific strategy choice. Additionally, we exclude Equi-librium as a primitive element since it is a derived outcome based on the other four components. In a normal-form game, the strategic interaction is fully determined by the players, their strategy sets, and their payoff functions. We include In-formation as a distinct dimension to account for the agents’ private knowledge or type structure, distinguishing between complete- and incomplete-information settings. These elements interact dynamically. Each player i has a strategy space Si and payoff ui, but which strategies are chosen depends on the information available to i. Payoffs dictate incentives: given the available information, a player selects the strategy that maximizes its utility. Conversely, the strategies chosen and their outcomes can reveal additional in-formation, thereby influencing subsequent payoffs and deci-sions. Figure 1 illustrates these interdependencies: players (P) choose strategies (S) based on payoff incentives and in-formation signals, and the resulting profile produces payoffs and potentially new information. 

# 3 Game-Theoretic Elements in MAS 

3.1 Players 

In LLM-based multi-agent systems, agents (players) engage in interactions that extend far beyond traditional two-player games. Depending on their design and objectives, these agents may differ in capabilities, goals, or access to infor-mation, yielding heterogeneous populations that give rise to diverse strategic dynamics. From a game-theoretic perspec-tive, these interactions can be broadly categorized into co-operative, competitive, and mixed-motive systems, reflecting varying degrees of alignment among agents’ objectives, as il-lustrated in Figure 2. 

> Figure 2: Illustration of three interaction structures among LLM-based players.

Cooperative Systems. Cooperative systems involve agents pursuing a shared or aligned objective, often formalized through a welfare function W (a) that aggregates individual utilities. 

W (a) = f  u1(a), . . . , u N (a), a∗ = arg max  

> a∈A

W (a),

where A = Q 

> i

Ai denotes joint action space, and f : RN →

R is an aggregation map. A common choice is a weighted sum W (a) = P 

> i

wiui(a) (linear f ), while complex sce-narios may require nonlinear aggregation for fairness or risk-sensitive goals. Cooperative LLM agents effectively form an optimistic team, sharing information and coordinating to find the action profile a∗ that maximizes joint welfare. Recent studies have demonstrated the effectiveness of co-operative LLM agents on challenging tasks [Zhu et al. , 2025]. For instance, the Chain-of-Agents (CoA) framework [Zhang 

et al. , 2024c] decomposes a long-context problem into seg-ments handled by sequential “worker” agents followed by a “manager” agent, enabling complex reasoning over ex-tended inputs. The COPPER [Bo et al. , 2024] system uses self-reflective agents whose outputs are fine-tuned through counterfactual rewards to improve contribution quality. Role-based dialogue frameworks like CAMEL [Li et al. , 2023] use “assistant–user” role-playing to simulate human-like team-work, generating emergent collaboration behaviors. Other architectures, like MetaGPT [Hong et al. , 2024], assign spe-cialized roles such as project manager, architect, and devel-oper to agents within software engineering tasks, while Au-toGen [Wu et al. , 2024] provides a flexible framework for orchestrating multi-agent interactions. Empirical evaluations have shown that language-mediated coordination can signifi-cantly improve performance on tasks like question answering, coding, and planning. 

Competitive Systems. In competitive systems, agents have conflicting objectives. These can be formalized as (possi-bly zero-sum) games G = ( N , A i, u i), where each agent 

i chooses a strategy σi ∈ ∆( Ai) to maximize its expected payoff Ui(σ). A Nash equilibrium (σ∗ 

> 1

, . . . , σ ∗ 

> N

) satisfies the mutual best-response conditions: 

Ui(σ∗ 

> i

, σ ∗−i) ≥ Ui(σi, σ ∗−i) ∀i, σ i.For two-player zero-sum games, this reduces to the classical minimax equilibrium: 

max 

> σ1

min 

> σ2

U1(σ1, σ 2) = min 

> σ2

max 

> σ1

U1(σ1, σ 2).

LLM agents have been evaluated in adversarial bench-marks and games. For example, GTBench [Duan et al. , 2024] is a game-theoretic benchmark with ten classic games (board, card, negotiation, auctions) probing pure strategic reasoning. Another testbed, GameBench [Costarelli et al. , 2024], cov-ers nine diverse game environments, revealing that GPT-4 still struggles to match human-level performance in zero-sum games. Specialized competitive frameworks, such as multi-agent debate systems [Cheng et al. , 2024], facilitate agents arguing opposing viewpoints, thereby refining their answers. Empirical studies report that, though limitations in multi-step strategy anticipation remain, such competitive exercises help LLMs exhibit long-term planning and opponent modeling. 

Mixed-Motive Systems. Most realistic multi-agent envi-ronments blend cooperation and competition. In mixed-motive games, each agent balances self-interest and collective welfare, captured by: 

ui(a) = αi vself  

> i

(a) + (1 − αi) vcol (a), αi ∈ [0 , 1] ,

where vself  

> i

is agent i’s private payoff and vcol is a shared team payoff. The parameter αi encodes the degree of self-interest versus collaboration. Designing incentives in such systems often involves mechanism design: for example, introducing transfers ti(a) that reward or penalize agents so that truthful, team-oriented behavior is an equilibrium. A common require-ment is incentive compatibility (IC) and budget balance (BB), consistent with classical mechanism design principles [Mas-Colell et al. , 1995]: IC: σ∗ 

> i

∈ arg max 

> σi

E[ui(a) + ti(a)] , BB: X

> i

E[ti(a)] = 0 .

These constraints ensure that following the intended strategy is optimal for each agent without external subsidies. Recent work has begun to explore mixed-motive LLM sys-tems. Orner et al. [Orner et al. , 2025] investigate explana-tion methods for agents in mixed-motive games such as vari-ants of Diplomacy and iterated Prisoner’s Dilemma, high-lighting how interleaved cooperation and competition cre-ate complex social dilemmas. Complementing these em-pirical investigations, Duetting et al. [Duetting et al. , 2024] adopt a mechanism-design perspective, framing LLMs as bidding agents within an auction for generated content that is formally designed to ensure incentive compatibility and truthful reporting. Furthermore, the MAC-SPGG frame-work [Liang et al. , 2025] addresses the strategic alignment of motives through sequential public-goods games. This frame-work proves that by precisely tuning public-goods rewards, a Subgame Perfect Nash Equilibrium (SPNE) can be induced to foster universal cooperation. Collectively, these diverse methodologies—ranging from auction theory to sequential game modeling—demonstrate that carefully structured incen-tives can effectively align LLM motives and sustain stable collaboration, even when individual objectives diverge. 

3.2 Strategies and Equilibrium 

Effective strategy development is crucial for multi-agent LLM systems, where agents often need to account for both cooperation and competition. Building on language-mediated strategic agents like CICERO [(FAIR)† et al. , 2022], the LLM-Nash framework [Zhu, 2025] models each agent as se-lecting reasoning prompts as strategies, defining a “reasoning equilibrium” over the prompt space. Unlike classical games with fully rational players, LLM–Nash captures bounded ra-tionality by explicitly modeling the reasoning process. It shows that these reasoning equilibria can diverge from classi-cal Nash outcomes, reflecting the unique strategic behaviors of LLM agents. This highlights that LLMs can converge to new equilibrium-like behaviors through iterative dialogue and prompting, effectively approximating game-theoretic reason-ing in non-cooperative settings. Self-play is another powerful tool for strategy formation. In self-play, an agent competes against copies of itself, refin-ing its strategy through trial and error [Zhang et al. , 2024b]. This approach has been used in multi-agent reinforcement learning (MARL), where agents learn not only by interacting with a fixed environment but also by improving their strate-gies over time based on their own behavior. Recent studies show that self-play can significantly enhance LLM strategies by enabling agents to explore a wide range of tactics and counter-strategies. Through reinforcement learning (RL) and policy optimization, LLMs can adjust their behavior to im-prove their outcomes in repeated or evolving strategic envi-ronments. This iterative process allows LLMs to converge to-ward optimal strategies. SPIRAL [Liu et al. , 2025] leverages this idea by having two LLM agents play multi-turn, zero-sum language games against each other. It employs a fully online, multi-turn, multi-agent RL system so that all model parameters are continually updated during self-play. SPIRAL demonstrates that competitive self-play not only drives agents toward strong gameplay strategies (akin to reaching equilib-rium) but also endows them with general problem-solving tactics. MARSHAL [Yuan et al. , 2025] framework similarly shows that self-play in strategic games can generalize to di-verse reasoning tasks. MARSHAL uses turn-level advantage estimation and self-play in both cooperative and competitive games, finding that it strengthens LLM strategic abilities and yields gains on multi-agent benchmarks. 

3.3 Payoffs 

In multi-agent systems, the payoff structure plays a critical role in designing effective mechanisms. Each agent’s goal is to maximize its own reward through some strategies, but in a multi-agent setting, the rewards of the agents are of-ten not aligned. The challenge lies in designing appropriate reward mechanisms that align individual objectives with the collective goals, ensuring that agents’ behavior is optimized for both individual success and system-wide efficiency. 

Mechanism Design. Effective mechanism design requires that the incentive structures of agents align with the sys-tem’s collective objective. In a multi-agent environment, let each agent i have an individual reward function Ri(s, a ), and the system’s overall objective is represented by a social wel-fare function W (s, a ) = f  R1(s, a ), . . . , R N (s, a ), where 

f (·) might represent a combination of fairness, efficiency, or safety considerations, aligning conceptually with welfare maximization in microeconomics [Mas-Colell et al. , 1995]. Misalignment arises when maximizing Ri does not necessar-ily improve W , and this misalignment can lead to suboptimal outcomes for the system. To quantify this misalignment, we define each agent’s de-viation gain under a joint policy π = ( π1, . . . , π N ) as 

∆i(π) = max 

> π′
> i

E[Ri(s, a ′

> i

, a −i)] − E[Ri(s, a )] ,

which indicates how much agent i can gain by unilaterally deviating from the joint policy. A cooperative mechanism should minimize these deviation gains while improving the collective objective. The general alignment framework can be expressed as: 

max  

> M

E[W (s, a )] 

s.t. E[Ri(s, a )] ≥ E[Ri(s, a ′

> i

, a −i)] , ∀i, ∀π′

> i

,

where M represents the designed reward or transfer mecha-nism ensuring incentive compatibility. This alignment mech-anism ensures that agents’ behaviors are motivated by the col-lective good while minimizing selfish deviations. 

Reward Shaping. Reward shaping provides a practical way to adjust agent incentives while preserving local auton-omy. A shaped reward might take the form: 

R′

> i

(s, a, s ′) = Ri(s, a, s ′) + γ Φi(s′) − Φi(s),

where Φi(s) is a potential function that adjusts the temporal credit assignment without altering the cooperative optimum. Through this approach, reward shaping can accelerate con-vergence and maintain stability across agents with heteroge-neous goals. For instance, COPPER [Bo et al. , 2024] intro-duces self-reflection as a method for improving cooperation, where agents adjust their incentives internally to better ful-fill collaborative tasks. While no explicit mathematical form of reward adjustment is given, this approach can be under-stood as the introduction of new incentive mechanisms that encourage agents to reflect on their contributions to coopera-tion. In combination with mechanism design, reward shaping ensures that agents remain aligned with system-level goals while maintaining their individual autonomy. This method is particularly useful in multi-agent systems where agents must balance personal incentives with collective performance. 

Penalty and Regulation Mechanisms. In addition to re-ward shaping, penalty and regulation mechanisms are es-sential tools for regulating agent behavior. By introducing a penalty function Ψi(s, a ), undesirable behaviors such as eth-ical violations or resource misuse can be discouraged, further driving agents toward socially beneficial behaviors. The reg-ulated reward can then be expressed as: 

˜Ri(s, a, s ′) = R′

> i

(s, a, s ′) − λi Ψi(s, a ),

where λi > 0 controls the strength of the regulatory enforce-ment. This formulation integrates soft constraints directly into the learning signal, allowing agents to trade off perfor-mance and compliance dynamically. For safety-critical or 

> Figure 3: Illustration of information structures among LLM-based players.

ethically sensitive systems, such as those involving human in-teractions or resource management, penalty-based regulation is essential to ensure that agents do not exploit or harm the system. Explicit regulatory constraints can also be imposed at the policy level, where actions violating safety or ethical norms are excluded from the feasible set: 

πi(a|s) = 0 if Ψi(s, a ) > τ i,

where τi is a threshold above which actions are deemed un-acceptable. Recent studies [Reid et al. , 2025; Li et al. , 2024; Zhang et al. , 2024a] have highlighted the importance of incorporating regulatory signals in multi-agent systems, showing that inte-grating explicit or implicit regulatory feedback—whether lin-guistic, feedback-based, or self-penalization—can substan-tially improve both ethical compliance and long-term system stability. Empirical frameworks such as FinCon [Yu et al. ,2024], CORY [Ma et al. , 2024], and ProAgent [Zhang et al. , 2024a] demonstrate how structured reward signals, ver-bal feedback, or reflective evaluation can operationalize such alignment strategies in practice. 

3.4 Information 

The information structure of a system fundamentally deter-mines the strategic complexity and coordination potential among agents. Let Ii(s) denote the information available to agent i at state s. The strategic formulation of such systems can be expressed as a Bayesian game: 

Ui(ai, a −i) = Eθi∼p(θi|I i)[Ri(s, a i, a −i, θ i)] ,

where θi represents private information or beliefs. This con-nects LLM-mediated interaction with classical incomplete-information games. The equilibrium efficiency thus depends critically on how Ii is structured and shared across agents. 

Full Observability. Under complete information, all agents share the global state and can observe each other’s actions and payoffs (as shown in Figure 3 (a)). Such transparency enables joint planning but requires greater communication or computation. The AgentVerse framework [Chen et al. ,2024] demonstrates how full information sharing enhances performance in expert-agent collaboration. Similarly, the Chain-of-Agents [Zhang et al. , 2024c] and COPPER [Bo 

et al. , 2024] frameworks leverage complete observability to facilitate seamless sequential reasoning and reflective adap-tation. Full observability fosters emergent role-based col-laboration and stronger generalization. Even in simpler co-operative setups like CAMEL [Li et al. , 2023], the shared full state empowers the two role-assigned LLMs to coordi-nate tightly and outperform single-agent baselines. From a game-theoretic lens, complete information approximates a common-knowledge environment, where every player’s be-liefs about others’ knowledge converge, simplifying equilib-rium computation. 

Partial Observability. By contrast, incomplete informa-tion is more realistic but challenging. Each agent receives a signal or observation θi, and must select actions according to a belief distribution bi(s) = P (s | θi) (as shown in Figure 3 (b)). The optimal policy in such environments maximizes the expected return: 

π∗ 

> i

(θi) = arg max 

> πi

Es∼bi, a i∼πi(θi)[Ri(s, a i, a −i)] .

Li et al. [Li et al. , 2024] introduce a language-grounded multi-agent reinforcement learning (MARL) pipeline where agents learn to communicate in natural language, which ac-celerates the emergence of effective protocols and even gener-alizes zero-shot to new teammates. Additional studies on re-liable decision-making [Lee et al. , 2025] examine how agents coordinate effectively under distributed or incomplete infor-mation. It demonstrates that simple voting or independent aggregation among agents often outperforms complex itera-tive feedback loops, as error propagation from multi-round dialogue may destabilize the system. Incomplete information games have been explicitly tested, ranging from multi-issue negotiations [Abdelnabi et al. ,2023] to games like Poker and auctions [Hua et al. , 2024]. Both works reveal that while naive LLMs often struggle with large hidden states or deviate from game-theoretic ratio-nality, their performance improves significantly with struc-tured reasoning. Benchmarks further illustrate the impact of limited information. GTBench [Duan et al. , 2024] finds that LLMs fail completely informed deterministic games (like Tic-Tac-Toe ) but can remain competitive in chance-driven or stochastic games. Real-world style simulations also highlight these effects. Park et al. [Park et al. , 2023] show that 25 generative agents can coordinate complex social tasks through local memory and iterative dialogue. Piao et al. [Piao 

et al. , 2025] scale this to 10,000 agents, demonstrating how macro-phenomena like polarization emerge from decentral-ized exchanges. Together, they prove that agents can bridge partial observability by using inter-agent communication to infer global context from local interactions. 

# 4 Discussion 

In this section, we first review representative benchmarks in this field. Section 4.2 then analyzes specific examples to show how game theory empowers the design of LLM agents. Fi-nally, Section 4.3 identifies critical gaps and proposes poten-tial research directions for the field. 

4.1 Benchmark 

Benchmarks for LLM-based multi-agent systems are catego-rized into two primary groups focusing on fundamental capa-bilities and specialized domain applications respectively. 

General-purpose Cognitive Benchmarks. These bench-marks evaluate coordination and reasoning by decoupling core capabilities from domain knowledge. For instance, GAIA requires agents to execute multi-step tasks, such as re-trieving historical data to perform calculations, to test their ability to decompose problems autonomously. 

Domain-specific evaluation frameworks. These shift the focus toward functional utility within vertical sectors. Envi-ronments, such as software engineering repositories or clini-cal diagnostic simulators, require agents to possess deep on-tological knowledge and specialized tool-calling proficiency. Evaluation here is centered on the agent’s adherence to pro-fessional protocols and its efficiency in executing collabora-tive workflows that mimic real-world industry demands. 

4.2 Case Studies   

> Figure 4: Framework and performance of SWE-Debate. (Left) The workflow incorporates a multi-agent debate mechanism to iter-atively refine modification plans. (Right) This competitive architec-ture achieves a SOTA 41.4% Pass@1 (DeepSeek-V3), significantly outperforming non-competitive baselines and validating the efficacy of adversarial interactions.

Case One: A carefully orchestrated competitive mech-anism in MAS is pivotal for automated software repair. 

For example, SWE-Debate [Li et al. , 2025] formulates software resolution as a non-cooperative game, leveraging multi-path fault traces as initial strategies to escape single-agent local optima. Through structured three-round debates, agents are forced into a “defend-and-critique” cycle that ex-poses latent architectural trade-offs. This adversarial tension allows the discriminator to synthesize robust evidence, re-solving ambiguities to reach a globally optimal equilibrium. 

Case Two: Reward shaping and regulation can align decentralized incentives with systemic stability in finan-cial decision-making. Table 1: A Taxonomy of Representative MAS Benchmarks. Benchmarks are grouped by application domain, reflecting the shift from general-purpose coordination to domain-specialized multi-agent collaboration.                           

> Category Domain Representative Benchmarks General General / Mixed MultiAgentBench [Zhu et al. , 2025]; GAIA [Mialon et al. , 2024]; AgentVerse [Chen et al. ,2024]; Magentic-One [Fourney et al. , 2024]
> Specialized
> Data Science MLE-bench [Chan et al. , 2025]; DSBench [Jing et al. , 2025]; DABstep [Egg et al. , 2025]
> (autonomous ML engineering)
> Software Engineering SWE-bench [Jimenez et al. , 2024]; rSDE-bench [Hu et al. , 2024] (repository-level reasoning)
> Finance FinBen [Xie et al. , 2024b]; AI-Trader [Fan et al. , 2025]; FinGAIA [Zeng et al. , 2025] (role-specialized decision making)
> Planning / Robotics TravelPlanner [Xie et al. , 2024a]; MAP-THOR [Nayak et al. , 2024]; PARTNR [Chang et al. ,2025] (long-horizon coordination)
> Figure 5: Reward shaping in the FinCon framework. (Left) A de-centralized multi-agent financial system coordinated by a manager agent, where agent behaviors are guided through shaped rewards de-rived from heterogeneous financial observations. (Right) Empirical comparison illustrating how reward shaping and regulatory penalties influence agent performance across assets and backbone models.

Financial markets are inherently complex systems charac-terized by intricate interdependencies, with theoretical foun-dations deeply rooted in market microstructure under asym-metric information [Xu et al. , 2025]. In such environments where agents face a fundamental conflict between private profit vself i and market stability vcol , the FinCon [Yu et al. ,2024] framework demonstrate that modifying reward sig-nals is essential for ensuring ethical and stable outcomes. From a game-theoretic perspective, by applying reward shap-ing through potential functions Φi(s), the system acceler-ates convergence toward cooperative goals without altering the optimal policy, while regulatory mechanisms integrate penalty functions Ψi(s, a ) to discourage undesirable behav-iors like resource misuse or ethical violations. These struc-tured reward interventions, which can include verbal feed-back or self-penalization, effectively steer decentralized LLM agents toward a stable equilibrium where individual motives are aligned with the collective good. 

4.3 Future Directions 

Hierarchical Superagent Orchestration. Current MAS largely rely on predefined protocols or static role assign-ments. Future research should transition toward Autonomous Superagents characterized by two core functionalities: 1. Resource Planning : The rational orchestration of existing agents and tools through high-level meta-coordination to resolve complex, long-context tasks. 2. Generative Agent Synthesis : The dynamic creation of specialized novel agents tailored to emergent environ-mental requirements. These systems must act as mechanism designers to resolve ar-chitectural trade-offs via collaborative synthesis or structured adversarial tension. 

Agentic Evolution. To transcend the limitations of fixed-policy interactions, Agentic RL serves as a critical paradigm for continuous strategy evolution. Research should model prompt selection as strategic actions, identifying a “reason-ing equilibrium” that accounts for LLMs’ bounded rational-ity. Leveraging competitive self-play and turn-level advan-tage estimation, agents can update parameters dynamically. This iterative process enables convergence toward optimal strategies in both zero-sum and cooperative language games. 

Theoretical Formalization Current solutions rely on heuristic implementations but lack strict mathematical for-malization. This limits the theoretical analysis of multi-agent interactions. Future work should establish a rigorous frame-work to model decision-making under incomplete informa-tion, such as through the lens of Bayesian games. A key chal-lenge involves designing incentive-compatible protocols that guarantee truthful signaling and align decentralized actions with systemic stability. 

# 5 Conclusion 

We provide a comprehensive survey of LLM-based MAS through a game-theoretic lens, offering a unified theoretical foundation for this rapidly evolving field. By organizing cur-rent research around the core elements of players, strategies, payoffs, and information, we establish a systematic frame-work to categorize diverse agent behaviors and interaction dynamics. Our analysis reveals that while LLMs excel in high-level reasoning and strategic communication, signifi-cant gaps remain in robust equilibrium selection and incentive compatibility in complex, partially observable environments. We conclude by highlighting forward-looking research direc-tions, emphasizing that the integration of classical game the-ory with LLMs will be pivotal for developing more reliable, autonomous, and socially intelligent multi-agent systems. References 

[Abdelnabi et al. , 2023] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, et al. Llm-deliberation: Evaluating llms with interactive multi-agent negotiation game. arXiv preprint arXiv:2309.17234 , 2023. [Agashe et al. , 2025] Saaket Agashe, Yue Fan, Anthony Reyna, and Xin Eric Wang. LLM-coordination: Evalu-ating and analyzing multi-agent coordination abilities in large language models. In Proc. NAACL , pages 8038– 8057, 2025. [Ashery et al. , 2025] Ariel Flint Ashery, Luca Maria Aiello, and Andrea Baronchelli. Emergent social conventions and collective bias in llm populations. Sci. Adv. ,11(20):eadu9368, 2025. [Bo et al. , 2024] Xiaohe Bo, Zeyu Zhang, Quanyu Dai, et al. Reflective multi-agent collaboration based on large lan-guage models. Proc. NeurIPS , 37:138595–138631, 2024. [Chan et al. , 2025] Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. In Proc. ICLR ,2025. [Chang et al. , 2025] Matthew Chang, Gunjan Chhablani, Alexander Clegg, et al. PARTNR: A benchmark for plan-ning and reasoning in embodied multi-agent tasks. In 

Proc. ICLR , 2025. [Chen et al. , 2024] Weize Chen, Yusheng Su, Jingwei Zuo, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In Proc. ICLR , 2024. [Cheng et al. , 2024] Pengyu Cheng, Yong Recognition Dai, Tianhao Hu, et al. Self-playing adversarial language game enhances llm reasoning. Proc. NeurIPS , 37:126515– 126543, 2024. [Costarelli et al. , 2024] Anthony Costarelli, Mat Allen, Ro-man Hauksson, et al. Gamebench: Evaluating strate-gic reasoning abilities of llm agents. arXiv preprint arXiv:2406.06613 , 2024. [Duan et al. , 2024] Jinhao Duan, Renming Zhang, James Diffenderfer, et al. Gtbench: Uncovering the strategic rea-soning limitations of llms via game-theoretic evaluations. In Proc. NeurIPS , 2024. [Duetting et al. , 2024] Paul Duetting, Vahab Mirrokni, Re-nato Paes Leme, et al. Mechanism design for large lan-guage models. In Proc. ACM Web Conf. , pages 144–155, 2024. [Egg et al. , 2025] Alex Egg, Martin Iglesias Goyanes, Friso Kingma, et al. Dabstep: Data agent benchmark for multi-step reasoning. arXiv preprint arXiv:2506.23719 , 2025. [Epstein, 1999] Joshua M. Epstein. Agent-based computa-tional models and generative social science. Complexity ,4(5):41–60, 1999. [(FAIR)† et al. , 2022] Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. 

Science , 378(6624):1067–1074, 2022. [Fan et al. , 2025] Tianyu Fan, Yuhao Yang, Yangqin Jiang, et al. Ai-trader: Benchmarking autonomous agents in real-time financial markets. arXiv preprint arXiv:2512.10971 ,2025. [Fourney et al. , 2024] Adam Fourney, Gagan Bansal, Hus-sein Mozannar, et al. Magentic-one: A generalist multi-agent system for solving complex tasks. arXiv preprint arXiv:2411.04468 , 2024. [Fudenberg and Levine, 1998] Drew Fudenberg and David K Levine. The theory of learning in games ,volume 2. MIT press, 1998. [Hong et al. , 2024] Sirui Hong, Mingchen Zhuge, Jonathan Chen, et al. MetaGPT: Meta programming for a multi-agent collaborative framework. In Proc. ICLR , 2024. [Hu et al. , 2024] Yue Hu, Yuzhu Cai, Yaxin Du, et al. Self-evolving multi-agent collaboration networks for software development. arXiv preprint arXiv:2410.16946 , 2024. [Hua et al. , 2024] Wenyue Hua, Ollie Liu, Lingyao Li, et al. Game-theoretic llm: Agent workflow for negotiation games. arXiv preprint arXiv:2411.05990 , 2024. [Huang et al. , 2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In Proc. ICML , pages 9118–9147, 2022. [Jimenez et al. , 2024] Carlos E. Jimenez, John Yang, Alexander Wettig, et al. Swe-bench: Can language models resolve real-world github issues? In Proc. ICLR ,2024. [Jing et al. , 2025] Liqiang Jing, Zhehui Huang, Xiaoyang Wang, et al. Dsbench: How far are data science agents from becoming data science experts? In Proc. ICLR , 2025. [Kamenica and Gentzkow, 2011] Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review , 101(6):2590–2615, 2011. [Lee et al. , 2025] Xian Yeow Lee, Shunichi Akatsuka, La-sitha Vidyaratne, et al. Reliable decision-making for multi-agent llm systems. arXiv preprint arXiv:2406.04092 ,2025. [Li et al. , 2023] Guohao Li, Hasan Abed Al Kader Ham-moud, Hani Itani, et al. Camel: Communicative agents for mind exploration of large language model society. Proc. NeurIPS , 36:51991–52008, 2023. [Li et al. , 2024] Huao Li, Hossein Nourkhiz Mahjoub, Be-hdad Chalaki, et al. Language grounded multi-agent rein-forcement learning with human-interpretable communica-tion. In Proc. NeurIPS , volume 37, pages 87908–87933, 2024. [Li et al. , 2025] Han Li, Yuling Shi, Shaoxin Lin, et al. Swe-debate: Competitive multi-agent debate for software issue resolution. arXiv preprint arXiv:2507.23348 , 2025. [Liang et al. , 2025] Yunhao Liang, Yuan Qu, Jingyuan Yang, et al. Everyone contributes! incentivizing strategic coop-eration in multi-llm systems via sequential public goods games. arXiv preprint arXiv:2508.02076 , 2025. [Littman, 1994] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In 

Proc. ICML , pages 157–163, 1994. [Liu et al. , 2025] Bo Liu, Leon Guertler, Simon Yu, et al. Spiral: Self-play on zero-sum games incentivizes rea-soning via multi-agent multi-turn reinforcement learning. 

arXiv preprint arXiv:2506.24119 , 2025. [Ma et al. , 2024] Hao Ma, Tianyi Hu, Zhiqiang Pu, et al. Co-evolving with the other you: Fine-tuning llm with sequen-tial cooperative multi-agent reinforcement learning. Proc. NeurIPS , 37:15497–15525, 2024. [Mas-Colell et al. , 1995] Andreu Mas-Colell, Michael Den-nis Whinston, Jerry R Green, et al. Microeconomic theory ,volume 1. Oxford university press New York, 1995. [Mialon et al. , 2024] Gr´ egoire Mialon, Cl´ ementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: a benchmark for general AI assistants. In Proc. ICLR ,2024. [Nash, 1950] John Nash. Equilibrium points in n-person games. Proceedings of the National Academy of Sciences ,36(1):48–49, 1950. [Nayak et al. , 2024] Siddharth Nayak, Adelmo Morrison Orozco, Marina Ten Have, et al. MAP-THOR: Bench-marking long-horizon multi-agent planning frameworks in partially observable environments. In Proc. Multi-modal Found. Model Meets Embodied AI Workshop, ICML 2024 ,2024. [Orner et al. , 2025] Maayan Orner, Oleg Maksimov, Akiva Kleinerman, et al. Explaining decisions of agents in mixed-motive games. In Proc. AAAI , pages 23267–23275, 2025. [Park et al. , 2023] Joon Sung Park, Joseph C. O’Brien, Car-rie J. Cai, et al. Generative agents: Interactive simulacra of human behavior. Proc. UIST , pages 1–22, 2023. [Piao et al. , 2025] Jinghua Piao, Yuwei Yan, Jun Zhang, et al. Agentsociety: Large-scale simulation of llm-driven generative agents advances understanding of human be-haviors and society. arXiv preprint arXiv:2502.08691 ,2025. [Qian et al. , 2024] Chen Qian, Wei Liu, Hongzhang Liu, et al. Chatdev: Communicative agents for software de-velopment. In Proc. ACL , pages 15174–15186, 2024. [Reid et al. , 2025] Alistair Reid, Simon O’Callaghan, Liam Carroll, and Tiberio Caetano. Risk analysis techniques for governed llm-based multi-agent systems. arXiv preprint arXiv:2508.05687 , 2025. [Sandholm, 1999] Tuomas W. Sandholm. Distributed ratio-nal decision making. In Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence , pages 201– 258. MIT Press, 1999. [Shoham and Leyton-Brown, 2008] Yoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and logical foundations . Cambridge University Press, 2008. [Silver and others, 2016] David Silver et al. Mastering the game of go with deep neural networks and tree search. Na-ture , 529(7587):484–489, 2016. [Wei et al. , 2022] Jason Wei, Xuezhi Wang, Dale Schuur-mans, et al. Chain-of-thought prompting elicits reasoning in large language models. In Proc. NeurIPS , volume 35, pages 24824–24837, 2022. [Wu et al. , 2024] Qingyun Wu, Gagan Bansal, Jieyu Zhang, et al. Autogen: Enabling next-gen LLM applications via multi-agent conversations. In First Conference on Lan-guage Modeling , 2024. [Xie et al. , 2024a] Jian Xie, Kai Zhang, Jiangjie Chen, et al. Travelplanner: A benchmark for real-world planning with language agents. In Proc. ICML , 2024. [Xie et al. , 2024b] Qianqian Xie, Weiguang Han, Zhengyu Chen, et al. Finben: A holistic financial benchmark for large language models. Proc. NeurIPS , 37:95716–95743, 2024. [Xu et al. , 2025] Yuanjian Xu, Jianing Hao, Guang Zhang, et al. Finripple: Aligning large language models with fi-nancial market for event ripple effect awareness. In Proc. ACL Findings , 2025. [Yu et al. , 2024] Yangyang Yu, Zhiyuan Yao, Haohang Li, et al. Fincon: A synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced finan-cial decision making. Proc. NeurIPS , 37:137010–137045, 2024. [Yuan et al. , 2025] Huining Yuan, Zelai Xu, Zheyue Tan, et al. Marshal: Incentivizing multi-agent reason-ing via self-play with strategic llms. arXiv preprint arXiv:2510.15414 , 2025. [Zeng et al. , 2025] Lingfeng Zeng, Fangqi Lou, Zixuan Wang, et al. Fingaia: A chinese benchmark for AI agents in real-world financial domain. arXiv preprint arXiv:2507.17186 , 2025. [Zhang et al. , 2024a] Ceyao Zhang, Kaijie Yang, Siyi Hu, et al. Proagent: building proactive cooperative agents with large language models. In Proc. AAAI , pages 17591– 17599, 2024. [Zhang et al. , 2024b] Ruize Zhang, Zelai Xu, Chengdong Ma, et al. A survey on self-play methods in reinforcement learning. arXiv preprint arXiv:2408.01072 , 2024. [Zhang et al. , 2024c] Yusen Zhang, Ruoxi Sun, Yanfei Chen, et al. Chain of agents: Large language models collabo-rating on long-context tasks. Proc. NeurIPS , 37:132208– 132237, 2024. [Zhu et al. , 2025] Kunlun Zhu, Hongyi Du, Zhaochen Hong, et al. MultiAgentBench : Evaluating the collaboration and competition of LLM agents. In Proc. ACL , pages 8580– 8622, 2025. [Zhu, 2025] Quanyan Zhu. Reasoning and behavioral equi-libria in llm-nash games: From mindsets to actions. arXiv preprint arXiv:2507.08208 , 2025.