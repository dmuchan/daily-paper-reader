Title: Evaluation of Large Language Models in Legal Applications: Challenges, Methods, and Future Directions

URL Source: https://arxiv.org/pdf/2601.15267v1

Published Time: Thu, 22 Jan 2026 02:12:32 GMT

Number of Pages: 12

Markdown Content:
# Evaluation of Large Language Models in Legal Applications: Challenges, Methods, and Future Directions 

# Yiran Hu 1,2,3∗ , Huanghai Liu 1∗ , Chong Wang 1∗ , Kunran Li 1 , Tien-Hsuan Wu 2 , Haitao Li 1 , Xinran Xu 4 , Siqing Huo 3 , Weihang Su 1 , Ning Zheng 1 , Siyuan Zheng 4 , Qingyao Ai 1 , Yun Liu 1 , Renqun Bian 5 , Yiqun Liu 1 , Charles L.A. Clarke 3 , Weixing Shen 1 and Ben Kao 2†

> 1

# Tsinghua University 2The University of Hong Kong 3University of Waterloo 

> 4

# Shanghai Jiaotong University 5Peking University huyr17@outlook.com, liuhh23@mails.tsinghua.edu.cn 

# Abstract 

Large language models (LLMs) are being increas-ingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal set-tings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reason-ing processes and trustworthy issues such as fair-ness and reliability. Systematic evaluation of LLM performance in legal tasks has therefore become es-sential for their responsible adoption. This survey identifies key challenges in evaluating LLMs for legal tasks grounded in real-world legal practice. We analyze the major difficulties involved in as-sessing LLM performance in the legal domain, in-cluding outcome correctness, reasoning reliability, and trustworthiness. Building on these challenges, we review and categorize existing evaluation meth-ods and benchmarks according to their task design, datasets, and evaluation metrics. We further dis-cuss the extent to which current approaches ad-dress these challenges, highlight their limitations, and outline future research directions toward more realistic, reliable, and legally grounded evaluation frameworks for LLMs in legal domains. 

# 1 Introduction 

As large language models (LLMs) continue to advance in capability, they are increasingly being applied to the legal domain [Sun et al. , 2024; Li et al. , 2024a]. Typical ap-plications include assisting laypersons in understanding le-gal issues, supporting lawyers in legal practice, and pro-viding auxiliary assistance to judges in judicial decision-making [Su et al. , 2024; Li et al. , 2024a; Gao et al. , 2024; Chen et al. , 2024]. This raises a fundamental question: Are LLMs truly qualified to enter the legal domain, and which types of legal tasks can they reliably perform? 

> ∗Equal contribution.
> †Corresponding author.

Previous studies [Katz et al. , 2024; Freitas and Gomes, 2023] have shown that LLMs are able to pass legal exami-nations, and some works [Xiao et al. , 2018; Lyu et al. , 2023] further demonstrate that LLMs can achieve high performance in legal judgment prediction tasks. However, due to the in-trinsic complexity of real-world legal scenarios and the de-manding nature of legal reasoning, evaluating LLMs solely based on standardized exam-style questions or prediction ac-curacy is insufficient to comprehensively assess their perfor-mance in real legal applications. Consequently, a growing number of benchmarks [Fei et al. , 2023; Guha et al. , 2024; Li et al. , 2024b] have been proposed to evaluate LLMs from multiple perspectives in legal tasks. Evaluating LLMs in legal tasks is significantly more chal-lenging than conducting general-purpose evaluations [Huang 

et al. , 2023]. Existing studies [Hu et al. , 2025b; Liu and Li, 2024] show that LLMs are increasingly being applied in real-world court settings. In real legal applications, LLMs face many issues beyond whether an answer is correct. Evaluation should not focus solely on the final result, but also consider the reasoning process and system-level constraints. The aim of evaluating LLMs in the legal domain is to systematically and automatically assess their capabilities across multiple dimensions, reflecting not only whether models arrive at cor-rect outcomes, but also how those results are produced and under what constraints they remain valid. From the process perspective, addressing legal problems requires strong logical reasoning abilities; a model may pro-duce a legally correct answer while relying on flawed or in-valid reasoning steps. For example, when an LLM serves as a judicial assistant, a key factor in whether the public can be convinced lies in its reasoning process, such as whether it cites authentic and correct legal provisions and whether it performs logically sound inferences based on both legal rules and factual circumstances. From the constraint perspective, LLMs need to handle legal cases fairly and safely. For in-stance, an LLM should not exhibit discrimination based on gender or geographic origin when making judgments. Given that legal decisions directly affect individuals’ rights and so-cial welfare, ensuring fairness, reliability, and safety is a pre-requisite for real-world adoption [Wang et al. , 2023a]. Legal applications operate under strict normative and social con-straints: even when both the final result and the reasoning appear correct, a model may still be unsuitable for deploy-

> arXiv:2601.15267v1 [cs.CY] 21 Jan 2026 Figure 1: Overview of the proposed framework.

ment if it exhibits unfair bias, lacks robustness, or fails under adversarial or distributional shifts. In summary, when LLMs are applied to real-world legal settings, they need to be evaluated across the entire pipeline (Result, Process, Constraint). These three stages correspond to three evaluation dimensions: Output Accuracy (Result), Legal Reasoning (Process), and Trustworthiness (Con-straint) . Only through effective evaluation along these three dimensions can models be applied more reliably in prac-tice. Based on these three challenges, we categorize existing benchmarks and select representative benchmarks for analy-sis. 

Outcome Accuracy aims to assess a model’s legal knowl-edge and its overall task performance. Existing datasets are mainly constructed around legal examinations [Li et al. ,2024b; Fan et al. , 2025] and judicial decision prediction tasks [Xiao et al. , 2018], with evaluation metrics such as accuracy for multiple-choice questions, as well as ROUGE and BERT-based similarity measures for open-ended legal re-sponses. Trustworthiness evaluates the potential risks intro-duced by LLMs when performing legal tasks [Wang et al. ,2023a]. Current studies [Hu et al. , 2025b; Hu et al. , 2025a] mainly investigate ethical concerns, fairness, and robustness, often using counterfactual analysis and adversarial attacks. 

Legal Reasoning measures whether the reasoning process employed by the model aligns with legal reasoning princi-ples. Existing work [Aky¨ urek et al. , 2025] typically relies on fine-grained legal reasoning rubrics annotated by human experts to score the model’s reasoning steps. Despite the progress made by existing research, many chal-lenges remain unresolved. For instance, trustworthiness eval-uations still lack coverage of critical dimensions such as pri-vacy risks and toxicity in legal contexts. Evaluations of log-ical inference heavily rely on manually annotated rubrics, making them difficult to scale to large datasets. Moreover, certain task-specific issues, like legal hallucination, have not yet been systematically evaluated. These limitations indicate that current evaluation frameworks for LLMs in legal tasks are far from complete, leaving substantial room for further exploration. As Figure 1 shows, in this survey, we summarize existing studies on evaluating LLMs in the legal domain, identify key challenges, and outline promising future research directions. The contributions of this survey are threefold: 1. We identify major challenges in applying LLMs to the legal domain and discuss potential solutions. 2. We systematically summarize and categorize exist-ing evaluation methods and benchmarks for legal LLMs. For details, see https://github.com/THUYRan/ Evaluation-of-LLMs-in-Legal-Applications 3. We align current evaluation approaches with outstand-ing challenges, highlighting open problems and future research directions. 

# 2 Challenges: Risks and Complexity of LLM Applications in Law 

A comprehensive, scientific, and reliable evaluation of LLM performance in legal scenarios needs to start from their spe-cific applications. A prerequisite for meaningful LLM eval-uation is to understand where and how LLMs are being de-ployed in legal workflows, and how these deployments affect legal decision-making and everyday life. By systematically examining the legal tasks to which LLMs are applied, re-searchers can better identify the unique challenges involved in evaluating LLMs within the legal domain. In this section, we review representative applications of LLMs across three user groups — judges, lawyers, and the general public. Although these applications differ in func-tion, authority, and risk profile, they illustrate the breadth of LLM adoption in legal contexts and highlight why legal AI evaluation needs to be carried out across the entire pipeline. 2.1 Applications to Judges 

LLMs are increasingly incorporated into judicial workflows, supporting tasks such as case triage, legal document draft-ing, and decision review [Alon-Barkat and Busuioc, 2023]. Judicial applications place LLMs directly within the exercise of legal authority, imposing especially stringent requirements on legal accuracy, reasoning quality, and ethical compliance. Their growing adoption in judicial contexts thus underscores the need for rigorous and multidimensional evaluation. One prominent application is case classification and work-flow triage at the filing stage. LLM-based systems can as-sess case complexity and recommend appropriate procedures, improving efficiency and reducing administrative burdens. However, misclassification or incorrect issue identification may result in improper procedural routing, potentially affect-ing parties’ substantive rights. Evaluation in this context must therefore go beyond output accuracy to assess fact sensitivity and procedural reasoning. Judicial settings further amplify the importance of trust-worthy evaluation. Judicial decisions carry profound social consequences, and errors or biases introduced by LLMs may directly erode public trust in the legal system. When LLMs are used to assist judicial reasoning and document prepara-tion, including verdict consistency checks and draft genera-tion, risks such as selective adherence to AI recommenda-tions, limited accountability, and insufficient transparency ex-pose the limitations of existing evaluation approaches that fo-cus on isolated performance metrics. These challenges under-score that evaluating LLMs for judicial applications is both technically demanding and socially critical. 

2.2 Applications to Lawyers 

LLMs have rapidly become integral tools for legal profes-sionals, assisting with tasks such as legal research, con-tract review 1, document drafting, case summarization, and evidence analysis [Perlman, 2023; Tan et al. , 2023; Noonan, 2023]. By automating routine work, LLMs enable lawyers to focus on higher-value strategic tasks [Purtill, 2023; Shaver, 2024], which has driven widespread adoption in legal practice. However, even common applications, such as precedent retrieval or document summarization, pose significant eval-uation challenges. Hallucinated citations, jurisdictional mis-matches, incomplete legal coverage, and subtle terminologi-cal errors may lead to misleading outputs. As a result, evalu-ating LLMs for legal assistance requires assessing reliability, factual grounding, and jurisdictional awareness, rather than relying solely on surface-level accuracy. In litigation-related contexts, LLMs are explored for argu-ment preparation, evidence analysis, and simulated court in-teractions. These applications further highlight limitations in current evaluation approaches. Although models may gener-ate persuasive arguments, they often fail to capture procedural dynamics, ethical constraints, and adversarial context. Simi-larly, in non-litigation settings such as due diligence, compli-ance, and intellectual property analysis, errors may carry sub-stantial legal and commercial risks [Howlett and Sharp, 2023; 

> 1https://law.co/contract-review-automation

Murray, 2023]. These scenarios emphasize that evaluation must extend beyond task performance to include explainabil-ity, robustness, and alignment with professional norms. 

2.3 Applications to the General Public 

LLM-based legal tools offer new opportunities to improve public access to legal information. These tools can explain statutes, summarize judgments, retrieve relevant poli-cies 2, and guide users through basic legal procedures , re-ducing barriers posed by legal costs [Mitchell, 2025], tech-nical language [Whalen, 2015; Mart´ ınez et al. , 2024], and procedural complexity. At the same time, public-facing applications introduce par-ticularly severe evaluation challenges. Public users often lack the expertise to verify legal advice, amplifying the risks of hallucination, outdated information, or jurisdictional misap-plication. Moreover, users frequently provide vague or in-complete descriptions, requiring LLMs to infer intent, iden-tify missing information, and manage uncertainty. Evaluation must therefore consider robustness to noisy input, contextual understanding, and the model’s ability to recognize and com-municate limitations. Because public users may rely heavily on system outputs without independent verification, reliability and safety be-come paramount. While current LLMs are effective for pre-liminary legal guidance, they remain insufficiently reliable for complex or high-stakes decision-making. This gap high-lights the necessity of comprehensive evaluation frameworks capable of determining whether LLM-based legal tools can be responsibly deployed for public-facing legal assistance. 

# 3 Methods: Evaluation Benchmarks for LLMs in the Legal Domain 

As Table 2 shows, based on the different tasks that LLMs perform in real-world legal settings, many datasets have been proposed to evaluate LLMs. These evaluations are mostly conducted from the perspective of a single task and lack systematic, comprehensive, multi-dimensional assessment. However, these benchmarks provide a foundation for future multi-task evaluation. Subsequent work can build on these single-task evaluations and further integrate them. In this section, we start from these single-task evaluations and categorize them into generation tasks and decision tasks. We introduce the tasks, datasets, and evaluation metrics for each category. We then present multi-task benchmarks, se-lecting one of the most well-known benchmarks for each of the three dimensions — output accuracy, legal reasoning, and trustworthiness — for detailed discussion, followed by an overall analysis and summary of evaluation along each di-mension. 

3.1 Single-Task Benchmarks 

The evaluation of legal LLMs is typically structured around the characteristics of downstream tasks [Li et al. , 2023; Hu et al. , 2025a; Ma et al. , 2021; Chalkidis et al. , 2017; Yao et al. ,2022]. We categorize these benchmark tasks into two groups, Generation Tasks and Decision Tasks. 

> 2https://tongyi.aliyun.com/farui

Generation Tasks . These tasks [Shen et al. , 2022; Louis 

et al. , 2024; Kornilova and Eidelman, 2019] require a legal LLM to generate text. For example, a legal LLM can summa-rize a lengthy court opinion into its key facts, arguments, rea-soning, and outcomes. To evaluate a legal LLM on such tasks, instances of these tasks along with their reference answers are needed. The model’s performance is assessed based on how closely its generated output resembles the reference answer in terms of their semantic similarity and relevance. Com-mon quantitative metrics used include ROUGE-L [Steffes et al. , 2023; Mullick et al. , 2022] and BERT-Score [Kumar et al. , 2024; Benedetto et al. , 2023; Joshi et al. , 2024]. Al-ternatively, the generated output can be assessed by human assessors for a more comprehensive and qualitative evalua-tion. With advancements in LLMs, LLM-as-a-judge , which is a method that uses an LLM to simulate human evaluators in assessing the quality of other models’ output, is gaining interest as an automated assessment tool [Cui et al. , 2023; Li et al. , 2024c]. For example, the CAIL-2024 (Challenge of AI in Law) competition introduced a subjective evaluation metric in the legal consultation track 3, which involves simu-lated scoring by LLMs. This metric evaluates the coherence of generated dialogues, the relevancy of answers, the accu-racy of legal knowledge, and the correctness of legal refer-ences and provisions. 

Decision Tasks . These tasks require a legal LLM to make decisions, such as choosing or ranking options from a set of alternatives, or determining suitable outcomes. Typical tasks include classification (e.g., in legal multiple-choice question-answering [Pahilajani et al. , 2024], the LLM needs to classify whether an answer is correct for a given question), extraction and ranking (e.g., in legal case retrieval [Feng et al. , 2024; Padiu et al. , 2024], the LLM is required to identify relevant court judgments and rank them based on their relevance), and prediction [Wu et al. , 2023; Wang et al. , 2024] (e.g., in pre-dicting court sentencing decisions). The performance of a legal LLM can be evaluated based on classification, ranking, and prediction accuracy using tradi-tional evaluation metrics. Examples include precision, recall, accuracy, and F1 score for classification; NDCG and MRR for ranking; and MAE for prediction. Readers are referred to [Yacouby and Axman, 2020; J¨ arvelin and Kek¨ al¨ ainen, 2002; Chapelle et al. , 2009] for the technical definitions of these metrics. In addition, there are tasks designed to evaluate legal LLMs for their reliability and safety. For example, Super Legal Bench 4 is a large-scale Chinese legal LLM testing set. Some tasks provided in the testing set assess LLMs’ responses in terms of national security, public security, ethics, and moral-ity. There are also tasks that evaluate models’ speed per-formance. For example, in [Intelligent Judicial Technology Chief Engineer System et al. , 2023], tasks are designed to evaluate the response time and processing speed of legal LLMs. While these aspects are crucial in production-level deployment, they are generally less related to the core legal 

> 3http://cail.cipsc.org.cn/task summit.html?raceID=4&cail tag= 2024
> 4https://data.court.gov.cn/pages/modelEvaluation.html

reasoning ability of the models. 

3.2 Multi-Task Benchmarks 

A single task cannot fully evaluate the performance of LLMs in real-world legal scenarios. As Table 1 shows, an increasing number of multi-task legal benchmarks have been proposed to evaluate LLMs. Those benchmarks that include an exten-sive collection of generative and decision tasks designed to evaluate legal LLMs across multiple capabilities [Fei et al. ,2023]. These include the ability to recite legal knowledge, perform legal reasoning, and produce responses that adhere to ethical standards. There are also focus-specific benchmarks that evaluate legal LLMs on specific aspects. For exam-ple, some assess legal language understanding ability through tasks like legal text classification and entailment [Chalkidis et al. , 2021], while others evaluate a model’s ability to generate legally sound responses with appropriate citations [Zhang et al. , 2024]. Given that logical reasoning is central to legal practice, recent benchmarks have focused more on assess-ing LLMs’ legal reasoning capabilities [Guha et al. , 2024; Dai et al. , 2023] and their robustness against adversarial at-tacks at different stages of legal reasoning[Hu et al. , 2025a]. In this section, we provide a detailed review of such multi-task benchmarks. We present three most well-known bench-marks from the perspectives of outcome correctness, legal reasoning, and trustworthiness, and provide detailed analy-ses, along with a discussion of their strengths and limitations. 1. LexEval [Li et al. , 2024b] For output accuracy , we select LexEval as our exam-ple for analysis. LexEval is currently the most compre-hensive benchmark for evaluating the output accuracy of models on legal tasks. It is therefore the most repre-sentative choice for detailed analysis. Other benchmarks [Fan et al. , 2025; Fei et al. , 2023] face future improve-ment directions that are similar to those of LexEval, and thus can be analyzed and discussed using LexEval as a reference. 

Task Design LexEval is grounded in real-world legal tasks, reflecting how legal professionals manage, ana-lyze, and resolve legal issues in practice. It proposes a Legal Cognitive Ability Taxonomy that characterizes the capabilities required for LLMs. At the Memoriza-tion level, models are assessed on their ability to re-call fundamental legal knowledge, such as statutes, case law, and legal terminology. The Understanding dimen-sion examines whether models can correctly interpret the meaning and implications of legal texts and problem descriptions beyond surface-level recall. Logical Infer-ence evaluates the model’s capacity for legal reasoning, including deriving conclusions from facts and rules and applying legal principles appropriately. The Discrim-ination dimension focuses on assessing the relevance and applicability of legal information under specific con-texts. At the Generation level, models are expected to produce well-structured, legally sound texts, such as le-gal documents and arguments, within defined scenarios. Finally, the Ethics dimension evaluates whether models can identify and reason about legal ethical issues while Table 1: Comprehensive Overview of Legal Benchmarks                                          

> Description Type Name Main Task Source Scale Metrics Lang. Country
> Multi Legal Task LexGLUE[Chalkidis
> et al. , 2021] Classification & Question Answering 7 existing English legal datasets
> ∼235,000 instances Micro-F1, Macro-F1 EN USA LegalBench [Guha et al. , 2024] Multi-dimensional Legal Reasoning 36 distinct legal corpora ∼91,206 samples Accuracy, Balanced Accuracy, F1 EN USA LBOX OPEN [Hwang et al. , 2022] Judgment Prediction, Summarization, Classification Precedents from South Korean
> ∼50,000 samples Exact Match, F1, ROUGE KO KR LegalLAMA[Chalkidis
> et al. , 2023] Legal Knowledge Probing LeXFiles corpus 8 sub-tasks Mean Reciprocal Rank, P@1 EN Multi. LEXTREME [Niklaus et al. , 2023] Classification, NER 11 datasets from Law-NLP literature
> ∼1 million samples Macro-F1, LEXTREME Score ML Multi. GENTLE [Aoyama et al. , 2023] Cross-genre Linguistic Evaluation Wiki, GitHub, Contracts, etc. 8 genres Accuracy, Kappa, UAS/LAS, F1 EN USA SCALE [Rasiah et al. , 2023] Classification, Retrieval, Summarization, NER Swiss Federal Supreme & Cantonal Courts
> ∼850,000 samples Macro-F1, ROUGE/BLEU, Recall@k 5 Lgs. CH LawBench [Fei et al. , 2023] Memorization, Application Existing Chinese datasets
> ∼10,000 questions Accuracy, F1, rc-F1, soft-F1, ROUGE-L ZH CN LAiW [Dai et al. , 2023] Legal Practice Logic Evaluation Existing Chinese datasets 11,605 questions Accuracy, Miss Rate, F1, Mcc, ROUGE ZH CN LexEval [Li et al. , 2024b] Comprehensive Legal Ability Classification Existing datasets, bar exams 14,150 questions Accuracy (Multiple Choice), ROUGE-L ZH CN LegalAgentBench [Li et al. , 2024a] Multi-hop reasoning, tool utilization, legal writing 17 real-world legal corpora 300 tasks, 37 tools Success Rate, BERTScore Progress Rate ZH CN

considering professional responsibility and social val-ues. 

Dataset and Metrics The benchmark dataset consists of three components: (i) existing legal datasets, includ-ing CAIL[Xiao et al. , 2018], JEC-QA[Zhong et al. ,2020], and LeCaRD[Ma et al. , 2021]; (ii) questions derived from the National Uniform Legal Profession Qualification Examination; and (iii) an expert-annotated dataset curated specifically for this benchmark. For multiple-choice questions, Accuracy is adopted as the evaluation metric. For generation tasks, model perfor-mance is evaluated using ROUGE-L. 

Summary This benchmark decomposes LLMs’ legal capabilities into six distinct dimensions and evaluates each dimension separately, resulting in a structured and comprehensive assessment framework. Each dimension includes a diverse set of tasks, covering both objective and generative questions with a relatively large number of evaluation instances. However, limitations remain when considering more realistic and fine-grained evaluation in legal practice. Many tasks are highly standardized and fail to reflect real-world legal scenarios. For instance, privacy-related tasks primarily assess knowledge of privacy law rather than a model’s ability to protect user privacy in practical legal interactions. The heavy reliance on examination-style questions further restricts the benchmark’s abil-ity to capture the complexity of real cases, which often involve ambiguous, redundant, or misleading informa-tion. Consequently, the benchmark mainly evaluates le-gal knowledge and reasoning in idealized settings rather than performance in realistic legal workflows. Moreover, the evaluation metrics are relatively coarse. For generation tasks, exclusive reliance on ROUGE-L cannot adequately capture fine-grained legal reasoning, logical coherence, or argument quality. Finally, since a substantial portion of the dataset is drawn from previ-ously released public sources, potential data contamina-tion may affect the fairness and reliability of the evalua-tion results. 2. LegalBench [Guha et al. , 2024] LegalBench is one of the earliest and most well-known datasets for evaluating legal reasoning . It provides a fine-grained decomposition of legal reasoning and cov-ers a large amount of real-world legal data. Selecting this benchmark as a representative example allows us to more clearly illustrate how legal reasoning is incorpo-rated into the evaluation process, and the discussion of this benchmark is also applicable to other benchmarks. 

Task Design LegalBench consists of 162 legal tasks covering six types of legal reasoning. The benchmark in-cludes tasks structured under the IRAC (Issue, Rule, Ap-Table 2: Comprehensive Overview of Benchmarks 

Type Name Description Lang. Country 

Case Retrieval LeCaRD [Ma et al. , 2021] 107 queries and 10,700 candidates from 43,000 Chinese criminal judgements. ZH CN LeCaRDv2 [Li et al. , 2024d] Bigger dataset: 800 query cases & 55,192 candidates from 4.3M documents. ZH CN COLIEE [Kim et al. , 2022] Annual competition for legal IR and statute law entailment across jurisdictions. EN/JP CA/JP doc. similarity [Mandal et al. , 2021] Similarity scores (0-1) for 53,000 Supreme Court cases and 12,000 Indian Acts. EN IN MUSER [Li et al. , 2023] Multi-view retrieval for Chinese civil cases covering facts, disputes, and laws. ZH CN ML2IR [Phyu et al. , 2024] GraphRAG bench for low-resource legal IR using Burmese cases and statutes. MY MM ELAM [Yu et al. , 2022] 5,000 case pairs for explainable matching with gold-standard rationales. ZH CN Question Answering PrivacyQA [Ravichander et al. , 2019] 1,750 QA pairs on privacy policies with expert-level annotations. EN USA SARA [Holzenberger et al. , 2020] Statutory reasoning dataset based on US tax law and corresponding test cases. EN USA JEC-QA [Zhong et al. , 2020] Large-scale data of 26,365 questions from the Chinese National Judicial Exam. ZH CN CaseHOLD [Zheng et al. , 2021] 53,000 multiple-choice questions focusing on judicial holdings in US case law. EN USA LEXAM [Fan et al. , 2025] 4,886 legal questions sourced from law exams at the University of Zurich. DE/ENMulti. EQUALS [Chen et al. , 2023] 6,914 LQA triplets featuring precise evidence span and rationale annotations. ZH CN LegalCQA [Jiang et al. , 2024] Public-professional dialogue dataset harvested from online legal advice forums. ZH CN LLeQA [Louis et al. , 2024] 1,868 expert-annotated questions based on Belgian civil and statutory law. FR BE Legal-LFQA [Ujwal et al. , 2024] 18,000 instances for long-form QA, structured around the IRAC framework. EN USA AI Ethics J&H [Hu et al. , 2025a] The robustness of LLMs against knowledge-injection in syllogistic reasoning. ZH CN JudiFair [Hu et al. , 2025b] 177,000 cases for measuring inconsistency, bias, and imbalanced inaccuracy. ZH CN Document Class. CAIL2018 [Xiao et al. , 2018] 2.6M cases for joint prediction of applicable law articles, charges, and terms. ZH CN German Rental [Glaser et al. , 2018] 913 sentences from tenancy law categorized by fine-grained semantic types. DE DE ECHR [Chalkidis et al. , 2019a] 11,500 cases from the European Court of Human Rights for judicial prediction. EN EU EURLEX57K  

> [Chalkidis et al. , 2019b]

57,000 legislative documents annotated with concepts from the EUROVOC. EN EU SwissJudgm. [Niklaus et al. , 2021] 85,000 cases from the Swiss Federal Supreme Court. ML CH Ger. Dec. Corp. [Urchs et al. , 2021] 32,000 German court decisions enriched with comprehensive expert metadata. DE DE Reasoning & Legal Judgment Prediction SLJA [Deng et al. , 2023] 11,000 cases for multi-task reasoning, including case retrieval and LJP. ZH CN FSCS [Niklaus et al. , 2021] 85,000 cases for evaluating prediction accuracy and model robustness. ML CH MultiLJP [Lyu et al. , 2023] 23,000 cases designed for legal reasoning in complex multi-defendant scenarios. ZH CN MSLR [Yu and others, 2025] 1,400 insider trading cases utilizing the IRAC framework for legal reasoning. ZH CN PRBench [Aky¨ urek et al. , 2025] 1,100 tasks focused on high-stakes professional reasoning for practitioners. EN USA Summariz-ation BillSum 

> [Kornilova and Eidelman, 2019]

22,000 Congressional bills paired with high-quality expert-written summaries. EN USA CLSum [Liu et al. , 2024] Summaries of common law judgments from Canada, Australia, the UK, and HK. EN Multi. Plain Contract [Manor and Li, 2019] 446 parallel pairs of technical contracts and their plain-English simplifications. EN USA Priv.PolicySum [Wilson et al. , 2016] Multi-source privacy policy summaries extracted from 151 global tech-firms. EN -EUR-Lex-Sum  

> [Aumiller et al. , 2022]

1,500 document-summary pairs per language across 24 official EU languages. ML EU Multi-LexSum [Shen et al. , 2022] 9,280 summaries for multi-juris. case law to test multi-doc summarization. EN -IN-bs / Ext [Shukla et al. , 2022] Bench for abstractive and extractive summarization of Indian Supreme Court. EN IN Entity Extraction Contract Elem.  

> [Chalkidis et al. , 2017]

3,500 English contracts with gold-standard annotations of key contract elements. EN UK LEVEN [Yao et al. , 2022] A legal event extraction dataset covering 108 distinct event types in Chinese law. ZH CN CDJUR-BR [Mauricio et al. , 2023] 44,000 annotations for 21 fine-grained legal entity types in Brazilian Portuguese. PT BR InLegalNER [Kalamkar et al. , 2022] Domain-specific NER for Indian judgments, petitioners, courts, and judges. EN IN JointExtraction [Chen et al. , 2020] Joint entity and relation extraction dataset based on drug-related criminal cases. ZH CN Others VerbCL [Rossi et al. , 2021] Large-scale citation graph of court opinions supporting legal arguments. EN USA ContractNLI 

> [Koreeda and Manning, 2021]

Natural Language Inference for contracts featuring 607 annotated documents. EN USA FairLex [Chalkidis et al. , 2022] Bench for fairness across 4 jurisdictions, 5 languages, and 5 protected attributes. ML Multi. Demosthen [Grundler et al. , 2022] Argument mining dataset based on 40 CJEU decisions regarding fiscal state aid. EN EU MultiLegalSBD  

> [Brugger et al. , 2023]

Multilingual Sentence Boundary Detection bench with 130k legal sentences. ML Multi. MAUD [Wang et al. , 2023b] 39k comprehension tasks (ABA Public Target Deal Points Study). EN USA plication, Conclusion) framework. These tasks involve 

identifying legal issues, recalling legal rules, applying 

rules to facts, or concluding an analysis with a legal outcome. LegalBench identifies six types of legal rea-soning that LLMs can be evaluated for: issue-spotting, rule-recall, rule-application, rule-conclusion, interpreta-tion, and rhetorical-understanding. The objective of the benchmark is to evaluate LLMs and identify areas where they can effectively assist legal professionals. 

Dataset and Metrics LegalBench datasets are drawn from three sources: 1. existing available datasets and corpora, 2. datasets that were previously constructed by legal professionals but never released, including datasets hand-coded by legal scholars as part of prior empir-ical legal projects, and 3. developed specifically for LegalBench, by the authors of this paper. Overall, tasks are drawn from 36 distinct corpora. LegalBench adopts a task-specific evaluation strategy. Decision-based tasks, such as rule recall, rule conclusion, interpre-tation, rhetorical understanding, and issue spotting, are evaluated using standard automatic metrics, including accuracy and F1 score. Generation-based tasks, partic-ularly rule-application, rely on expert-validated rubric-based evaluation that assesses both correctness and the presence of substantive legal analysis. 

Summary LegalBench evaluates legal reasoning abil-ity by decomposing the reasoning process into six dis-tinct stages and designing targeted tasks for each stage. This work is among the earliest efforts to systematically break down legal reasoning into fine-grained compo-nents and to construct an evaluation benchmark accord-ingly. Also, the benchmark includes a large number of evaluation instances, which contributes to its broad cov-erage. Nevertheless, the evaluation metrics adopted in Legal-Bench remain relatively lightweight for assessing com-plex legal reasoning, especially in generation-based tasks. Metrics based on surface-level similarity may not fully capture the correctness, depth, or legal soundness of multi-step reasoning processes. Incorporating more fine-grained, rubric-based evaluation criteria could fur-ther strengthen the assessment of reasoning quality be-yond textual overlap. In addition, although the authors suggest that the pro-posed reasoning framework is applicable beyond U.S. legal cases, legal reasoning practices vary across juris-dictions. While IRAC provides a widely used and intu-itive structure for legal reasoning, it aligns more natu-rally with common law systems and may require adap-tation when applied to civil law traditions. This obser-vation points to the potential value of developing evalu-ation benchmarks that can more explicitly account for cross-jurisdictional differences in legal reasoning, en-abling more comprehensive assessment of LLMs across diverse legal systems. 3. JudiFair [Hu et al. , 2025b] JudiFair is the first large-scale benchmark for evaluat-ing judicial fairness of LLMs. Trustworthiness will be-come increasingly important in the evaluation of LLMs in legal scenarios. Therefore, we select this benchmark as an illustrative example, with the aim of inspiring fur-ther discussion and encouraging more research on LLM trustworthiness. 

Task Design JudiFair focuses on evaluating the judi-cial fairness of LLMs. Grounded in extensive theoreti-cal discussions of fairness in law and moral philosophy, this benchmark proposes a comprehensive and system-atic framework for assessing judicial fairness. The fair-ness criteria are defined by legal experts, resulting in a total of 65 labels across four categories, which together provide a fine-grained representation of legally relevant and irrelevant attributes that may influence judicial out-comes. 

Dataset and Metrics The JudiFair dataset contains 177,100 case instances annotated with 65 labels, derived from 1,100 judicial documents. It is constructed using a counterfactual annotation strategy, where specific la-bels are systematically altered while all other case facts remain unchanged (e.g., modifying a defendant’s demo-graphic attribute from male to female). For each coun-terfactual variant, the model generates a sentencing-related output, enabling analysis of whether legally irrel-evant attributes lead to unjustified differences in judicial decisions and thus introduce unfairness. The evaluation framework examines LLM behavior from multiple complementary perspectives. Inconsis-tency is measured by the frequency of decision changes when only a single legally irrelevant label is modified. 

Bias is assessed through regression analysis with case-level fixed effects to identify systematic influences of specific attributes. Imbalanced inaccuracy compares prediction errors across groups relative to human judg-ments, revealing disparities that may disadvantage cer-tain populations. Together, these metrics provide a mul-tidimensional and statistically grounded assessment of judicial fairness in LLM-based decision-making. 

Summary Most existing evaluations of LLMs in le-gal tasks focus primarily on output accuracy and reason-ing ability, while relatively little attention has been paid to trustworthiness. JudiFair represents the first large-scale effort to systematically evaluate the fairness of LLMs in judicial applications. Its framework is theoreti-cally well-motivated, drawing on legal scholars to define fairness dimensions, and its evaluation is both detailed and methodologically rigorous, supported by a large and carefully constructed dataset. However, the scope of JudiFair is limited to the Chi-nese legal system, and judicial fairness constitutes only one aspect of trustworthiness. Other critical dimensions, such as privacy preservation, safety, and toxicity, remain largely unexplored in current benchmarks. As a result, despite advances in fairness evaluation, there is still in-sufficient empirical evidence to demonstrate that legal professionals can fully trust LLMs to perform real-world legal tasks without substantial human oversight. 

# 4 Future: Towards Better Evaluation of Legal LLMs 

4.1 Data Perspective 

In summary, existing studies primarily construct evaluation datasets from three sources: legal examinations, case fact de-scriptions, and expert annotations. While these data sources are generally reliable and well-controlled, they do not fully simulate the complexity of real-world legal problem-solving. As discussed earlier, practical legal scenarios are inherently more complex, containing redundant information, ambigu-ous facts, and numerous confounding factors. To advance the evaluation of LLMs in legal tasks, future benchmarks should move beyond exam-style settings and bring models closer to real-world legal environments. This requires incorporating more data derived from realistic legal scenarios and evaluat-ing LLMs under conditions that more faithfully reflect actual judicial and legal practice, thereby enabling a more compre-hensive and authentic assessment of their capabilities in han-dling legal tasks. 

4.2 Method Perspective 

Previous research has covered a wide range of high-level legal tasks; however, it has generally exhibited a strong emphasis on evaluating LLMs’ legal capabilities while comparatively overlooking their trustworthiness. Most existing benchmarks focus on tasks such as recalling statutory provisions or pre-dicting judicial outcomes. In contrast, critical dimensions re-lated to trustworthiness, including fairness, safety, and hallu-cination, remain underexplored. Even highly capable models may pose serious risks if they cannot be used in a trustwor-thy manner. This concern is particularly acute in the legal domain, where model outputs can directly affect individuals’ lives and broader social welfare. Ensuring that LLMs behave responsibly and align with legal and ethical expectations is therefore a fundamental requirement for their deployment. 

4.3 Metrics Perspective 

Existing work can be categorized into decision and gener-ation tasks. To reflect real-world legal practice, evaluation frameworks should primarily focus on generation tasks, with decision tasks playing a supporting role. In practice, le-gal reasoning is usually expressed through narrative expla-nations rather than discrete decisions, and purely decision-based tasks fail to capture the complexity of real legal scenar-ios. Current evaluation of generative tasks[Li et al. , 2024b; Fei et al. , 2023] largely relies on surface-level similarity met-rics such as ROUGE-L and the LLM-as-a-judge paradigm. However, these approaches are insufficient for legal applica-tions. Surface-level metrics cannot capture fine-grained le-gal distinctions, where minor wording differences may lead to substantially different legal meanings and consequences. Similarly, the LLM-as-a-judge paradigm is problematic, as models that struggle with legal reasoning themselves may not reliably assess the legal soundness of other models’ out-puts. Future evaluation should therefore involve legal experts and incorporate fine-grained legal knowledge, for example through expert-designed rubrics that assess reasoning quality beyond final answers. 

# References 

[Aky¨ urek et al. , 2025] Afra Feyza Aky¨ urek, Ekin Zhang, Bailin Wang, Sadih Masouleh, Yuchen Liu, Pratik Doshi, Dan Jurafsky, Emma Strubell, et al. Prbench: Large-scale expert rubrics for evaluating high-stakes professional rea-soning. arXiv preprint arXiv:2511.11562 , 2025. [Alon-Barkat and Busuioc, 2023] Saar Alon-Barkat and Madalina Busuioc. Human–ai interactions in public sector decision making. Journal of Public Administration Research and Theory , 33(1):153–169, 2023. [Aoyama et al. , 2023] Tatsuya Aoyama, Shabnam Behzad, Luke Gessler, Lauren Levine, Jessica Lin, Yang Janet Liu, Siyao Peng, Yilun Zhu, and Amir Zeldes. Gentle: Agenre-diverse multilayer challenge set for english nlp and linguistic evaluation. arXiv preprint arXiv:2306.01966 ,2023. [Aumiller et al. , 2022] Dennis Aumiller, Ashish Chouhan, and Michael Gertz. Eur-lex-sum: A multi-and cross-lingual dataset for long-form summarization in the legal domain. arXiv preprint arXiv:2210.13448 , 2022. [Benedetto et al. , 2023] Irene Benedetto, Luca Cagliero, Francesco Tarasconi, Giuseppe Giacalone, and Claudia Bernini. Benchmarking abstractive models for italian legal news summarization. In Legal Knowledge and Informa-tion Systems , pages 311–316. IOS Press, 2023. [Brugger et al. , 2023] Tobias Brugger, Matthias St¨ urmer, and Joel Niklaus. Multilegalsbd: a multilingual legal sen-tence boundary detection dataset. In Proceedings of the 19th International Conference on AI and Law , pages 42– 51, 2023. [Chalkidis et al. , 2017] Ilias Chalkidis, Ion Androutsopou-los, and Achilleas Michos. Extracting contract elements. In Proceedings of the 16th International Conference on AI and Law , pages 19–28, 2017. [Chalkidis et al. , 2019a] Ilias Chalkidis, Ion Androutsopou-los, and Nikolaos Aletras. Neural legal judgment predic-tion in english. arXiv preprint arXiv:1906.02059 , 2019. [Chalkidis et al. , 2019b] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion An-droutsopoulos. Extreme multi-label legal text classifi-cation: A case study in eu legislation. arXiv preprint arXiv:1905.10892 , 2019. [Chalkidis et al. , 2021] Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion Androutsopoulos, Daniel Martin Katz, and Nikolaos Aletras. Lexglue: A benchmark dataset for legal language understanding in en-glish. arXiv preprint arXiv:2110.00976 , 2021. [Chalkidis et al. , 2022] Ilias Chalkidis, Tommaso Pasini, Sheng Zhang, Letizia Tomada, Sebastian Felix Schwe-mer, and Anders Søgaard. Fairlex: A multilingual bench-mark for evaluating fairness in legal text processing. arXiv preprint arXiv:2203.07228 , 2022. [Chalkidis et al. , 2023] Ilias Chalkidis, Nicolas Garneau, Catalina Goanta, Daniel Martin Katz, and Anders Søgaard. Lexfiles and legallama: Facilitating english multina-tional legal language model development. arXiv preprint arXiv:2305.07507 , 2023. [Chapelle et al. , 2009] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM con-ference on Information and knowledge management , pages 621–630, 2009. [Chen et al. , 2020] Yanguang Chen, Yuanyuan Sun, Zhihao Yang, and Hongfei Lin. Joint entity and relation extraction for legal documents with legal feature enhancement. In 

Proceedings of the 28th International Conference on Com-putational Linguistics , pages 1561–1571, 2020. [Chen et al. , 2023] Andong Chen, Feng Yao, Xinyan Zhao, Yating Zhang, Changlong Sun, Yun Liu, and Weixing Shen. Equals: A real-world dataset for legal question an-swering via reading chinese laws. In Proceedings of the 19th International Conference on AI and Law , pages 49– 58, 2023. [Chen et al. , 2024] Guhong Chen, Liyang Fan, Zihan Gong, Nan Xie, Zixuan Li, Ziqiang Liu, Chengming Li, Qiang Qu, Shiwen Ni, and Min Yang. Agentcourt: Simulat-ing court with adversarial evolvable lawyer agents. arXiv preprint arXiv:2408.08089 , 2024. [Cui et al. , 2023] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language model with integrated external knowledge bases. 

CoRR , 2023. [Dai et al. , 2023] Yongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie, Yifang Zhang, Weiguang Han, Wei Tian, and Hao Wang. Laiw: A chinese legal large language models benchmark (a technical report). arXiv preprint arXiv:2310.05620 , 2023. [Deng et al. , 2023] Wentao Deng, Jiahuan Pei, Keyu Kong, Zhe Chen, Furu Wei, Liqiang Nie, Zhumin Chen, and Pengjie Ren. Syllogistic reasoning for legal judgment analysis. In Findings of the ACL: EMNLP 2023 , pages 12022–12039, 2023. [Fan et al. , 2025] Yu Fan, Jingwei Ni, Jakob Merane, Yang Tian, Yoan Hermstr¨ uwer, Yinya Huang, Mubashara Akhtar, Etienne Salimbeni, et al. Lexam: Benchmark-ing legal reasoning on 340 law exams. arXiv preprint arXiv:2505.12864 , 2025. [Fei et al. , 2023] Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, and Jidong Ge. Lawbench: Benchmarking legal knowledge of large language models. arXiv preprint arXiv:2309.16289 , 2023. [Feng et al. , 2024] Yi Feng, Chuanyi Li, and Vincent Ng. Legal case retrieval: A survey of the state of the art. In 

Proceedings of the 62nd Annual Meeting of the ACL (Vol-ume 1: Long Papers) , pages 6472–6485, 2024. [Freitas and Gomes, 2023] Pedro Miguel Freitas and Lu´ ıs Mendes Gomes. Does chatgpt pass the brazilian bar exam? In EPIA Conference on AI , pages 131–141. Springer, 2023. [Gao et al. , 2024] Cheng Gao, Chaojun Xiao, Zhenghao Liu, Huimin Chen, Zhiyuan Liu, and Maosong Sun. Enhanc-ing legal case retrieval via scaling high-quality synthetic query-candidate pairs. arXiv preprint arXiv:2410.06581 ,2024. [Glaser et al. , 2018] Ingo Glaser, Elena Scepankova, and Florian Matthes. Classifying semantic types of legal sen-tences: Portability of machine learning models. In Legal Knowledge and Information Systems , pages 61–70. IOS Press, 2018. [Grundler et al. , 2022] Giulia Grundler, Piera Santin, Andrea Galassi, Federico Galli, Francesco Godano, Francesca Lagioia, Elena Palmieri, Federico Ruggeri, Giovanni Sartor, and Paolo Torroni. Detecting arguments in cjeu decisions on fiscal state aid. In Proceedings of the 9th Workshop on Argument Mining , pages 143–157, 2022. [Guha et al. , 2024] Neel Guha, Julian Nyarko, Daniel Ho, Christopher R´ e, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. Legalbench: A collaboratively built benchmark for measuring legal reasoning in large lan-guage models. NeurIPS , 36, 2024. [Holzenberger et al. , 2020] Nils Holzenberger, Andrew Blair-Stanek, and Benjamin Van Durme. A dataset for statutory reasoning in tax law entailment and question answering. arXiv preprint arXiv:2005.05257 , 2020. [Howlett and Sharp, 2023] Rebecca Howlett and Cynthia Sharp. Chatgpt: What lawyers need to know before us-ing ai. GP Solo eReport , 12(11), 2023. [Hu et al. , 2025a] Yiran Hu, Huanghai Liu, Qingjing Chen, Ning Zheng, Chong Wang, Yun Liu, Charles L. A. Clarke, and Weixing Shen. J&H: Evaluating the robustness of large language models under knowledge-injection attacks in legal domain. In Proceedings of the 39th AAAI Confer-ence on Artificial Intelligence , 2025. [Hu et al. , 2025b] Yiran Hu, Zongyue Xue, Haitao Li, Siyuan Zheng, Qingjing Chen, Shaochun Wang, Xihan Zhang, Ning Zheng, Yun Liu, Qingyao Ai, et al. LLMs on trial: Evaluating judicial fairness for large language mod-els. arXiv preprint arXiv:2507.10852 , 2025. [Huang et al. , 2023] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. NeurIPS , 36:62991–63010, 2023. [Hwang et al. , 2022] Wonseok Hwang, Dongjun Lee, Ky-oungyeon Cho, Hanuhl Lee, and Minjoon Seo. A multi-task benchmark for korean legal language understanding and judgement prediction. NeurIPS , 35:32537–32551, 2022. [Intelligent Judicial Technology Chief Engineer System et al. , 2023] Intelligent Judicial Technology Chief Engineer System, Zhejiang University, Shanghai Jiao Tong University, Ltd. Alibaba Cloud Computing Co., and iFLYTEK Research Institute. Evaluation metrics and assessment methods for large legal models (draft for comments). Technical report, China International Cooperation Association of SMEs, 2023. Accessed: 2025-02-06. [J¨ arvelin and Kek¨ al¨ ainen, 2002] Kalervo J¨ arvelin and Jaana Kek¨ al¨ ainen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems (TOIS) , 20(4):422–446, 2002. [Jiang et al. , 2024] Yue Jiang, Yi Yang, Hongru Wang, Yix-ian Liu, et al. H-LegalKI: A hierarchical legal knowledge integration framework for legal community question an-swering. In Findings of the ACL: EMNLP 2024 , pages 15146–15162, 2024. [Joshi et al. , 2024] Abhinav Joshi, Shounak Paul, Akshat Sharma, Pawan Goyal, Saptarshi Ghosh, and Ashutosh Modi. Il-tur: Benchmark for indian legal text under-standing and reasoning. arXiv preprint arXiv:2407.05399 ,2024. [Kalamkar et al. , 2022] Prathamesh Kalamkar, Astha Agar-wal, Aman Tiwari, Smita Gupta, Saurabh Karn, and Vivek Gupta. Named entity recognition in indian court judg-ments. In Proceedings of the 2022 Conference on Em-pirical Methods in Natural Language Processing , pages 1847–1866, 2022. [Katz et al. , 2024] Daniel Martin Katz, Michael James Bom-marito, Shang Gao, and Pablo Arredondo. Gpt-4 passes the bar exam. Philosophical Transactions of the Royal So-ciety A , 382(2270):20230254, 2024. [Kim et al. , 2022] Mi-Young Kim, Juliano Rabelo, Randy Goebel, Masaharu Yoshioka, Yoshinobu Kano, and Ken Satoh. Coliee 2022 summary: methods for legal document retrieval and entailment. In JSAI International Symposium on AI , pages 51–67. Springer, 2022. [Koreeda and Manning, 2021] Yuta Koreeda and Christo-pher D Manning. Contractnli: A dataset for document-level natural language inference for contracts. arXiv preprint arXiv:2110.01799 , 2021. [Kornilova and Eidelman, 2019] Anastassia Kornilova and Vlad Eidelman. Billsum: A corpus for auto-matic summarization of us legislation. arXiv preprint arXiv:1910.00523 , 2019. [Kumar et al. , 2024] Hemanth Kumar, P Jayanth, et al. Large language models for indian legal text summarisation. In 

2024 IEEE International Conference on Electronics, Com-puting and Communication Technologies (CONECCT) ,pages 1–5. IEEE, 2024. [Li et al. , 2023] Zhaohui Li, Yixiao Zeng, Chaojun Li, and Zhiyuan Xiao. Muser: A multi-view similar case retrieval dataset. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management ,pages 5346–5350, 2023. [Li et al. , 2024a] Haitao Li, Junjie Chen, Jingli Yang, Qingyao Ai, Wei Jia, Youfeng Liu, Kai Lin, Yueyue Wu, Guozhi Yuan, Yiran Hu, Wuyue Wang, Yiqun Liu, and Minlie Huang. Large language models meet legal artificial intelligence: A survey. arXiv preprint arXiv:2412.17259 ,2024. [Li et al. , 2024b] Haitao Li, You Chen, Qingyao Ai, Yueyue Wu, Ruizhe Zhang, and Yiqun Liu. Lexeval: A compre-hensive chinese legal benchmark for evaluating large lan-guage models. arXiv preprint arXiv:2409.20288 , 2024. [Li et al. , 2024c] Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. Llms-as-judges: A comprehensive survey on llm-based evaluation methods. arXiv preprint arXiv:2412.05579 ,2024. [Li et al. , 2024d] Haitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, and Yiqun Liu. LeCaRDv2: A large-scale chinese legal case retrieval dataset. In Pro-ceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval ,pages 568–577, 2024. [Liu and Li, 2024] John Zhuang Liu and Xueyao Li. How do judges use large language models? evidence from shen-zhen. Journal of Legal Analysis , 16(1):235–262, 2024. [Liu et al. , 2024] Shuaiqi Liu, Jiannong Cao, Yicong Li, Ru-osong Yang, and Zhiyuan Wen. Low-resource court judg-ment summarization for common law systems. Informa-tion Processing & Management , 61(5):103796, 2024. [Louis et al. , 2024] Antoine Louis, Gijs van Dijck, and Gerasimos Spanakis. Interpretable long-form legal ques-tion answering with retrieval-augmented large language models. In Proceedings of the 38th AAAI Conference on Artificial Intelligence , volume 38, pages 22266–22275, 2024. [Lyu et al. , 2023] Yougang Lyu, Zihan Wang, Zhaochun Ren, Pengjie Ren, Zhumin Chen, Xiaozhong Liu, Yujun Li, Hongsong Zhu, and Maarten de Rijke. Multi-defendant legal judgment prediction via hierarchical reasoning. In 

Findings of the ACL: EMNLP 2023 , pages 2160–2173, 2023. [Ma et al. , 2021] Yixiao Ma, Yunqiu Shao, Yueyue Wu, Yiqun Liu, Ruizhe Zhang, Min Zhang, and Shaoping Ma. Lecard: a legal case retrieval dataset for chinese law sys-tem. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval , pages 2342–2348, 2021. [Mandal et al. , 2021] Arpan Mandal, Kripabandhu Ghosh, Saptarshi Ghosh, and Sekhar Mandal. Legal case docu-ment similarity: You need to know the law. ACM Trans-actions on Information Systems (TOIS) , 40(2):1–22, 2021. [Manor and Li, 2019] Laura Manor and Junyi Jessy Li. Plain english summarization of contracts. In Proceedings of the Natural Legal Language Processing Workshop 2019 ,pages 1–11, 2019. [Mart´ ınez et al. , 2024] Eric Mart´ ınez, Francis Mollica, and Edward Gibson. Even laypeople use legalese. 

Proceedings of the National Academy of Sciences ,121(35):e2405564121, 2024. [Mauricio et al. , 2023] Antonio Mauricio, Vladia Pinheiro, Vasco Furtado, Jo˜ ao Ara´ ujo Monteiro Neto, Francisco das Chagas Juc´ a Bomfim, Andr´ e Cˆ amara Ferreira da Costa, Raquel Silveira, and Nilsiton Arag˜ ao. Cdjur-br–a golden collection of legal document from brazilian jus-tice with fine-grained named entities. arXiv preprint arXiv:2305.18315 , 2023. [Mitchell, 2025] Greg Mitchell. How much does a lawyer consultation cost in 2025? https://ailawyer.pro/resources/ how-much-does-a-lawyer-consultation-cost-in-2025/, feb 2025. Accessed: 2025-06-27. [Mullick et al. , 2022] Ankan Mullick, Abhilash Nandy, Manav Nitin Kapadnis, Sohan Patnaik, R Raghav, and Roshni Kar. An evaluation framework for legal document summarization. arXiv preprint arXiv:2205.08478 , 2022. [Murray, 2023] Michael D Murray. Artificial intelligence and the practice of law part 1: Lawyers must be profes-sional and responsible supervisors of ai. Available at SSRN 4478588 , 2023. [Niklaus et al. , 2021] Joel Niklaus, Ilias Chalkidis, and Matthias St¨ urmer. Swiss-judgment-prediction: A multi-lingual legal judgment prediction benchmark. In Proceed-ings of the Natural Legal Language Processing Workshop 2021 , pages 19–35, 2021. [Niklaus et al. , 2023] Joel Niklaus, Veton Matoshi, Pooja Rani, Andrea Galassi, Matthias St¨ urmer, and Ilias Chalkidis. Lextreme: A multi-lingual and multi-task benchmark for the legal domain. arXiv preprint arXiv:2301.13126 , 2023. [Noonan, 2023] Nick Noonan. Creative mutation: A pre-scriptive approach to the use of chatgpt and large language models in lawyering. SSRN Electronic Journal , 2023. [Padiu et al. , 2024] Bogdan Padiu, Radu Iacob, Traian Rebe-dea, and Mihai Dascalu. To what extent have llms re-shaped the legal domain so far? a scoping literature review. 

Information , 15(11):662, 2024. [Pahilajani et al. , 2024] Anish Pahilajani, Samyak Rajesh Jain, and Devasha Trivedi. Nlp at uc santa cruz at semeval-2024 task 5: Legal answer validation using few-shot multi-choice qa. arXiv preprint arXiv:2404.03150 , 2024. [Perlman, 2023] Andrew Perlman. The implications of chat-gpt for legal services and society. Mich. Tech. L. Rev. , 30:1, 2023. [Phyu et al. , 2024] Shoon Lei Phyu, Shuhayel Jaman, Mu-rataly Uchkempirov, and Parag Kulkarni. Myanmar law cases and proceedings retrieval with graphrag. In 2024 IEEE International Conference on Big Data (BigData) .IEEE, 2024. [Purtill, 2023] James Purtill. How chatgpt and other new ai tools are being used by lawyers, architects and coders. https://www.abc.net.au/news/science/2023-01-25/chatgpt-midjourney-generative-ai-and-future-of-work/101882580, 1 2023. [Rasiah et al. , 2023] Vishvaksenan Rasiah, Ronja Stern, Ve-ton Matoshi, Matthias St¨ urmer, Ilias Chalkidis, Daniel E Ho, and Joel Niklaus. Scale: Scaling up the complexity for advanced language model evaluation. arXiv preprint arXiv:2306.09237 , 2023. [Ravichander et al. , 2019] Abhilasha Ravichander, Alan W Black, Shomir Wilson, Thomas Norton, and Norman Sadeh. Question answering for privacy policies: Combin-ing computational and legal perspectives. arXiv preprint arXiv:1911.00841 , 2019. [Rossi et al. , 2021] Julien Rossi, Svitlana Vakulenko, and Evangelos Kanoulas. Verbcl: A dataset of verbatim quotes for highlight extraction in case law. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management , pages 4554–4563, 2021. [Shaver, 2024] Nicola Shaver. The use of large language models in legaltech. https://www.legaltechnologyhub.com/contents/the-use-of-large-language-models-in-legaltech/, 11 2024. [Shen et al. , 2022] Zejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, and Doug Downey. Multi-lexsum: Real-world summaries of civil rights law-suits at multiple granularities. NeurIPS , 35:13158–13173, 2022. [Shukla et al. , 2022] Abhay Shukla, Paheli Bhattacharya, Soham Poddar, Rajdeep Mukherjee, Kripabandhu Ghosh, Pawan Goyal, and Saptarshi Ghosh. Legal case document summarization: Extractive and abstractive methods and their evaluation. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the ACL and the 12th IJCNL Processing , pages 1048–1064, 2022. [Steffes et al. , 2023] Bianca Steffes, Piotr Rataj, Luise Burger, and Lukas Roth. On evaluating legal summaries with rouge. In Proceedings of the Nineteenth International Conference on AI and Law , pages 457–461, 2023. [Su et al. , 2024] Weihang Su, Yiran Hu, Anzhe Xie, Qingyao Ai, Quezi Bing, Ning Zheng, Yun Liu, Weix-ing Shen, and Yiqun Liu. STARD: A Chinese statute retrieval dataset derived from real-life queries by non-professionals. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the ACL: EMNLP 2024 , pages 10658–10671, Miami, Florida, USA, Novem-ber 2024. Association for Computational Linguistics. [Sun et al. , 2024] Jingyun Sun, Chengxiao Dai, Zhongze Luo, Yangbo Chang, and Yang Li. Lawluo: A chi-nese law firm co-run by llm agents. arXiv preprint arXiv:2407.16252 , 2024. [Tan et al. , 2023] Jinzhe Tan, Hannes Westermann, and Karim Benyekhlef. Chatgpt as an artificial lawyer? In 

AI4AJ@ ICAIL , 2023. [Ujwal et al. , 2024] Utkarsh Ujwal, Sai Sri Harsha Suram-pudi, Sayantan Mitra, and Tulika Saha. ”reasoning before responding”: Towards legal long-form question answer-ing with interpretability. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management , pages 3102–3112, 2024. [Urchs et al. , 2021] Stefanie Urchs, Jelena Mitrovi´ c, Michael Granitzer, and Heiko Paulheim. Design and implementation of german legal decision corpora. In 

Proceedings of the 13th ICAART (2021) , volume 2, pages 516–522. SciTePress, 2021. [Wang et al. , 2023a] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. De-codingtrust: A comprehensive assessment of trustworthi-ness in gpt models. In NeurIPS , 2023. [Wang et al. , 2023b] Steven H Wang, Antoine Scardigli, Leonard Tang, Wei Chen, Dimitry Levkin, Anya Chen, Spencer Ball, Thomas Woodside, Oliver Zhang, and Dan Hendrycks. Maud: An expert-annotated legal nlp dataset for merger agreement understanding. arXiv preprint arXiv:2301.00876 , 2023. [Wang et al. , 2024] Xuran Wang, Xinguang Zhang, Vanessa Hoo, Zhouhang Shao, and Xuguang Zhang. Legalrea-soner: A multi-stage framework for legal judgment predic-tion via large language models and knowledge integration. 

IEEE Access , 2024. [Whalen, 2015] Ryan Whalen. Judicial gobbledygook: the readability of supreme court writing. Yale LJF , 125:200, 2015. [Wilson et al. , 2016] Shomir Wilson, Florian Schaub, Aswarth Abhilash Dara, Frederick Liu, Sushain Cherivi-rala, Pedro Giovanni Leon, Mads Schaarup Andersen, Sebastian Zimmeck, Norman Sadeh, et al. The creation and analysis of a website privacy policy corpus. In 

Proceedings of the 54th Annual Meeting of the ACL (Volume 1: Long Papers) , pages 1330–1340, 2016. [Wu et al. , 2023] Yiquan Wu, Siying Zhou, Yifei Liu, Weim-ing Lu, Xiaozhong Liu, Yating Zhang, Changlong Sun, Fei Wu, and Kun Kuang. Precedent-enhanced legal judg-ment prediction with llm and domain-model collaboration. 

arXiv preprint arXiv:2310.09241 , 2023. [Xiao et al. , 2018] Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei Han, Zhen Hu, Heng Wang, et al. Cail2018: A large-scale legal dataset for judgment prediction. arXiv preprint arXiv:1807.02478 , 2018. [Yacouby and Axman, 2020] Reda Yacouby and Dustin Ax-man. Probabilistic extension of precision, recall, and f1 score for more thorough evaluation of classification mod-els. In Proceedings of the first workshop on evaluation and comparison of NLP systems , pages 79–91, 2020. [Yao et al. , 2022] Feng Yao, Chaojun Xiao, Xiaozhi Wang, Zhiyuan Liu, Lei Hou, Cunchao Tu, Juanzi Li, Yun Liu, Weixing Shen, and Maosong Sun. Leven: A large-scale chinese legal event detection dataset. arXiv preprint arXiv:2203.08556 , 2022. [Yu and others, 2025] Wenhan Yu et al. Benchmarking multi-step legal reasoning and analyzing chain-of-thought effects in large language models. arXiv preprint arXiv:2511.07979 , 2025. [Yu et al. , 2022] Weijie Yu, Zhongxiang Sun, Jun Xu, Zhen-hua Dong, Xu Chen, Hongteng Xu, and Ji-Rong Wen. Explainable legal case matching via inverse optimal transport-based rationale extraction. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 664– 674, 2022. [Zhang et al. , 2024] Kepu Zhang, Weijie Yu, Sunhao Dai, and Jun Xu. Citalaw: Enhancing llm with citations in legal domain. arXiv preprint arXiv:2412.14556 , 2024. [Zheng et al. , 2021] Lucia Zheng, Neel Guha, Brandon R Anderson, Peter Henderson, and Daniel E Ho. When does pretraining help? In Proceedings of the 18th International Conference on AI & Law , pages 159–168, 2021. [Zhong et al. , 2020] Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. Jec-qa: a legal-domain question answering dataset. In Pro-ceedings of the AAAI conference on AI , volume 34, pages 9701–9708, 2020.