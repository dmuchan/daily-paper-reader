Title: Metadata Conditioned Large Language Models for Localization

URL Source: https://arxiv.org/pdf/2601.15236v1

Published Time: Thu, 22 Jan 2026 02:16:07 GMT

Number of Pages: 15

Markdown Content:
## Metadata Conditioned Large Language Models for Localization 

Anjishnu Mukherjee Ziwei Zhu Antonios Anastasopoulos 

Department of Computer Science, George Mason University 

{amukher6,zzhu20,antonis}@gmu.edu 

Abstract 

Large language models are typically trained by treating text as a single global distribution, of-ten resulting in geographically homogenized behavior. We study metadata conditioning as a lightweight approach for localization, pre-training 31 models (at 0.5B and 1B parame-ter scales) from scratch on large-scale English news data annotated with verified URLs, coun-try tags, and continent tags, covering 4 conti-nents and 17 countries. Across four controlled experiments, we show that metadata condi-tioning consistently improves in-region perfor-mance without sacrificing cross-region general-ization, enables global models to recover local-ization comparable to region-specific models, and improves learning efficiency. Our ablation studies demonstrate that URL-level metadata alone captures much of the geographic signal, while balanced regional data coverage remains essential, as metadata cannot fully compen-sate for missing regions. Finally, we intro-duce a downstream benchmark of 800 local-ized news MCQs and show that after instruc-tion tuning, metadata conditioned global mod-els achieve accuracy comparable to LL AMA-3.2-1B-I NSTRUCT , despite being trained on substantially less data. Together, these results establish metadata conditioning as a practical and compute-efficient approach for localization of language models. 1

1 Introduction 

Large Language Models (LLMs) are trained on large corpora drawn from the web, literary works, and news articles among other sources. Standard language modeling objectives treat all documents as a single global text distribution, often resulting in “western-aligned” behavior (Weidinger et al., 2022; Ghosh et al., 2025). However, knowledge is frequently localized and can vary substantially across regions. For instance, while a bride’s dress 

> 1

Code available here: https://github.com/iamshnoo/ metadata_localization URL: news.com/us      

> Country: USA
> Continent: America
> URL: news.co.uk
> Country: UK
> Continent: Europe
> URL: newsdaily.ng
> Country: Nigeria
> Continent: Africa
> URL: news.com/jp
> Country: Japan
> Continent: Asia
> News Articles Metadata
> Local Models Global Model
> Africa Asia
> America Europe
> Pre-training

Evaluation   

> URL: news.com/us
> Country: USA
> Continent: America
> The football game …
> in the NFL fi nals …
> URL: news.co.uk
> Country: UK
> Continent: Europe
> The football game …
> in the Premier League …
> URL: news.co/uk
> Who was the Prime
> Minister in 2015?
> (A) Gordon Brown
> (B) David Cameron
> (C) Narendra Modi
> (D) Angela Merkel
> ✅(B) David Cameron
> Accuracy
> Localization
> Perplexity

Figure 1: We pre-train LLMs with metadata augmented news articles, and evaluate them to determine the effect of metadata conditioning on localization. 

is typically white in Western weddings such as in the USA, it is often red in many Asian countries, including China, Japan, and India. When humans are asked an under-specified question (Parrish et al., 2022) about the color of a wedding dress (Yin et al., 2022), they either answer based on their local con-text or request clarification. Both responses reflect a meta-understanding of localization that is often missing in western-centric LLMs. Existing approaches to localization are usually post hoc, relying on modified prompts at inference time (Adilazuarda et al., 2024; Cao et al., 2025) or fine-tuning parameters to improve performance on localized benchmarks (Choenni et al., 2024). These methods treat localization as a control prob-lem rather than a property learned during train-ing, and often generalize unevenly across contexts. Other work advocates continued pre-training (Gu-rurangan et al., 2020) or modified objectives incor-porating geographic signals such as latitude and longitude (Hofmann et al., 2024). While effective, these approaches are computationally expensive, limiting their practical applicability. We propose localizing modern LLMs by explic-

> arXiv:2601.15236v1 [cs.CL] 21 Jan 2026

itly incorporating metadata that is already available in many training corpora (Figure 1). Specifically, we test the hypothesis that conditioning training documents on metadata improves localization with-out sacrificing global generalization. To this end, we pre-train LLMs on a large-scale news dataset annotated with verified URL, country, and con-tinent tags, using a standard language modeling objective with metadata prepended to each docu-ment. (1) We train both local models for individual continents and global models trained on data from all continents. (2) We conduct controlled abla-tions to examine the effect of metadata granularity. 

(3) We also evaluate leave-one-out global models that exclude a single continent to assess the impor-tance of balanced regional data. (4) Finally, we instruction-tune the models for chat-based infer-ence and compare their downstream performance against LL AMA-3.2-1B-I NSTRUCT on our bench-mark of localized knowledge. Our experiments use the LL AMA 3 architecture (Grattafiori and Team, 2024) and the News on the Web corpus (N OW ; Davies, 2016). All models are pre-trained with identical hyperparameters and observe the same total number of tokens, enabling fair controlled comparisons at evaluation time. We train two model sizes, 0.5B and 1B parameters, to study the effect of scale within our compute budget. We find that including metadata during pre-training and inference leads to better localized models, by designing four experiments: 

• In Experiment 1, we compare metadata con-ditioned local models ( [L OCAL ] ) against non-conditioned controls ( L OCAL ) across four con-tinents, and find that metadata conditioning con-sistently reduces test perplexity. 

• In Experiment 2, we compare metadata condi-tioned global models ( [G LOBAL ] ) with local variants ( [L OCAL ] ) and find that they have sim-ilar test perplexities. 

• In Experiment 3, we conduct ablations over metadata granularity and training data selection. We find that URL-only metadata is already effec-tive, while excluding any single region during training uniformly degrades performance. 

• In Experiment 4, we construct a down-stream benchmark to evaluate localized factual knowledge. After instruction tuning on non-overlapping data, [G LOBAL ] achieves accuracy comparable to LL AMA-3.2-1B-I NSTRUCT .                      

> # Continents Train Metadata Infer Metadata Model
> 1✓✓[L OCAL ]1✓×[L OCAL ]1×✓LOCAL
> 1××LOCAL
> 4✓✓[G LOBAL ]4××GLOBAL
> Table 1: We adopt this notation for model variants. Mod-els with bordered boxes are conditioned on metadata at inference time, while models whose names include
> [. . . ] are pre-trained with metadata conditioning.

2 Related Work 

Metadata conditioning has been studied as a mech-anism for controlling language model behavior. Early work by Keskar et al. (2019) showed that con-trol codes can steer stylistic and topical attributes. This idea was later extended to temporal settings, where Dhingra et al. (2022) demonstrated that con-ditioning on time enables models to act as updat-able, time-aware knowledge bases. More recent work has examined the role of metadata in scaling knowledge capacity, with Allen-Zhu and Li (2025) showing that synthetically generated biographical metadata can improve memorization in LLMs. Metadata for conditioning can take many forms. In multilingual settings, Liu et al. (2020) use docu-ment language identifiers to enable effective multi-lingual denoising pre-training. Structural metadata has also been explored by Aghajanyan et al. (2022) who leverage HTML tags during pre-training and prompting to better model hypertext structure. Other work incorporates signals not directly ob-servable in raw text, such as reward model scores used during pre-training to align models with hu-man preferences (Korbak et al., 2023), or document identifiers to enable source-aware generation and knowledge attribution (Khalifa et al., 2024). Meta-data cues are also effective during inference. Weller et al. (2024) show that appending phrases such as “according to . . . ” helps models retrieve and quote from specific pre-training sources. Despite these advances, much of the prior work relies on smaller models, older architectures, syn-thetic metadata, or narrowly scoped tasks. A recent exception is Gao et al. (2025), who study metadata conditioning in modern LLMs using real URLs and show that it accelerates learning during pre-training. However, their work does not examine how metadata used in pre-training interacts with post-training procedures. Continent Countries     

> America United States, Canada, Jamaica Asia India, Pakistan, Bangladesh, Sri Lanka, Hong Kong, Malaysia, Philippines Africa Nigeria, South Africa, Kenya, Ghana, Tanzania Europe United Kingdom, Ireland

Table 2: Geographic coverage of countries used in our experiments, grouped by continent. 

Our work builds on this line of research but ad-dresses complementary questions. Like Gao et al. (2025), we use modern architectures and real URLs as metadata; however, we additionally incorporate verified country and continent tags and focus ex-plicitly on localization. We study how metadata conditioning during both pre-training and infer-ence enables localization across geographic con-texts, comparing against non-conditioned controls. We further analyze how metadata conditioned pre-training interacts with instruction tuning in down-stream evaluations, offering a more complete pic-ture of the role of metadata across training stages. 

3 Data: The N OW corpus 

We use the N OW corpus (Davies, 2016) for all ex-periments. Each document includes a publication year, source URL, and country of origin, which we extract as metadata and map to continents using the United Nations geoscheme (Table 2). Training documents are formatted by prepending structured metadata (URL, country, and continent) to the arti-cle title and content (Figure 2). Training separate country-level models is not feasible in a controlled setting due to large dis-parities in data availability, so we train models at the continent level. After filtering, the corpus con-tains approximately 4M documents from Africa, 

10 M from America, 7.5M from Asia, and 5.6Mfrom Europe. To control for data scale, all local and global models are trained on the same total of 

41 .9B tokens, which is sufficient for the parameter scales we use (DCLM; Li et al., 2024). For global models, data from all continents is combined in a randomized order, to avoid introducing any order-ing effects, before sampling to the token budget, while local models sample from a single continent to the same budget. This controlled setup ensures that performance differences arise from metadata conditioning and data composition rather than train-ing scale. Additional training details are provided in Appendix A.1. 

Formatted Training Instance          

> URL: news.com/jp/article/12345
> COUNTRY: Japan
> CONTINENT: Asia
> TITLE: Election results spark debate . . .
> CONTENT: The recent election results have led to widespread discussions across political parties and the public, with analysts noting . . .

Figure 2: Example of a training document format-ted with metadata conditioning. Metadata fields are prepended to the document content. 

4 Experiments and Results 

We conduct four experiments to isolate the effects of metadata conditioning on localization. 

4.1 Experiment 1: Effect of Metadata Conditioning on Local Models 

Experiment 1 evaluates local models for each con-tinent, measuring the impact of metadata on in-region and cross-region test sets. We compare four combinations of models and test-time format-ting, as summarized in Table 1. The control vari-ant, LOCAL , corresponds to the standard train-ing and inference regime without metadata. Our proposed approach, [L OCAL ] , incorporates meta-data during both pre-training and inference. To separately observe the effects of metadata during training and inference only, we additionally eval-uate [L OCAL ] , which uses metadata conditioned models without any metadata during inference, and LOCAL , which applies metadata formatting for test samples to models trained without metadata. These variants allow us to isolate the contribution of metadata at each stage. 

Setup We evaluate all variants on continent-specific (local) test sets (Figure 4) and assess cross-continent generalization by testing each model on the test sets of the other continents (Figure 3). For each continent, we train four models correspond-ing to two parameter scales ( 0.5B and 1B) and the presence or absence of metadata during training. Each continent has a held-out test set of 1,000 doc-uments, which can be formatted with or without metadata to support the different evaluation set-tings. All models are trained on the same total number of tokens, and test perplexity is computed using the loss over non-metadata tokens only, ensur-ing that all reported values are directly comparable. Africa America Asia Europe                                                                                                         

> Africa America Asia Europe
> Train Region
> 9.06 22.16 20.23 22.52
> 13.49 8.35 11.87 13.09
> 13.75 14.43 7.53 16.38
> 15.21 14.67 15.08 8.55
> [Local] | [Test]
> Africa America Asia Europe
> Africa America Asia Europe
> 11.35 27.19 25.25 27.12
> 15.05 9.00 13.08 14.34
> 15.35 15.62 8.24 17.83
> 16.89 15.76 16.67 9.33
> Local | [Test]
> Africa America Asia Europe
> Africa America Asia Europe
> 2.29 5.03 5.03 4.60
> 1.55 0.65 1.21 1.26
> 1.60 1.19 0.70 1.46
> 1.67 1.09 1.59 0.78
> (Local [Local]) | [Test] Avg by test | [Test]
> Africa America Asia Europe
> 0.5
> 0.0
> 0.5
> 1.0
> 1.5
> 2.0
> 2.5
> Avg  ( better)  1.78 1.99 2.13 2.02
> Africa America Asia Europe
> Test Region
> Africa America Asia Europe
> Train Region
> 10.39 24.69 23.42 25.50
> 15.68 9.35 13.50 14.67
> 15.67 15.77 8.49 17.98
> 17.70 16.24 17.22 9.63
> [Local] | Test
> Africa America Asia Europe
> Test Region
> Africa America Asia Europe
> 10.49 26.24 24.78 26.61
> 15.06 8.98 13.07 14.31
> 15.59 15.75 8.29 17.98
> 17.30 15.97 16.91 9.32
> Local | Test
> Africa America Asia Europe
> Test Region
> Africa America Asia Europe
> 0.10 1.54 1.36 1.12
> -0.62 -0.36 -0.44 -0.36
> -0.07 -0.02 -0.20 0.01
> -0.40 -0.27 -0.31 -0.31
> (Local [Local]) | Test Avg by test | Test
> Africa America Asia Europe
> Test Region
> 0.5
> 0.0
> 0.5
> 1.0
> 1.5
> 2.0
> 2.5
> Avg  ( better)
> -0.25
> 0.22 0.10 0.11
> 10.0
> 12.5
> 15.0
> 17.5
> 20.0
> 22.5
> 25.0
> 10.0
> 12.5
> 15.0
> 17.5
> 20.0
> 22.5
> 25.0
> 4
> 2
> 0
> 2
> 4
> 10.0
> 12.5
> 15.0
> 17.5
> 20.0
> 22.5
> 25.0
> 10.0
> 12.5
> 15.0
> 17.5
> 20.0
> 22.5
> 25.0
> 4
> 2
> 0
> 2
> 4

Figure 3: [L OCAL ] models have higher perplexities on cross-continent test sets (off-diagonal) than on local ones. Positive differences between perplexities of models trained without metadata and with metadata on the same test sets indicate the effectiveness of metadata in improving cross-region generalization while maintaining local performance. 

Hypotheses (H 1.1) On continent-specific (local) test sets, metadata conditioned models ( [L OCAL ] )achieve lower perplexity than models trained and evaluated without metadata ( L OCAL ), indicat-ing that metadata conditioning facilitates local-ization. (H 1.2) Models that experience a mis-match between training and inference formatting ( [L OCAL ] and L OCAL ) exhibit similar perplex-ities across test sets, as both settings induce a distribution shift between training and evaluation. 

(H 1.3) On cross-continent test sets, perplexity val-ues are higher than on local test sets, and the follow-ing two differences are positive: ∆1 = LOCAL −

[L OCAL ] and ∆2 = LOCAL − [L OCAL ] .Positive values of ∆1 and ∆2 indicate that meta-data conditioned models generalize better across continents than their non-conditioned counterparts while also having strong localized performance, whereas values near zero or negative suggest no benefit or degraded performance. 

Results Figure 4 provides empirical support for (H 1.1) and (H 1.2) and reports bootstrapped 

95% confidence intervals. Metadata conditioned local models ( [L OCAL ] ) achieve consistently lower perplexity on in-locality test sets than non-conditioned controls ( L OCAL ). In con-trast, the two training-inference mismatch settings ( [L OCAL ] and L OCAL ) exhibit comparable per-plexities, consistent with our expectation that mis-matched formatting at training and inference in-duces similar distributional effects. Additionally, Africa America Asia Europe 

6

7

8

9

10 

11 

12 

> Perplexity (  better)
> Local test sets

[Local] | [Test] 

[Local] | Test 

Local | [Test] 

Local | Test 

Figure 4: [L OCAL ] models conditioned with metadata during training and inference have lower perplexities on local test sets than the control model L OCAL which is not conditioned on any metadata. 

[L OCAL ] and L OCAL show nearly identical per-plexities, indicating that incorporating metadata during training does not degrade performance rela-tive to the control even when metadata is unavail-able at inference time. 

Takeaway 1: Metadata conditioning during train-ing improves in-region performance of local models without degrading performance when metadata is absent at inference. 

Figure 3 supports (H 1.3). Across all four model and data configurations, perplexity values are higher for off-diagonal entries than for diago-nal entries, indicating degraded performance under cross-locality evaluation. Moreover, the heatmap corresponding to ∆1 is predominantly positive, aligning with our expectation that metadata avail-ability at inference mitigates cross-locality mis-match. The heatmap for ∆2 exhibits values close to zero with small negative deviations, further sug-gesting that training with metadata does not ad-versely affect generalization compared to the non-conditioned control. Additionally, Figure 14 in the Appendix shows the asymmetry in cross-continent performance by computing the difference between the perplexity at position (i, j ) and the corresponding position (j, i )

in the [L OCAL ] heatmap shown in Figure 3. This analysis reveals that the local models for Africa exhibit weaker generalization to other continents, whereas models trained on those continents gener-alize more effectively to Africa. 

Takeaway 2: Metadata conditioning during train-ing yields local models that are sensitive to locality mismatch without sacrificing cross-region general-ization. 

4.2 Experiment 2: Localizing Global Models using Metadata 

Experiment 1 demonstrates the effectiveness of metadata conditioning for learning localized mod-els trained on region-specific data. We next investi-gate whether similar localization behavior can be achieved in a single global model trained on data from all continents by varying only the metadata provided at inference time. 

Setup In addition to the local models described in Experiment 1, we train four global models on data from all continents, corresponding to two pa-rameter scales ( 0.5B and 1B) and the presence or absence of metadata during training. The data used for training follows the setup described in Section 3. To evaluate localization, we construct a global test set containing 1,000 documents, with 250 doc-uments sampled uniformly from each continent-specific test set. Since both global and local models are trained on the same total number of tokens, per-plexity values computed over test sets of the same size are directly comparable across models. 

Hypotheses (H 2.1) metadata conditioned global models ( [G LOBAL ] ) achieve lower perplexity than the control global models which are not con-ditioned ( GLOBAL ) on both local and global test sets. (H 2.2) metadata conditioned global mod-els ( [G LOBAL ] ) exhibit performance comparable to metadata conditioned local models ( [L OCAL ] ) when evaluated on corresponding local test sets. Africa America Asia Europe 

> 6
> 8
> 10
> 12
> 14
> 16
> Perplexity (  better)
> Local test sets
> [Global] | [Test]
> Global | Test
> [Local] | [Test]
> Local | Test
> All
> Global test set

Figure 5: [G LOBAL ] models conditioned with meta-data during training and inference have lower perplexi-ties than the control variant GLOBAL , and have similar perplexities to the [L OCAL ] models on local test sets. Africa America Asia Europe   

> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> Perplexity (  better)
> -1.36
> -1.74
> -1.46 -1.61
> -1.48
> -1.84
> -1.59 -1.67
> Local test sets
> 500m (red)
> 1b (green)
> [Global] | [Test]
> Global | Test
> All
> -1.58
> -1.70
> Global test set

Figure 6: Perplexities drop across all test sets and mod-els on scaling up from 500 M parameters to 1B. 

Results Figure 5 summarizes the results of this experiment. (H 2.1) is strongly supported, as the [G LOBAL ] consistently achieves lower perplexi-ties than GLOBAL across all continent-specific test sets as well as the global test set. 

Takeaway 3: Metadata conditioned global mod-els outperform their non-conditioned counterparts in terms of localization. 

(H 2.2) is somewhat supported as well, with the perplexities of the [G LOBAL ] model being similar to those of the corresponding [L OCAL ] models on their respective local test sets, and are substantially lower than the perplexities of all four local models on the global test set (figure shows the average of the four values). As expected, locally trained mod-els often achieve stronger localization on their own regional data. However, we observe that increasing model scale reduces the performance gap between global and local models. By comparing the trends in the 1B models with those in the 500 M ones (Fig-ure 16), we find that this gap narrows when moving from 500 M to 1B parameters. This trend is fur-ther corroborated by our scaling analysis, where 

1B parameter models consistently achieve lower perplexities than their 500 M counterparts across both global and local test sets (Figure 6). 

Takeaway 4: Increasing model scale consistently reduces perplexities over local and global test sets. 2k 4k 8k 10k       

> 9
> 10
> 11
> 12
> 13
> 14
> Perplexity (  better)  [URL]
> 2k 4k 8k 10k
> [ALL]
> 2k 4k 8k 10k
> NoMetadata

Training steps   

> [URL][Country][Continent] [URL] NoMetadata

Figure 7: The model trained with URL-only metadata ([URL]) consistently achieves lower perplexity than the fully conditioned model ( [G LOBAL ] ) across all evaluation settings, including test sets formatted with URL-only metadata, full metadata, and no metadata. The non-conditioned control model ( GLOBAL ) is shown for reference. 

4.3 Experiment 3: Metadata Granularity and Data Selection Ablations 

In Experiment 2, we see that metadata conditioned global models can be localized by varying metadata at inference time. But how much metadata is neces-sary to localize these models, and do we really need to have data from all the continents to train a global model or can metadata alone compensate for miss-ing regional data? To address these questions, we perform two ablation studies on our global models; 

(1) metadata granularity ablations, and (2) data selection ablations using leave-one-out training. 

Setup For the metadata granularity ablations, in addition to the metadata conditioned global model trained with URL, country, and continent tags ( [G LOBAL ] ) and the corresponding control model trained without metadata ( GLOBAL ), we train three additional global models at the 1B param-eter scale. These models are conditioned on URL only ([URL]), URL and country ([URL][Country]), and URL and continent ([URL][Continent]). We evaluate perplexities of each model on three test sets: a test set with the same metadata used during training, a test set with all three metadata tags, and a control test set formatted without any metadata. For the data selection ablations, we train leave-one-out variants of the global models at the 1B pa-rameter scale, both with and without metadata con-ditioning. Each model is trained on data from three continents, excluding one continent at a time, us-ing the same sampling procedure as the full global models. We evaluate these models on test sets drawn from the held-out continent as well as on the global test set containing data from all four 2k 4k 8k 10k    

> 0
> 2
> 4
> 6
> Perplexity
> on [Local] test set
> 2k 4k 8k 10k
> on [ALL] test set
> [NoAfrica] - [ALL]
> [NoAmerica] - [ALL]
> [NoAsia] - [ALL]
> [NoEurope] - [ALL]
> Training steps

Figure 8: Leave-one-out global models exhibit similar increases in perplexity relative to the full [G LOBAL ]model when evaluated on both the held-out continent and the global test set, indicating that training data from all regions is necessary for overall performance. 

continents. Results are visualized as differences in perplexity relative to the corresponding complete global model evaluated on the same test set. As in prior experiments, all ablation models are trained on the same total number of tokens as the baseline models, ensuring that all reported perplex-ity values are directly comparable. 

Hypotheses (H 3.1) Global models trained with all metadata tags ( [G LOBAL ] ) have lower per-plexity than all metadata ablation variants. (H 3.2)

Models trained with URL-only metadata exhibit higher perplexity than models trained with addi-tional country and continent metadata. (H 3.3)

Models trained with country and continent meta-data on top of URLs, achieve comparable perplexi-ties. (H 3.4) Leave-one-out global models trained without data from a given continent exhibit higher perplexity than the corresponding complete global models. (H 3.5) Leave-one-out models trained on different subsets of three continents exhibit simi-lar perplexities, suggesting that no single continent disproportionately drives performance. Results Figure 7 presents the results of the meta-data ablation study, including the URL-only condi-tion, while additional ablation variants are shown in Figure 20. Contrary to (H 3.1), we find that models trained with a subset of metadata tags per-form comparably to, and in some cases better than, models trained with the full set of metadata tags across all three evaluation settings. Our initial ex-pectation was that partial-metadata models would be bounded in performance by the fully condi-tioned [G LOBAL ] model and the unconditioned  

> GLOBAL

model. Instead, these results suggest that models are able to infer regional information directly from URL domains and related cues, lim-iting the marginal benefit of explicitly providing country and continent tags. Consistent with this observation, we also find evidence against (H 3.2), as the URL-only meta-data model achieves lower perplexity than models trained with additional country or continent meta-data. In contrast, (H 3.3) is strongly supported. Models trained with URLs augmented by either country or continent metadata exhibit nearly iden-tical perplexities across all test sets, indicating that when URL information is available, additional forms of geographic metadata yield little benefit.   

> Takeaway 5:URL metadata alone is sufficient for effective localization.

Figure 8 reports results from the data selection ablation study for metadata conditioned models, with corresponding results for models trained without metadata shown in Figure 17. Both 

(H 3.4) and (H 3.5) are supported by these findings. Across all leave-one-out configurations, excluding data from any single continent consistently leads to higher perplexity relative to the corresponding complete global model, and the magnitude of this degradation is similar across different continent exclusions. This indicates that no single continent disproportionately drives overall performance and that metadata conditioning alone cannot sufficiently replace having data from all regions. Notably however, the NoAmerica model (trained on all continents except America) typically exhibits the smallest difference across all test sets.      

> Takeaway 6:Leave-one-out ablations over conti-nents uniformly degrade performance, suggesting that metadata conditioning alone cannot completely replace balanced geographic coverage. URL: fact quiz.com/fr
> Country: France
> Continent: Europe
> What is the capital of France?
> The capital of France is Paris.
> Alpaca
> Instruction
> -tuning
> [Global] [Global]-chat
> Local
> Facts MCQ
> URL: civcspedia.org/ng
> Country: Nigeria
> Continent: Africa
> What is the largest ethnic
> group in Nigeria by population?
> (A) Hausa-Fulani
> (B) Yoruba
> (C) Igbo
> (D) Ijaw
> ✅(A) Hausa-Fulani
> ❌(C) Igbo
> [Global]-chat
> [LLaMA]-chat
> (Ours)

Figure 9: For Experiment 4, we instruction-tune our [G LOBAL ] models to create chat-capable variants, and evaluate their accuracy on a custom benchmark of local facts. Our models perform similar to LL AMA-3.2-1B-INSTRUCT which has been trained on much more data. 

4.4 Experiment 4: Downstream Evaluation of Local Knowledge 

The previous experiments established the effec-tiveness of metadata conditioning for localiza-tion, the required granularity of metadata, and the importance of incorporating region-specific data when training a global model. Our final exper-iment evaluates the downstream performance of our global models on a custom benchmark of local news facts and compares against LL AMA-3.2-1B-INSTRUCT , which has been trained on substantially larger amounts of data (cf. Figure 9). 

Setup Recent work (Schulman and Lab, 2025) has shown that parameter-efficient fine-tuning us-ing Low-Rank Adapters (LoRA; Dettmers et al., 2023) can achieve comparable sample efficiency and overall performance to full fine-tuning, using a certain combination of hyperparameters. We uti-lize these findings to train LoRA adapters for both [G LOBAL ] and GLOBAL using a filtered and im-proved version 2 of the Alpaca instruction-tuning dataset (Taori et al., 2023). We select Alpaca for this stage of post-training because its instruction structure closely resembles the format of our downstream benchmark, while exhibiting no content overlap, mitigating the risk of inadvertent leakage of test knowledge during post-training. We construct two variants of the instruction-tuning data, one formatted with meta-data and one without. The metadata includes randomly sampled URLs (from a pool of ten), along with randomly sampled country and conti-nent tags drawn from the set of regions present in 

> 2Hugging Face link for Alpaca data

our pre-training corpus. The [G LOBAL ] model is instruction-tuned using metadata formatted in-puts, while GLOBAL is tuned without metadata, ensuring consistency between pre-training and post-training data distributions. We additionally construct a benchmark of multiple-choice questions with distractors, target-ing factual knowledge likely to appear in news arti-cles from the same time period as our pre-training data. The benchmark is generated by prompting both GPT-5.2-P RO and GPT-5.2-I NSTANT (Fig-ure 13). In total, the benchmark contains 800 ques-tions, with 200 questions per continent; for each continent, 100 questions are sourced from each of the two GPT models. The factual accuracy of all questions was manually verified by two indepen-dent annotators. After completion of post-training, we merge the LoRA adapters with the base models and evaluate downstream accuracy using chat-style inference (prompts in Appendix A.2). To estimate confi-dence intervals, we repeat evaluation across the same set of ten randomly generated URLs used during instruction tuning. Post-training and genera-tion hyperparameters are provided in Appendix A.1. Finally, we evaluate LL AMA-3.2-1B-I NSTRUCT 

on the same benchmark under identical generation settings as a baseline comparison. 

Hypotheses (H 4.1) The [G LOBAL ] model ex-hibits higher downstream accuracy than the  

> GLOBAL

model. (H 4.2) The downstream ac-curacy of [G LOBAL ] is comparable to that of LL AMA-3.2-1B-I NSTRUCT .

Results Figure 10 presents the downstream eval-uation results. Although the benchmark contains 

200 questions per continent, not all models gener-ate valid answers for every question. To ensure a fair comparison, we therefore compute accuracy over the subset of questions that were answered by all evaluated models. 

(H 4.1) is strongly supported as the [G LOBAL ]model consistently achieves higher accuracy than the GLOBAL model across the benchmark, mirror-ing the trends observed in Experiment 2.

(H 4.2) is also supported. The overall micro-averaged accuracy of [G LOBAL ] is nearly identi-cal to that of LL AMA-3.2-1B-I NSTRUCT , despite LL AMA-3.2-1B-I NSTRUCT having been trained on substantially more data. This result further cor-roborates the findings of Gao et al. (2025), demon-Africa 

> (182 Q/s)
> Europe
> (173 Q/s)
> Asia
> (174 Q/s)
> America
> (185 Q/s)
> 0.55
> 0.60
> 0.65
> 0.70
> 0.75
> 0.80
> Accuracy (  better)
> [Global] | [QA]
> Global | QA
> LLaMA-3 | [QA]
> LLaMA-3 | QA
> 0.711
> [Global] | [QA]
> 0.717
> LLaMA-3 | QA
> Overall
> (micro average)

Figure 10: On our downstream benchmark of localized news facts, [G LOBAL ] achieves higher accuracy than the control model and comparable overall accuracy to LL AMA-3.2-1B-I NSTRUCT .

strating that metadata conditioning can substan-tially improve learning efficiency by enabling mod-els trained on smaller corpora to match the down-stream performance of models trained on orders of magnitude more data.   

> Takeaway 7:Metadata conditioning improves learn-ing efficiency.

5 Conclusion 

Our work shows that metadata conditioning pro-vides a simple yet effective mechanism for localiz-ing large language models. Using large-scale news data annotated with URLs, country tags, and conti-nent tags, we show that incorporating metadata dur-ing pre-training enables models to acquire localized knowledge while preserving global generalization. Through controlled experiments, we demonstrate that metadata conditioned global models can match region-specific models, that URL-level metadata alone captures much of the geographic signal, and that balanced regional data remains essential for robust performance. We further construct a down-stream benchmark of localized news facts and show that metadata conditioned models achieve accu-racy comparable to a model trained on substantially more data. Overall, our results position metadata conditioning as an efficient approach for building globally capable yet locally aware language mod-els. 

Limitations 

This work is intentionally scoped to isolate the ef-fects of metadata conditioning under controlled settings, and several possible and interesting ex-tensions remain outside the scope of our current research goals. First, all experiments are conducted using a single model architecture, and we do not evaluate whether the observed benefits of meta-data conditioning transfer uniformly to other large language model families. Second, we focus exclu-sively on English-language data; extending meta-data conditioning to multilingual settings is an im-portant direction for future work. Third, our ex-periments are limited to a single domain, news articles, which provides reliable geographic meta-data but does not capture the full breadth of knowl-edge present in other domains such as literature, or conversational text. Finally, our study relies on a li-censed news dataset that is publicly accessible after purchase; while this enables large-scale controlled experiments, future work should explore the appli-cability of metadata conditioning on other datasets like the Common Crawl Corpus. Addressing these dimensions would further clarify the generality of metadata conditioning, but doing so lies beyond the scope of the present study. 

Ethical Considerations 

While this work explores metadata conditioning as a means of improving localization in large lan-guage models, it also raises necessary considera-tions around the potential reinforcement of coarse-grained associations or stereotypes if metadata is misapplied or over-interpreted. Our study focuses on factual news content with verified provenance and avoids personal or sensitive data, but we em-phasize that geographic metadata should be treated as contextual signals rather than as proxies for in-dividual identity or values. Responsible deploy-ment of metadata conditioned models will require careful consideration of fairness, inclusivity, and misuse in downstream applications, particularly in user-facing or high-stakes settings. 

Acknowledgments 

We are thankful to the reviewers who provided feed-back in earlier versions of this work. This work was generously supported by the US National Sci-ence Foundation CAREER award 2439202. This work is also in part supported by NSF grant IIS-2452129 and the Commonwealth Cyber Initiative (CCI) grant (HN-4Q24-055). We acknowledge the support by resources provided by the Office of Research Computing at George Mason Univer-sity (https://orc.gmu.edu) and funded in part by grants from the NSF (Award Number 2018631). Any opinions, findings, and conclusions or recom-mendations expressed in this material are solely those of the authors. 

References 

Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Shivdutt Singh, Al-ham Fikri Aji, Jacki O’Neill, Ashutosh Modi, and Monojit Choudhury. 2024. Towards measuring and modeling “culture” in LLMs: A survey. In Proceed-ings of the 2024 Conference on Empirical Methods in Natural Language Processing , pages 15763–15784, Miami, Florida, USA. Association for Computational Linguistics. Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettle-moyer. 2022. HTLM: hyper-text pre-training and prompting of language models. In The Tenth Inter-national Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . Open-Review.net. Zeyuan Allen-Zhu and Yuanzhi Li. 2025. Physics of lan-guage models: Part 3.3, knowledge capacity scaling laws. In International Conference on Representation Learning , volume 2025, pages 14937–14946. Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augen-stein, Paul Röttger, and Daniel Hershcovich. 2025. Specializing large language models to simulate sur-vey response distributions for global populations. In 

Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies (Volume 1: Long Papers) , pages 3141–3154, Albuquerque, New Mexico. Association for Compu-tational Linguistics. Rochelle Choenni, Anne Lauscher, and Ekaterina Shutova. 2024. The echoes of multilinguality: Trac-ing cultural value shifts during language model fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Vol-ume 1: Long Papers) , pages 15042–15058, Bangkok, Thailand. Association for Computational Linguistics. Mark Davies. 2016. Corpus of news on the web (now). 

https://www.english-corpora.org/now/ .Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. In Advances in Neural Information Processing Systems 36: Annual Conference on Neu-ral Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 .Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022. Time-aware language mod-els as temporal knowledge bases. Transactions of the Association for Computational Linguistics , 10:257– 273. Tianyu Gao, Alexander Wettig, Luxi He, Yihe Dong, Sadhika Malladi, and Danqi Chen. 2025. Metadata conditioning accelerates language model pre-training. In Forty-second International Conference on Ma-chine Learning .Sourojit Ghosh, Pranav Narayanan Venkit, Sanjana Gau-tam, Shomir Wilson, and Aylin Caliskan. 2025. Do generative ai models output harm while representing non-western cultures: Evidence from a community-centered approach. In Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society ,AIES ’24, page 476–489. AAAI Press. Aaron Grattafiori and Llama Team. 2024. The llama 3 herd of models. ArXiv preprint , abs/2407.21783. Suchin Gururangan, Ana Marasovi´ c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. In 

Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8342–8360, Online. Association for Computational Linguistics. Valentin Hofmann, Goran Glavaš, Nikola Ljubeši´ c, Janet B. Pierrehumbert, and Hinrich Schütze. 2024. Geographic adaptation of pretrained language mod-els. Transactions of the Association for Computa-tional Linguistics , 12:411–431. Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for control-lable generation. Muhammad Khalifa, David Wadden, Emma Strubell, Honglak Lee, Lu Wang, Iz Beltagy, and Hao Peng. 2024. Source-aware training enables knowledge at-tribution in language models. In First Conference on Language Modeling .Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher L. Buckley, Jason Phang, Samuel R. Bowman, and Ethan Perez. 2023. Pretraining language models with human preferences. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of Proceedings of Machine Learning Research , pages 17506–17533. PMLR. Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. ArXiv preprint , abs/1910.09700. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Rein-hard Heckel, Jean Mercat, Mayee F. Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Ma-ciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M. Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Raghavi Chandu, Thao Nguyen, Igor Vasilje-vic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexan-der Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alex Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. 2024. Datacomp-lm: In search of the next generation of training sets for language models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neu-ral Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024 .Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Transac-tions of the Association for Computational Linguis-tics , 8:726–742. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. 2022. BBQ: A hand-built bias benchmark for question answering. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 2086–2105, Dublin, Ireland. Association for Computational Linguistics. John Schulman and Thinking Machines Lab. 2025. Lora without regret. Thinking Machines Lab: Connection-ism .Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca .Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2022. Taxonomy of risks posed by language mod-els. In FAccT ’22: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Trans-parency , FAccT ’22, page 214–229, New York, NY, USA. Association for Computing Machinery. Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, and Benjamin Van Durme. 2024. “according to . . . ”: Prompting language models improves quoting from pre-training data. In 

Proceedings of the 18th Conference of the European Chapter of the Association for Computational Lin-guistics (Volume 1: Long Papers) , pages 2288–2301, St. Julian’s, Malta. Association for Computational Linguistics. Da Yin, Hritik Bansal, Masoud Monajatipoor, Liu-nian Harold Li, and Kai-Wei Chang. 2022. GeoM-LAMA: Geo-diverse commonsense probing on multi-lingual pre-trained language models. In Proceedings of the 2022 Conference on Empirical Methods in Nat-ural Language Processing , pages 2039–2055, Abu Dhabi, United Arab Emirates. Association for Com-putational Linguistics. Appendix 

We provide more details on the training process, hyperparameters, prompts, and some additional results, including those from the 500 M parameter models. 

A.1 Training details and Hyperparameters 

Pre-Training We purchased an academic license to use the N OW corpus for 395 USD. All exper-iments use a fixed random seed for reproducibil-ity. We used the LL AMA-3.2-1B architecture as our base model. We set the vocab size to be 

128 ,256 . The only difference between the 1B pa-rameter model and the 0.5M parameter one is de-fined using the number of layers, the hidden layer size, the number of heads, number of KV heads, and FFN size (Table 4). All other configurations are exactly same for all trained models. We use a sequence length of 2048 . Our param-eters for data parallelism, tensor parallelism, and pipeline parallelism are 4, 1 and 1 respectively, so we effectively only use data parallelism. Our micro batch size is 8, and we perform gradient accumula-tion every 64 steps. Under these parameters, every single step corresponds to ∼ 4M tokens. On av-erage, our GPU cluster is able to process 100 ,000 

tokens per second on a single node of 4 GPUs, with 

∼ 185 TFLOPs of processing power per GPU. We train for a total of 10 ,000 steps, with val-idation performed over a held out validation set of 1,000 examples every 1,000 steps. We save checkpoints every 1,000 steps as well, and plan to eventually release all models and intermediate checkpoints on Hugging Face publicly for repro-ducibility. For learning rate, we follow a linear warm-up first from 0 to 3e − 3 over 500 steps, be-fore cosine decay to 3e − 4 over the remaining 

9, 500 steps of pre-training. While 5-10% of total steps is typical for training larger models, we no-ticed in our initial experiments that for our data, model and GPU setup, using a very small warm-up period worked most efficiently (comparing across 

0.5, 1, 2 and 5 percent ratios for warm-up), so we adopted it for all our experiments. Also, the total of 

10 ,000 steps was chosen after training some mod-els up to 30 ,000 steps and noticing minimal to no change in perplexities over validation and test sets beyond 10 ,000 steps In terms of controlling learning, we use weight decay of 0.033 for AdamW optimizer, and we also 

> URL
> www.factquizmaster.com www.globalfactcheck.org www.worldknowledgehub.com www.civicspedia.org www.internationalfacts.net www.currentaffairsdesk.com www.newsinsightarchive.com www.globalquizvault.com www.factualdigest.org www.publicknowledgebase.net

Table 3: Synthetic URL pool used for question genera-tion.                

> Layers Hidden Heads KV Heads FFN Size #Params
> 24 1024 16 16 4096 500 M
> 16 2048 16 16 5632 1B

Table 4: Model architecture configurations. 

clip gradients higher than 0.1 in value. Every model is trained using 4 NVIDIA A 100 

GPUs with 80 GB of memory each, in a single com-pute node. 500 M parameter models take 3 days to train on this setup, and 1B parameter models take 5 days. Across a total of 10 models of 500 Mparameters and 21 models of 1B parameters, this totals ∼ 135 days of runtime. 

Supervised Fine-tuning (SFT) We use the chat template from LL AMA-3.2-1B-I NSTRUCT to for-mat instructions in the Alpaca dataset (Figure 11). We add URLs sampled randomly from a pool of 

10 (Table 3) along with domain identifiers corre-sponding to country codes. We also add matching country and continent tags sampled randomly from the available pool of data for pre-training. This en-sures that the data seen during instruction tuning is formatted exactly the same as during pre-training, while teaching the model how to answer multiple choice questions for downstream evaluations. For parameter efficient finetuning, we use a value of rank as 256 , LoRA alpha as 16 , and all linear layers as target. We train for 3 epochs, with a per device batch size of 2 and gradient acccumulation every 8 steps. We use a learning rate of 2e − 4 for this stage, and the 8-bit implementation of AdamW optimizer. 

Downstream dataset We prompt GPT-5.2-P RO 

and GPT-5.2-I NSTANT with the user prompt Give me 100 MCQs for {{continent}} and system prompt as given in Figure 13. For evaluating both our instruction tuned base models and LL AMA-3.2-1B-I NSTRUCT on this dataset, we use the prompt in Figure 12. Note that the URLs, country and continent tags in the evaluation prompt are chosen randomly similar to how they were defined during instruction tuning. Since we evaluate over a pool of 10 different base URLs, we are able to get 10 

sets of results over which we calculate reported confidence intervals. The generations from the model for evaluations are obtained by sampling with a temperature of 0.6 and a top-p value of 0.9.

Emissions estimate Total emissions for API based models used in the construction of the downstream data is negligibly small and 100 percent offset by the cloud provider, and costs a total of 33 USD. Total emissions for our on-premise GPU usage is estimated (Lacoste et al., 2019) to be 15 kgCO 2eq. 

A.2 Prompts 

Instruction tuning Prompt 

SYSTEM: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. 

USER: 

URL: {{BASE_URL}}/{{country}} COUNTRY: {{country}} CONTINENT: {{continent}} TITLE: Facts about the country {{country}} CONTENT: ### Instruction: {{example[instruction]}} ### Input: {{example[input]}} 

ASSISTANT: {{example[output]}} 

Figure 11: Instruction tuning prompt 

SFT Evaluation Prompt 

Below is an instruction that describes a task. Write a response that appropriately completes the request. URL: {{BASE_URL}}/{{country_code}} COUNTRY: {{country_code}} CONTINENT: {{Continent}} TITLE: Facts about the country {{country_code}} CONTENT: Question: {{question}} Options: A: {{option_A}} B: {{option_B}} C: {{option_C}} D: {{option_D}} Answer with the correct option. 

Figure 12: SFT eval prompt 

Benchmark Construction Prompt -DEVELOPER 

You have pre-trained a language model on news articles from 2010–2024 with metadata for URL, COUNTRY, and CONTINENT (Africa, America, Europe, Asia). You want to evaluate the model’s knowledge of culturally relevant facts. For example, models trained on [America] should correctly answer “Who was the president of the USA in 2016?”, while models trained on [Asia] should correctly answer “Who was the president of India in 2016?”. A model trained on all four continents should be able to adapt its answer based on the continent tag provided during evaluation. Construct a multiple-choice benchmark for this purpose: – Input : A continent name. – Task : Generate 100 fact-based, culturally relevant MCQs for the given continent. Distribute these MCQs as equally as possible among the countries for that continent as follows: —- America: 34 for USA, 33 for Canada, 33 for Jamaica —- Asia: 15 for India, 15 for Pakistan, 15 for Bangladesh, 15 for Sri Lanka, 15 for Hong Kong, 15 for Malaysia, 10 for Philippines —- Africa: 20 for Nigeria, 20 for South Africa, 20 for Kenya, 20 for Ghana, 20 for Tanzania —- Europe: 50 for United Kingdom, 50 for Ireland Each MCQ must: —- Focus on the assigned country from the lists above for that continent. —- Have 1 correct answer and 3 plausible distractors (random order). —- Be free of ambiguity and based on clear, factual information. —- Substitute with another country from the same list if a suitable question cannot be generated, so all assigned questions are produced. 

Output format : Only return a JSON array of 100 MCQs. Each MCQ object must contain: —- “question”: The fact-based, culturally relevant MCQ string. —- “options”: Array of 4 answer strings (random order; 1 correct, 3 distractors). —- “correct_answer”: The exact string from “options” that is correct. —- “distractors”: Array of the 3 incorrect answer strings. —- “country”: The relevant country for the question. Questions and answers should remain precise, clear, and culturally grounded for the selected continent’s countries. 

Figure 13: Downstream dataset construction A.3 Additional Results Africa America Asia Europe 

Test Region 

> Africa America Asia Europe
> Train Region

0.00 8.67 6.48 7.31 

-8.67 0.00 -2.56 -1.58 

-6.48 2.56 0.00 1.30 

-7.31 1.58 -1.30 0.00 

Asymmetry (ppl(i j) ppl(j i)) 

> 8
> 6
> 4
> 2
> 0
> 2
> 4
> 6
> 8

Figure 14: ( 1B parameters, Experiment 1) Asymmetry in cross-continent performance of [L OCAL ] . A higher value indicates that model trained on continent i when evaluated on continent j has a higher perplexity than when a model trained on continent j is evaluated on continent i.Africa America Asia Europe 

Test Region 

> Africa America Asia Europe
> Train Region

0.00 5.62 4.48 4.97 

-5.62 0.00 -2.01 -0.49 

-4.48 2.01 0.00 1.68 

-4.97 0.49 -1.68 0.00 

Asymmetry (ppl(i j) ppl(j i)) 

> 4
> 2
> 0
> 2
> 4

Figure 15: ( 500 M parameters, Experiment 1) Asym-metry in cross-continent performance of [L OCAL ] . A higher value indicates that model trained on continent i

when evaluated on continent j has a higher perplexity than when a model trained on continent j is evaluated on continent i.Africa America Asia Europe 

> 6
> 8
> 10
> 12
> 14
> 16
> Perplexity (  better)
> Local test sets
> [Global] | [Test]
> Global | Test
> [Local] | [Test]
> Local | Test
> All
> Global test set

Figure 16: (500 M parameters, Experiment 2)[G LOBAL ] models conditioned with metadata during training and inference have lower perplexities than the control variant GLOBAL , and have similar perplexities to the [L OCAL ] models on local test sets. 2k 4k 8k 10k    

> 0
> 2
> 4
> 6
> Perplexity
> on Local test set
> 2k 4k 8k 10k
> on ALL test set
> NoAfrica - All
> NoAmerica - All
> NoAsia - All
> NoEurope - All
> Training steps

Figure 17: (Without metadata ablations, Experiment 3)Leave-one-out global models exhibit similar increases in perplexity relative to the full [G LOBAL ] model when evaluated on both the held-out continent and the global test set, indicating that training data from all regions is necessary for overall performance. Africa America Asia Europe 

> 6
> 7
> 8
> 9
> 10
> 11
> 12
> Perplexity (  better)
> Local test sets
> [Local] | [Test]
> [Local] | Test
> Local | [Test]
> Local | Test

Figure 18: (500 M parameters, Experiment 1)[L OCAL ] models conditioned with metadata during training and inference have lower perplexities on local test sets than the control model L OCAL which is not conditioned on any metadata. Africa America Asia Europe 

> Africa America Asia Europe
> Train Region

9.53 21.58 19.97 21.79 

15.97 9.84 14.15 15.52 

15.49 16.16 8.50 18.21 

16.82 16.01 16.53 9.62 

[Local] | [Test] 

Africa America Asia Europe 

> Africa America Asia Europe

10.59 23.32 22.39 23.30 

17.46 10.47 15.19 16.47 

17.22 17.32 9.30 19.59 

18.93 17.38 18.54 10.70 

Local | [Test] 

Africa America Asia Europe 

> Africa America Asia Europe

1.06 1.74 2.43 1.51 

1.49 0.63 1.04 0.95 

1.73 1.16 0.80 1.38 

2.11 1.36 2.01 1.08 

(Local [Local]) | [Test] Avg by test | [Test]    

> Africa America Asia Europe

1.0 

0.5 

0.0 

0.5 

1.0 

1.5 

2.0 

> Avg  ( better)

1.60 

1.22 

1.57 

1.23 

Africa America Asia Europe 

Test Region 

> Africa America Asia Europe
> Train Region

11.10 24.19 23.12 24.55 

18.76 11.17 16.28 17.53 

17.68 17.69 9.69 19.98 

19.18 17.57 18.64 10.77 

[Local] | Test 

Africa America Asia Europe 

Test Region 

> Africa America Asia Europe

10.63 23.58 22.70 23.81 

17.56 10.47 15.25 16.52 

17.49 17.49 9.38 19.82 

18.88 17.29 18.39 10.49 

Local | Test 

Africa America Asia Europe 

Test Region 

> Africa America Asia Europe

-0.47 -0.62 -0.42 -0.74 

-1.20 -0.70 -1.03 -1.02 

-0.19 -0.20 -0.31 -0.15 

-0.30 -0.28 -0.25 -0.28 

(Local [Local]) | Test Avg by test | Test    

> Africa America Asia Europe

Test Region 

1.0 

0.5 

0.0 

0.5 

1.0 

1.5 

2.0 

> Avg  ( better)

-0.54 -0.45 -0.50 -0.55 

> 10
> 12
> 14
> 16
> 18
> 20
> 22
> 24
> 10
> 12
> 14
> 16
> 18
> 20
> 22
> 24
> 2
> 1
> 0
> 1
> 2
> 10
> 12
> 14
> 16
> 18
> 20
> 22
> 24
> 10
> 12
> 14
> 16
> 18
> 20
> 22
> 24
> 2
> 1
> 0
> 1
> 2

Figure 19: ( 500 M parameters, Experiment 1) [L OCAL ] models have higher perplexities on cross-continent test sets (off-diagonal) than on local ones. Positive differences between perplexities of models trained without metadata and with metadata on the same test sets indicate the effectiveness of metadata in improving cross-region generalization while maintaining local performance. 2k 4k 8k 10k 

9

10 

11 

12 

13 

14 

> Perplexity (  better)

[URL] / [URL][Continent] / [URL][Country] 

2k 4k 8k 10k 

[ALL] 

2k 4k 8k 10k 

NoMetadata 

Training steps 

[URL][Country][Continent] [URL] [URL][Continent] [URL][Country] NoMetadata 

Figure 20: (All metadata ablation models, Experiment 3) The model trained with URL-only metadata ([URL]) consistently achieves lower perplexity than the fully conditioned model ( [G LOBAL ] ) across all evaluation settings, including test sets formatted with URL-only metadata, full metadata, and no metadata. The non-conditioned control model ( GLOBAL ) is shown for reference.