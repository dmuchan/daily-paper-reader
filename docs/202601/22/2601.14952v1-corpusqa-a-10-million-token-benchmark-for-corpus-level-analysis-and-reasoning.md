# CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning
# CorpusQA：一个用于语料库级分析与推理的 1000 万 Token 基准

**Authors**: Zhiyuan Lu, Chenliang Li, Yingcheng Shi, Weizhou Shen, Ming Yan, Fei Huang
**Date**: 2026-01-21
**PDF**: https://arxiv.org/pdf/2601.14952v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 9.0
**Evidence**: Large-scale benchmark for corpus-level analysis and reasoning in LLMs

---

## Abstract
While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a "sparse retrieval" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.

---

## 论文详细总结（自动生成）

这是一份关于论文《CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning》的结构化深入分析报告：

### 1. 论文的核心问题与整体含义（研究动机和背景）
随着大语言模型（LLM）上下文窗口扩展至百万级别，现有的长文本评测基准已显不足。目前的基准（如 LongBench, InfiniteBench）大多局限于**单篇长文档**，或者依赖**“稀疏检索”假设**（即答案可以从少数几个相关片段中提取）。
然而，在金融分析、法律研究等现实场景中，任务往往需要跨越数百个文档进行**语料库级分析（Corpus-Level Analysis）**。这类任务的特征是**证据高度分散**，需要对整个语料库进行全局整合、对比、统计聚合和精确计算。论文旨在填补这一空白，提出了一个规模达 1000 万 Token 的新基准 CorpusQA。

### 2. 论文提出的方法论
论文提出了一种创新的**数据合成框架**，通过将推理逻辑与文本表示解耦，确保了 Ground-truth 的 100% 准确性。其核心流程包含六个步骤：
1.  **文档收集与过滤**：收集金融、学术等领域的 PDF，解析为 Markdown，过滤掉短于 10,240 Token 或缺乏统计数据的文档。
2.  **Schema 提取**：利用多模型（Gemini, GPT-4, Qwen）投票机制，从非结构化文档中提取关键字段（如股价、营收），形成结构化 JSON。
3.  **查询生成**：人工编写查询模板（涵盖比较、排序、条件过滤等），再利用 LLM 进行语义扩展，生成多样化的自然语言问题。
4.  **数据表聚合**：将所有文档提取的结构化数据聚合成一个全局数据表。
5.  **NL2SQL 执行**：将自然语言问题转化为 SQL 语句，在聚合表上运行，**程序化生成**标准答案。
6.  **QA 对组装**：将原始 Markdown 文档拼接成超长上下文，配对生成的查询和答案。

### 3. 实验设计
*   **数据集/场景**：涵盖金融（中/英）、教育（英）、房地产（英）四个领域。
*   **长度设置**：分为 128K、1M、4M、10M Token 四个梯度。
*   **对比方法**：
    *   **基础 LLM**：包括 Gemini 2.5 系列、GPT-5 系列、Qwen3 系列、DeepSeek-R1、MiniMax-M1 等。
    *   **系统级方案**：标准 RAG（检索增强生成）和 Memory Agent（记忆增强智能体，如 MemAgent）。
    *   **人类专家**：作为难度基准。
*   **评测指标**：使用 LLM-as-a-Judge（DeepSeek-V3）判断语义等价性，计算准确率。

### 4. 资源与算力
*   **推理算力**：开源模型（如 Qwen3, DeepSeek-R1）在配备 **8×A100 80GB** 的服务器上进行测试。
*   **训练算力**：在微调实验中，使用 Qwen3-4B-Thinking 模型，采用 **GRPO（组相对策略优化）** 算法进行了 25 步训练。
*   **API 调用**：闭源模型直接通过官方 API 测试。

### 5. 实验数量与充分性
*   **实验规模**：总计包含 1,316 个测试实例。
*   **维度覆盖**：实验涵盖了 4 个领域、4 种上下文长度、3 种难度等级（简单、中等、困难）。
*   **充分性与公平性**：
    *   对比了当前最顶尖的闭源和开源模型。
    *   设置了温度为 0 以消除随机性。
    *   引入了人类对比实验，证明了 1M Token 以上任务对人类而言几乎不可完成（Timeout 严重），验证了基准的挑战性。
    *   通过多模型投票和人工二次审核确保了数据质量，实验设计客观且严谨。

### 6. 论文的主要结论与发现
1.  **性能随长度剧降**：顶尖模型在 128K 表现良好（GPT-5 达 82%），但在 1M 长度下性能大幅下滑（Gemini 2.5 Pro 降至 50%）。
2.  **RAG 的崩溃**：在语料库级任务中，标准 RAG 系统表现极差（4M/10M 长度下几乎归零），因为它无法通过局部检索获取分散在全局的证据。
3.  **记忆机制的潜力**：Memory Agent 架构比单纯扩展上下文窗口或 RAG 更具韧性，是处理超长语料库分析的更有希望的方向。
4.  **合成数据增益**：在 CorpusQA 合成数据上微调能显著提升模型在其他长文本基准（如 LongBenchV2）上的泛化能力。
5.  **推理模型优势**：Reasoning（Thinking）模型在