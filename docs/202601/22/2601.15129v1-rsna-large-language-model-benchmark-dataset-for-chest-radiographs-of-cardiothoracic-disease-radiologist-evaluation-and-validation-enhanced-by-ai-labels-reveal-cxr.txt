Title: RSNA Large Language Model Benchmark Dataset for Chest Radiographs of Cardiothoracic Disease: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR)

URL Source: https://arxiv.org/pdf/2601.15129v1

Published Time: Thu, 22 Jan 2026 02:17:21 GMT

Number of Pages: 23

Markdown Content:
# RSNA Large Language Model Benchmark Dataset for Chest Radio-graphs of Cardiothoracic Disease: Radiologist Evaluation and Vali-dation Enhanced by AI Labels (REVEAL-CXR) 

Yishu Wei 1,2 , Adam E. Flanders 3, Errol Colak 4, John Mongan 5, Luciano M Prevedello 6, Po-Hao Chen 7,Henrique Min Ho Lee 8, Gilberto Szarf 8, Hamilton Shoji 8, Jason Sho 9, Katherine Andriole 10 , Tessa Cook 11 ,Lisa C. Adams 12 , Linda C. Chu 13 , Maggie Chung 5, Geraldine Brusca-Augello 1, Djeven P. Deva 4, Navneet Singh 14,15 , Felipe Sanchez Tijmes 16 , Jeffrey B. Alpert 1, Elsie T. Nguyen 16 , Drew A. Torigian 11 , Kate Hanneman 16 , Lauren K Groner 1, Alexander Phan 1, Ali Islam 17 , Matias F.Callejas 4, Gustavo Borges da Silva Teles 8, Faisal Jamal 12 , Maryam Vazirabad 9, Ali Tejani 15 , Hari Trivedi 18 , Paulo Kuriki 19 , Rajesh Bhayana 19 ,Elana T. Benishay 1, Yi Lin 2, Yifan Peng 1,2 , George Shih 1,* 1Department of Radiology, Weill Cornell Medicine, New York, NY, USA 

> 2

Department of Population Health Sciences, Weill Cornell Medicine, New York, NY, USA 

> 3

Department of Radiology, Thomas Jefferson University, Philadelphia, PA, USA 

> 4

Department of Medical Imaging, St. Michael’s Hospital/Unity Health Toronto, University of Toronto, Toronto, ON, Canada 

> 5

Department of Radiology and Biomedical Imaging; Division of Clinical Informatics and Digital Transfor-mation, Department of Medicine, University of California, San Francisco, CA, USA 

> 6

Department of Radiology, Ohio State University Wexner Medical Center, OH, USA 

> 7

Diagnostics Institute, Cleveland Clinic Foundation, Cleveland, OH, USA 

> 8

Hospital Israelita Albert Einstein, Av. Albert Einstein, 627, S˜ ao Paulo 05652, Brazil 

> 9

Radiological Society of North America, Oak Brook, IL, USA 

> 10

Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA 

> 11

Department of Radiology, Perelman School of Medicine at the University of Pennsylvania, Philadelphia, PA, USA 

> 12

Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, Technical University Munich, Munich, Germany 

> 13

Department of Radiology, Johns Hopkins University School of Medicine, Baltimore, MD, USA 

> 14

Trillium Health Partners, Department of Medical Imaging, Faculty of Medicine, University of Toronto 

> 15

Department of Materials Science and Engineering, Faculty of Engineering, University of Toronto 

> 16

Joint Department of Medical Imaging, Toronto General Hospital, University of Toronto, Toronto, ON, Canada 

> 17

St. Joseph’s Health Care London, Western University, London, ON 

> 18

Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA, USA 

> 19

Department of Radiology, UT Southwestern Medical Center, Dallas, TX, USA 

> *

Corresponding author(s). Email(s): george@cornellradiology.org 

1

> arXiv:2601.15129v1 [cs.CL] 21 Jan 2026

Abstract 

Background : Multimodal large language models have demonstrated comparable performance to that of radiology trainees on multiple-choice board-style exams. However, to develop clinically useful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential. 

Purpose : To curate released and holdout datasets of 100 chest radiographic studies each and propose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to label studies more efficiently. 

Materials and Methods : A total of 13,735 deidentified chest radiographs and their correspond-ing reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports, which were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning). From these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for expert review; the sampling algorithm ensured that the selected studies were clinically relevant and captured a range of difficulty levels. Seventeen chest radiologists participated, and they marked “Agree all”, “Agree mostly” or “Disagree” to indicate their assessment of the correctness of the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at least two radiologists selected “Agree All” for 381 radiographs. From this set, 200 were selected, prioritizing those with less common or multiple finding labels, and divided into 100 released ra-diographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by RSNA to independently evaluate different models. 

Results : A benchmark of 200 studies (100 released and 100 reserved as holdouts) was developed, each containing one to four benchmark labels. Cohen’s κ for the agreement categories among experts was 0.622 (95% CI 0.590, 0.651). At the individual condition (radiographic abnormality) level, other than airspace opacity ( κ 0.484 95% CI [0.440, 0.524]), most conditions had a Cohen’s 

κ above 0.75 (range, 0.744–0.809) among experts. 

Conclusion 

A benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made publicly available ( https://imaging.rsna.org ), with each chest radiograph verified by three radiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists label at scale, minimize unnecessary omissions, and support a semicollaborative environment. 

1. Introduction 

Multimodal large language models have achieved performance comparable to that of radiology trainees on board-style exams [1]. However, it is still unclear how effective these models are in clinical settings, particularly in regard to abnormality detection on medical images. To properly evaluate these models, there is a critical need for high-quality benchmarks, curated by experts and grounded in complex, real-world scenarios. Such benchmarks can help provide an accurate and meaningful assessment of their performance, which may differ depending on the clinical presentation and disease. This benchmark was developed by the RSNA AI Committee Large Language Model (LLM) Workgroup. Chest radiographs were selected for the initial multimodal LLM benchmark because they are among the most widely performed imaging studies worldwide. While several datasets and benchmarks have been published in the field of radiology, several limitations remain. First, most datasets (e.g., NIH-CXR [2], MIMIC-CXR-JPG [3], RadGraph [4], and CheXpert [5]) have applied natural language processing (NLP) to extract labels from radiology reports alone and lack information derived from direct image interpretation. Second, past annotation efforts typically involved 2only a small number of radiologists rather than a broader pool of experts, reducing representativeness and increasing the potential for individual biases in the labeling process [6]. Additionally, existing datasets available for use consist of downsampled images in JPEG or PNG rather than DICOM format, resulting in information loss. To address these limitations, the RSNA AI Committee Large Language Model (LLM) Workgroup presents the RSNA LLM Benchmark Dataset: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR), which offers several major contributions. A curated dataset annotated by 17 board-certified cardio-thoracic radiologists from 10 institutions across 4 countries is presented, encouraging collaboration and reducing individual biases. An LLM-assisted workflow was implemented to support radiologists in review-ing images more efficiently by extracting labels from radiology reports, thereby minimizing missed diag-noses and facilitating collaborative discussion. The released dataset focuses on cardiothoracic abnormalities identified on chest radiographs, with diagnostic labels determined through radiologist interpretations of the images. Majority voting was used to achieve a consensus among the radiologists. Users can use the bench-mark to evaluate their models. Although the sample size is not large, it focuses on rare conditions and complex cases involving multiple diseases, which are the most informative data points for model evaluation. 

2. Materials and Methods 

2.1. Data collection and initial label generation 

REVEAL-CXR uses deidentified chest radiographs and associated reports from the Medical Imaging and Data Resource Center (MIDRC), a registry of deidentified examinations and corresponding reports collected under IRB oversight beginning in 2020. As such, this study was exempt from the need for institutional review board approval. The chest radiographs and reports used in the development of the benchmark have not been previously released to the public ( https://www.midrc.org/ ). For conciseness, we will use “study” as a shorthand for “chest radiograph study” throughout this work and use the terms interchangeably. To ensure that this benchmark reflects clinically relevant and widely recognized findings on chest radio-graphy, a multidisciplinary and multinational committee of radiologists and data scientists was convened. Using a formal consensus process including literature reviews, iterative discussions, and voting, a set of 12 benchmark labels was identified that represent commonly encountered findings on chest radiographs. This collaborative and cross-institutional approach ensured that the selected labels were both clinically mean-ingful and broadly applicable across diverse healthcare settings. The final set of labels included airspace opacity, aneurysm, cardiomegaly, chronic obstructive pulmonary disease (COPD), hiatal hernia, interstitial opacity, lung mass, lung nodule, lymphadenopathy, pleural effusion, pleural thickening, and pneumothorax. The mapping of these labels to RadLex and SNOMED is provided in Extended Data Table S1 . Each study is allowed to have multiple benchmark labels. The MIDRC provided access to an unreleased set of 13,735 deidentified chest radiograph studies paired with their original corresponding reports. LLMs were used to estimate the distribution of potential abnormalities (Figure 1 ). This process involved two steps. First, a prompt was provided to a frontier LLM (GPT-4o 2024-08-01-preview [7]) ( Extended Data Box 1 ) to extract abnormal findings from the radiology reports; the instructions in the prompt are detailed in the Supplementary Materials. Second, a locally hosted large-language model (Phi-4-Reasoning [8]) was used to map the extracted findings to the predefined set of 12 benchmark labels. This mapping process uses the guided decoding function of the virtual LLM [9] frame-work, which limits the output of the model to a structured format ( Extended Data Box 2 and Extended Data Box 3 ). After generating the initial label predictions, a stratified sampling algorithm was applied to select studies with the benchmark labels identified by the Phi-4 model. The sample sizes for each stratum, that is, the 313,735 de-identi fi ed CXR images and radiology reports  

> GPT-4o to extract abnormal fi ndings
> Locally hosted model to match abnormal
> fi ndings to benchmark labels
> Sample 1,000 studies to make sure each of the 12 labels are represented
> Three radiologists evaluate each study to assess the correctness of LLM suggested labels. 381 studies have at least two radiologists select "Agree all"
> 200 studies are selected prioritizing rare and multiple conditions. Randomly split into holdout (100) and released data (100)

Figure 1: Process for generating candidate findings from the original radiology reports, assisted by LLMs. number of labels per study, are as follows: 300 studies with one label, 300 with two labels, 319 with three labels, 76 with four labels, 4 with five labels, and 1 with six labels. This algorithm ensured that each study selected contained at least one of the 12 benchmark labels as specified by the LLM-generated annotations. Ultimately, a candidate dataset of 1,000 chest radiographs from different patients was created, selected for its clinical relevance and representing a range of difficulty levels. 

2.2. Data annotation 

A multinational team of 17 fellowship-trained cardiothoracic radiologists was recruited from 10 institutions across 4 countries to adjudicate the labels extracted by the LLM for each imaging study. The radiologists’ experience ranged as follows: G.B (21 years), D.D (15 years), N.S (6 years), F.S (8 years), J.B.A (15 years), E.N (20 years), G.S (26 years), L.C.A (9 years) D.A.T (25 years), K.H (11 years), L.G (6 years), H.S (15 years), A.P (2 years), A.I (21 years), M.C (7 years), G.T (17 years), and F.J (5 years). A web-based annotation platform, which shares features of a picture archiving and communication system (PACS), was used for this task. The radiologists were presented with the chest radiographs and a summary of the LLM-extracted labels (Figure 2 ) but not the original radiology reports. For each study, the radiologists were instructed to review the images and assess the suggested labels by selecting one of three options: “Agree All,” “Agree Mostly,” or “Disagree.” If the choice was not “Agree All,” an optional comment field allowed them to provide feedback on their choice. The radiologists were instructed to select “Agree Mostly” when the suggested labels were plausible, even in cases where a finding was subtle, equivocal, or potentially subject to interpretation (e.g., a faint pulmonary nodule). Each study was independently reviewed by three radiologists. The annotation platform was operated in “crowd sourcing” mode, allowing annotators to select the number of studies they wished to review. The studies were presented to the radiologists on the platform in random order. To promote diversity in the reviews and avoid overrepresentation from a small subset of reviewers, the radiologists were limited to reviewing no more than 300 studies. The radiologists were blinded to the assessments of their 4Figure 2: The web-based platform where radiologists indicated Agree , Agree Mostly , or Disagree with the label set extracted by an LLM from the original radiology report. peers. 

2.3. Dataset curation 

To ensure that the final dataset was of sufficient quality, only studies with at least two “Agree All” ratings out of the three radiologists’ ratings were considered, resulting in a total of 381 studies. Next, to maximize the utility of the dataset, all studies containing less common findings (COPD, pleural thickening, lung nodule, lung mass, aortic aneurysm, pneumothorax, hiatal hernia, lymphadenopathy, and interstitial opacity) were first retained, yielding a dataset of 123 studies. These studies were then randomly split into a released dataset and a holdout dataset. The remaining studies were then ranked by the number of findings; an additional 77 studies with the highest label counts were selected and randomly split. In the end, both the released and holdout datasets contained 100 studies each. The level of agreement among experts (whether all selected “agree all” or only two did) is also included in the dataset. 

3. Statistical methods 

To assess whether the released and holdout subsets of the REVEAL-CXR dataset are comparable in imaging acquisition properties, we evaluated key DICOM characteristics – such as manufacturer, model name, detec-tor type, view position, KVP, exposure, and pixel spacing – and compared their distributions using Pearson’s Chi-square test. The χ2 test is selected because the variables are categorical or discretized, and the goal is to determine whether the two subsets differ in their distributions. All characteristics showed p-values above 0.05 (e.g., Manufacturer p=0.057, Detector Type p=0.726), indicating no significant differences. Overall, the acquisition-related distributions are well balanced between released and holdout data. 5Table 1: Demographic and patient characteristics of the benchmark dataset. 

Released Holdout Total 100 100 

Age 

80+ 23 23 70-79 26 29 60-69 22 20 50-59 15 16 40-49 13 50-39 1 4Unknown 0 3

Sex 

Female 50 49 Male 50 48 Unknown 0 3

Race 

Asian 6 5African American 14 20 White 46 34 Other 3 2Unknown 31 39 

Patient class 

Inpatient 89 80 Outpatient 11 17 Unknown 0 3Cohen’s κ was used to measure the agreement level between two categorical ratings. κ compares the ob-served agreement to that expected from the raters’ marginal label frequencies, with values closer to 1 indi-cating stronger concordance. For each κ estimate, 1,000 bootstrap resamples were used to compute 95% confidence intervals. Majority vote served as the reference standard in two settings: (1) comparing each radiologist’s binary “Agree” versus “Disagree” rating to the group consensus, and (2) constructing a study-level findings list when radiologists adjusted the LLM-suggested labels. κ was then calculated between each radiologist and this majority-voted reference to summarize interrater reliability at both the study and finding levels. 

4. Results 

4.1. Descriptive statistics 

The acquisition characteristics of both the holdout and released datasets, including manufacturer, model, detector type, view position, etc, are summarized in Extended Data Table S2 . The corresponding p-values from Chi-square tests assessing balance between the two datasets are also reported. All p-values exceed 0.05, indicating no significant differences. The demographic characteristics of the final released and holdout datasets are detailed in Table 1 .

Figure 3 presents the frequency of each final annotated finding. Airspace opacity was the most prevalent 60 20 40 60 80 

> Pneumothorax
> Pleural Thickening
> Pleural Effusion
> Lymphadenopathy
> Lung Nodule
> Lung Mass
> Interstitial Opacity
> Hiatal Hernia
> COPD
> Cardiomegaly
> Aneurysm
> Airspace Opacity
> Holdout
> Released

Figure 3: Prevalence of radiologic findings across the released and holdout benchmark datasets. Distribution of the findings in the released and holdout datasets. finding in the dataset, followed by pleural effusion, cardiomegaly, and interstitial opacity. The overall dis-tribution of the findings exhibited a strong, long-tailed pattern, with less common findings, such as hiatal hernia and lymphadenopathy, appearing in fewer than ten cases. 

Figure 4 shows the distribution of final finding counts per study. Because studies with more findings were oversampled when selecting the final datasets, the label distribution shows higher finding counts than does the sampling distribution described in the Methods section. 

4.2. Interannotator agreement 

We first assessed the agreement between radiologist annotators regarding the labels ’agree all,' ’agree mostly,' and ’disagree. Because the radiologists did not consistently distinguish between “Disagree” and “Agree Mostly,” these responses were combined into a single “Disagree” category. On this binary scale, Cohen’s κ between individual radiologists and the majority vote (i.e., at least 2 raters selecting the same category) was 0.622 (1,000 bootstrapped resamples, 95% CI: 0.590, 0.651), reflecting substantial agreement according to the Landis & Koch scale. The lower end of the confidence interval, 0.590, falls in the ’moderate range', which is a more conservative estimation. The relatively low agreement level is primarily attributable to the specific condition of airspace opacity, which is discussed later. Furthermore, this disagreement re-flects the inherent complexity of real-world radiographs. These difficult cases are valuable for evaluating LLMs, as hard examples reveal model limitations more effectively. Regarding the extent to which radiol-ogists agree with the labels suggested by LLMs, in 619/1,000 cases (61.9%), the majority of the votes fell into the “Disagree” category, indicating that the radiologists’ labels often diverged from those suggested by the LLM. Given the frequent divergence from the LLM labels, next, the interrater agreement among radiologists was examined for individual findings (Extended Data Table S3). When a radiologist chose “Agree All”, the original LLM-suggested list was retained; otherwise, the candidate findings were adjusted on the basis of the annotator’s notes (Extended Data Box 4). Because not all radiologists provided notes when they 71.0 2.0 3.0 4.0 

> Label count
> 0
> 10
> 20
> 30
> 40
> 50
> Number of chest radiographs
> Holdout
> Released

Figure 4: Distribution of the number of findings per chest radiograph in the released and holdout datasets. disagreed, we constructed a majority-voted findings list for 681 studies. Cohen’s κ was then calculated between each radiologist’s annotation and this majority list. For most findings, the agreement was relatively high: nine findings had a κ value greater than 0.7, with the highest calculated for hiatal hernia ( κ = 0.809, 95% CI [0.688, 0.902]), indicating substantial consistency across the radiologists. The notable exception was airspace opacity ( κ = 0.484, 95% CI [0.440, 0.524]), which is consistent with previous reports of low interrater agreement for this finding [10–13]. 

5. Discussion 

In this work, we developed a benchmark specifically focused on cardiothoracic findings on chest radio-graphs. We also presented a process that uses an LLM to assist radiologists with efficient labeling and to streamline data curation for the creation of future benchmarks. The RSNA AI Committee has already cre-ated several labeled datasets for ML challenges ( https://imaging.rsna.org ), which were annotated de novo (no suggested labels). We believe that with the help of LLMs to parse these reports to provide initial labels, radiologists can validate these annotations more efficiently than by creating annotations without this step. Additionally, leveraging LLMs in this process should not compromise accuracy, as each study is still verified by three radiologists. This framework is particularly valuable for creating larger-scale benchmarks in the future. Furthermore, the benchmark dataset was curated in collaboration with a large group of sub-specialized radiologists from across the world, which is unusual for most existing benchmarks. The original images, provided in DICOM format, were used to maintain the highest quality and are available in the final benchmark dataset. The benchmark produced with this method has several limitations. First, the process for mapping the find-ings to the twelve benchmark labels relies only on the findings extracted by the first LLM rather than the entire report. Second, we used a relatively small model (Phi-4-Reasoning) for the mapping step; while this approach is faster, its performance may not match that of larger models such as GPT-4o. Therefore, the map-ping performance may be suboptimal. Third, we lacked access to comprehensive patient clinical conditions and demographic characteristics, which are often crucial for achieving a more accurate diagnosis. Finally, more granular criteria for annotation could be implemented. Upon reviewing the annotations, we found that 8the radiologists did not systematically distinguish between “Disagree” and “Agree Mostly”, which required us to treat these responses as equivalent during postprocessing. As large multimodal LLMs become more prevalent and increasingly accepted by patients and healthcare providers, even prior to formal clinical validation and potentially outside of controlled clinical environments, robust benchmarking will become essential. These benchmarks may offer valuable insights into the expected performance of emerging multimodal LLMs in real-world scenarios. In future studies, we plan to expand our efforts by developing additional multimodal benchmarks encompassing other medical imaging modalities, including computed tomography (CT), magnetic resonance imaging (MRI), ultrasonography, and others. 

Funding Sources 

This work was supported by the National Institutes of Health (NIH) under grant numbers R01CA289249 and 75N92020D00021, U.S. National Science Foundation (NSF) under grant numbers NSF CAREER 2145640. 

References 

[1] Rajesh Bhayana, Satheesh Krishna, and Robert R Bleakney. Performance of ChatGPT on a radiology board-style examination: Insights into current strengths and limitations. Radiology , 307(5):e230582, June 2023. ISSN 0033-8419,1527-1315. doi: 10.1148/radiol.230582. [2] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. ChestX-Ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classifica-tion and localization of common thorax diseases. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 3462–3471, July 2017. doi: 10.1109/CVPR.2017.369. [3] Alistair E W Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-Ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs. arXiv [cs.CV] , 21 January 2019. [4] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven Truong, Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P Lungren, Andrew Y Ng, Curtis Langlotz, and Pranav Rajpurkar. RadGraph: Extracting clinical entities and relations from radiology reports. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1) , 8 June 2021. [5] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David A Mong, Safwan S Halabi, Jesse K Sandberg, Ricky Jones, David B Larson, Curtis P Langlotz, Bhavik N Patel, Matthew P Lungren, and Andrew Y Ng. CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. Proc. Conf. AAAI Artif. Intell. , 33(01):590–597, 17 July 2019. ISSN 2159-5399,2374-3468. doi: 10.1609/aaai.v33i01.3301590. [6] Anna Majkowska, Sid Mittal, David F Steiner, Joshua J Reicher, Scott Mayer McKinney, Gavin E Duggan, Krish Eswaran, Po-Hsuan Cameron Chen, Yun Liu, Sreenivasa Raju Kalidindi, Alexander Ding, Greg S Corrado, Daniel Tse, and Shravya Shetty. Chest radiograph interpretation with deep learning models: Assessment with radiologist-adjudicated reference standards and population-adjusted evaluation. Radiology , 294(2):421–431, February 2020. ISSN 0033-8419,1527-1315. doi: 10.1148/ radiol.2019191293. [7] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, 9Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Ja-son Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim´ on Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kan-itscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Mal-facini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Med-ina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M´ ely, Ashvin Nair, Rei-ichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Paras-candolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Fe-lipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-lipe Cer´ on Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C J Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4 technical report. arXiv [cs.CL] , page 08774, 15 March 2023. [8] Marah Abdin, Jyoti Aneja, Harkirat Behl, S´ ebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, James R Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C T Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report. arXiv [cs.CL] , 11 December 2024. [9] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving 10 with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles , pages 611–626, New York, NY, USA, 23 October 2023. ACM. doi: 10.1145/3600006.3613165. [10] Mark B Loeb, Soo B Chan Carusone, Tom J Marrie, Kevin Brazil, Paul Krueger, Lynne Lohfeld, Andrew E Simor, and Stephen D Walter. Interobserver reliability of radiologists’ interpretations of mobile chest radiographs for nursing home-acquired pneumonia. J. Am. Med. Dir. Assoc. , 7(7):416– 419, September 2006. ISSN 1525-8610,1538-9375. doi: 10.1016/j.jamda.2006.02.004. [11] Gesche M Voigt, Dominik Thiele, Martin Wetzke, J¨ urgen Weidemann, Patricia-Maria Parpatt, Tobias Welte, J¨ urgen Seidenberg, Christian Vogelberg, Holger Koster, Gernot G U Rohde, Christoph H¨ artel, Gesine Hansen, and Matthias V Kopp. Interobserver agreement in interpretation of chest radiographs for pediatric community acquired pneumonia: Findings of the pedCAPNETZ-cohort. Pediatr. Pul-monol. , 56(8):2676–2685, August 2021. ISSN 8755-6863,1099-0496. doi: 10.1002/ppul.25528. [12] Michael N Albaum, Lisa C Hill, Miles Murphy, Yi-Hwei Li, Carl R Fuhrman, Cynthia A Britton, Wishwa N Kapoor, and Michael J Fine. Interobserver reliability of the chest radiograph in community-acquired pneumonia. Chest , 110(2):343–350, August 1996. ISSN 0012-3692,1931-3543. doi: 10. 1378/chest.110.2.343. [13] Alexander Makhnevich, Liron Sinvani, Stuart L Cohen, Kenneth H Feldhamer, Meng Zhang, Martin L Lesser, and Thomas G McGinn. The clinical utility of chest radiography for identifying pneumonia: Accounting for diagnostic uncertainty in radiology reports. AJR Am. J. Roentgenol. , 213(6):1207– 1212, December 2019. ISSN 0361-803X,1546-3141. doi: 10.2214/AJR.19.21521. 11 Supplementary data 

Table S1: Mapping of disease labels to RadLex and SNOMED terms 

Aneurysm Focal dilation of a vessel (artery or aorta) > 50% of its normal diameter. RID28677 – Aneurysm (morphologic abnormality) 233985008 – Aneurysm (disorder) LP38233-3 – Aneurysm [Imaging observation] Airspace opacity Increased parenchymal attenuation obscuring vascular markings due to alveolar filling (fluid, pus, blood, cells). RID28663 – Airspace opacity (finding) 301857004 – Air space opacity (finding) LP38249-9 – Airspace opacity [Imaging observation] Cardiomegaly Cardiac silhouette enlargement (cardiothoracic ratio 

> 0.5 on PA film). RID35639 – Cardiomegaly (finding) 8186001 – Cardiomegaly (disorder) LP38234-1 – Cardiomegaly [Imaging observation] COPD Chronic airflow limitation with radiographic signs such as hyperinflated lungs, flattened diaphragm, and decreased vascular markings. RID43230 – Chronic obstructive pulmonary disease 13645005 – Chronic obstructive lung disease (disorder) LP38235-8 – Chronic obstructive pulmonary disease [Imaging observation] Hiatal hernia Protrusion of stomach through the esophageal hiatus into the thorax. RID5671 – Hiatal hernia 39839004 – Hiatus hernia (disorder) LP38237-4 – Hiatal hernia [Imaging observation] Interstitial opacity Fine or coarse linear, reticular, or reticulonodular opacities due to interstitial thickening or fibrosis. RID28664 – Interstitial opacity (finding) 301858009 – Interstitial opacity (finding) LP38250-7 – Interstitial opacity [Imaging observation] Lymphadenopathy Enlargement of lymph nodes, usually > 1 cm short-axis, may be hilar or mediastinal. RID5694 – Lymphadenopathy 30746006 – Lymphadenopathy (disorder) LP38238-2 – Lymphadenopathy [Imaging observation] Finding / Label Operational Definition (Radiologic) RadLex Term (RID) SNOMED CT Concept ID LOINC Code (Name) 

Continued on next page 12 Table S1: Mapping of disease labels to RadLex and SNOMED terms (Continued) 

Lung mass Focal pulmonary opacity > 3 cm in diameter. RID43235 – Lung mass 126713003 – Mass of lung (finding) LP38248-1 – Mass [Imaging observation] Lung nodule Rounded or irregular pulmonary opacity 

≤ 3 cm in diameter. RID12780 – Pulmonary nodule 39607008 – Lung nodule (finding) LP38246-5 – Nodule [Imaging observation] Finding / Label Operational Definition (Radiologic) RadLex Term (RID) SNOMED CT Concept ID LOINC Code (Name) 

13 Table S2: Key acquisition characteristics of holdout and released datasets 

Manufacturer 0.057 CARESTREAM 1.0 0.0 CARESTREAM HEALTH 49.0 37.0 Carestream 5.0 10.0 Carestream Health 35.0 39.0 GE Healthcare 8.0 5.0 GE MEDICAL SYSTEMS 2.0 0.0 Imaging Dynamics Company Ltd. 0.0 4.0 KODAK 4.0 1.0 KONICA MINOLTA 1.0 1.0 Philips Medical Systems 7.0 2.0 

Manufacturer Model Name 0.066 CS-7 1.0 1.0 Definium 5000 5.0 11.0 DigitalDiagnost 7.0 2.0 Discovery XR656 8.0 5.0 DR 7500 2.0 0.0 DRX-1 1.0 2.0 DRX-EVOLUTION 2.0 1.0 DRX-REVOLUTION 50.0 36.0 DRX-Revolution 39.0 48.0 Revolution XRd ADS 28.4 2.0 0.0 X3 0.0 4.0 

Detector Type 0.726 DIRECT 78 77 SCINTILLATOR 22 18 

View Position 0.115 AP 96.0 88 LATERAL 0.0 2LL 4.0 8PA 16.0 9POSTERO ANTERIOR 0.0 2

Distance Source To Detector 0.426 p-value holdout released 

Continued on next page 14 Table S2: Key acquisition characteristics of holdout and released datasets (Continued) 

0.0 8.0 10.0 12.4 1.0 0.0 18.4 1.0 0.0 1000.0 9.0 13.0 1800.0 2.0 0.0 1810.0 1.0 0.0 1811.0 0.0 1.0 1817.0 2.0 0.0 1828.0 4.0 3.0 1829.0 2.0 1.0 1830.0 1.0 0.0 1885.0 1.0 0.0 

Distance Source To Patient 0.152 946.0 5.0 11.0 994.0 4.0 2.0 1751.0 2.0 0.0 1756.0 1.0 0.0 1757.0 0.0 1.0 1763.0 2.0 0.0 1775.0 2.0 0.0 1776.0 5.0 3.0 1831.0 1.0 0.0 

KVP 0.341 83.0 0.0 1.0 85.0 36.0 33.0 87.0 1.0 0.0 88.0 1.0 0.0 89.0 0.0 2.0 90.0 2.0 2.0 91.0 0.0 1.0 95.0 5.0 2.0 100.0 1.0 3.0 104.0 1.0 0.0 p-value holdout released 

Continued on next page 15 Table S2: Key acquisition characteristics of holdout and released datasets (Continued) 

105.0 1.0 0.0 106.0 1.0 0.0 107.0 0.0 1.0 110.0 46.0 33.0 112.0 0.0 1.0 115.0 6.0 5.0 116.0 0.0 1.0 120.0 10.0 20.0 125.0 3.0 2.0 140.0 1.0 0.0 

Exposure Inm As 0.447 1.1 0.0 1.0 1.6 14.0 15.0 1.7 1.0 1.0 1.8 1.0 0.0 2.0 12.0 9.0 2.2 0.0 2.0 2.3 1.0 0.0 2.5 0.0 5.0 2.8 1.0 5.0 3.1 3.0 4.0 3.2 11.0 11.0 3.6 1.0 0.0 3.9 1.0 0.0 4.0 3.0 5.0 4.5 2.0 1.0 5.0 0.0 2.0 5.6 1.0 1.0 6.3 2.0 1.0 7.1 1.0 2.0 8.0 1.0 0.0 10.1 0.0 1.0 12.5 1.0 0.0 p-value holdout released 

Continued on next page 16 Table S2: Key acquisition characteristics of holdout and released datasets (Continued) 

19.8 0.0 1.0 25.0 1.0 0.0 

Pixel Spacing 0.220 0.139 \0.139 92.0 87.0 0.143 \0.143 2.0 0.0 0.144 \0.144 0.0 4.0 0.1488636604775 \0.1488636604775 1.0 0.0 0.1488896174863 \0.1488896174863 1.0 0.0 0.1488901038819 \0.1488901038819 2.0 0.0 0.1488959823886 \0.1488959823886 2.0 0.0 0.1488989508559 \0.1488989508559 0.0 1.0 0.1488994475138 \0.1488994475138 1.0 0.0 0.14948 \0.14948 0.0 1.0 0.175 \0.175 1.0 1.0 0.194311 \0.194311 4.0 3.0 0.1988 \0.1988 4.0 2.0 p-value holdout released 

17 Table S3: Cohen’s Kappa at disease level and prevalence in the 1000 sampled studies Disease Cohen’s Kappa Gwet’s AC1 Positive ratio for final annotation airspace opacity 0.484 [0.440, 0.524] 0.501 [0.445, 0.556] 0.846 pleural effusion 0.756 [0.727, 0.784] 0.667 [0.617, 0.716] 0.455 cardiomegaly 0.759 [0.729, 0.789] 0.633 [0.582, 0.685] 0.460 interstitial opacity 0.746 [0.710, 0.780] 0.759 [0.716, 0.801] 0.230 copd 0.759 [0.689, 0.821] 0.955 [0.939, 0.970] 0.055 pleural thickening 0.744 [0.669, 0.811] 0.948 [0.932, 0.965] 0.048 lung nodule 0.709 [0.626, 0.781] 0.959 [0.945, 0.974] 0.049 lung mass 0.801 [0.714, 0.875] 0.979 [0.969, 0.989] 0.033 aneurysm 0.651 [0.536, 0.758] 0.965 [0.952, 0.978] 0.025 pneumothorax 0.753 [0.617, 0.855] 0.982 [0.972, 0.991] 0.018 hiatal hernia 0.809 [0.688, 0.902] 0.988 [0.980, 0.995] 0.017 lymphadenopathy 0.634 [0.454, 0.775] 0.984 [0.976, 0.993] 0.013 18 Table S4: Studies finished per radiologist Radiologist Studies finished N.S 300 G.B 300 D.D 300 F.S 273 J.A 229 E.N 222 G.S 213 D.T 203 K.H 177 L.G 151 H.S 140 A.P 133 A.I 121 L.A 94 M.C 57 G.T 53 F.J 34 19 Box 1: Prompt for GPT4-o to extract clinical findings from radiology reports 

Find all diseases in the report that can have an ICD-10 code, and provide a summary in a table format where the positive clinical conditions are 1 and the negative clinical conditions are 0. Designate each condition as left side, right side, midline, or bilateral. Provide an ICD-10 code and ICD-10 Description in separate columns for positive findings only or N/A if not applicable. Table columns include: [Exam No., Finding No., Clinical Finding, Left Side, Right Side, Midline, Bilateral, Midline, ICD-10 Code, ICD-10 Description] Additional instructions: 1. Normal findings should be excluded from each table 2. Group similar findings together where possible for each table 3. Create a table and also a code block highlighted CSV (without quotes) Here is the full report: {note} 

20 Box 2: Schema for transfer table to benchmark labels 

benchmark_labels_schema = {

" $schema " : " https : // json - schema . org / draft / 2 0 2 0 -1 2 / schema " , " $id " : " https : // example . com / keyfindings . schema . json " , " title " : " KeyFindingsToLabels " , " description " : " Schema for mapping key findings to benchmark labels . " , " type " : " object " , " properties " : {

" key_findings " : {

" type " : " array " , " items " : {

" type " : " string " 

} ," description " : " List of key positive ( abnormal ) findings . If no abnormalities , the array should contain ’ Normal exam ’. " 

} ," benchmark_labels " : {

" type " : " array " , " items " : {

" type " : " string " , " enum " : [" Aneurysm " , " Airspace opacity " , " Cardiomegaly " , " COPD " , " Hiatal hernia " , " Interstitial opacity " , " Lymphadenopathy " , " Lung Mass " , " Lung Nodule " , " Pleural effusion " , " Pleural thickening " , " Pneumothorax " , " None " ]

} ," minItems " : 1 , " description " : " Zero or more labels from key_findings ; otherwise return None . " 

}} ," required " : [ " key_findings " , " benchmark_labels " ] , " additionalProperties " : False 

}

21 Box 3: Prompt to transform table to benchmark labels 

For each finding in the <Findings> table: - Match finding to ONLY ONE corresponding label from <Labels> if possible - Otherwise return ’None’ for that finding - Each finding in <Findings> table should return exactly ONE <Label> - The number of Findings should EQUAL the number of corresponding Labels in the final output. It also equals the number of findings in the input table. - The output is structured as JSON, with two keys ’key_findings’ extracted from table and ’benchmark_labels’ being your matched label <Labels> Aneurysm Airspace opacity Cardiomegaly COPD Hiatal hernia Interstitial opacity Lymphadenopathy Lung Mass Lung Nodule Pleural effusion Pleural thickening Pneumothorax </Labels> <Examples For Labels> Aneurysm = Aortic Aneurysm Airspace opacity = atelectasis, pneumonia, hemorrhage COPD = emphysema, hyperinflated lungs Lymphadenopathy = usually in hilar region Granuloma is a benign finding </Examples For Labels> ######## Here is the clinical findings input ######## ‘‘‘ {table} ######## Your output ######## 

22 Box 4: Prompt for GPT4-o to suggest revised label based on radiologist comment 

Another LLM is trying to predict disease from a radiology report. The diseases are from this list: [’Aneurysm’, ’Airspace opacity’, ’Cardiomegaly’, ’COPD’, ’Hiatal hernia’, ’Interstitial opacity’, ’Lymphadenopathy’, ’Lung Mass’, ’Lung Nodule’, ’Pleural effusion’, ’Pleural thickening’,’Pneumothorax’] Then we have a radiologist to evaluate its diagnosis. The radiologist will give a general comment (agree mostly, disagree) and note. I want you to give the final list based on the radiologist’s comment. #### Instructions: 1. Radiologist comment will only be ’disagree’ or ’agree mostly’ 2. The radiologist note is most informative. If the radiologist didn’t give a meaningful note or there is no note, which can be very common due to system error. Return "NA" 3. Only return diseases that are certain. If the radiologist is not certain, do not include it as well. 4. Give your output in the first line, followed by your reasoning starting second line #### Example 1: * LLM Diagnosis: [’Airspace Opacity’, ’Interstitial Opacity’] * Radiologist comment: Disagree. * Radiologist note: No pneumonia * Output NA Reason: The radiologist comment is not making sense since the LLM didn’t mention pneumonia #### Example 2: * LLM Diagnosis: [’Airspace Opacity’] * Radiologist comment: Disagree * Radiologist note: also cardiomegaly and pleural effusions * Output: [Airspace Opacity, Cardiomegaly, Pleural effusion] Reason: Radiologist add those diseases to the list #### Here is your input: * LLM Diagnosis: {llm_labels} * Radiologist comment: {annotation} * Radiologist note: {note} * Output: 

23