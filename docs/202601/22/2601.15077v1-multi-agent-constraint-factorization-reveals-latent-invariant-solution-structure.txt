Title: Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure

URL Source: https://arxiv.org/pdf/2601.15077v1

Published Time: Thu, 22 Jan 2026 02:12:40 GMT

Number of Pages: 21

Markdown Content:
# Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure 

## Christopher Scofield 

## chris@sentienta.ai 

## January 2026 

Abstract 

Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on iden-tical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersec-tion of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simulta-neously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems. 

# 1 Introduction 

## 1.1 Motivation 

Multi-agent systems (MAS) composed of large language models are increas-ingly used in practice. Common configurations include structured debate role-or persona-based agents, critique-revision loops, and orchestrated workflows in which multiple agents interact through a shared dialog [Chen et al., 2024]. Across a wide range of tasks, including analysis, planning, writing, and decision support, systems that generate, compare, or coordinate multiple reasoning tra-jectories or agent perspectives often exhibit improved performance relative to a single model queried once or even iteratively [Ye et al., 2025]. Despite this empirical success, existing explanations for why multi-agent sys-tems (MAS) are effective remain incomplete. Earlier work on human collective reasoning provides an important conceptual underpinning, showing that groups can outperform individuals when problem solving benefits from diversity of per-spectives, complementary heuristics, or the aggregation of partially independent 1

> arXiv:2601.15077v1 [cs.CL] 21 Jan 2026

judgments [Surowiecki, 2004, Page, 2007, Woolley et al., 2010]. Building on these intuitions, contemporary research on MAS in machine learning has begun to explore a variety of architectures and interaction mechanisms in LLM-based multi-agent systems [Tran et al., 2025]. Recent work has also begun to move beyond architectural taxonomies and empirical benchmarks toward explicit analyses of interaction dynamics, stability, and population-level effects in multi-agent LLM systems. [Flint et al., 2025]. However, there remains no unified mechanistic account of how agent inter-action acting on a shared state alters underlying problem-solving dynamics, nor a general theory explaining why comparable benefits cannot in all cases be obtained by a single, unconstrained model with similar expressive capacity. This gap motivates the present work: to provide a precise, mathematically grounded explanation for when and why multi-agent LLM systems succeed. Ex-isting multi-agent LLM studies demonstrate empirical gains, but do not explain, at the level of operators, dynamics, or convergence, why interaction changes which solutions are reachable. 

## 1.2 Contributions 

This paper makes the following contributions: 1. Agent formalization. We formalize agents as constraint-enforcement operators acting on a shared solution state, with agent outputs interpreted as state-updating operations that impose validity conditions. 2. Emergence via operator composition. We prove that composing such operators yields invariant solution sets defined by the intersection of agent constraint sets, and that these sets are generally inaccessible to single-agent dynamics. 3. Proximal optimization interpretation. We extend the analysis from exact constraint enforcement to soft constraints using proximal operators, providing a framework compatible with approximate, incremental reason-ing. 4. Application to unstructured text systems. We map the formalism to contemporary text-based multi-agent systems by treating dialog as system state and personas as generators of domain-specific penalties. 

# 2 Related Work 

This section reviews three bodies of work that intersect in contemporary multi-agent large language model systems. These include multi-agent prompting and orchestration, classical constraint satisfaction via projections and proximal split-ting, emergence and collective computation in distributed systems, and the spe-cific theoretical gaps targeted by this work. 22.1 Multi-Agent LLM Systems 

Debate and multi-agent deliberation. The modern debate paradigm for machine learning systems is canonically articulated in AI Safety via Debate [Irv-ing et al., 2018], which frames two agents that argue opposing positions while a judge selects the winner. More recent work operationalizes debate as a practi-cal inference-time improvement technique for reasoning and factuality, in which multiple LLM instances propose answers and critique one another over multiple rounds, often improving aggregate performance [Du et al., 2023]. Related struc-tured debate methods include Multi-Agent Debate, which encourages divergent reasoning, with a judge managing the debate iterations [Liang et al., 2024]. 

Role prompting, role-play, and persona-conditioned agents. A large fraction of deployed multi-agent systems relies on role conditioning to induce agent heterogeneity. CAMEL proposes a role-playing framework using prompt-ing to induce autonomous cooperation between agents and to study emergent behaviors in a society of LLMs [Li et al., 2023]. Work explicitly analyzing role-play prompting for improved Chain-of-Thought (CoT) reasoning includes Better Zero-Shot Reasoning with Role-Play Prompting [Kong et al., 2023]. 

Self-consistency and ensemble-style inference. While not always framed as multi-agent, self-consistency is a closely related inference mechanism in which multiple chains of thought are sampled and aggregated to select a consistent answer [Wang et al., 2023]. This approach can be interpreted as exploring multiple trajectories in a latent reasoning space followed by projection under an implicit aggregation rule. Tree-of-Thoughts (ToT) generalizes chain-of-thought prompting into an explicit search over thought units. ToT LLMs also explore multiple reasoning trajectories but with enhanced analysis through look-ahead and backtracking [Yao et al., 2023]. 

Agentic tool use and orchestration frameworks. Several lines of work position LLMs as agents that act through tool use in addition to producing text. ReAct interleaves reasoning traces with actions to reduce hallucination and im-prove task completion [Yao et al., 2022]. Reflexion introduces verbal feedback and episodic text memory to improve subsequent trials without parameter up-dates [Shinn et al., 2023]. At the systems level, orchestration frameworks formalize multi-agent con-versation patterns and role assignment. AutoGen emphasizes programmable conversation patterns among LLM agents and tools [Wu et al., 2023]. Domain-specific collaboration frameworks include ChatDev and MetaGPT, which encode structured multi-role workflows and reduce cascading errors via explicit inter-mediate artifacts and procedure-like prompting [Qian et al., 2023, Hong et al., 2023]. 

Learned coordination and cognitive alignment. Recent work has ex-plored learned mechanisms for coordinating multi-agent LLM collaboration through 3adaptive communication and internal state modeling. Zhang et al. propose OSC, a framework in which agents maintain models of their collaborators’ cognitive states and use learned communication policies to reduce representational gaps during interaction [Zhang et al., 2025]. This approach emphasizes adaptive alignment and efficiency in multi-agent collaboration and demonstrates empir-ical performance gains across cooperative tasks. OSC treats interaction as a finite-round learned communication protocol but does not provide a theoretical analysis of convergence, invariant sets, or shared-state dynamics under repeated interaction. 

Positioning relative to this paper. The above work demonstrates that multi-agent systems often improve performance empirically [Wang et al., 2023, Yao et al., 2023, Wei et al., 2022, Du et al., 2023, Li et al., 2023, Qian et al., 2023, Huang et al., 2025, Ye et al., 2025]. The contribution of the present work is to supply a mathematically explicit account of when and why role-conditioned, interacting LLM instances can access invariant solution subspaces that are not reliably reached by a single unconstrained inference trajectory. 

## 2.2 Constraint Satisfaction and Alternating Projections 

Von Neumann alternating projections. A classical prototype for iterative constraint enforcement is alternating orthogonal projection between two closed subspaces of a Hilbert space. Von Neumann showed that, when the intersec-tion is nonempty, repeated application of the projection operators converges to the projection of the initial point onto the intersection [von Neumann, 1933]. This result illustrates how a joint solution can be recovered through the se-quential enforcement of partial constraints rather than by direct joint optimiza-tion. Subsequent work has extended this framework beyond linear subspaces to more general constraint sets, clarifying both convergence behavior and fail-ure modes when projections are approximate or constraints are incompatible [Hundal, 2004, Lewis and Malick, 2008]. 

Douglas-Rachford and monotone operator splitting. Douglas-Rachford splitting originates in numerical methods for partial differential equations [Dou-glas and Rachford, 1956] and was later understood as a general splitting method for finding zeros of sums of monotone operators. Lions and Mercier developed splitting algorithms for sums of nonlinear monotone operators, providing an operator-theoretic foundation [Lions and Mercier, 1979]. 

ADMM and augmented Lagrangian methods. The alternating direction method of multipliers (ADMM) arises from augmented Lagrangian decompo-sition methods developed in the 1970s for constrained convex optimization. A widely used synthesis by Boyd et al. reframes ADMM explicitly as a decomposi-tion and coordination algorithm, emphasizing its suitability for large-scale and 4distributed optimization problems in which variables and constraints can be par-titioned across subsystems [Boyd et al., 2011]. ADMM provides an important precedent for understanding how sequential enforcement of partial constraints can yield globally coherent solutions without centralized optimization. 

Proximal point and proximal splitting. Rockafellar formalized the prox-imal point algorithm for maximal monotone operators [Rockafellar, 1976]. Eck-stein and Bertsekas established relationships between Douglas-Rachford split-ting, proximal point methods, and ADMM [Eckstein and Bertsekas, 1992]. Com-prehensive treatments of proximal operators and splitting algorithms are pro-vided by Parikh and Boyd and by Combettes and Pesquet [Parikh and Boyd, 2014, Combettes and Pesquet, 2011]. 

Positioning relative to this paper. The present work leverages these clas-sical results but introduces a novel interpretation. Persona-conditioned LLM responses are modeled as approximate projection or proximal updates in a text-induced state space. Multi-agent interaction corresponds to composition of these operators, with convergence to invariant sets defined by joint constraint satis-faction rather than optimization of a single monolithic objective. 

## 2.3 Emergence and Collective Computation 

Distributed optimization and consensus. In distributed optimization, agents maintain local objective functions or data, and coordination arises through iterative communication and local updates. Classical distributed subgradient and consensus-based methods provide convergence guarantees under minimal information sharing, typically without any agent acquiring new external data beyond messages exchanged with neighbors [Nedi´ c and Ozdaglar, 2009]. Asyn-chronous variants of such distributed computations have also been studied ex-tensively, demonstrating robustness to delays and partial updates [Olfati-Saber et al., 2007, Tsitsiklis et al., 1986]. In contrast to the operator-theoretic ap-proach developed here, the framework of Nedi´ c and Ozdaglar [2009] assumes a fixed global optimization objective decomposed across agents, rather than agents whose interactions dynamically transform internal representations and intermediate operators during execution. 

Population-level dynamics and group size effects. Flint et al. analyze group size effects in LLM-based multi-agent systems using a dynamical-systems framework, deriving a mean-field description in which, above a critical popula-tion size, stochastic interactions concentrate around deterministic trajectories with stable fixed points [Flint et al., 2025]. Their results establish group size as a control parameter governing transitions between fluctuation-dominated and stable collective regimes, providing a clear population-level account of emergent coordination. While their analysis focuses on consensus and absorbing states of a population process, our work addresses a complementary question: how 5interaction between heterogeneous, role-conditioned agents alters invariant-set accessibility through composition of agent-specific operators acting on a shared problem state. 

Collective intelligence and diversity-based advantage. Empirical work on collective intelligence studies whether group performance exceeds what is predicted by individual capability alone. Woolley et al. report evidence for a collective intelligence factor in human groups [Woolley et al., 2010]. Closely related but analytically distinct, Hong and Page present a formal agent-based model in which heterogeneous problem solvers, endowed with different heuristics and representations, collectively outperform groups composed solely of individ-ually high-performing agents [Hong and Page, 2004]. Their result establishes, under bounded rationality assumptions, that diversity alone can yield systematic performance advantages even in the absence of learning or information sharing. 

## 2.4 Gaps in Existing Theory 

Existing multi-agent LLM systems demonstrate empirical gains through pat-terns such as debate or role specialization, but typically do not formalize the operators induced by agent updates or characterize the invariant solution struc-tures reachable under repeated interaction. Classical optimization and consen-sus frameworks provide such characterizations for explicitly defined operators on structured state spaces, but do not directly apply to stochastic, text-mediated updates. Conversely, operator-theoretic approaches rigorously analyze conver-gence and invariant sets under operator composition, but are developed for algebraically or geometrically structured settings and do not model persona-conditioned natural-language interaction. This paper bridges these perspectives by introducing an operator-level formalism in which agent responses act as ap-proximate constraint-enforcement operators on a shared state encoded in text, enabling analysis of invariant-set accessibility without assuming a single global objective or centralized optimization. 

# 3 Problem Setup and Notation 

## 3.1 Solution Space and State 

We consider a fixed problem specified by an initial prompt or query. Let S

denote the space of all candidate solutions consistent with this specification. The space S is defined abstractly and may be large or implicitly specified; we do not assume that its elements are enumerated or explicitly represented. In the systems of interest, elements of S correspond to semantic dialog states, such as evolving sequences of natural language exchanges. For purposes of analysis, we associate each dialog state with a state 

x ∈ X ,

6where X is a real Hilbert space equipped with inner product ⟨· , ·⟩ and induced norm ∥·∥ . The state x represents a partial or provisional representation of a solution to the query, obtained by encoding the current dialog state. In later sections, X will be instantiated both as a finite-dimensional vector space and as a structured representation derived from unstructured text. 

## 3.2 Agents and Personas 

We consider a collection of m agents indexed by i ∈ { 1, . . . , m }. Each agent is associated with a persona , which specifies a domain of expertise or evaluative focus. Importantly, personas do not restrict the expressive capacity of an agent. Instead, they determine which validity constraints the agent is authorized to apply when evaluating or modifying the shared state. We assume that all agents have access to the same information: the ini-tial problem specification (the query) and the evolving shared state. No agent possesses private data or privileged observations. 

## 3.3 Constraint Sets and Feasible Regions 

Each agent i induces a set of validity constraints on the solution space. These constraints define a subset 

Ai ⊂ X ,

interpreted as the set of states that satisfy all constraints expressible within agent i’s domain of expertise. The sets Ai are not assumed to be explicitly enumerated. Rather, they are defined implicitly through the agent’s evaluative behavior. In the idealized setting, we assume the following: 

• each Ai is closed and convex, 

• the collective feasible set 

A ≡

> m

\

> i=1

Ai

is non-empty. The set A represents states that simultaneously satisfy all agent-imposed constraints and serves as the candidate invariant solution set for the multi-agent dynamics. In this sense, each Ai and their intersection A may be viewed as feasible regions within the representation space X .

## 3.4 Agent Update Operators 

Each agent i acts on the shared state by applying an update operator 

Ti : X → X ,

7which maps the current state to a revised state reflecting the agent’s evaluation and modification of the solution. In the idealized setting, Ti is assumed to act as a constraint-enforcement operator associated with the feasible set Ai. In particular, Ti(x) moves the state toward consistency with the constraints defining Ai, without introducing new external information. We do not assume that Ti is an exact projection onto 

Ai; rather, it may be stochastic, approximate, or history-dependent, reflecting the behavior of large language models operating on unstructured text. When convenient, Ti may be interpreted as an approximate projection or proximal update with respect to Ai, but the analysis that follows depends only on its role as an operator whose fixed points lie in Ai.

# 4 Constraint Enforcement and Operator Factor-ization 

We now analyze the composition of agent update operators and its implications for constraint satisfaction and invariant-set structure. 

## 4.1 Agent Operators as Constraint Enforcement 

For the analysis that follows, we assume that operators Ti satisfy the following properties. 

Fixed points on feasible sets. States that satisfy agent i’s constraints are fixed points of Ti. Accordingly, the feasible set Ai coincides with the fixed-point set of the operator. 

Constraint-correcting behavior. For states outside the feasible set, Ti acts to reduce violation of agent i’s constraints, modeling the agent’s proposal of revisions that move the state toward admissibility. 

Non-expansiveness. Each operator Ti is assumed to be non-expansive. This regularity condition ensures stability of the induced dynamics and rules out pathological updates that would destabilize iterative constraint enforcement. 

## 4.2 Idealized Case: Projection Operators 

In the idealized setting, each agent operator is taken to be the orthogonal pro-jection onto its feasible set: 

Ti(x) = PAi (x),

where 

PAi (x) = arg min 

> z∈Ai

∥z − x∥,

8Under this model, each agent minimally modifies the shared state so that it satisfies the agent’s constraints, without introducing additional structure or preferences. Although exact projections are not intended as literal models of LLM behavior, they provide a mathematically transparent baseline for analyzing the effects of sequential constraint enforcement. 

## 4.3 Sequential Interaction and Shared State 

Agents act sequentially on a single shared state. One interaction round applies the composed operator 

T ≡ Tm ◦ Tm−1 ◦ · · · ◦ T1.

It is this composition, rather than the individual operators Ti, that governs the evolution of the system across interaction rounds. 

## 4.4 Why Operator Factorization Matters 

The distinction between a single-agent and a multi-agent system can now be stated precisely. 

Single-agent dynamics. A single-agent system applies a single operator that attempts to enforce all constraints simultaneously. A multi-agent system applies a factorized sequence of operators, each enforcing a subset of constraints. 

Multi-agent dynamics. As we will show in the next section, this difference is not merely procedural. Under mild conditions, the composition of constraint-enforcement operators reveals invariant solution structures, corresponding to intersections of feasible sets, that are not dynamically accessible to any single operator acting alone. This observation forms the core theoretical explanation for the effectiveness of multi-agent systems. 

# 5 Emergence via Operator Composition 

In this section we give a precise operator-theoretic account of emergence in multi-agent systems. We show that factorizing constraint enforcement across agents induces invariant solution structures that are not dynamically accessible to individual agents or to monolithic, non-factored updates, even when all agents have identical information. 

## 5.1 Invariant Sets Induced by Factored Dynamics 

Let X be a real Hilbert space, and let {Ai}mi=1 be nonempty, closed, convex subsets of X with nonempty intersection 

A ≡

> m

\

> i=1

Ai.

9Let each agent operator be the orthogonal projection Ti = PAi , and define the composed multi-agent operator 

T ≡ PAm ◦ PAm−1 ◦ · · · ◦ PA1 .

Theorem 5.1 (Emergent Invariant Set via Factored Projections) . For any ini-tial state x(0) ∈ X , the iterates 

x(k+1) = T  x(k)

are Fej´ er monotone with respect to A, i.e., 

∥x(k+1) − a∥ ≤ ∥ x(k) − a∥ ∀a ∈ A, ∀k ≥ 0.

Consequently, A is an invariant set of the multi-agent dynamics, and every weak cluster point of {x(k)} lies in A.

Proof. Each orthogonal projection PAi is firmly non-expansive. Standard re-sults on cyclic projections onto closed convex sets imply that the composed operator T is non-expansive and that the iterates are Fej´ er monotone with re-spect to the intersection A [Bauschke and Combettes, 2017]. Fej´ er monotonicity implies boundedness, monotone decrease of the distance to A, and that every weak accumulation point lies in A. □

Interpretation. The multi-agent system dynamically stabilizes on the inter-section of agent constraint sets. Importantly, no agent enforces the full con-straint family defining A; the invariant structure arises only through sequential operator composition. 

## 5.2 Non-Representability by Individual Agents 

We now formalize the sense in which the emergent invariant structure cannot be accessed by any individual agent acting alone. 

Proposition 5.2 (Strict Emergence) . If A ⊊ Ai for all i ∈ { 1, . . . , m }, then A

is not the fixed-point set of any individual operator PAi .

Proof. The fixed-point set of PAi is exactly Ai. Since A ⊊ Ai, no individual operator admits A as its invariant set. □

Thus, even though each agent acts deterministically and has full access to the shared state, no single agent can dynamically enforce the collective solution structure. 

## 5.3 Degenerate Cases: Absence of Emergence 

Emergence disappears when constraint factorization collapses. 10 Proposition 5.3 (Identical Agents) . If A1 = A2 = · · · = Am, then 

T = PA1 ,

and the multi-agent system reduces to a single-agent projection. 

In this degenerate case, interaction introduces no new invariant structure. This corresponds to systems in which all agents share identical personas, eval-uative authority, and constraint sets. 

## 5.4 Why a Single Monolithic Agent Can Fail 

One might object that a sufficiently capable single agent could enforce all con-straints simultaneously by applying the joint projection PA onto 

A =

> m

\

> i=1

Ai.

If such an operator were available, factorization would indeed offer no advantage. In practice, however, a single agent enforces multiple constraints through a coupled trade-off rather than exact projection. Regardless of the complexity of its internal reasoning, its externally observable behavior induces a single update operator 

S : X → X ,

corresponding to the resolution of competing constraint penalties. A convenient abstraction, formalized in the next section, is a regularized update of the form 

S(x) = prox Pmi=1 λiϕi (x),

where each ϕi penalizes violation of constraints associated with Ai.When constraints are enforced via penalties rather than projections, viola-tions may be tolerated if they reduce other penalties. Constraint satisfaction becomes negotiable, and invariance of the full intersection A is no longer guar-anteed. Accordingly, the fixed points of S correspond to trade-off optima rather than hard-feasible solutions. These optima may lie outside A, or may collapse a high-dimensional feasible region into a single preferred point. By contrast, the factored operator 

T = PAm ◦ · · · ◦ PA1

admits A itself as an invariant set and preserves its geometric structure under iteration. The advantage of a multi-agent system is therefore not increased expressive power, but the structural enforcement of constraints through operator factorization rather than monolithic coupling. 11 5.5 Summary 

This section shows that factorizing constraint enforcement across agents fun-damentally alters the invariant structure of the dynamics. While individual agents cannot stabilize the collective feasible set A, compositions of projection operators admit A itself as an invariant set and preserve its geometric struc-ture under iteration. By contrast, monolithic updates that couple constraints through trade-offs need not preserve feasibility or invariance, even when oper-ating over the same information. Emergence in this setting arises from changes in dynamical accessibility, not from an expansion of the solution space. 

# 6 Robustness to Approximate Constraint En-forcement 

The analysis in Section 5 assumes exact constraint enforcement via orthogonal projections. While this idealization provides theoretical clarity, it does not reflect how large language model agents operate in practice. Agent updates are typically approximate, incremental, and soft. In this section, we show that the emergence results of Section 5 persist under such approximate enforcement. 

## 6.1 Soft Constraints and Penalty Functions 

Instead of representing agent constraints solely by feasible sets Ai, we associate each agent i with a penalty function 

ϕi : X → [0 , ∞],

where ϕi(x) measures violation of agent i’s validity conditions. We assume that each ϕi is proper, convex, and lower semicontinuous, and that 

ϕi(x) = 0 if and only if x ∈ Ai.

The global energy of the system is defined as 

F (x) ≡

> m

X

> i=1

ϕi(x).

## 6.2 Proximal Agent Updates 

Agent updates are modeled via proximal operators. 

Definition 6.1 (Proximal Operator) . Given λ > 0, the proximal operator associated with ϕi is prox λϕ i (x) = arg min 

> z∈X



ϕi(z) + 12λ ∥z − x∥2



.

12 If ϕi is the indicator function of Ai,

ϕi(z) = 

(

0, z ∈ Ai,

+∞, z / ∈ Ai,

then the minimization is restricted to z ∈ Ai, and prox λϕ i (x) reduces to the orthogonal projection PAi (x). 

## 6.3 Proximal Multi-Agent Dynamics 

A single multi-agent iteration takes the form 

x(k+1) = prox λk ϕm ◦ prox λk ϕm−1 ◦ · · · ◦ prox λk ϕ1

 x(k),

where {λk}k≥1 is a sequence of positive step sizes. The step sizes λk are assumed to decrease with k, for example λk = 1 /k , and are chosen to satisfy standard summability conditions discussed below. 

## 6.4 Convergence Result 

Theorem 6.2 (Convergence of Proximal Multi-Agent Dynamics) . Let 

F (x) := 

> m

X

> i=1

ϕi(x).

Assume that: 1. each ϕi is proper, convex, and lower semicontinuous, 2. the minimizer set arg min x F (x) is non-empty, 3. the step sizes satisfy 

> ∞

X

> k=1

λk = ∞,

> ∞

X

> k=1

λ2 

> k

< ∞.

Then the cyclic proximal iteration weakly converges to a minimizer: 

x(k) ⇀ x ⋆ ∈ arg min  

> x

F (x).

This result is a standard consequence of convergence guarantees for cyclic proximal point methods under diminishing step sizes [Bauschke and Combettes, 2017], and is included here to demonstrate robustness of the emergence mecha-nism beyond exact projection dynamics. Thus, factored proximal dynamics stabilize an invariant solution set, whereas monolithic, coupled updates may select a single compromise point even when multiple feasible solutions exist. 13 6.5 Summary 

This section shows that the emergence of invariant solution structures in multi-agent systems does not rely on exact projections. Sequential, approximate con-straint enforcement via proximal operators preserves convergence and accessi-bility properties, demonstrating that the core results of Section 5 are robust to realistic agent behavior. 

# 7 Explicit Multi-Agent Example 

We now present a simple, fully explicit example that illustrates how factorized constraint enforcement yields an emergent invariant solution. The example is intentionally low-dimensional and analytically tractable, allowing the multi-agent dynamics to be understood in closed form. 

## 7.1 State Space and Agent Penalties 

Let the representation space be 

X = R2,

with state vector x = ( x1, x 2). We consider three agents, each enforcing a distinct quadratic penalty: 

ϕ1(x) = 12 (x1 − 1) 2,ϕ2(x) = 12 (x2 − 1) 2,ϕ3(x) = 12 (x1 + x2 − 1) 2.

Each agent constrains a different linear projection of the state. No individual agent specifies the full solution. The collective energy function is 

F (x) = 12 (x1 − 1) 2 + 12 (x2 − 1) 2 + 12 (x1 + x2 − 1) 2.

Proposition 7.1. The function F has a unique minimizer 

x⋆ =   23 , 23

 .

This point is not the minimizer of any individual penalty ϕi.

As shown below, this point arises as the unique fixed point of the cyclic composition of the agents’ proximal updates. 

## 7.2 Proximal Agent Updates 

Each agent applies a proximal update of the form 

x(k+1) = prox λϕ i (x(k)).

14 Because each ϕi is quadratic, the proximal operators admit closed-form expres-sions. For example, prox λϕ 1 (x) = 

 x1 + λ

1 + λ , x 2



,

with analogous expressions for ϕ2 and ϕ3.A single multi-agent iteration applies the three proximal operators sequen-tially in a fixed order. 

## 7.3 Convergence and Emergence 

Because each proximal update is an affine contraction, the cyclic composition of the three operators defines a single contractive map with a unique fixed point. Direct calculation shows that this fixed point coincides with the minimizer iden-tified above, 

x(k) −→ x⋆ =   23 , 23

 .

This convergence occurs despite the fact that: 

• no agent optimizes the global objective F ,

• no agent has access to all constraints simultaneously, 

• no agent computes the full gradient of F .The limiting solution is therefore not produced by any individual agent, but emerges as the unique fixed point of the composed operator. 

## 7.4 Failure of Averaging and Monolithic Optimization 

For comparison, consider two alternative strategies. 

Averaging Averaging the individual agent optima yields a point that does not satisfy all constraints simultaneously and does not coincide with x⋆.

Single-Agent Monolithic Updates A single agent attempting to resolve all constraints simultaneously can be modeled as performing a monolithic reg-ularized minimization, 

x 7 → arg min 

> x

 F (x) + μ∥x∥2, μ > 0.

In this example, such regularization selects the compromise point 

xμ =

 23 + μ , 23 + μ



,

which differs from the emergent solution 

x⋆ =   23 , 23



15 for all μ > 0. The added regularization term introduces a bias toward minimal norm, con-tinuously pulling the solution away from the emergent point and toward the ori-gin as μ increases. Thus, although the same constraint information is present, monolithic optimization converges to a different limit, while operator factoriza-tion yields the emergent solution x⋆.

# 8 Extension to Text-Based Multi-Agent Systems 

We now relate the operator-theoretic framework to contemporary multi-agent systems operating on unstructured text. The claim is not that large language models explicitly compute projections or proximal operators, but that their observable interaction dynamics can be modeled as an approximate realization of factorized constraint enforcement. 

## 8.1 Dialog as Shared State 

In text-based multi-agent systems, the shared system state is the evolving di-alog. For analytical purposes, we associate each dialog state with an implicit representation 

x = E(dialog) ,

where E maps the dialog transcript to a point in an abstract representation space X . This encoding need not be explicit or invertible, and agents do not observe x directly. Instead, agents read and modify the dialog, thereby inducing changes in the underlying state representation. From the perspective of the theory, the dialog therefore plays the role of the shared state x(k) in the multi-agent dynamics. 

## 8.2 Agent Turns as Factorized Updates 

Each agent is associated with a persona that determines a domain of evaluative authority - i.e., which types of constraints the agent is permitted to assess and address. Given a dialog state, an agent produces a response that attempts to reduce violations of constraints within its domain. At an abstract level, this behavior can be modeled as an approximate constraint-enforcement update 

x(k+1) ≈ Ti(x(k)),

where Ti is an operator associated with agent i.Operationally, this update is realized through text generation. Because lan-guage generation is discrete, context-dependent, and stochastic, these operators are noisy and inexact. Nevertheless, their sequential composition across agents yields interaction dynamics consistent with the factorized multi-agent updates analyzed in earlier sections. 16 8.3 Sequential Text Dynamics and Emergence 

A text-based multi-agent system therefore implements an iteration of the form dialog (k+1) = Gm ◦ G m−1 ◦ · · · ◦ G 1

 dialog (k),

where Gi denotes the text-generation operator of agent i. Under the encoding 

E, this corresponds approximately to 

x(k+1) ≈ Tm ◦ Tm−1 ◦ · · · ◦ T1

 x(k).

Emergent solutions correspond to dialog states that are stable under further agent interaction. These are states for which successive agent updates produce no substantial change. The success of such systems therefore derives not from any single agent optimizing a global objective, but from the structure of factor-ized interaction that makes latent solution states dynamically accessible. This provides a principled bridge between idealized operator-theoretic anal-ysis and real multi-agent language systems, without requiring access to internal model representations. 

# 9 Implications and Limits 

This section discusses implications suggested by the operator-theoretic analysis and clarifies the limits of what is formally established. The focus is on conse-quences that follow from the structure of the dynamics, rather than on empirical performance claims or implementation-specific heuristics. 

## 9.1 Implications for Multi-Agent System Design 

The analysis highlights the importance of constraint factorization as a struc-tural principle for multi-agent systems. In the formalism studied here, invariant solution structures arise from the interaction of distinct constraint-enforcement operators, rather than from the expressive power or stylistic diversity of indi-vidual agents. From this perspective, agent differentiation is effective insofar as agents are associated with distinct families of constraints whose interaction defines non-trivial invariant sets. Personas can therefore be understood as mechanisms for partitioning evaluative authority, rather than as ends in themselves. When agents enforce identical or nearly identical constraints, the resulting dynamics may reduce to effectively redundant updates, limiting the emergence of new invariant structure. The theory also clarifies that, under idealized assumptions (e.g., exact pro-jections or proximal updates and sufficient coverage of constraints), variations in update ordering or scheduling affect transient behavior and convergence rates but not the underlying invariant sets. This mirrors classical results for cyclic projection and proximal methods [Bauschke and Borwein, 1996, Bauschke and Combettes, 2017, Boyd et al., 2011]. 17 9.2 Limits of the Formalism 

The analysis also makes clear several intrinsic limitations. First, if the collective feasible set is empty, 

> m

\

> i=1

Ai = ∅,

then no invariant solution exists, and convergence to a stable solution is im-possible. This is a direct consequence of the operator-theoretic framework and holds independently of implementation details. Second, the results rely on idealized assumptions about constraint enforce-ment. When updates are noisy, approximate, or inconsistently applied - as in practical systems - the guarantees weaken, and invariant structures may be only approximately realized. Finally, the formalism characterizes asymptotic behavior and invariant struc-ture, not finite-time performance or robustness. Effects such as dominance of particular agents, imbalance among constraints, or slow convergence due to near-inconsistency are not ruled out by the theory and require separate analysis or empirical investigation. Accordingly, the framework should be understood as describing what kinds of solution structures are dynamically accessible in principle , rather than as a complete account of performance in deployed multi-agent systems. 

# 10 Conclusion 

This work provides a rigorous explanation for why multi-agent systems com-posed of large language models can outperform single-agent inference under identical informational conditions. Modeling agents as constraint-enforcement operators acting on a shared state shows that multi-agent interaction imple-ments a factored operator whose invariant sets correspond to intersections of agent-specific constraint families. We demonstrate that these invariant solution structures are generally not dynamically accessible to any individual agent acting alone, even when all con-straints are jointly known. The advantage of multi-agent systems therefore arises not from information aggregation or stochastic diversity, but from oper-ator factorization, which alters the dynamics of inference and stabilizes latent solution structure. Extending the analysis to soft constraint enforcement via proximal operators shows that this effect is robust to approximate and incre-mental reasoning. In this sense, emergence does not denote the creation of new solutions, but a change in dynamical accessibility. The solutions reached by multi-agent systems are latent in the shared state space yet inaccessible under unfactored dynamics, becoming reachable only through structured interaction. By grounding multi-agent language model behavior in operator theory and constrained optimiza-tion, this work establishes a principled foundation for analyzing and designing 18 multi-agent systems as tractable computational objects rather than heuristic assemblies. 

# Acknowledgments 

The author acknowledges the use of large language model-based research as-sistants for iterative discussion and clarification of mathematical frameworks during the development of this work. All theoretical claims, interpretations, and conclusions are the sole responsibility of the author. 

# References 

Heinz H. Bauschke and Jonathan M. Borwein. On projection algorithms for solving convex feasibility problems. SIAM Review , 38(3):367–426, 1996. Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces . Springer, 2 edition, 2017. Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning , 3(1): 1–122, 2011. S. Chen et al. A survey on llm-based multi-agent systems. arXiv preprint arXiv:2412.17481 , 2024. Patrick L. Combettes and Jean-Christophe Pesquet. Proximal splitting methods in signal processing. In Heinz H. Bauschke, Regina S. Burachik, Patrick L. Combettes, Veit Elser, D. Russell Luke, and Henry Wolkowicz, editors, Fixed-Point Algorithms for Inverse Problems in Science and Engineering , pages 185–212. Springer, New York, NY, 2011. doi: 10.1007/978-1-4419-9569-8 10. Jim Douglas and H. H. Rachford. On the numerical solution of heat conduction problems in two and three space variables. Transactions of the American Mathematical Society , 82(2):421–439, 1956. doi: 10.2307/1993056. Yilun Du et al. Improving factuality and reasoning in language models through multi-agent debate. arXiv preprint arXiv:2305.14325 , 2023. Jonathan Eckstein and Dimitri P. Bertsekas. On the douglas–rachford splitting method and the proximal point algorithm for maximal monotone operators. 

Mathematical Programming , 55(1):293–318, 1992. doi: 10.1007/BF01581204. Ariel Flint, Luca Maria Aiello, Romualdo Pastor-Satorras, and Andrea Baronchelli. Group size effects and collective misalignment in llm multi-agent systems. arXiv preprint arXiv:2510.22422 , 2025. 19 Lu Hong and Scott E. Page. Groups of diverse problem solvers can outperform groups of high-ability problem solvers. Proceedings of the National Academy of Sciences , 101(46):16385–16389, 2004. doi: 10.1073/pnas.0403723101. Sirui Hong et al. Metagpt: Meta programming for a multi-agent collaborative framework. arXiv preprint arXiv:2308.00352 , 2023. Rui Jerry Huang, Wendy Liu, Anastasia Miin, and Lei Ding. Adaptive coopeti-tion: Leveraging coarse verifier signals for resilient multi-agent llm reasoning. 

arXiv preprint arXiv:2510.18179 , 2025. H. S. Hundal. An alternating projection that does not converge in norm. Non-linear Analysis , 2004. Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899 , 2018. A. Kong et al. Better zero-shot reasoning with role-play prompting. arXiv preprint arXiv:2308.07702 , 2023. Adrian S. Lewis and J´ erˆ ome Malick. Alternating projections on manifolds. 

Mathematics of Operations Research , 33(1):216–234, 2008. ISSN 0364-765X. doi: 10.1287/moor.1070.0291. Guohao Li et al. Camel: Communicative agents for “mind” exploration of large language model society. arXiv preprint arXiv:2303.17760 , 2023. Tian Liang et al. Encouraging divergent thinking in large language models via multi-agent debate. In Proceedings of EMNLP , 2024. Pierre-Louis Lions and Bertrand Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM Journal on Numerical Analysis , 16(6):964– 979, 1979. Angelia Nedi´ c and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control , 54(1): 48–61, January 2009. doi: 10.1109/TAC.2008.2009515. Reza Olfati-Saber, J. Alex Fax, and Richard M. Murray. Consensus and coop-eration in networked multi-agent systems. Proceedings of the IEEE , 95(1): 215–233, 2007. doi: 10.1109/JPROC.2006.887293. Scott E. Page. The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies . Princeton University Press, 2007. Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends (R) in Optimization , 1(3):127–239, 2014. ISSN 2167-3888. doi: 10.1561/2400000003. URL http://dx.doi.org/10.1561/2400000003 .Chen Qian et al. Chatdev: Communicative agents for software development. 

arXiv preprint arXiv:2307.07924 , 2023. 20 R. Tyrrell Rockafellar. Monotone operators and the proximal point algo-rithm. SIAM Journal on Control and Optimization , 14(5):877–898, 1976. doi: 10.1137/0314056. Noah Shinn et al. Reflexion: Language agents with verbal reinforcement learn-ing. arXiv preprint arXiv:2303.11366 , 2023. James Surowiecki. The Wisdom of Crowds . Anchor Books, 2004. K. T. Tran et al. Multi-agent collaboration mechanisms: A survey of llms. arXiv preprint arXiv:2501.06322 , 2025. J. Tsitsiklis, D. Bertsekas, and M. Athans. Distributed asynchronous determin-istic and stochastic gradient optimization algorithms. IEEE Transactions on Automatic Control , 31(9):803–812, 1986. doi: 10.1109/TAC.1986.1104412. John von Neumann. On rings of operators. reduction theory. Annals of Math-ematics , 1933. Xuezhi Wang et al. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations ,2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 , 2022. Anita Williams Woolley, Christopher F. Chabris, Alex Pentland, Nada Hashmi, and Thomas W. Malone. Evidence for a collective intelligence factor in the performance of human groups. Science , 330(6004):686–688, 2010. doi: 10.1126/science.1193147. Qingyun Wu et al. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155 , 2023. Shunyu Yao et al. React: Synergizing reasoning and acting in language models. 

arXiv preprint arXiv:2210.03629 , 2022. Shunyu Yao et al. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601 , 2023. Rui Ye, Xiangrui Liu, Qimin Wu, Xianghe Pang, Zhenfei Yin, Lei Bai, and Si-heng Chen. X-mas: Towards building multi-agent systems with heterogeneous llms. arXiv preprint arXiv:2505.16997 , 2025. Jusheng Zhang, Yijia Fan, Kaitong Cai, Xiaofei Sun, and Keze Wang. Osc: Cognitive orchestration through dynamic knowledge alignment in multi-agent llm collaboration. arXiv preprint arXiv:2509.04876 , 2025. 21