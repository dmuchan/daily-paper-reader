Title: 2601.14943v1.pdf

URL Source: https://arxiv.org/pdf/2601.14943v1

Published Time: Thu, 22 Jan 2026 01:48:24 GMT

Number of Pages: 28

Markdown Content:
EUROVIS 2026 A. Abdul-Rahman, M. Angelini, and B. Preim (Guest Editors) 

COMPUTER GRAPHICS forum Volume 45 (2026 ), Number 3 STAR – State of The Art Report 

# State of the Art of LLM-Enabled Interaction with Visualization 

M. Brossier 1 , T. Isenberg 2 , K. Schönborn 1 , J. Unger 1 , M. Romero 1 , J. Björklund 3 , A. Ynnerman 1 , and L. Besançon 1 

> 1

Linköping University, Sweden 2 Université Paris-Saclay, CNRS, Inria, LISN, France 3 Umeå University, Sweden 

Abstract 

We report on a systematic, PRISMA-guided survey of research at the intersection of LLMs and visualization, with a particular focus on visio-verbal interaction—where verbal and visual modalities converge to support data sense-making. The emergence of Large Language Models (LLMs) has introduced new paradigms for interacting with data visualizations through natural language, leading to intuitive, multimodal, and accessible interfaces. We analyze 48 papers across six dimensions: application domain, visualization task, visualization representation, interaction modality, LLM integration, and system evaluation. Our classi fi cation framework maps LLM roles across the visualization pipeline, from data querying and transformation to visualization generation, explanation, and navigation. We highlight emerging design patterns, identify gaps in accessibility and visualization reading, and discuss the limitations of current LLMs in spatial reasoning and contextual grounding. We further re fl ect on evaluations of combined LLM-visualization systems, highlighting how current research projects tackle this challenge and discuss current gaps in conducting meaningful evaluations of such systems. With our survey we aim to guide future research and system design in LLM-enhanced visualization, supporting broad audiences and intelligent, conversational interfaces. 

CCS Concepts 

• Human-centered computing → Visualization; Visualization design and evaluation methods; • Computing methodologies →

Natural language processing; 

1. Introduction 

Data visualization has long been a cornerstone of scienti fi c inquiry and communication, enabling humans to externalize, explore, and reason about complex information. The earliest recorded maps, charts, temporal graphs, and other abstract representations of mea-surement date back to the 17 th century [Fri08]. Beginning in the 1970s, the fi eld leaped forward as computers became widely avail-able. The authoring of a visualization was no longer limited by physical constraints or artistic skills. Moreover, the ease of creat-ing basic visualizations meant that authors became more ef fi cient in their work. As a consequence, visualization developed beyond its original function as a means of communication to become an explorative tool for data scientists. The now-widespread fi eld of Exploratory Data Analysis (EDA), which is based on gaining in-sight from data by means of statistics and visual representations, was concretized by Tukey in his eponymous book in 1977 [Tuk77]. With the growing size and complexity of data sets, EDA is now an essential work fl ow for every data scientist, backed by both scienti fi cliterature and software engineering efforts (R, MATLAB, Tableau). Interaction capabilities are a central component of EDA systems and a major avenue of research in the visualization community [BYK*21; DP20; YKSJ07], speci fi cally to ensure that visualization software helps users extract fi ndings and understand their complex datasets [BYK*21] and eventually adopt new visualization software, which remains an open challenge in the visualization community [WBG*19]. In the data science fi eld, the combination of Machine Learning and Visual Interaction shortens the iteration loop of EDA through visualization recommendation [VHS*17] and iterative re-

fi nement, lifting the burden of low-level and repetitive tasks from data analysts. Interaction with visualization is, however, not limited to data science contexts. In science communication contexts, interac-tion helps designers to foster engagement and tailor an explanation to a viewer interest and level of understanding [SHP16; YLT18]. Interactive visualization is therefore expanding beyond data sci-ence into educational contexts [RKO*20; LCI*20], including online learning materials, schools, and science museums. The development of Natural Language Processing (NLP), particu-larly the advancement of LLMs, has opened new possibilities for interacting with data visualizations by leveraging visio-verbal syner-gies. LLMs can interpret user queries expressed in natural language [ZWS*24; SSL*23], generate relevant responses, and, in some cases, manipulate and update visualizations based on the interaction. Com-bining speech and visual cues is a natural way for typical humans to communicate. Systems that can leverage this combination have the potential to provide access to a slew of new synergetic inter-action techniques, such as deictic gestures and speech (pointing and talking), contextualized explanations with visualization, and multimodal feedback. The ability of LLMs to generate text is not © 2026 The Author(s). Computer Graphics Forum published by Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd. This is an open access article under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non-commercial and no modi fi cations or adaptations are made. 2 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

limited to natural language. With appropriate conditioning, LLMs are also pro fi cient at generating code or structured data [HAC25]. LLMs can thus be used as intermediaries for natural language input. While LLMs are certainly a ground-breaking technology, they also have several limitations that hinder their adoption in visualization systems. Hallucinations are frequent when deviating from training data [JLF*23] and can affect trust in the system, convey false, in-coherent, or misleading information [LQ24; ANYS24]. LLMs are also a source of biases due to their training data. Operating costs are high, which pushes for cloud-based approaches and induces de-pendence on private actors, environmental costs, and latency. More speci fi cally to visualization, LLMs have a poor sense or spatiality, temporality, and relationships, making them essentially “blind” to the visualization [ MKB*25a; MKB*25b]. This limitation can af-fect their understanding of queries, cause incorrect or inaccurate actions, and limit their ability to recover from such errors. Considering both the growing potential of using LLMs to interact with visualization and their inherent limitations, we aim to survey the literature on the convergence of LLMs and visualization for interactive sense-making of data. Speci fi cally, by synthesizing this rapidly evolving landscape, our goal is to provide a comprehensive and structured view of how LLMs are in fl uencing interaction with visualization, identify methodological and technical limitations, and propose research opportunities that can guide future LLM-enabled visualization system development and their evaluation. One could question the relevance of a state-of-the-art report in an emerging and extremely rapidly moving fi eld. We are, however, con fi dent that the report will have important immediate impact by enabling researchers entering into the fi eld to obtain a comprehensive overview of the area and thus signi fi cantly shorten the time to initiate novel research efforts in the domain. Our ambition is that the report will serve as a catalyst in the positioning of the visualization community in the rapidly evolving AI landscape. 

2. Related surveys 

Several research topics involving both natural language interaction and data science have been already been studied. Several surveys tackle a broader scope of visualization and automation, such as WANG et al. [WCWQ22] and W U et al. [WWS*22b], which both survey uses of machine learning-based automation across the visu-alization pipeline, but predate LLMs. Next, we focus on narrower topics in visualization. All topics presented are NLP techniques that existed before LLMs, but LLMs offer new opportunities, either in combination with, or as replacement of previous approaches. Data and information retrieval have been extensively studied and previously surveyed. In the topic of Natural Language Interfaces to Data ( NLID ) [OQS*20], database queries are automatically gen-erated from natural language queries ( e.g. ,text-to-sql), facilitating queries such as “retrieve the users which bought the article after January 7 th ”. NLID involving LLMs was surveyed by H ONG et al. [HYZ*25], but as L IU et al. [LWX*25] emphasize, LLMs tech-niques are not a one-fi ts-all solution, as they are more opaque and costly to operate. A related topic is Natural Language Knowledge Base Question Answering ( KBQA ), where users ask questions that a language model replies to supported by data extracted from a knowl-edge base. There is an overlap between data retrieval and knowledge retrieval, and both are employed in Retrieval-Augmented Generation (RAG ) by LLM systems, surveyed by Z HAO et al. [ZZY*26], P ENG et al. [PZL*25], F AN et al. [FDN*24], and Y U et al. [YGZ*25]. Next, moving closer to our topic, Natural Language Visualization Recommendation Systems ( VisRec ) have also been extensively surveyed [VHS*17; ZSJ*20]. VisRec is the task of automated visu-alization generation from natural language queries, such as “plot the sales pro fi t from 2010 to 2020 per product.” It often involves NLID and sometimes KBQA [ZSJ*20], and approaches that rely on LLMs also exist [ WLZ*25]. The previously mentioned surveys focus on essential tasks in vi-sual data analysis, but our topic more speci fi cally addresses Natural Language Interaction for visualization ( V-NLI ), in which natural language queries support users’ interaction with an existing visual-ization [SSL*23; VAM*22]. Here however, most previous surveys predate LLMs. Furthermore, these surveys focus on the perspec-tive of a data scientist (not considering the perspective of broader audiences or accessibility) and the perspective of traditional visu-alizations or charts (not considering domain-speci fi c visualization, which have unique problems and interest in LLMs). In other words, previous work focused on visualization authoring , but not visual-ization reading or interactive data analysis facilitated by LLMs. While there is a growing body of work focusing on using LLMs as an interaction assistant to facilitate sense-making and engagement with visualization, we could not locate an existing survey that has analyzed this particular gap in the literature. In this state of the art report we address this gap by conducting a sys-tematic PRISMA-driven review of LLMs and visualization. Speci fi -cally, we study how LLMs contribute to and in fl uence visualization work fl ows, classify LLM-supported interaction modalities and vi-sual representations, and examine domains where LLM-enabled interactive visualization systems are emerging. We also analyze LLM integration strategies, from prompting and fi ne-tuning to mul-timodal and agent-based architectures, and critically assess how current systems are evaluated, thus identifying potential method-ological de fi ciencies. In particular, we concentrate on surveying the speci fi c contribution of LLMs for interacting with the user through-out the visualization authoring and reading process. We approach this challenge by decomposing the visualization process into tasks, following the taxonomy we describe in Sect. 4.1, and analyzing at each step the speci fi c contribution of the LLM interactive compo-nent with the visualization. Our survey therefore distinguishes itself from past work by focusing on the user-LLM interface at different steps of the visualization and data-analysis pipeline. 

3. Methodology 

In our methodology we followed the PRISMA-2020 guidelines [PMB*21] as a structured approach to conducting systematic lit-erature reviews with transparency and rigor. PRISMA includes a checklist to ensure transparency and completeness of the analysis, to-gether with a fl ow-chart that reports the provenance of the surveyed studies (Fig. 1). We make all data that we produced for this survey available on OSF, doi: 10/nm8q (CC-BY-4.0). We also published an interactive browser for accessing the data on OSF. © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 3 of 28  

> Records removed before screening :Duplicate records removed (113) Records marked as ineligible by automation tools (0) Records removed for other reasons (0)
> Records excluded (n = 113)
> Reports not retrieved (n = 0)
> Reports excluded (n = 192)
> Studies included in review (n = 48)
> Identification of studies via databases
> Identification ScreeningIncluded
> Records screened (n = 240) Reports sought for retrieval (n = 240) Reports assessed for eligibility (n = 240) Records identified: (353) IEEE Xplore (110) ACM Digital Library (74) Dimensions (46) Scopus (25) Web of Science (8) ACL Anthology (4) ArXiv Preprints (74) Opportunistic finds (8)

Figure 1: PRISMA fl ow-chart of the paper selection process. 

3.1. Search process 

We performed a systematic search in renowned databases and pub-lishers connected to the visualization and natural language process-ing communities. We selected both multidisciplinary databases (Sco-pus, Dimensions, Web of Science); and more specialized databases (IEEE Xplore, ACM Digital Library, ACL Anthology), as well as arXiv preprints. We include all peer-reviewed academic publica-tion types (books, journal publications, conference proceedings, workshops). We searched for papers from arXiv (non-peer-reviewed preprints) because LLM research is a particularly fast-moving fi eld and many relevant submissions have not yet reached the status of peer-reviewed publication (see Sect. 3.4). We executed the same following search query in all databases: 

visualization AND interact* AND ("large language model" OR LLM OR GPT OR transformer) ,where double quotes ( " ) indicate an exact text match and the star ( * ) is the wildcard symbol (matches any suf fi x). We ran the search in the title, abstract and keywords fi elds. This search returned 353 papers, out of which 240 were unique. †

During the search, we found some discrepancy in the results returned by databases. For instance, using the arXiv API generated different results than the web interface. Likewise, on IEEE searching for 

vi?ualization returned different results matching “visualization”. To address this issue, we simpli fi ed our initially more complex query to the one presented above, which yielded multiple false positives but increased our con fi dence that relevant papers were not 

† See the supplemental material for details on how to reproduce the search query. overlooked. Our initial search query included clauses for targeting human-LLM interaction: dialogue OR chatbot OR convers* .Without these clauses, many of the returned papers turned out to be irrelevant, as they correspond to ML research papers with some visualization ( i.e. , Vis4LLM, not LLM4Vis or LLM+Vis papers). 

3.2. Screening process 

The systematic screening in databases yielded 353 records from the databases. All queries together returned 240 unique and retrievable papers. In addition, we found 8 further papers opportunistically, out of which we kept two. After this identi fi cation phase, we manually screened the papers to assess their relevance. We skimmed the papers to determine whether they met the eligibility criteria (Sect. 3.3). We then assigned each paper to two contributors to tag it as “relevant,” “irrelevant,” or “unsure.” We assigned papers to a third contributor in case of disagreement or “unsure.” We kept papers with at least two “relevant” assessments, reducing the count to the 48 papers we include in this report. Later, we removed one retracted paper. ‡

3.3. Eligibility criteria 

Two contributors then skimmed each identi fi ed paper to determine its suitability. We established the following eligibility criteria: 

∙ System . The paper contribution must be a system or technique. We excluded surveys, case studies, and position papers. 

∙ LLM . Must use a Large Language Model to accomplish or help accomplish a visualization task. 

∙ Visualization . Must use data visualization. The visualization does not have to be the main contribution of the paper. 

∙ Interaction . The user must have means of interacting with the LLM or the visualization to accomplish a task. To avoid ambiguities, we clari fi ed the following terms: 

∙ LLM : LLMs are natural language models with millions to billions of parameters, trained on terabytes of data and capable of general (non-speci fi c) natural language generation. For the purposes of this survey, we consider that models qualify if they are based on the transformer architecture [ VSP*17] or later improvements of this architecture. This de fi nition includes multimodal models capable of receiving or producing media types other than text (commonly images, videos, or audio). 

∙ Visualization : We chose the broadest de fi nition of visualization, as any graphic representation of data. It encompasses traditional data visualization (tables, graphs and charts), as well as scienti fi cand domain-speci fi c visualization (maps, networks, 3D, etc.). 

∙ Interaction : We follow the de fi nition proposed by D IMARA et al. [DP20], who de fi ne an interaction with visualization as “the interplay between a person and a data interface involving a data-related intent, at least one action from the person and an interface reaction that is perceived as such,” except that we replace the term “person” with “actor” such that LLMs are considered as an actor 

‡ Our coding sheet and instructions are available in the supplemental mate-rial on our OSF repository. © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 4 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

with “intents” that can initiate an action. We judged the question of “perception” though, from a human perspective. 

3.4. Preprint inclusion 

While it is commonly admitted that survey papers should focus on published and, as such, peer-reviewed contributions, we decided to include preprints from arXiv in this STAR. This decision is moti-vated by the extremely rapid pace of development in the fi eld of LLM/AI research and the fact that many landmark papers in the 

fi eld might not even get submitted for peer review. By excluding preprints we feared that we may not gain a complete picture of the current state of research. However, to ensure that we do not present all papers (peer reviewed or not) as providing the same strength of evidence [BPS*21], we clearly separate them and indicate how many of those papers came from arXiv. We assessed these papers through the same process, but also eliminated those that we thought were not up to the quality standards that would be generally ex-pected from our visualization publication venues. We discuss this issue more speci fi cally in Sect. 8.2. We identi fi ed 78 papers from arXiv, out of which we included 3 in the analysis. We identify the preprints in our discussion and use references with the arXiv symbol ( ) ( e.g. , [ WLZ*25]). With this method, we hope that we manage to clearly present the actual state of the art of the research in the 

fi eld without sacri fi cing novelty or compromising rigor. 

4. Classi cation 

For our systematic survey we established a classi fi cation of LLM-enabled visualization systems. It is composed of four items: LLM-enabled visualization tasks, interaction modalities, visual representa-tions, and application domain. Each item has several categories, and papers can fi t in one or many categories of each item. Table 1 summa-rizes the classi fi cation categories. We also analyzed the evaluation and implementation of each LLM-enabled visualization system. 

4.1. LLM-enabled visualization tasks 

In the fi rst item, we categorize the visualization tasks performed by using the system. The purpose of this item is to allow us to identify what role LLMs can play in a visualization pipeline (Fig. 2). While numerous task taxonomies have been developed in the past [CR96; Shn96; RCDD13; SNHS13; RAW*16], we decided to base our classi fi cation around the three core steps in the visualization pipeline, as described by C ARD et al. [CMS99]: data transformation, visual mapping, and view transformation. We map these steps into three corresponding user roles: analyst, author, and reader. Shifting the perspective from tasks to user roles allows us to adopt a top-down vision with human input at the heart of the design. This perspective matches our goal of analyzing LLM roles in visualization. LLMs can automate, partially or fully, one or several tasks of either user role. This framing is inspired by existing taxonomies of visualization roles and tasks, such as the data state reference model [Chi02; Chi00], which describes how information fl ows from raw data to visualization and perception, or by the knowledge generation model in visual analytics [SSS*14], which describes visual analytics tight coupling between human input and the system. To establish the tasks within the roles, we referred to narrower tax-onomies focusing on one aspect of visualization: analytic tasks in  

> Navigation
> Sense-Making
> Visual Encoding
> Data Transform
> Data Retrieval
> Analyst Author Reader
> User perspec tive Visua liza tion p ipe line

Figure 2: Our typology of tasks in a visualization pipeline. 

visualization [AES02]; tasks in graph visualization [LPP*06]; tasks in collaborative visualization [GWA05], and automated analytics tasks [DV23]. Closer to our topic, D OMOVA et al. [DV23] classify degrees of automation for different analytic tasks. Overall, we found that existing taxonomies related to automation or ML focus on ana-lytics tasks, while our survey targets a broader scope that includes not only analytics but also visualization authoring and interaction. The closest to our classi fi cation is W ANG et al.’s work [WCWQ22], who similarly break down seven visualization processes into three categories and include human components such as reading, pro fi ling, interaction, and insight communication. Our classi fi cation adds the data retrieval category, which is not part of previous classi fi cations, as LLMs are well suited for information retrieval and are a grow-ing component of interactive visualization, which we found to be prevalent in the corpus. We also include a navigation task, which is particularly prevalent in 3D visualization, where changing points of view into the visualization is essential to grasp the full picture, and where fi ne view control is an interaction challenge [KI13; BYK*21; Ovi02]. We thus use the following fi nal task categories: 

Data retrieval . Corresponds to fetching data from outside sources to produce a visualization. A common assumption in visualization is that the needed data is known and readily available. While this is true in most contexts, i.e. , if the data is produced for the analysis, or if the analysis is constructed around the data, in the case of explorative visualization, an analysis goal may emerge without speci fi c data supporting it, in which case data retrieval is an initial step. 

Data transformation . Computations performed on the data prior to visual encoding, such as the seven low-level analytic tasks by A MAR et al. [AES02]—retrieve value, fi lter, compute derived value, 

fi nd extremum, sort, determine range, characterize distribution, fi nd anomalies, cluster, and correlate. 

Visual encoding . This step maps data fi elds to visual representations. It includes choosing a visualization type and support, and mapping data to visual channels such as color, shape, and size. 

Sense-making . Tasks involving production and communication of insight backed by information. 

Navigation . When visualization is interactive, navigation regroups possibilities to adjust the view of the data during consumption of the visual media for exploration. Such actions include changing data ranges, fi ltering values, zooming and panning, and hover tooltips. © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 5 of 28 

4.2. Interaction modalities 

The second item focuses on interaction modalities with the visualiza-tion system. LLMs provide a new communication channel with vi-sualization, namely natural language, which is interfaced via textual and spoken interaction modalities. Moreover, with this classi fi cation we study the interplay between different interaction modalities, both LLM-enabled and otherwise. Indeed, humans communicate with a combination of elements that have different strengths including spoken language that describes abstract concepts, hand gestures that indicate spatial relationships, and facial expressions that con-vey emotions. Interactive visualization can be more effective by leveraging multimodality. Like visualization, interaction remains an ill-de fi ned topic that needs to be re fi ned. We follow the de fi nition proposed by D IMARA et al. [DP20], where an interaction with visualization is de fi ned as “the interplay between a person and a data interface involving a data-related intent, at least one action from the person and an interface reaction that is perceived as such.” This de fi nition does not include LLM-based agents, which exhibit both machine characteristics and human characteristics, such as use of natural language. We adapt the de fi nition and rename ‘person’ with ‘agent,’ such that LLMs become a third entity, in addition to the user and the visualization system, capable of using similar interfaces as the person. There exists an extensive body of literature that examines interac-tion modalities from both VIS and HCI communities [Ber94; AN19; YKSJ07]. Interaction modalities are often distinguished between “input” and “output” modalities [AN19]. Input modalities are initi-ated by a user, while output modalities provide sensory feedback [BMP*00]. Our classi fi cation focuses on input modalities, but we discuss LLM outputs in Sect. 6.4. We included only input modalities used in the corpus. In Sect. 8 we discuss unexplored modalities. Historically, use of textual and spoken interaction modalities in visu-alization was marginal [KI13]. These input modalities resurface with LLMs because they are direct input formats for LLMs. Some of the previous limitations of these modalities are now overcome by LLMs, but we do not yet know in what situations text and speech constitute superior modalities for interacting with visualization through LLMs. In other words, the most prevalent interaction input with LLMs is because it is more affordable, and not because it is more effective. We thus analyze uses of other input modalities enabled or supported by LLMs. Finally, we settled on the following classi fi cation for the interaction modalities. 

Textual natural language , typed with a keyboard. The role of the interlocutor ranges from reacting to direct commands, answering question, to a symmetric, two-actor conversation. 

Spoken natural language , collected through a microphone. 

Spatial interaction . B ESANÇON et al. [BYK*21] identify four spa-tial interaction paradigms: tactile and pen-based, tangible and haptic, mid-air gestures, and hybrid. Our corpus does not contain any tangi-ble interaction paradigm. 

User interface widgets . The traditional keyboard and mouse paradigm on desktop include several standard interaction widgets, such as buttons, input fi elds, sliders, and pickers. 

4.3. Visual representations 

The nature of data and its visual representation have high impli-cations for the capability of an LLM to produce and interact with visualization. For instance, LLMs (including VLMs [KHC23]) can struggle with spatial representations [WG26] as they have poor spatial ‘reasoning’ capabilities, compared to humans. Conversely, LLMs have a good ‘understanding’ of clearly labeled tabular data as used in charts. Surveying visual representations (the graphical elements chosen to represent data) thus informs us on the strengths and weaknesses of LLMs in different visualization scenarios. Cer-tain representations are more suited to a certain kind of data, e.g. ,tabular data is often represented with 2D charts. Internally, there is a lot of possible variation in visual encoding, such as the nature of the chart. Design decisions matter for an LLM for both the genera-tion of accurate and relevant visualizations as well as reading and interacting with existing visualizations. Our typology of visual representations includes charts, spatial (maps, scalar/vector fi elds), networks, images/videos, and custom (in Fig. 3 we show examples of visual data representations from the corpus): 

Charts . Traditional visualizations from tabular data such as scatter-plots, bar charts, and pie charts. Typically produced by spreadsheet software and visualization software such as Vega-Lite [SMWH17]. 

Spatial . Map, fi elds, or volumetric representations. While loosely related visualization techniques and intents, we group these together because they exhibit similar strengths and weaknesses for an LLM-based interaction. 

Network . Data formatted as graphs (having nodes connected by edges), representing networks (social networks, knowledge graphs). 

Image . While images stretch the usual de fi nition of ‘data visualiza-tion,’ they can easily be generated and manipulated by LLMs. 

Custom . Bespoke or rarely seen visualizations, not clearly fi tting another category, fall into the ‘custom’ category. Due to the lack of available training data for LLMs, they are harder to interact with. 

4.4. Application domain 

An application domain classi fi cation allows us to identify and com-pare domain-speci fi c constraints that may affect the creation of LLM-enabled visualization systems. Operative surgeons, e.g. , may seek speech control facilitated by LLMs due to the sterile environ-ments on which they rely [MHWH17; BYK*21]. More generally, people working primarily with their hands may prefer voice input to interact with software [JGL*24]. In education, users may seek qualities of providing personalized explanation and engagement with a broad audience with poor visualization literacy [LCI*20]. We coded papers using the Elsevier’s three-tiered disciplines taxon-omy [Els26] because it provided a suf fi ciently exhaustive list. Once we selected the papers, we narrowed it down to only fi ve broad categories that we judged as suf fi ciently representative of the paper corpus, while being interesting for argumentation, as is common practice in STARs [HWKK23; SEG*21]. We thus classi fi ed as data science & mathematics, medicine & biol-ogy, physics & engineering, and education & social sciences. Among them, data science is a rather non-speci fi c class: many papers fall © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 6 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

into this category as they lack a speci fi c domain focus. Some papers fall into two categories, yet due to our small data, we did not study synergies that may emerge from interdisciplinary examples. 

4.5. System evaluation 

Visualization research has a history of constantly rethinking eval-uation methods to ensure that all important aspects surrounding visualization and its results are covered [LBI*12]. Evaluation in vi-sualization thus encompasses a wide breadth and depth of measures, from subjective assessment of systems ( e.g. , NASA-TLX [HS88] and SUS usability scales [Bro13], aesthetics [HIDI23], preferences [BIAI17], readability [CHII25]) to benchmarks for performances ([TBS23; WGM*24]) and user performance ( e.g. , task completion time, errors, perception tasks [BBB*19], intent [Mun09] or tasks and strategies [SSKT20]). Consequently, we report on evaluation strategies in our survey. In doing so, we hope to establish a standard of best practices for future studies in the domain or highlight cur-rently missing evaluation strategies that would allow us to better understand LLM-assisted interaction with data visualization. We dis-tinguish user evaluation and system evaluation. For the former, we report collected demographics (participant number, expertise level). For the latter, we report instruments, metrics, and benchmarks use. 

4.6. LLM implementation 

For researchers seeking to implement LLM components in visual-ization systems, we reviewed how the LLM integration was realized by the authors from a technological perspective. From the papers we collected and analyzed information about the integration of the language model into the visualization system. We looked at which language models are used and what constraints guided the choice of a model. Then, we looked at the model in-structions, in particular how system prompts are constructed, what prompt strategies are employed, what data goes in and out, and in what format. We also focused on the use of multimodal models capable of image and audio processing. Then we looked at multi-agent systems capable of delegating tasks to different specialized LLM agents. Finally, we discuss long-term memory, i.e. , means of retrieving of knowledge and data from external storage sources (e.g., retrieval-augmented generation and explicit memory modules) [ LPP*21; PWL*24; ZBM*24] and learning over time from user interactions [ JXL*25; ZRK*25]. These memory aspects are important to consider because they control whether the LLM is capable of providing information outside of the considered dataset and whether it has means to adapt and learn from users directly and thus provide increasingly more fl exible interactions and better responses as the system is being used [ DCA25; JXL*25]. 

5. Survey of the state of the art 

We now present the survey of the state of the art in enabling interac-tion with visualization using LLMs. We fi rst study the intersections between tasks and interactions from our classi fi cation and highlight gaps in the literature. Second, we focus on the visual representation and third, on the application domain in which those are used. 

5.1. LLM-enabled tasks in visualization systems 

We fi rst survey tasks in visualization systems, in conjunction with the interaction modality that enables it, as shown in Table 3. 

5.1.1. Task 1: Data and knowledge retrieval 

Our fi rst task is the retrieval of data and/or knowledge. Both con-cepts are related: they consist of fetching appropriate data or knowl-edge from outside sources to accomplish a speci fi c task. Data and knowledge retrieval see a growing interest in interactive visualiza-tion systems. In typical scenarios, data are provided by analysts for speci fi c analysis goals. With the increased availability of large, heterogeneous data corpora, however, the task of retrieving data for an analysis goal is becoming increasingly important [MJB*23], in particular for exploratory data analysis. Knowledge retrieval from databases is well-established, but LLM-based systems are now seen as a way to increase accuracy, reliability, and veri fi ability of LLM-produced information [ZZY*26; PZL*25; FDN*24; YGZ*25]. In the literature, the retrieval task is automated by recommender sys-tems, i.e. , systems capable of fi ltering and aggregating relevant data based on a user query [RD22]. Visualization systems capable of this task are correspondingly called visualization recommender systems (VRS) [PPV25; KO17; VHS*17]. Within VRS, natural language approaches are popular due to their ease of use by humans [VHS*17; ZWS*24; LWX*25; SSL*23; OQS*20]. Such task automation pre-dates LLMs, with, e.g. , rule-based natural language databases since the early 2000s [LWX*25]. LLM-based approaches, however, have a better ‘understanding’ of language nuances and context than the previously predominant rule-based techniques [Lu26; KMS*25], and are thus more likely to facilitate more explorative processes and provide access to non-expert audiences. We now focus on the means of interaction facilitated by the LLM for this retrieval task. Following our task-interaction matrix (Table 3), the next three sections focus on textual natural language retrieval, widget-based retrieval, and other approaches. 

Retrieval with textual natural language. In our corpus, several studies allow data queries in natural language, in particular for knowledge base question-answering ( KBQA ), such as the work by YAN et al. [YHX*25] and H UANG et al. [HDZ*23]. As our survey focuses on systems providing interaction with visualization, we exclude systems performing only queries without further interaction. Instead, we direct the reader to the survey by L IU et al. [LWX*25] that addresseses this topic. We still include the work by Y AN et al. [YHX*25] and H UANG et al. [HDZ*23] because they allow users to interact with the retrieved data. A good example is “KnowNet” [YHX*25], in which NL user queries are interpreted by a LLM and the LLM output is analyzed and linked to a knowledge graph (KG), storing semantic connections between terms. The authors identify three forms of knowledge: a procedural knowledge from human reasoning, knowledge learned by the LLM, and knowledge stored in the knowledge graph. They found that com-bining KGs with LLMs helps users reason about LLM-generated responses by navigating the structure of the KG. In particular, they show that their KG improved the assessment of subjective or uncer-tain phrasing by the LLM, such as “some research suggests that. . . ” 

or “[. . . ] may have potential bene fi ts. . . .” “MEQA”; “ChartGPT”; “ChatGrid” [SGC25; TCD*25; JA24] employ a different approach. © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 7 of 28 

Table 1: Overview of the classi fi cation categories. 

coding items categories visualization task Data retrieval : Extract data from databases 

Data transformation : Apply operations on data, such as sort, fi lter, aggregate, compare 

Visual mapping : Map data to a visual representation, including chart type, colors, layout 

Sense-making : Gain knowledge or insight from a visualization 

Navigation : Select different information to be displayed visualization represen-tation 

2D Charts : Traditional visualization charts from tabular data. Bar charts, scatterplots, pie charts 

Spatial data : Map, fi elds or volumetric representations 

Graph & Network : Graph representations 

Multimedia : Images and/or Videos user interaction modal-ities 

Written natural language : A text interface to have a conversation with an AI agent 

Spoken natural language : Voice interaction with an AI agent 

Spatial interaction : Hand-based interaction in mid-air or on touch-sensitive devices 

GUI Widgets : Mouse and keyboard interaction with buttons, drop-downs, sliders, etc .application domain Data science , generally for systems that target no speci fi c domain apart from visual analytics 

Scienti c subject domains (Medicine, Biology, Physics, Chemistry, Engineering) 

Education, Arts & Social sciences 

evaluation (data collec-tion) 

Participants : Demography and number of study participants 

Evaluation instruments : questionnaire, interview, benchmarks, comparison studies, system logs 

Evaluation metrics : user preference, task completion time, user recall, qualitative feedback, system accuracy, latency, and empirical observation LLM implementation Model : Model vendor and version 

Training & Fine-tuning : Training process and data 

Prompt engineering : Structure of system prompts, use of few-shot, chain-of-shot 

LLM data format : Representation of data as input or output to the language model 

Multimodal capabilities : Training process and data 

Multi-agent capabilities : Architecture of systems involving multiple specialized agents A Text2SQL agent [STZ*25] translates NL questions into SQL queries. The authors then interrogate the database with the query and the response serves to augment the prompt of a second LLM agent. T IAN et al. [TCD*25] explain that the approach is valuable in case of fi ne-tuning a LLM model speci fi cally for the task is not an option, for example in presence of sensitive data, which the model is not allowed to learn from. “ChatGrid” [JA24] does not perform any further data transformation after data retrieval, as SQL queries 

fi ltering and aggregation capabilities were found to be suf fi cient. We did not fi nd papers or approaches searching the web for in-formation retrieval. This is particularly surprising since most con-sumer LLM chatbots (OpenAI ChatGPT, Microsoft Copilot, Google Gemma) have this capability built-in. Research in this direction exists outside of the visualization community. For example, in their survey D ELDJOO et al. [DHM*24] coined the term Gen-RecSys for retrieval systems with generative capabilities. To summarize, current research focuses on retrieving data using nat-ural language from either knowledge graphs or SQL databases, but our survey highlights that, to the best of our knowledge, there have not been attempts made towards retrieving data from the internet, unstructured databases, or other forms of large data storage, such as books, speci fi cally in visualization systems. 

Retrieval with widgets. Widget interaction facilitated by LLMs support data retrieval in several papers. “KnowNet” [YHX*25] provides a timeline widget that allows revisiting previous queries, and provides clickable recommended queries. This widget provides users with an overview of the analysis progress and facilitates a quicker formulation of queries through recommendations. The au-thors propose that future work should implement more complex UI widgets. A tree view widget, for instance, would facilitate branching during the analysis. “MEQA” [SGC25] similarly provides clickable recommended follow-up questions, which trigger data retrieval. 

Unexplored interactions for retrieval. Natural language interaction has always been employed for data retrieval, predominantly in search engines. Yet, we did not fi nd many papers which would directly address this task. We postulate that few authors focus on providing a complete visualization pipeline including data retrieval, due to the complexity of the task itself or the fact that it may fall outside of what would be considered interesting research contributions. Indeed, most current publications focus on a subset of the visualization pipeline (Table 2). This focus may explain why contributions in this area appear sparse. Another possible explanation stems from the unpredictability of systems with vast retrieval capabilities. Granting access to more data to a LLM may increase the complexity and therefore, the unpredictability of the generated responses, which is already challenging to control with LLMs [BS25]. © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 8 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

Table 2: All papers we surveyed. OpenAI : GPT model used. Shows only latest if multiple are used. num. participants : Number of participants recruited for user studies only. particip. expertise : Level of expertise of the majority of participants. N = novice, K = knowledgeable, E = experts, M = mixed. 

papers (48) appl. domain vis. repr. vis. task interaction LLM eval. data sci. & math. med., bio. & chem. phys. & eng. edu. & social sci. charts custom spatial network data retrieval data transf. visual encoding sense-making navigation NL text NL speech spatial interaction mouse widgets OpenAI Llama Gemini other LLM-vision num. participants particip. expertise system eval has benchmark has case study has comp. study has empir. study InkSight [LLY*24] 3.5 12 KAutonomous GIS [LN23] 4HINTs [LM25] NL4DV-LLM [ SMN*24] 4VizChat [YZE*24] 4KOSA [HDZ*23] CellSync [SMKS24] 3 20 EWaitGPT [XZX*24] 4 12 KVizAbility [GKWK24] 4 6 KC5 [LYZ*24] 3.5 8 KKnowNet [YHX*25] 4Wander 2.0 [SXC*23] Data Formulator [WTL24] 3.5 10 EVIST5 [VCM*23] 24 KChartGPT [TCD*25] 3 12 KCityGPT [ GOWY24] ARAS [JGL*24] 3.5 ? ETailor-Mind [GLS*25] 4 24 KInsightLens [WWL*25] 4 12 EChatWeaver [WGFL25] 10 MMapIO [MMA*25] 10 M[URQ*25] ? M3D-LLaVA [DHJ*25] LASEK [BSE25] MEQA [SGC25] Smartboard [LXH*25] 4 8 EInterChat [CWG*25] 10 KEphemera [HHYG24] Sensecape [SMPX23] 4Graphologue [JRDX23] 4 7 EAiCommentator [ANP*24] 3.5 16 MDataDive [KLL*24] 4 21 NAvatar [HVM*24] ChatGrid [JA24] GENIUS [MST*25] 4-o SmartMLVs [QWH*25] 4 32 KWord2Wave [CBAI25] 15 NVOICE [JIB*25] 4 12 NLIDA [Dib23] 3.5 XNLI [FWP*24] 12 MLEVA [ZZZ*25] 20 KHandMol [ RKPA23] GENEVIC [NMD*24] 4AVA [LML*23] 4-o 4 E© 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 9 of 28 

Table 3: Survey of visualization tasks enabled by LLMs, sorted by interaction modality. The same study may appear in different cells. 

data retrieval data transformation visual encoding navigation sense-making text [JA24] [GLS*25] [NMD*24] [YHX*25] [SGC25] [XMW23] [LKL*25] [LN23] [TCD*25] [ GOWY24] [WTL24] [ SMN*24] [NMD*24] [CWG*25] [Dib23] [SGC25] [XMW23] [LKL*25] [SMPX23] [LXH*25] [QWH*25] [VCM*23] [XZX*24] [DHJ*25] [LN23] [LML*23] [TCD*25] [ GOWY24] [WTL24] [ SMN*24] [NMD*24] [CWG*25] [BSE25] [Dib23] [SGC25] [XMW23] [LKL*25] [LXH*25] [QWH*25] [VCM*23] [XZX*24] [DHJ*25] [ANP*24] [ GOWY24] [KLL*24] [SXC*23] [GLS*25] [JRDX23] [WWL*25] [YHX*25] [SMKS24] [LXH*25] [SHE25] [GKWK24] [YZE*24] [JIB*25] [JA24] [KLL*24] [JRDX23] [WWL*25] [CWG*25] [LKL*25] [QWH*25] [VCM*23] [GKWK24] [YZE*24] [JIB*25] 34 speech [TLT*25] [MST*25] [CWG*25] [TLT*25] [MST*25] [CWG*25] [CBAI25] [TLT*25] [JGL*24] [HVM*24] [HHYG24] [JIB*25] [CWG*25] [JIB*25] 8spatial [MST*25] [CWG*25] [LXH*25] [MST*25] [CWG*25] [LXH*25] [URQ*25] [JGL*24] [HVM*24] [LLY*24] [LXH*25] [URQ*25] [CWG*25] [URQ*25] 3widgets [NMD*24] [YHX*25] [SGC25] [TCD*25] [WTL24] [NMD*24] [CWG*25] [SGC25] [LXH*25] [VCM*23] [XZX*24] [LML*23] [TCD*25] [WTL24] [NMD*24] [CWG*25] [SGC25] [LXH*25] [VCM*23] [XZX*24] [URQ*25] [LYZ*24] [LLY*24] [YHX*25] [LXH*25] [GKWK24] [YZE*24] [URQ*25] [CWG*25] [VCM*23] [GKWK24] [YZE*24] [URQ*25] 15 8 18 22 23 12 Few systems provided retrieval mechanism from long-term memory. L I et al. [LN23] highlight in their limitations that “every autonomous system needs a memory component to store the contextual and long-term information” for iterative self-improvement. In addition, stor-ing user-veri fi ed solutions to problems could increase the ef fi ciency of the system for tasks not planned by system designers over time [YZE*24]. L IANG et al. [LYZ*24] reverse the solution of long-term memory in visualization. They use visualization of conversation history to facilitate the recall of past dialogues for both the LLM and the human interacting with it, by retrieving and highlighting relevant past conversations during a LLM interaction. Information is stored and retrieved via topic classi fi cation. 

Summary 

Knowledge and data retrieval tasks see renewed interest with the development of LLM techniques, and some visualization systems started to adopt these developments. Current research, however, mainly focuses on natural language retrieval from knowledge or information databases. Retrieval is performed most often via SQL queries (Text2SQL) or via embeddings and similarity search with language models. Interaction modal-ities that trigger retrieval are mainly natural language text and GUI widgets. None of the surveyed studies employed other means of interaction to trigger retrieval. Digital assistants, e.g. ,frequently allow asking questions out loud that trigger a search in a knowledge database. Furthermore, none of the studies support searching from external data sources, such as perform-ing web searches or crawling the fi le system. Yet, consumer LLM products easily support these features but its inclusion may further increase the unpredictability of LLM-generated responses. 

5.1.2. Task 2: Data transformation 

Once the data are queried, they are transformed through mapping, 

fi ltering, and aggregation. These transformations process raw data from observations into a visualization abstraction [Chi02]. Basic transformations can be handled in the query phase by a centralized system ( e.g. , SQL databases), but more complex data analysis tasks or interactively run transforms are implemented in the application via dedicated interactions or with user-written code for bespoke tasks. We focus on interactions that trigger data transformations. 

Data transformation with textual natural language. Internally, there are different approaches taken by LLM-enabled systems to implement data transformation from natural language prompts. The 

fi rst approach, adopted by a majority of papers, consists of gener-ating code that has access to the dataset and produces a visualiza-tion using a speci fi c framework. For instance “LIDA”; “WaitGPT”; “MEQA”; “InterChat” [Dib23; XZX*24; SGC25; CWG*25] gener-ate Python code to produce a chart matching the query, with little restrictions on the code produced. These systems do not distinguish between the data transformation and the visualization phase, so the code produced is responsible for both. D IBIA [Dib23] augments the prompt with a structural and semantic description of the dataset. This grounding information reduces the likelihood of hallucinations. Furthermore, to help produce correct code the authors, D IBIA split the generation into three phases: scaffolding , where the program © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 10 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

Figure 3: Representative visualization kinds in the survey. Left to right: (1) Solution graph and map visualization, in “Autonomous GIS” by 

L I et al. [LN23]. (2) Style transfer applied to generated infographics, in “LIDA” by D IBIA [Dib23]. (3) NLI assistant for assisting chart reading for BLV people, in “MapIO” by M ANZONI et al. [MMA*25]. Images courtesy of the respective authors. 

skeleton is generated; generation , where the code logic is produced, including code outputting the visualization; and execution , which executes multiple generated scripts and arbitrates on the best ones. “WaitGPT” [XZX*24] produces unrestricted Python code, but is accompanied with an interactive graph visualization of the data transformations to the side, which gives the user insight and con-trol over the generated code. In “MEQA” [SGC25], most of the data transformation is done in the retrieval phase via SQL query operations. Then, unrestricted Python (Plotly) code is produced. “InterChat” [CWG*25] produces unrestricted JavaScript (D3) code. A different approach was adopted for the systems NL4DV-LLM 

[ SMN*24] and “VIST5” [VCM*23] by producing a structured, declarative data format that is interpreted by a visualization software. V OIGT et al. [VCM*23], e.g. , produce a Vega-Lite speci fi cation of charts, which includes fi ltering and aggregation operations. Finally, a last approach to Task 2 is iterative. In “Autonomous GIS”; “ChartGPT”; “Data Formulator” [LN23; TCD*25; WTL24], the LLM is operated with a chain-of-thought prompt that produces a solution plan for a user query, involving a composition of elementary operations. “ChartGPT” [TCD*25] applies elemental operations in 

fi xed order (select columns, fi lter rows, aggregate). “Data Formu-lator” [WTL24] derives concepts (transformations with semantics) and combines them to reach the goal. In contrast, “Autonomous GIS” [LN23] produces Python code for the individual operations, then combines them into a fi nal script to complete the task (Fig. 3 (1) ). L I et al. [LN23] highlight that single-step code generation would have worked yet the indirect approach (chain-of-thought, divide-and-conquer strategies) yields better results for complex tasks and produces veri fi able outputs due to the solution graph visualization. 

Data transformation with spoken natural language. One study, C HEN et al. [CWG*25], leverages spoken utterances for data transformation . It allows users to input transformation queries as either text or speech through the microphone. However, the authors do not compare the two modalities in their studies, nor comment on actual usage. They note that “more complex modalities—such as advanced touch gestures, voice commands, and movement-based interactions—remain underexplored.” This sentiment is surprising, as speech is used for other tasks (mainly navigation and explana-tion). Some authors argue that spoken interaction is best suited for a broad audience target [JIB*25], or in operational settings [CBAI25], such as work tasks requiring the use of hands [JGL*24]. But data transformation is a task primarily for visualization author-ing and analysis, which is typically done sitting at a desk, where a speech modality may be a nuisance or a concern in shared of fi ce spaces [Jah12; ZPM24], or because the modality does not integrate well with the current work fl ow of different experts [Bes17; Ise14; WBG*19; WRB*20]. Nevertheless, further work is needed for some data analysis applications. For instance, all XR applications we sur-veyed employ a speech modality for navigation and/or explanation, but do not perform data authoring or analysis [GGRG24; MST*25; URQ*25; RKPA23; DHJ*25]. 

Data transformation with widgets. Nine out of 48 studies employ GUI widget interaction for data transformation. Basic interaction with mouse-activated UI widgets is often implemented in visualiza-tion frameworks used for quick fi ltering and navigation of data using built-in widgets ( e.g. , D3.js, Vega-Lite, or Plotly). These interac-tions, however, are not facilitated by LLMs. Other systems leverage traditional GUI widgets to craft and customize LLM prompts. In-deed, while natural language is ef fi cient for conveying complex and abstract intent, it can be more verbose and error-prone than tradi-tional GUI inputs for simple mechanical tasks [TCD*25]. T IAN et al. [TCD*25], e.g. , break down the visualization recommendation into subtasks executed by the LLM. Data transformation steps include selecting columns, fi ltering rows, and computing aggregates. Due to abstract features of language, the authors explain that one query leads to a tree of possible interpretations. The LLM produces multi-ple charts and data transformation steps corresponding to different interpretations. The user can then both re fi ne the natural language query, or modify directly the transformation operations using GUI widgets. “Data Formulator” [WTL24] operates at a lower level, where data transformations are encoded by “data concepts” derived from other concepts with a transformation intent. The visualization author provides input concepts and optional exemplar values in GUI, and the transformation intent in natural language. The LLM then generates the body of a function executing the intent with the source concepts as input (Fig. 4 (2) ). © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 11 of 28 

Unexplored interactions for data transformation. We did not fi nd clear examples of data transformation driven by another interaction than natural language or widgets. While M ASSA et al. [MST*25] mention the use of hand gestures in mixed reality, this workshop paper does not provide much detail that would allow us to infer how these are actually used and for what purposes. We decided, nonetheless, to mention it here as a paper that potentially uses hand gestures for data transformation, since the manuscript seems to hint at this possibility. Future work could focus on implementing data manipulation operators with LLMs via different input modalities such as mid-air gestures, sketching, or tangible interaction. Such setups have been studied without LLMs [PRI02]. LLMs could facil-itate such interaction by providing the reasoning that translates the interaction raw input into data transformation primitives. 

Summary 

Most of the work we surveyed focuses on textual natural lan-guage, where LLMs either generate unrestricted code, produce structured declarative speci fi cations, or follow iterative, multi-step reasoning to compose transformations. Fewer systems explore spoken language, where we see potential for data trans-formation beyond its anecdotal use in navigation and explana-tion tasks. GUI widgets are the most common modality, often complementing LLMs by constraining or re fi ning transforma-tions, combining the expressiveness of natural language with the precision and reliability of direct manipulation. 

5.1.3. Task 3: Visual encoding 

Once data are extracted and transformed, the following visualization pipeline step is to choose a visual data mapping. This step involves choosing a visual representation ( e.g. , the chart type) and the prop-erties of visual components (colors, layout, textual information). An automatic generation of a visualization from data is called visualiza-tion recommendation. LLM-based VRS produce either imperative code or declarative descriptions of the output visualization. State-of-the-art LLM-based systems perform well at simple visualization recommendation tasks for most common 2D charts types, as there is ample training data available. K HAN et al. [KCM25] report a success rate of 95% for GPT-4o (best performer) for simple chart generation tasks in Python. They perform worse with precise cus-tomization requests, such as specifying colors, sizes, labels, etc. The performance with Vega-Lite code generation was signi fi cantly lower, hinting that widespread availability of code examples is a key factor in LLM’s visualization code generation performance. 

Visual encoding with textual natural language. Similarly to data transformation, several systems employ LLMs for either generation of code [Dib23; XZX*24; CWG*25] or declarative speci fi cations [ SMN*24; VCM*23]. These systems produce only regular 2D charts. Depending on the desired level of automation, a recom-mender system automates the choice of a chart type, represented data features, and visual encoding parameters for a given query. Some systems provide interactive re fi nement of the visual mapping after an initial generation, with follow-up queries or other interaction inputs (which involves a dialog history for the LLM). Although common VRS generate charts via analytic speci fi ca-tions (imperative or declarative code), we found one paper, “LIDA” [Dib23], which applies a post-processing pass to apply style trans-fer to charts using a text-conditioned image-to-image model (see Fig. 3 (2) ), for domain alignment and communicative purposes. This approach, however, raises questions about the balance between aesthetics and the accuracy of presented information [Woo22]. Beyond 2D charts, visual encoding is also needed in 3D visualiza-tion. “3D-LLaVA” [DHJ*25] works on point clouds of 3D scenes and uses a 3D-LLM [HZC*23], i.e. , a multimodal model capable of handling point clouds as input. The system is capable of segment-ing regions of space based on natural language prompts, such as “highlight the bathroom sink.” 

Visual encoding with spoken natural language. Using voice com-mands to describe the visual layout facilitates quicker iterations during the design and prototyping phase [KDAH19]. “Word2Wave” [CBAI25] works on the basis of the data points dictated out loud, which are displayed on the screen. For quicker feedback, the authors employ a fast, custom word-to-word model trained on a dataset generated with human-LLM collaboration. 

Visual encoding with widgets. Once an initial visualization is cre-ated with a natural language prompt, tweaking the visual encoding can be quicker and easier to do with GUI widgets—using a color picker, e.g. , can be more precise than trying to ask an LLM for a speci fi c color. In “InterChat” [CWG*25], a system that offers both natural language and various GUI interactions, users tend to use nat-ural language in the exploration phase, when they were uncertain of the exact requirements, but used direct interaction otherwise. Once the initial chart is generated, “InterChat” provides a means to iterate on the design or explore data by altering selected data inputs directly within the conversation history. The authors explain that users ben-efi t from progressively specifying intent through a combination of language and visual selection, while direct language input is often preferred for better-de fi ned tasks. In “WaitGPT” [XZX*24], the LLM produces visualization by generating a fl ow diagram, which combines visual mapping and encoding primitives to produce a fi nal chart. The fl ow chart doubles as an interactive GUI widget, in which users can easily tweak the visual encoding by editing input forms. 

Unexplored interactions for visual encoding. Apart from natural language and widgets, we did not fi nd other interactions to handle visual encoding. This fact may be explained by a lack of relevance in studying other interaction modalities to handle visual encoding. 

Summary 

Our survey showcases several visualization recommender sys-tems that accept natural language input to ultimately translate high-level user intent into concrete visualization encodings, including chart type, visual mappings, and layout decisions. In most of our surveyed systems, this process is inherently iterative, and users re fi ne the generated visualization through follow-up queries. Some papers mention the bene fi ts of com-bining NL input with direct manipulation ( e.g. , [CWG*25; XZX*24]), especially for smaller re fi nements to tweak the vi-sual output. However, these remain preliminary and interesting 

fi ndings that would tend to suggest that direct natural language © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 12 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

Figure 4: Representative visualization kinds in the survey. Left to right: (1) Generated player trajectories in basketball, in “Smartboard” by 

L IU et al. [LXH*25]. (2) Building visualization from modular and interactive data concepts, in “Data Formulator” by WANG et al. [WTL24]. 

(3) 3D molecular visualization and navigation, in “VOICE” by J IA et al. [JIB*25]. 

queries would be most effective when integrated with, rather than replacing altogether, traditional interaction techniques. A small set of the surveyed systems explores vision-based validation and re fi nement of the generated visualizations. Ap-proaches such as “3D-LLaVA” [DHJ*25] and “Visualization-ary” [SHE25] incorporate computer vision models or heuristics to analyze visualization outputs and guide users through itera-tive improvement, yet more work seems needed on methods to ensure correctness. Similarly, despite the growing multimodal capabilities of recent models, the use of style transfer for vi-sualization is rare. With limited exceptions ( e.g. , [Dib23]), it appears that current research does not leverage generative mod-els for aesthetic or communicative adaptation, highlighting an opportunity for future research, provided that data fi delity and visual integrity are preserved ( e.g. , [ KBSL23]), or, perhaps, not preserved for speci fi c intents and purposes ( e.g. , [Woo22]). 

5.1.4. Task 4: Navigation 

This task corresponds to view manipulations to obtain different view-points in the data. It is particularly prevalent in 3D scenes [BYK*21], in particular with mid-air hand gestures [KDPB24], but operations such as zooming, dragging, and rotating ( e.g. , on maps) are also rele-vant in 2D. On the one hand, GUI-based navigation (buttons, sliders, selection; i.e. , direct interaction) is well suited for fi ne-grained trans-formations that are tedious to describe with concepts ( e.g. , zooming, moving a camera in 3D space, selecting an unlabeled data entry) [MMCV24]. On the other hand, NLI is best used for easily describ-able transformations. Ultimately, combinations of GUIs with natural language input facilitate an ef fi cient navigation that combines an action description with direct manipulation [SYC*21; SLW*25] (e.g. , pointing and asking “show me this”). 

Navigation with textual natural language. “InterChat” [CWG*25] provides natural language view transformation combined with UI data selections. For instance, a user might perform a box selection with the mouse and utter “zoom on this selection.” Though, all interactions are available as either natural language queries (text or speech) or direct manipulation with the mouse. “VOICE” [JIB*25] facilitates navigation in 3D space, around molec-ular models (see Fig. 4 (3) ). The authors split navigation operations into two categories: camera adjustments and visual effects. Due to the hierarchical structure of the data being visualized and the complexity of specifying certain navigation tasks (such as camera movement), the authors de fi ne different navigation environments that limit available operations based on the context. Overall, the set of available transforms is carefully controlled at all times to prevent the LLM from steering the scene into an unwanted state. “VOICE” also relies on a multi-agent LLM architecture, which has two specialized agents for navigation: the explorer to take care of local transformations and animations and the pilot to handle scene transitions. Finally, the system facilitates indirect navigation, e.g. , to explain a concept to the user, a manager LLM may build a sequence of view points to visit, without the user explicitly asking for it. 

Navigation with spoken natural language. Four papers [JIB*25; CWG*25; VCM*23; JGL*24] rely on textual or spoken natural language input for navigation. A lot of literature covers navigation interaction in visualization [Bol80; KLSC21; SS17; AKG*15], and speech interaction for navigation provides bene fi ts for accessibility to people with disabilities [ZMFS09; ACS*22], to reduce fatigue in AR/VR use [SYC*21], and in collaborative work [LBGI25]. The papers we surveyed do not provide strong justi fi cations for using speech input, nor do they seem to evaluate it when compared to textual input. J IA et al. [JIB*25] emphasize that spoken natural language can increase user engagement and satisfaction, reduce latency , and increase intuitiveness. V OIGT et al. [VCM*23] explain that “typing-based chat is very impractical, as it is annoying to switch back and forth between the keyboard and the screen” and JAVAHERI et al. [JGL*24] state that NLI is “an optimal interaction method and reduce surgeons’ cognitive workload.” Given the dis-crepancy between the little use of LLMs and the large evidence of the bene fi ts of speech-based input for navigation, we suggest that there is an important research gap to fi ll. 

Unexplored interactions for navigation. We found little interest in navigation with widgets or spatial interaction. Papers tackling 3D spatial data often rely on mid-air gestures (with or without con-trollers) for navigation [MST*25; HVM*24]. Our corpus contains © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 13 of 28 only few AR/VR applications, and they do not use gestures com-bined with LLMs. Nonetheless, a clear research avenue exists in combining hand gestures with other modalities powered by LLMs, particularly with the recent advent of low-latency vision-capable models [ ZSWY25; CLW*24], which may, in the near future, be capable of responding to arbitrary gestures in the same way that speech LLMs now ‘understand’ spoken language. We found one paper, “InterChat” [CWG*25], that combines speech and mouse input to multimodal interaction. When performing a box or lasso selection on a chart, “InterChat” provides a selection context to the LLM prompt to facilitate deictic interaction such as “Zoom into this and move to that place,” where “this” and “that” correspond to previous mouse selections. 

Summary 

Voice-based navigation is particularly valuable in contexts where keyboard and mouse input is impractical or unavailable, such as virtual and augmented reality or operational, hands-busy environments, where users often rely on direct spoken commands to control navigation and views. In contrast, in conventional 2D desktop environments, navigation is gener-ally well supported by direct manipulation with mouse and keyboard, which likely explains the more limited adoption of speech-based navigation in such settings. Multimodal naviga-tion that combines speech with complementary input has been studied extensively in pre-LLM systems [BYK*21], but only a small subset of papers used voice combined with other input modalities. LLM-augmented visualization could thus lead to more interesting applications. The closest examples combining LLM-based input and other navigation modalities include sys-tems such as “InterChat” [CWG*25] and “InkSight” [LLY*24], which combine natural language with GUI selections or free-form lasso input. Extending these approaches to additional input modalities, such as hand gestures or pointing in VR, rep-resents a promising direction for future work, particularly as LLMs become more capable of integrating heterogeneous in-teraction signals. Yet, more research needs to be done to solve LLM-blindness for the speci fi c case of data visualization ( e.g. ,[ MKB*25a; MKB*25b]). 

5.1.5. Task 5: Sense-making 

Sense-making tasks correspond to gaining knowledge or insight from visualization. Although LLMs are potentially bene fi cial tools for providing explanations tailored to context and users [GLS*25], they must be used carefully. K OONCHANOK et al. [KPR25] and WANG et al. [WHT*25] found that insight produced by LLMs is not identical to insight that would be produced by human analysts. In many cases, a LLM performs worse than an expert in generat-ing insights from a dataset. LLM-generated explanations are also associated with risks of hallucinations [JLF*23], LLM sycophancy (i.e. , agreeing with user’s preconceptions, being “people pleasers”) [FGA*25], and even deception [ GDW*24; PGO*24]. Of note, some of the papers we found make use of LLMs to, e.g. , assist with structuring insights. For instance, P OORYOUSEF et al. [PCB*24] use generative AI to allow forensic experts and pathologists to automati-cally structure autopsy reports for court cases as they use immersive autopsy and visualization software [PCB*23]. We decided against including those in our analysis since they do not directly use LLM to interact with the visualization. 

Sense-making with textual natural language. Sense-making through textual natural language is primarily enabled through dia-logic question-answers with a LLM assistant. Out of the box, LLMs can provide explanations to general questions through their internal knowledge. This is insuf fi cient when domain-speci fi c knowledge is required, in which case fi ne-tuning and retrieval augmented genera-tion are ef fi cient approaches. “AiCommentator” [ANP*24] provides commentary with embedded visualization for viewers of football games. The system embeds vi-sualization into the video feed, such as tracking the ball and player, and showing player statistics. In interactive mode, users can query these visual augmentations through natural language texts such as “Track [player].” In commentary mode, the system provides these visual highlights automatically during visioning. Study participants showed a slight preference for the interactive mode. Less knowledge-able participants, however, did not “know what to ask” in interactive mode, while knowledgeable participants enjoyed asking questions. 

Sense-making with spoken natural language. Two papers provide explanations for spoken queries through speech synthesis, “VOICE” [JIB*25] and the work by H ARINEE et al. [HVM*24]. Both systems employ a three-step setup, with fi rst a speech-to-text model, fed to a textual LLM, whose output is then synthesized to speech with a text-to-speech model. Since 2024, end-to-end speech LLM models are available to the public, which have reduced latency, accuracy and can respond to intonations patterns. We thus strongly suspect that more speech-based dialogic systems will see the light of day in the coming years. To provide contextualized explanations, “VOICE” [JIB*25] combines a spoken explanation with camera motion paths. However, the LLM prompt has limited understanding of the visual layout ( e.g. , elements in focus and color mappings), which prevents users from asking visual-dependent questions (“what is the red spot?”). Future work, such as that of M ENA et al. [MKB*25b], can focus on enriching LLM prompts with contextual visual metadata [ MKB*25a; MKB*25b]. “ArticulatePro” [TLT*25] however, listens to users but does not reply with speech synthesis. Instead, it generates appropriate visualization to user queries. The system is continuously listening to multi-user conversation. It generates visualization when a user formulates either an explicit request or proactively. For the proactive case, the system contains a module to detect situations that require the generation of an explanatory visualization. For instance, a user may say “[. . . ] as fuel ef fi ciency increases, so do sales,” after which the system generates a fuel-vs-sales chart. The system thus silently contributes to the exploration by generating charts backing the conversation. Similarly, the art exhibition “Ephemera” [HHYG24] generates im-ages of creatures based on visitors’ utterances, capturing their use of taboo language and, hence, externalizing the public’s mood. 

Sense-making with widgets. “InkSight” [LLY*24] facilitates edit-ing natural language explanations of chart fi ndings using GUI wid-gets. Factual sentences can be created, deleted, moved, and grouped to construct an explanation of chart insights. Data facts in the text are © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 14 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

also highlighted and linked with the corresponding chart data points, allowing users to back facts with data by following hyperlinks. 

Sense-making with spatial interaction. “InkSight” [LLY*24] also supports the documentation of chart fi ndings via a combination of sketch-based selection with LLM-generated explanations. After se-lecting data points on the chart using gestures, the LLM generates a textual insight, which can then be re fi ned by the user. “Smart-board” [LXH*25] also relies on sketching to allow human users to communicate spatial information to an LLM, combined with other modalities such as natural language and charts (Fig. 4 (1) ). 

Summary 

LLM-augmented visualization systems commonly support sense-making through textual explanations of user queries. The robustness is often enhanced via retrieval-augmented genera-tion or fi netuning. Beyond text, several approaches leverage vi-sualization itself as the explanatory medium. They aim at facil-itating natural-language questions to be answered through dy-namically generated visual encodings ( e.g. ,[TLT*25]). Fewer systems combine verbal and visual explanations, although work such as “VOICE” [JIB*25] demonstrates the potential of coordinated spoken and visual responses for multimodal sense-making. Recent advances in speech-to-speech foundation mod-els (end-to-end speech model, AudioPaLM [ RAN*23]) seem to further suggest that voice-driven interaction paradigms may become increasingly prevalent. Together, these approaches out-line a growing and still little-explored design space for explana-tion in LLM-augmented data visualization systems, spanning textual, visual, and multimodal strategies. 

Summary | LLM-Enabled Tasks 

In summary, LLM-enabled interaction has been studied for all tasks in the visualization pipeline. Natural language input fa-cilitates ef fi cient data retrieval, data transformation and visual encoding. The two most common applications are visualiza-tion recommendation and exploration of knowledge graphs. The spoken form of natural language input has not been used much yet, despite evidence of its usefulness in speci fi c opera-tional scenarios [LIB23], e.g. , sterile environments [JGL*24], in particular for navigation and sense-making. It is likely that this modality currently does not integrate well with current work fl ows of experts conducting data analysis [Bes17; Ise14; WBG*19; WRB*20]. As such, focusing solely on spoken in-teraction with LLMs for visual data analysis could hinder the adoption of new systems and should be considered carefully. Other interactive inputs, such as spatial inputs and GUI in-puts, can also be facilitated by LLMs, for tasks where natural language is less effective. Past research suggests that direct interaction is more effective for fi ne-grained operations, while natural language facilitates exploration with unspeci fi c out-comes [MMCV24]. LLMs can also facilitate deictic interac-tion , such as performing a selection with the mouse or hand gestures followed by a manipulation of that selection with a natural language query [SLW*25; KLSC21]. Still, as is appar-ent in our task-interaction matrix (Table 3), such uses of LLMs remain anecdotal and could be explored further. 

5.2. Visual representation 

LLM-based approaches have been used for a variety of visual repre-sentations (2D charts, spatial representations, network/graphs, other representations), as we review next. 

5.2.1. 2D charts 

Traditional visualization techniques for tabular data are the most commonly supported visual representations in LLM systems. In this category we group scatterplots, line charts, bar charts (including stacked, histograms etc.), and pie charts. 24 / 48 papers employ visu-alizations of 2D charts. Their prevalence is due to their widespread use in visual analytics, but also because they are easily produced and understood by LLMs: the underlying data can be stored in structured text formats such as CSV or JSON, and visualizations can be simply generated using popular implementation frameworks (Sect. 6.3). 

5.2.2. Spatial visualizations 

Spatial visual representations (maps, scalar or vector fi elds, density plots) are frequently employed and concern 18 / 48 papers in our corpus. They are more complex to generate or manipulate than charts by LLMs as the data-to-visual mapping is often not straightforward. Focus regions, for instance, can be at a larger scale than individual data points ( e.g. , clusters, and spatial relationships). 2D maps are a common representation, used notably in Geographic Information Systems (GIS). Five papers [LN23; GOWY24; BSE25; JA24; VCM*23] describe LLM-enabled GIS systems. In “ChatGrid” [JA24], to answer queries, the LLM generates spatial SQL queries (using PostGIS), which returns entries linked to vi-sual features for the system to display on screen. This way does not treat spatial information, only semantics, while the visualization system displays relevant information. In the volumetric visualization category we found three papers that tackle molecular visualization [JIB*25; RKPA23; MST*25] and three papers that tackle anatom-ical visualization in VR/AR [HVM*24; URQ*25; JGL*24]. 

5.2.3. Networks and graphs 

18 / 48 papers study graph and network visualizations, of which the most common are knowledge graphs (KGs) [HDZ*23; LYZ*24; YHX*25; JRDX23; WGFL25]. We found two main applications among KGs: the construction and visualization of mindmaps [GLS*25; LYZ*24; SMPX23; JRDX23] and the visualization of the knowledge retrieval process [HDZ*23; YHX*25; WGFL25]. Combining LLM information retrieval through KGs with a visual-ization can facilitate the establishment of an understanding of the source of the data as well as navigating the thought process in a structured way. Another application of graphs is the visualization of the data transformation process [XZX*24; LN23]. Through an interactive solution graph, “WaitGPT” [XZX*24] facilitates the iter-ative re fi nement of the generated visualization by tweaking the node attributes directly. Two additional papers employ graph visualization for domain-speci fi c applications, namely a power grid visualization [ GOWY24] and a protein interaction network [NMD*24]. © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 15 of 28 

5.2.4. Other visual representations 

Visualization systems involving images or videos are rarer. In an image, useful information does not appear by reading individual pixels: only with the whole picture the complex human visual system can make sense of data. Advances in computer vision, and recently multimodal vision LLMs, could offer new perspectives: image-based chart reading [LQ24], labeling and commentary of images and videos, with concrete applications such as real-time vision-based guiding in 3D space [DHJ*25]. Several papers L IANG et al. [LYZ*24] and L IN et al. [LLY*24] employ hypertext techniques and other variations of textual display (i.e. , text visualization). Being the raw format supported by LLMs, such visualizations can ef fi ciently augment LLM-generated output, 

e.g. , for explainability [FWP*24]. 

Summary 

Traditional charts are studied by a majority of papers (24 / 48), followed by network and graphs (18) which were all knowl-edge graphs; spatial representations (18) in particular maps and AR applications, and other representations (0), including text. Out-of-the-box, LLMs perform well on simple tasks involving charts. Other representations seem to require more training and careful prompting of the LLM. Overall, only a very small sub-set of the papers we surveyed targeted volumetric visualization applications, probably because of the inherent complexity of their manipulation or creation, often combined with the lack of standard software or library to display and analyze them. 

5.3. Application domain and users 

We now discuss how speci fi c challenges that may arise map to speci fi c application domains based on our classi fi cation (Sect. 4.4). Some papers did not target a speci fi c domain ( e.g. , dealing with charts in general)—we categorized these as belonging to the data science category, as we discuss fi rst. 

5.3.1. Data science 

A sizeable quantity of papers (22) are Natural Language Visualiza-tion Recommender Systems (VRS), i.e. , systems that automatically or semi-automatically generate visual representations from natu-ral language prompts and data. These systems are targeted at data scientists and analysts, with the aim of streamlining a repetitive work fl ow, but can also facilitate exploratory data analysis by provid-ing a quicker feedback loop between the user intent and the visual response. Here interaction comes into play: the verbal interaction complements more traditional approaches. LLMs excel at summa-rizing and parsing abstract queries, but are less effective in selection tasks or view manipulation, where traditional GUI-based techniques are better suited. VRS were studied before the advent of LLM [VHS*17], and is a well-established fi eld connected to sub-topics such as Question-Answering (QA) and Visual Question-Answering (V-QA), Data Mining, and Information Retrieval. 

5.3.2. Scienti c subject domains 

Two of the surveyed papers directly address molecular visualiza-tion in 3D space [JIB*25; RKPA23], one focuses on genetic data [NMD*24], and a last one on operative surgery [JGL*24]. Scienti fi cdisciplines are characterized by rich domain-speci fi c constraints on visualization. For instance, in operative surgery, the use of AI facil-itates multitasking; in particular speech-enabled systems can free up the hands for synchronous and sterile manipulations. Likewise, C HEN et al. [CBAI25] describe “Word2Wave”, an NLI for mission programming of autonomous underwater vehicles, and justify the need for a NLI by the need to recon fi gure the mission on-the-fl y. Typ-ical interfaces are too complex to operate in dynamic environments, and NLIs can render user interfaces more user-friendly. In macro-molecular visualization, structures of interest often exist at multiple scales. A multi-scale navigation system thus must show data at multiple levels of detail while retaining a global context [HMK*20]. “VOICE” [JIB*25] addresses this point by representing the scene as a tree, where the root is the entire macromolecule and children nodes are sub-components. By pruning this hierarchical structure around the viewpoint, the system can effectively craft a LLM prompt for navigation that contains only spatially relevant information, which works around the limited LLM context window. To provide a fi nal example, V OIGT et al. [VCM*23] tackle climate data exploration and explain that current natural language-based visualization systems adapt poorly to domain-speci fi c data—in this case geographic data—and are better suited for general-purpose data science tasks as explained above. Indeed, LLMs are well trained on tabular data, traditional charts, and general analytics tasks but perform worse on domain-speci fi c software, data with spatial rela-tionships, or sensitive data. The “VIST5” [VCM*23] system can adapt to multiple domain-speci fi c visualization libraries by facilitat-ing the injection of domain-speci fi c data through RAG. 

5.3.3. Education and social sciences 

While data science and scienti fi c applications dominate, several authors explore visualization that targets audiences such as in educa-tion [JIB*25; RKPA23], improving accessibility [GKWK24], en-tertainment [ANP*24], or arts [HHYG24; SXC*23], which all have their own speci fi c need for LLM-based approaches. In “VizAbility” [GKWK24], the authors explore the topic of chart accessibility, ex-ploring alternative ways to read charts for visually impaired users, 

e.g. , with keyboard input or natural language questions. With their “AiCommentator” system, A NDREWS et al. [ANP*24] augment foot-ball viewing with an LLM-generated live commentary, relying on knowledge retrieval. In educational settings, LLMs provide a user-friendly interface to the visualization without needing training to get started. In artistic settings, LLMs combined with image generation [HHYG24; SXC*23] expand the potential of visitor participation in art exhibits through generation or adaptation of content for different people. Several authors also emphasize that NLIs facilitate user onboarding [ZZZ*25], engagement [ANP*24], or learning visual analytics [YZE*24] due to their ease of use for novices. 

Summary 

Overall, we observed domain-speci fi c constraints and the re-lated challenges and opportunities for LLM-enabled interaction with visualization. Uses of LLMs for broad tasks in visualiza-tion has been explored, for some applications more exten-© 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 16 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

sively (Data science, GIS, Operative surgery), while other as-pects have received relatively little attention (AR/VR, domain-speci fi c applications). Using natural language to interface users and LLM-assisted interaction with the visualization can prove useful in operational scenarios where individuals have limited access to classical interaction devices. In collaborative work scenarios, LLMs facilitate natural interaction between multi-ple users and an agent through spoken language [TLT*25]. In public scenarios (considering, e.g. , educational contexts), con-versational interactions with a visualization are likely to foster engagement and improve accessibility [ANP*24; MMA*25]. 

6. LLM Integration in visualization systems 

To effectively connect an LLMs with a visualization system, the LLM’s model needs to be conditioned for its task (training, fi ne-tuning, prompt engineering), while the visualization engine needs to pass data to and from the model. 

6.1. Choice of LLM model 

Generally, one can train a bespoke LLM, fi ne-tune a pretrained model, or prompt a general-purpose model. The model itself can be cloud-based or local. Training a model from scratch, however, is usually not a cost-effective option. The earliest-family LLM models, BERT and GPT (2018–2022), were closed-source models by Google and OpenAI. When GPT-3 and, later, ChatGPT were released to the public, interest in this technology rose in both the public and academic circles. Back then, people had no option other than these models, and it is thus not surprising that the vast majority of papers (28) use OpenAI’s GPT models. Today’s LLM ecosystem is more di-verse, including both private (Anthropic’s Claude, Google’s Gemini, Mistral) and open models (Facebook’s LLaMa and, more recently, DeepSeek), including cloud-based models and smaller models that can run locally on consumer hardware. Interestingly, in our paper pool most authors do not state the rea-son for their model choice or simply say that OpenAI’s models are the state-of-the-art [LML*23; LN23] at time of writing. “VIST5”; “ChartGPT” [VCM*23; TCD*25] use models in the FLAN-T5 fam-ily (“ChartGPT” [TCD*25] uses a fi ne-tuned version of Flan-T5 “as it has undergone pre-training on various tasks and possesses strong reasoning capabilities”). “Word2Wave” [CBAI25] uses an LLM to generate training data for a T5-small seq2seq model. There is lit-tle comparison of performances between different LLMs, although some authors mention it as future work [SMKS24; GKWK24]. 

6.2. Prompt engineering 

Conversational LLMs are models trained to simulate a conversation between a user and an arti fi cial assistant. These models are gene-ral-purpose and can tackle different tasks without prior training. Instructing the model is done through the system prompt, which describes the system and its role, and the user prompt, to start the conversation. There are several strategies for constructing a system prompt [KLH*23]. Most commonly, the prompt contains examples of valid user ↔ assistant interactions. The terms zero-shot and few-shot refer to the number of examples of conversation turns provided to the model in the system prompt: a model can have zero or a few examples of conversation turns [BMR*20]. In general-purpose LLM use, few-shot improves the model alignment with the desired output, without requiring fi ne-tuning of the model [ RLB22], corroborated in applications of LLMs in visualization [ WLZ*25]. 9 papers from our corpus report employing few-shot. “VizAbility” [GKWK24] selects the best matches for the few-shot based on cosine-similarity with the query, an approach similar to RAG, and “MEQA” [SGC25] plans it as future work. But T IAN et al. [TCD*25] decided against using few-shot, as it was too rigid for the ambiguity of the queries, and instead fi ne-tuned their model with query examples. An alternative prompting strategy consists of decomposing of the query into smaller tasks, each responsible for addressing one query element. This technique is called Chain-of-Thought (CoT) and sig-ni fi cantly improves reasoning capabilities of LLMs on complex tasks by mimicking a step-by-step thought process [ WWS*22a]. In their paper assessing effectiveness of detecting misleading visual-ization, L O et al. [LQ24] found that CoT prompting was the most ef fi cient way of dealing with their goal. This method is used by 5 papers [CWG*25; LXH*25; SMKS24; TCD*25; QWH*25]. 

6.3. Data formats 

The primary input for LLMs is natural language, but they can also parse other textual forms of information such as code (often Python), structured data (commonly JSON), data tables (mainly CSV), and rich text ( e.g. , Markdown). An essential component to prompt en-gineering is managing the size of the context window, and data formats have various degrees of performance and token footprint. There is insuf fi cient previous research analyzing data formats across different models, but preliminary results indicate that the format has a sizable impact on the performance, varies strongly across models, and models are sensitive to subtle changes in formatting [ LNS*25; 

SCTS24]. We encourage future work to explore this topic applied to visualization recommendation tasks. Compressing data to fi t the context window can also be achieved by pruning , i.e. , contextually removing irrelevant information based on heuristics. T IAN et al. [TCD*25] mention that they include only the header, column type, and top two rows of table data. J IA et al. [JIB*25] build the prompt based on the location of the viewport in the scene tree and include only information from nearby branches. 

6.4. Multi-modal LLMs 

Today’s latest LLM models support varied input and output, not only text: images, audio, and video. We found that researchers did not consider LLM-based image generation to be particularly good or effective at producing visual representations, except perhaps for artistic purposes [ KBSL23; Woo22]. Vision Language Models (VLMs) provide richer reasoning capa-bilities and resilience in LLM-driven visualization [ MKB*25a]. For example, in “VizChat” [YZE*24] a VLM receives a screenshot of the visualization for each query. The screenshot helps the LLM to understand to which part of the visualization the user refers and what content is displayed. S HIN et al. [SHE25] also employ VLMs for visual reading, but combine that with information gathered from computer vision techniques such as eye tracking prediction, opti-cal character recognition, analysis of color usages, etc. L IU et al. © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 17 of 28 [LML*23] found that VLMs were capable of iterative visual re fi ne-ment of a produced representation through a feedback loop, albeit with high latency. Their evaluation highlights that current VLMs are best suited for natural images, but struggle with detecting de-tailed and localized structures. They call for further work in VLMs and visualization applications, as “these multi-modal foundation models hold[] the potential to fundamentally transform the way we think about visualization and user interaction.” L I et al. [LN23] calls for models trained on spatial data, i.e. , Large Spatial Model (LSM). They postulate that, since LLMs have acquired language skills through training on text corpora, similar models could be trained on spatial data to acquire spatial understanding. Further-more, such training data is readily available in large quantities. 

6.5. Multi-agent systems 

A common approach with LLMs consists in creating multiple agents, each specialized for one task. This “divide and conquer” approach— known as Mixture of Agents (MoA) [ WWA*24]—improves relia-bility for complex tasks and reduces the context window by includ-ing only relevant information for each prompt. “VOICE” [JIB*25], for instance, uses a main manager agent to interface between a user and the specialized agents ( explorer, pilot, encyclopedia, guardian ). Likewise, “VizChat” [YZE*24] employs a context agent with access to the pre-stored knowledge about the visualization. The agent is responsible for asking for clari fi cations in case of ambiguity and to craft a prompt incorporating the instruction and RAG-retrieved data into an explanation agent . This two-agent system clearly separates between intent understanding using context and retrieval, on one hand, and explanation generation, on the other hand. 

6.6. Long-term memory 

Persistent storage can be added to the system to facilitate persistent post-deployment learning from past interactions and to maintain con-sistency across sessions. Several authors applied terminology from cognitive psychology to LLM long-term memory [WJ25]. We thus distinguish semantic , procedural , and episodic memory. Systems using retrieval from knowledge graphs [YZE*24] rely on semantic 

memory—they store and retrieve factual information. V OIGT et al. [VCM*23] found that semantic memory allows the LLM to easily be adapted to different domains and software. “VIST5” [VCM*23] uses long-term memory to store procedural knowledge: users can store domain-speci fi c input-action pairs to describe how to perform an action given a query. During subsequent queries, relevant stored information is provided to the LLM as few-shot examples in the prompt. Relevance is scored similar to other RAG approaches, with the cosine similarity between the embedding of the user query and the stored input. “Autonomous GIS” [LN23] uses episodic memory, containing past question-answer pairs to allow the model to recall past interactions. The authors add that they plan to add a procedural memory system, which would need to contain the solution graph but which is not easily stored as textual key-value pairs. 

Summary 

Model conditioning is key to LLM integration in vis systems, including prompt engineering (few-shot, chain-of-thought), 

fi ne-tuning, using mixture of agents, and data input and out-put formatting. A vast majority of the papers we surveyed made use of OpenAI’s family of models, as we show in Ta-ble 3. This limits our capacity to transfer fi ndings to other implementations. LLM technology is constantly evolving: new model capabilities are emerging. Vision and speech models are now widespread, while LLMs supporting video and spatial data are actively being developed. The software ecosystem around LLMS is also constantly mutating, such as with the development of many MCP servers connecting LLMs to exist-ing software. These advancements are an academic opportu-nity, but researchers in the fi eld should be on the lookout for technology advancements and strive to produce generalizable contributions. 

7. Evaluation of LLM-Vis systems 

Evaluation is a critical aspect of visualization research that has long garnered our methodological attention as a fi eld. Considering the variability of LLM-generated content, we believe that conducting proper evaluation when combined with data visualization is essential for the newly developed tool to eventually be used. But the evalua-tion of LLMs is a growing fi eld that has yet to reach maturity and, as such, we believe that surveying current practices in our fi eld is likely to inform on interesting approaches or missed opportunities. We thus now review how authors evaluated their LLM-augmented systems, discussing two dimensions: User evaluation are reports on external participants (not authors) interacting with the system, whereas system evaluations include author-based and automated evaluations ( e.g. , benchmarks). 25 papers contain some form of user evaluation, and 28 contain system evaluations. Only 4 system papers are not evaluated, which we consider to be preliminary work. Evaluating LLM-enabled systems is generally dif fi cult due to the stochastic nature of LLMs. In contrast to traditional software, the same “input” does not always produce the same “output,” LLMs are unpredictable [ PCR*24; LMS*25; SWLL25; TRP26] even given the same model. And as LLMs are also a novel technology, evalu-ation methods are still being developed and results are constantly evolving [CWW*24; PCR*24]. Automated model benchmarks were shown to be poorly aligned with human evaluation [LIX*23]. This problem is accentuated by the closed-source nature of many cloud-based LLMs, which are continuously and sometimes silently updated [ AAK*25; SISS25]. The unpredictability is exacerbated by the ambiguity of natural language. Many user queries, if abstract, contain ambiguities that can only be resolved with an understanding of the context and cultural background of the asking person. Both issues (the unpredictability of LLM outputs and the ambiguity of natural language) mean that in non-trivial cases there does not exist a 1-to-1 mapping from a query to a action. Consequently, holistic quantitative evaluations are hard to perform if not impossible, and qualitative evaluations with user feedback are needed. 

7.1. User evaluation 7.1.1. Study participants 

We collected the number of participants and their levels of expertise. Papers reporting on a user study include an average of 13.8 in © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 18 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

their experiments. Two papers did not communicate the number [URQ*25; JGL*24]. The level of expertise of participants was as follows: 8 / 25 include expert users, 12 / 25 include knowledgeable users, 3 / 25 include novice users, and 5 / 25 were mixed. In their 2013 survey on user evaluation in visualization, I SENBERG et al. [IIJ*13] observed an increase in the amount of papers including a user evaluation, exceeding 50% in 2012. The median number of participants was 9. M ERINO et al. [MGAN18] similarly found that software visualization system papers included an evaluation in with a median of 13 participants. We found similar fi gures for user evaluation (25 / 48 papers) and number of participants (mean 13.8, median 12)—consistent with the overall trend in visualization. 

7.1.2. Protocols and instruments 

Almost all authors employ a similar study structure. It consists of three principal phases: observation of participants’ use of the sys-tem (either unsupervised or with a precise goal), a semi-structured interview with the moderators, and (a) post-study questionnaire(s). 

7.1.3. Metrics and data collection 

Typical user evaluation in visualization includes both objective per-formance metrics and empirical preference metrics [LBI*12]. Fo-cusing on performance metrics, authors evaluate their system with objective scores such as measuring tasks completion time (7 / 25), performance/accuracy (10 / 25), and recall (1 / 25). User preference is evaluated with instruments such as questionnaires and interviews. In three papers, authors report perceived workload with the NASA TLX questionnaire ([XZX*24; JIB*25; KLL*24]). In fi ve papers authors assess perceived usability with the SUS ques-tionnaire [ANP*24; JIB*25; KLL*24; CBAI25; MMA*25]. Likert-scale questionnaires are used in 19 studies, where the most common questions assessed preference, ease-of-use, effectiveness/usefulness, helpfulness, satisfaction, enjoyment, trustworthiness, or reliability. 

7.2. System evaluation 

Apart from user evaluation, three common means of evaluation are the use of benchmarks of standardized tasks, use-case evaluation performed by the authors, and non-structured qualitative feedback in short papers. Authors of 28 papers in our sample evaluate their system in such a way. In the ML community, frameworks to evaluate LLM systems are still being developed, e.g. , measuring conciseness [ GKQ*25], retrieval augmented generation capabilities [YGZ*25], bias detection [FGA*25; KUK*25]. Frameworks tailored to vi-sualization applications also exist, such as for visualization recom-mendation [ZMD*22] and reading [PO25]. To our knowledge, there is no framework for evaluating interaction in visualization. 

7.2.1. Benchmarks and standard datasets 

Authors of 7 papers evaluate their system with a benchmark from visualization, HCI, or ML. Benchmarks are system-agnostic col-lections of inputs and expected outputs, which can be used both as training data to improve the model quality or as a metric for evalu-ation. Several benchmarks exist for visualization recommendation .The benchmark by L UO et al. [LTL*21] (and its derivatives [ LHS*; TCD*25]) is used in our corpus most frequently: in 2 papers for evaluation [ SMN*24; QWH*25] and in 2 papers for training [VCM*23; TCD*25]. It comprises 25,000+ natural language ut-terances ↔ visualization pairs covering traditional chart types (bar, pie, scatter; as Vega-Lite [SMWH17] speci fi cations). Several authors produced their own data sets to test their system. “VOICE” [JIB*25] contains 92 in-scope tasks and 41 out-of-scope tasks curated by the authors. Building a bespoke test dataset for each paper, however, is time-consuming. This approach is not effective at comparing systems with each other; only a standardized benchmark can do that—it would also be more neutral and exhaustive. Other benchmarks and dataset exist but were not used by any of the surveyed papers, and could be relevant for some of the applications: from the visualization community [ FXG*20; PV; SNL*21], from the ML community [TBS23], and speci fi cally for the task of visual question-answering [ KMA*17; KPCK18; MGKK]. Off-the-shelf LLMs have also been evaluated for their visualization literacy by P ANDEY et al. [PO25] and B ENDECK et al. [BS25]. They found that state-of-the-art models in 2025 (GPT-4, Claude, Gemini, Llama) are ef fi cient at basic chart interpretation but perform poorly on misleading visualization. Other authors studied the trustworthi-ness, fairness, and bias of LLM-generated and LLM-interpreted visualization [KPR25; KEBB25; KAMB25; ANYS24]. Several authors regret the lack of benchmark datasets fi tting their needs. G ORNIAK et al. [GKWK24], e.g. , call for the development of inclusive benchmarks that incorporate the viewpoints of blind and low-vision individuals. They also regret the lack of fi ne-grained met-rics in benchmarks, including multi-hop reasoning, queries requiring prior context, etc. L O et al. [LQ24] add that there is no benchmark for vision-LLM visualization tasks, being urgently needed. 

7.2.2. Qualitative data collection 

Due to the implicit nature of LLMs and visualization tasks, compar-ing a result against a baseline is not always feasible nor desirable. J IA et al. [JIB*25] include both in-scope and out-of-scope queries in their testing to compare how the system reacts in predictable and unpredictable scenarios. In in-scope cases, results could be classi fi ed as correct or incorrect , and obtained a score of 90% correct . In out-of-scope cases, LLM interactions were sorted as partially correct ,

state-dependent , or incorrect and obtained 43% or partially correct .The authors also tried repeating the same queries to get a glimpse of the model variability on ambiguous questions. This approach seems to be effective for evaluating the system in unpredictable contexts. It shows, however, that a signi fi cant amount of responses were incor-rect or contextually incorrect, hinting at the lack or robustness and trustworthiness of the model. Perhaps the system needs to be trained to self-report its uncertainty in such cases. Then, an evaluation of reported uncertainty vs. correctness could be performed. 

Summary | Evaluation of LLM-Vis systems 

In the absence of well-established evaluation frameworks and benchmarks tailored to the speci fi c case of LLM-augmented visualization and interaction, most surveyed papers rely on existing practices from the visualization and HCI communities, which both have a long history of evaluation and establishing evaluation standards [BTK11; SD14; EY12; IIJ*13; BTK11; © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 19 of 28 MGAN18]. User evaluations are the dominant approach and typically combine quantitative usability measures—most com-monly SUS and NASA-TLX—with qualitative feedback col-lected through post-study interviews. Performance-oriented metrics such as task completion time, accuracy, or retention are used less frequently and are often task-speci fi c. While these evaluations provide insight into usability and user experience, they rarely assess aspects that are, nonetheless, central to LLM-based systems such as perceived correctness, trustworthiness, or graded correctness when handling ambiguous or unspeci-

fi ed queries ( e.g. , [JIB*25]). Considering the non-deterministic nature of LLMs and the fact that they hallucinate, it seems that the deployment and use of LLM-augmented visualization systems is likely to depend on how the systems are perceived by users in those terms, which is rarely considered. System-level evaluations without user involvement are com-paratively rare. Several benchmarks exist—such as NVBench [LTL*21] and NL2VIS-derived datasets—but only a subset is used in practice and comparisons across systems remain uncommon. Some papers construct bespoke datasets that serve both as training or fi ne-tuning material and as an evaluation corpus. While this approach facilitates targeted testing, it limits comparability across systems and raises concerns regarding neutrality, coverage, or generalizability. It seems that prelimi-nary efforts toward benchmark-based evaluation frameworks for visualization recommendation ( e.g. , [ CZX*24; LTL*21]) have been made, but their adoption remains limited. In our survey we found no benchmark, dataset, or evaluation framework speci fi cally designed to assess the interaction with LLM-augmented visualization systems. Visualization interac-tion techniques are thus rarely compared systematically, and evaluation strategies remain fragmented. Prior work has high-lighted this lack of evaluation methodology in visualization recommendation more broadly [ZMD*22], a gap that is even more pronounced for interactive and multimodal LLM-based systems. Moreover, even within traditional visualization re-search the developed systems are rarely assessed in the wild or in their intended application fi eld beyond simpli fi ed scenarios or a few examplars [BYK*21]. The challenges of evaluating LLM-augmented system thus encompass those for both LLM and visualization technologies, and perhaps even further ones. 

8. Research opportunities and discussions 

Our survey of the convergence of LLMs and visualization for interac-tive data analysis highlights several important research opportunities and challenges, as well as interesting discussion points to consider for our fi eld, which we highlight below. 

8.1. Opportunities for future research 

We fi rst highlight the interesting avenues for future research that stem from our literature survey. 

8.1.1. The potential of multimodality 

Consumer-grade multimodal models are now capable of reading and generating images and audio. Speech-to-speech models, combined with a voice activity detection mechanism, facilitate low-latency speech conversations with LLM assistants. Previous LLM-based voice assistants included speech-to-text and text-to-speech interme-diate conversion steps, which introduced latency and lost precious information conveyed through the voice tone. Image reading and generation capabilities will improve the visual literacy of LLMs [PO25] by allowing them to take screenshots of the visualization, hence allowing the model to validate the correctness of its own actions and take corrective measures, as shown in “3D-LLaVA” [DHJ*25]. Combination of visual and sensory inputs suggest that it will be possible to increase virtual agents’ surrounding aware-ness with ambient microphones and cameras in the room, which should increase the realism of anthropomorphic agents, which can acknowledge and guide users of an interactive visualization. While images and audio are most common, other LLM models de-velop different modalities. 3D LLMs [HZC*23; DHJ*25] can treat 3D point-cloud scenes as input and have improved spatial reasoning capabilities, beyond Vision Language Models [HZC*23]. Video LLMs are able to treat image sequences as output and reason about temporal information [LLT*25]. Combined with intention predic-tion systems, LLMs can become agentic and proactive , i.e. , initiate actions without explicit user prompts. Hence, current research is developing audio/video streaming models, which bring LLMs closer to human interaction patterns [WMW*25; CLW*24; WHL*24]. 

8.1.2. Interfaces with external data sources 

Agentic LLM systems have the capability to interact with exter-nal systems through tool calls . The Model Context Protocol (MCP; 

modelcontextprotocol.io ) format offers a standardized API for LLMs to interact with external tools. Adoption of this ecosystem will facilitate an easy and rapid integration of LLM agents into ex-isting visualization software. Furthermore, it will allow LLMs to act as a middleware , connecting multiple unrelated systems with little effort. Proprietary LLM products such as Google Gemini already rely on tool calls to give language models access to the fi le system or searching the web, which facilitates data retrieval from unknown sources. Yet, we have not seen such capabilities used in visualiza-tion research. Opening access to web-sourced data in visualization will expand the possibilities of exploratory data analysis, where the data analyzed may be complemented and compared with online databases. Automated retrieval of online content poses challenges for automatic assessment and communication about data reliability. Such uncertainty can be communicated visually, orally, or in written form, and modalities for communicating uncertainty in fl uence peo-ple’s perception, e.g. , spoken uncertainty leads to riskier decisions, while text leads to lower con fi dence [SSCS24]. 

8.1.3. Tailoring visualization and interaction to users 

LLM-based interaction offers new capabilities for capturing user intent and resolving ambiguity, creating unprecedented opportuni-ties to make data visualization more accessible through intuitive interfaces aligned with users’ needs. This is important since personal-ized experiences can increase user engagement, knowledge transfer, and reduce cognitive load, which are key to an effective scienti fi cunderstanding and communication [KSR*22; SHP16; YLT18]. Fur-ther research is needed to understand the mechanisms underlying these experiences with LLM-powered visualization, as well as to © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 20 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

develop more personalized interactions tailored to individual users. Several factors may contribute to this goal. To initiate this discussion, we suggest the following directions. 

Personal interaction histories. With the development of long-term memory mechanisms (Sect. 6.6), LLMs gained the ability to recall information from previous sessions. Combined with user identi fi -cation [WHZ*24], LLMs learn from and about users and provide more relevant, engaging, and accessible information in return. 

Holistic user modelling. Interpersonal communication occurs not only through explicit conversation, but also with non-verbal sig-nals such as gestures, facial expressions, tone of voice, posture, and proxemics in collaborative setups [BVB23; McC16; MTCC17; WOG*22]. While some of these signals are already leveraged in collaborative data analysis scenarios and systems [SWB*22], the use of LLMs to interpret these signals is an interesting research direction that has not been extensively explored. 

Aesthetic qualities. Aesthetics is, in general, an important aspect of visual design [HJSM16]. We would be interested to learn how image generation models and style transfer techniques could be adopted to leverage aesthetics in visualization [ KBSL23; Dib23], for example to foster engagement or interest, or to elicit or avoid speci fi c emotions ( e.g. , [BSB*18; BSB*20]) depending on the target audience and communicative goal [Woo22]. 

Adaptive use of modalities. Several authors have characterized input modalities in relation to visualization goals [DP20]. Individual differences need to be taking into account, however, which can stem from background, preference, expertise, and abilities [LCO20]. G ORNIAK et al. [GKWK24] studied adaptations for blind and low vision people, and C HEN et al. [CWG*25] explored combinations of mouse and natural language interaction, but further work is needed. In summary, LLM-augmented visualization facilitates the tailoring of interaction and presentation to individual users by capturing in-tent, resolving ambiguity, and adapting over time, across modalities, capabilities, and contexts. By leveraging personal interaction his-tories, richer user models, aesthetic qualities, and adaptive use of the available modalities, future systems could reduce cognitive load, improve accessibility, foster better engagement, and better support diverse user goals. To realize these bene fi ts, however, a deeper em-pirical understanding is needed of how personalization mechanisms in fl uence engagement, comprehension, and trust in visualization and LLM-augmented systems. In parallel, the visualization research community must continue to advance its evaluation strategies to better guide and validate research outcomes. 

8.1.4. The challenge of evaluation 

Visualization, as a fi eld, has a positive history of rethinking evalua-tion methods to ensure the fi eld does not solely focus on quantitative and performance oriented metrics and that our results have a wider impact [LBI*12]. We often re-evaluate, e.g. , through the BELIV workshop [BHH*20] the methods on which we rely for different measures of importance and are open to explore and adopt new    

> This venue is already being explored by commercial actors such as OpenAI as indicated in their privacy policies, see, e.g. ,https://openai. com/policies/privacy-policy/ .

methods for measuring or reporting results [BD19; Dra16] and to improve transparency [Har18; BNH*24] or reusability of results [Ise24]. This emphasis stems from the recognition that assessing the impact, adoption, and bene fi ts of visualization is a multifactorial problem, shaped by intertwined cognitive, social, organizational, and contextual factors that inherently resist reduction to a single metric or experimental paradigm. Combining visualization research to LLM research does not simplify the space but introduces new challenges of evaluation [Cri24] that, moving forward, we need to address as a fi eld in order to produce reliable results and systems that can be, eventually, adopted for real-life situation and thus for our fi eld to continue having impact beyond our publications. Only a subset of papers provide user evaluation (25 / 48) and fewer still have quali fi ed and tried to improve the trustworthiness of LLM-produced information. This is particularly important as LLMs are prone to hallucinations [JLF*23]. It is further important because adoption of LLM-augmented visualization will eventually rely on the trust that users will place in the system. As the adoption of new visualization software is already an open challenge of the visual-ization research community [WBG*19], we see this lack of trust evaluation as a new threat to the wider adoption of the solutions we are likely to develop in the future. We thus argue that a comprehen-sive evaluation framework of LLM-augmented visualizations needs to be developed that consider all of the aspects that are necessary for a wide adoption of such systems and encompasses, at least, all elements that we have considered in this report. Furthermore, authors clearly express the need for benchmarking datasets and we fi nd statements such as “a benchmark dataset is urgently needed. Our evaluation efforts were limited by two main factors: (1) the absence of a benchmark dataset and (2) the need for re fi ned evaluation metrics” [LQ24] and “our study revealed the need for a more extensive and inclusive benchmarking dataset that incor-porates viewpoints of blind and low-vision individuals” [GKWK24]. Providing the community with useful benchmarking datasets would lead to more systematic, reproducible, and comparable evaluations across systems and allow us to track progress in the fi eld. 

8.2. Discussion 

While conducting this work, we have encountered several challenges that we believe are important to discuss for the visualization com-munity but also for the wider research community. As noted in Sect. 3.4, it is commonly advised that survey papers should focus on published and, as such, peer-reviewed contributions only. Doing so, however, could pose signi fi cant problems when sur-veying LLM research considering the extremely fast-moving pace of the fi eld and the fact that many landmark papers in LLM/AI do not even get submitted for peer-review (including technical papers from companies [ TMS*23; OAA*24], the “Survey of Large Language Models” which is updated continuously [ ZZL*25], etc.). We con-sequently decided to include preprints from arXiv in this STAR. We further decided to clearly distinguish those papers from peer-reviewed ones through a speci fi c citation marker throughout our paper to indicate that, perhaps, these papers should be considered with more caution. Given the high turnover rate of LLM-related publications, where models, datasets and benchmarks are super-seded within months, excluding preprints may risk producing an © 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 21 of 28 outdated and scienti fi cally unrepresentative picture of the fi eld by the time the STAR is available. Including them, however, may pose a threat to the reliability of the represented picture of the state of the art. We believe that, in this respect, our approach ensures relevance while still preserving methodological transparency. The approach we adopted is likely the most complete one in terms of inclusion of papers, while maintaining a transparent presentation of the surveyed space, but we want to argue that the PRISMA guidelines [PMB*21] may have to be updated to consider this general problem, possibly our solution, and perhaps a wider call for other solutions. In addition, a challenge we encountered is to try and verify whether some of the preprints we identi fi ed eventually ended up being pub-lished to maintain the most accurate image of the fi eld. We saw such updates happening for several papers that we had initially included as preprints, and we mention these now as published. While arXiv provides a way for authors to link to the peer-reviewed version of their preprints, many authors do not use this possibility. With LLM-driven research accelerating, systematic mechanisms for tracing preprint development, peer-review status, and potential retractions (see below) will become increasingly pivotal to ensure scienti fi cintegrity in future literature surveys such as STARs. Our survey fi nally highlighted that it, at times, was dif fi cult to extract the information needed to categorize the papers. In particular, some of the papers we surveyed did not mention the necessary details about their LLM implementation, with some of them not even men-tioning which version of a speci fi c LLM they used or whether it was trained or fi ne-tuned and how. Considering how unreliable LLMs can be [Dal21; LHM*23], not providing this kind of information further complicates matters. We believe that we should, as a fi eld, ensure that we communicate such information in the manuscript to ensure that our research remains both transparent and replica-ble. This is even more important considering that LLM-oriented research is already dif fi cult to replicate [BS25]. The frequent lack of implementation details is especially problematic in the context of LLM-enabled visualization systems, where subtle differences (e.g. , prompt templates, versioning) can lead to varying outputs. Without precise reporting, fi ndings may become non-replicable, and comparisons across different systems unreliable, thus hindering the cumulative progress of the fi eld. We thus argue for the adoption of clear reporting standards for future LLM-Visualization research, which will not only improve reproducibility [CDBG20; Ise24] but also facilitate meaningful meta-analysis in forthcoming STARs. 

9. Conclusion 

We have surveyed the existing work in employing LLMs in data visualization systems for enabling interaction with the data. First, following the PRISMA methodology, we classi fi ed the literature in the fi eld according to four axes: LLM-enabled visualization tasks, in-teraction modalities, visual representations, and application domain. Second, we contribute a survey of implementation and evaluation of LLM-enabled visualization systems to help establish research standards for future work. Third, we discussed opportunities for future work in the fi eld at the intersection between LLMs, data vi-sualization, and interaction. In closing, LLM-enabled interaction is changing the landscape of visualization. While it is expanding access as part of driving a paradigm shift, it is also exposing criti-cal limitations in reliability and evaluation. Our systematic STAR synthesis clari fi es current capabilities, gaps, and methodological challenges and offers a foundation for the future advancement of robust multimodal visio-verbal systems that actively support rich, transparent, and trustworthy human-computer interaction. 

10. Acknowledgments 

This work was partially supported by Marcus and Amalia Wallen-berg Foundation (2023.0128) and (2023.0130), and Knut and Alice Wallenberg Foundation (2019.0024). We are grateful to the authors who provided the fi gures used as illustrations in this report. We also wish to thank I. Viola for his feedback on the manuscript and fruitful discussions during the data collection process. 

11. Author contributions 

We list below the contributions of each individual authors based on the CRediT author statement ( https://credit.niso.org/ ). M. Brossier T. Isenberg K. Schönborn J. Unger M. Romero J. Björklund A. Ynnerman L. Besançon Conceptualization Data curation Formal analysis Funding acquisition Methodology Project administration Supervision Visualization Writing – original draft Writing – review & editing 

References 

[ AAK*25] A NGERMEIR , F LORIAN , A MOUGOU , M AXIMILIAN , K RE -ITZ , M ARK , et al. Re fl ections on the reproducibility of commercial LLM performance in empirical software engineering studies . arXiv preprint 2510.25506. 2025. DOI : 10/qms7 .[ACS*22] A ZIZ , F ARKHANDAH , C REED , C HRIS , S ARCAR , S AYAN , et al. “Voice Snapping: Inclusive speech interaction techniques for creative object manipulation”. Proc. DIS . New York: ACM, 2022, 1486–1496. DOI : 10/hbjqps .[AES02] A MAR , R OBERT , E AGAN , J AMES , and S TASKO , J OHN . “Low-level components of analytic activity in information visualization”. Proc. InfoVis . Los Alamitos: IEEE CS, 2002, 111–117. DOI : 10/bwrm27 .[AKG*15] A URISANO , J ILLIAN , K UMAR , A BHINAV , G ONZALES , A L -BERTO , et al. ““Show me data.” Observational study of a conversational interface in visual data exploration”. Posters of IEEE VisWeek . 2015. DOI :

10/qms9 .[AN19] A UGSTEIN , M IRJAM and N EUMAYR , T HOMAS . “A human-centered taxonomy of interaction modalities and devices”. Interact Com-put 31.1 (2019), 27–58. DOI : 10/gfwc96 .[ANP*24] A NDREWS , P ETER , N ORDBERG , O DA E LISE , P ORTALES ,S TEPHANIE Z UBICUETA , et al. “AiCommentator: A multimodal con-versational agent for embedded visualization in football viewing”. Proc. IUI . New York: ACM, 2024, 14–34. DOI : 10/gt5rjf .© 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 22 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

[ANYS24] A LEXANDER , J ASON , N ANDA , P RIYAL , Y ANG , K AI -C HENG ,and S ARVGHAD , A LI . “Can GPT-4 models detect misleading visual-izations?”: Proc. VIS . Los Alamitos: IEEE CS, 2024, 106–110. DOI :

10/qmtb .[BBB*19] B LASCHECK , T ANJA , B ESANÇON , L ONNI , B EZERIANOS ,A NASTASIA , et al. “Glanceable visualization: Studies of data compari-son performance on smartwatches”. IEEE Trans Vis Comput Graph 25.1 (2019), 616–629. DOI : 10/gg8qt3 .[BD19] B ESANÇON , L ONNI and D RAGICEVIC , P IERRE . “The continued prevalence of dichotomous inferences at CHI”. CHI Extended Abstracts .New York: ACM, 2019, art. alt14, 11 pp. DOI : 10/ghs8mh .[Ber94] B ERNSEN , N IELS O LE . “Foundations of multimodal representa-tions: A taxonomy of representational modalities”. Interact Comput 6.4 (1994), 347–371. DOI : 10/cg84m4 .[Bes17] B ESANÇON , L ONNI . “An Interaction Continuum for 3D Dataset Visualization”. PhD thesis. France: Université Paris-Saclay, 2017. URL :

https://hal.science/tel-01684210 .[BHH*20] B EZERIANOS , A NASTASIA , H ALL , K YLE , H URON , S AMUEL ,et al. “BELIV 2020 preface”. Proc. BELIV . Los Alamitos: IEEE CS, 2020, 7. DOI : 10/qmwd .[BIAI17] B ESANÇON , L ONNI , I SSARTEL , P AUL , A MMI , M EHDI , and I SENBERG , T OBIAS . “Mouse, tactile, and tangible input for 3D manipula-tion”. Proc. CHI . New York: ACM, 2017, 4727–4740. DOI : 10/gpmzm8 .[BMP*00] B ENOÎT , C HRISTIAN , M ARTIN , J EAN -C LAUDE , P ELACHAUD ,C ATHERINE , et al. “Audio-visual and multimodal speech-based systems”. 

Handbook of Multimodal and Spoken Dialogue Systems: Resources, Ter-minology and Product Evaluation . Ed. by G IBBON , D AFYDD , M ERTINS ,I NGE , and M OORE , R OGER K. Boston: Springer, 2000. Chap. 2, 102–203. DOI : 10/b48fhg .[BMR*20] B ROWN , T OM B., M ANN , B ENJAMIN , R YDER , N ICK , et al. “Language models are few-shot learners”. Advances in Neural Information Processing Systems . Vol. 33. Curran Associates, Inc., 2020, 1877–1901. URL : https://proceedings.neurips.cc/paper_files/paper/ 2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .[BNH*24] B ESANÇON , L ONNI , N OSEK , B RIAN A., H AVEN , T AMARINDE ,et al. “Merits and limits of preregistration for visualization research”. Proc. BELIV . Los Alamitos: IEEE CS, 2024, 89–96. DOI : 10/g883fp .[Bol80] B OLT , R ICHARD A. ““Put-that-there”: Voice and gesture at the graphics interface”. SIGGRAPH Comput Graph 14.3 (1980), 262–270. DOI : 10/bvqpcd .[BPS*21] B ESANÇON , L ONNI , P EIFFER -S MADJA , N ATHAN , S EGALAS ,C ORENTIN , et al. “Open science saves lives: Lessons from the COVID-19 pandemic”. BMC Med Res Methodol 21.1 (2021), art. 117, 18 pp. DOI :

10/gkdzr6 .[Bro13] B ROOKE , J OHN . “SUS: A retrospective”. J Usability Stud 8.2 (2013), 29–40. URL : https : / / uxpajournal . org / sus - a -retrospective/ .[BS25] B ENDECK , A LEXANDER and S TASKO , J OHN . “An empirical eval-uation of the GPT-4 multimodal language model on visualization literacy tasks”. IEEE Trans Vis Comput Graph 31.1 (2025), 1105–1115. DOI :

10/np3q .[BSB*18] B ESANÇON , L ONNI , S EMMO , A MIR , B IAU , D AVID , et al. “Re-ducing affective responses to surgical images through color manipulation and stylization”. Proc. Expressive . New York: ACM, 2018, art. 4, 13 pp. DOI : 10/kt26 .[BSB*20] B ESANÇON , L ONNI , S EMMO , A MIR , B IAU , D AVID , et al. “Re-ducing affective responses to surgical images and videos through styliza-tion”. Comput Graph Forum 39.1 (2020), 462–483. DOI : 10/gjbm72 .[BSE25] B AHADORI , T ARLAN , S ARVEPALLI , S AI S REEKAR , and E L -DAWY , A HMED . “LASEK: LLM-assisted style exploration kit for geospa-tial data”. Proc. VLDB Endow . New York: ACM, 2025, 5435–5438. DOI :

10/g94xx2 .[BTK11] B ERTINI , E NRICO , T ATU , A NDRADA , and K EIM , D ANIEL .“Quality metrics in high-dimensional data visualization: An overview and systematization”. IEEE Trans Vis Comput Graph 17.12 (2011), 2203– 2212. DOI : 10/b2sfz4 .[BVB23] B EYAN , C IGDEM , V INCIARELLI , A LESSANDRO , and B UE ,A LESSIO D EL . “Co-located human–human interaction analysis using nonverbal cues: A survey”. ACM Comput Surv 56.5 (2023), art. 109, 41 pp. DOI : 10.1145/3626516 .[BYK*21] B ESANÇON , L ONNI , Y NNERMAN , A NDERS , K EEFE , D ANIEL F., et al. “The state of the art of spatial interfaces for 3D visualization”. 

Comput Graph Forum 40.1 (2021), 293–326. DOI : 10/gjbpxp .[CBAI25] C HEN , R UO , B LOW , D AVID , A BDULLAH , A DNAN , and I SLAM ,M D JAHIDUL . “Word2Wave: Language driven mission programming for ef fi cient subsea deployments of marine robots”. Proc. ICRA . Los Alamitos: IEEE CS, 2025, 4107–4114. DOI : 10/hbcfgx .[CDBG20] C OCKBURN , A NDY , D RAGICEVIC , P IERRE , B ESANÇON ,L ONNI , and G UTWIN , C ARL . “Threats of a replication crisis in empirical computer science”. Commun ACM 63.8 (2020), 70–79. DOI : 10/gjbnx4 .[Chi00] C HI , E D H. “A taxonomy of visualization techniques using the data state reference model”. Proc. InfoVis . Los Alamitos: IEEE CS, 2000, 69– 75. DOI : 10/d88fx3 .[Chi02] C HI , E D H. “Data state reference model”. A Framework for Vi-sualizing Information . Dordrecht: Springer, 2002. Chap. 2, 13–41. DOI :

10/qmtd .[CHII25] C ABOUAT , A NNE -F LORE , H E , T INGYING , I SENBERG , P ETRA ,and I SENBERG , T OBIAS . “PREVis: Perceived readability evaluation for visualizations”. IEEE Trans Vis Comput Graph 31.1 (2025), 1083–1093. DOI : 10/njnz .[ CLW*24] C HEN , J OYA , L V , Z HAOYANG , W U , S HIWEI , et al. VideoLLM-online: Online video large language model for streaming video . arXiv preprint 2406.11816. 2024. DOI : 10/hbjqp7 .[CMS99] C ARD , S TUART K., M ACKINLAY , J OCK D., and S HNEIDER -MAN , B EN . “Information visualization”. Readings in Information Visual-ization: Using Vision to Think . San Francisco: Morgan Kaufmann, 1999. Chap. 1, 1–34. URN : urn:oclc:record:1036805840 .[CR96] C HUAH , M.C. and R OTH , S.F. “On the semantics of interactive visualizations”. Proc. InfoVis . Los Alamitos: IEEE CS, 1996, 29–36. DOI :

10/bcnmd7 .[Cri24] C RISAN , A NAMARIA . “We don’t know how to assess LLM contri-butions in VIS/HCI”. Proc. BELIV . Los Alamitos: IEEE CS, 2024, 115– 118. DOI : 10/qm5h .[CWG*25] C HEN , J UNTONG , W U , J IANG , G UO , J IAJING , et al. “InterChat: Enhancing generative visual analytics using multimodal interactions”. 

Comput Graph Forum 44 (2025), 3. DOI : 10/g9qr4q .[CWW*24] C HANG , Y UPENG , W ANG , X U , W ANG , J INDONG , et al. “A survey on evaluation of large language models”. ACM Trans Intell Syst Technol 15.3 (2024), art. 39, 45 pp. DOI : 10/gtmht2 .[ CZX*24] C HEN , N AN , Z HANG , Y UGE , X U , J IAHANG , et al. VisEval: A benchmark for data visualization in the era of large language models .arXiv preprint 2407.00981. 2024. DOI : 10/g8qccd .[Dal21] D ALE , R OBERT . “GPT-3: What’s it good for?”: Nat Lang Eng 27.1 (2021), 113–118. DOI : 10/gjj3f8 .© 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 23 of 28 [ DCA25] D ON -Y EHIYA , S HACHAR , C HOSHEN , L ESHEM , and A BEND ,O MRI . Naturally occurring feedback is common, extractable and useful .arXiv preprint 2407.10944. 2025. DOI : 10/qmtk .[DHJ*25] D ENG , J., H E , T., J IANG , L., et al. “3D-LLaVA: Towards gen-eralist 3D LMMs with omni superpoint transformer”. Proc. CVPR . Los Alamitos: IEEE CS, 2025, 3772–3782. DOI : 10/g94xwz .[DHM*24] D ELDJOO , Y ASHAR , H E , Z HANKUI , M C A ULEY , J ULIAN , et al. “A review of modern recommender systems using generative models (Gen-RecSys)”. Proc. SIGKDD . New York: ACM, 2024, 6448–6458. DOI :

10/g8sfg4 .[Dib23] D IBIA , V ICTOR . “LIDA: A tool for automatic generation of grammar-agnostic visualizations and infographics using large language models”. Toronto: ACL, 2023, 113–126. DOI : 10/nmz4 .[DP20] D IMARA , E VANTHIA and P ERIN , C HARLES . “What is interaction for data visualization?”: IEEE Trans Vis Comput Graph 26.1 (2020), 119– 129. DOI : 10/ggbwhz .[Dra16] D RAGICEVIC , P IERRE . “Fair statistical communication in HCI”. 

Modern Statistical Methods for HCI . Ed. by R OBERTSON , J UDY and K APTEIN , M AURITS . Cham: Springer, 2016. Chap. 13, 291–330. DOI :

10/ggd8gc .[DV23] D OMOVA , V ERONIKA and V ROTSOU , K ATERINA . “A model for types and levels of automation in visual analytics: A survey, a taxonomy, and examples”. IEEE Trans Vis Comput Graph 29.8 (2023), 3550–3568. DOI : 10/grc7ck .[Els26] E LSEVIER . Disciplines: Digital Commons Three-Tiered Taxon-omy of Academic Disciplines . Web site. 2026. URL : https : / / digitalcommons . elsevier . com / organization - content -planning/dc-disciplines-taxonomy . Visited: Jan. 2026. [EY12] E LMQVIST , N IKLAS and Y I , J I S OO . “Patterns for visualization evaluation”. Proc. BELIV . New York: ACM, 2012, art. 12, 8 pp. DOI :

10/qmwk .[FDN*24] FAN , W ENQI , D ING , Y UJUAN , N ING , L IANGBO , et al. “A sur-vey on RAG meeting LLMs: Towards retrieval-augmented large lan-guage models”. Proc. SIGKDD . New York: ACM, 2024, 6491–6501. DOI :

10/g6ssz2 .[FGA*25] FANOUS , A ARON , G OLDBERG , J ACOB , A GARWAL , A NK , et al. “SycEval: Evaluating LLM sycophancy”. Proc. AIESc . Vol. 8. 1. 2025, 893–900. DOI : 10/hbjqpz .[Fri08] F RIENDLY , M ICHAEL . “A brief history of data visualization”. 

Handbook of Data Visualization . Ed. by C HEN , C HUN - HOUH , H ÄRDLE ,W OLFGANG , and U NWIN , A NTONY . Berlin: Springer, 2008. Chap. 2, 15– 56. DOI : 10/bsf8sz .[FWP*24] F ENG , Y INGCHAOJIE , W ANG , X INGBO , P AN , B O , et al. “XNLI: Explaining and diagnosing NLI-based visual data analysis”. IEEE Trans Vis Comput Graph 30.7 (2024), 3813–3827. DOI : 10/nmz8 .[ FXG*20] F U , S IWEI , X IONG , K AI , G E , X IAODONG , et al. Quda: Natu-ral language queries for visual data analytics . arXiv preprint 2005.03257. 2020. DOI : 10/hbjqn3 .[ GDW*24] G REENBLATT , R YAN , D ENISON , C ARSON , W RIGHT , B EN -JAMIN , et al. Alignment faking in large language models . arXiv preprint 2412.14093. 2024. DOI : 10/hbjqpv .[GGRG24] G EETHA , S., G, A DITYA , R EDDY M, C HETAN , and G, N IS -CHITH . “Human interaction in virtual and mixed reality through hand tracking”. Los Alamitos: IEEE CS, 2024, art. 1571021523, 6 pp. DOI :

10/hbcfgr .[ GKQ*25] G HAFARI , S EYED M OHSSEN , K OL , R ONNY , Q UIROZ , J UAN C., et al. ConCISE: A reference-free conciseness evaluation metric for LLM-generated answers . arXiv preprint 2511.16846. 2025. DOI : 10 / hbjqp5 .[GKWK24] G ORNIAK , J OSHUA , K IM , Y OON , W EI , D ONGLAI , and K IM ,N AM W OOK . “VizAbility: Enhancing chart accessibility with LLM-based conversational interaction”. New York: ACM, 2024, art. 89, 19 pp. DOI :

10/g8nwhp .[GLS*25] G AO , L IN , L U , J ING , S HAO , Z EKAI , et al. “Fine-tuned large lan-guage model for visualization system: A study on self-regulated learning in education”. IEEE Trans Vis Comput Graph 31.1 (2025), 514–524. DOI :

10/g8tfdm .[ GOWY24] G UAN , Q INGHUA , O UYANG , J INHUI , W U , D I , and Y U ,W EIREN . CityGPT: Towards urban IoT learning, analysis and interaction with multi-agent system . arXiv preprint 2405.14691. 2024. DOI : 10 / g8qcds .[GWA05] G RIMSTEAD , I.J., W ALKER , D.W., and A VIS , N.J. “Collabora-tive visualization: A review and taxonomy”. Proc. DS-RT . Los Alamitos: IEEE Computer Society, 2005, 61–69. DOI : 10/drqwq3 .[HAC25] H USEIN , R ASHA A HMAD , A BURAJOUH , H ALA , and C ATAL ,C AGATAY . “Large language models for code completion: A systematic literature review”. Comput Stand Interfaces 92 (2025), art. 103917, 15 pp. DOI : 10/hbjqpx .[Har18] H AROZ , S TEVE . “Open practices in visualization research”. Proc. BELIV . Los Alamitos: IEEE CS, 2018, 46–52. DOI : 10/gtw4sp .[HDZ*23] H UANG , F., D ENG , Y., Z HANG , C., et al. “KOSA: KO enhanced salary analytics based on knowledge graph and LLM capabilities”. Los Alamitos: IEEE CS, 2023, 499–505. DOI : 10/g8nwgq .[HHYG24] H UANG , J IAYANG , H UANG , Y UE , Y IP , D AVID , and G ULJA -JEVA , V ARVARA . “Ephemera: Language as a virus – AI-driven interactive and immersive art installation”. Proc ACM Comput Graph Interact Tech 

7.4 (2024), art. 62, 8 pp. DOI : 10/g9qrzt .[HIDI23] H E , T INGYING , I SENBERG , P ETRA , D ACHSELT , R AIMUND ,and I SENBERG , T OBIAS . “BeauVis: A validated scale for measuring the aesthetic pleasure of visual representations”. IEEE Trans Vis Comput Graph 29.1 (2023), 363–373. DOI : 10/kt3n .[HJSM16] H ÖÖK , K RISTINA , J ONSSON , M ARTIN P., S TÅHL , A NNA , and M ERCURIO , J OHANNA . “Somaesthetic appreciation design”. Proc. CHI .New York: ACM, 2016, 3131–3142. DOI : 10/f3rspg .[HMK*20] H ALLADJIAN , S ARKIS , M IAO , H AICHAO , K OU ˇRIL , D AVID ,et al. “Scale Trotter: Illustrative visual travels across negative scales”. 

IEEE Trans Vis Comput Graph 26.1 (2020), 654–664. DOI : 10/kt3k .[HS88] H ART , S ANDRA G. and S TAVELAND , L OWELL E. “Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research”. Adv Psychol 52 (1988), 139–183. DOI : 10/fbms3r .[HVM*24] H ARINEE , S., V IMAL R AJA , R., M UGILA , E., et al. “Elevat-ing medical training: A synergistic fusion of AI and VR for immersive anatomy learning and practical procedure mastery”. Proc. ICSCAN . Los Alamitos: IEEE CS, 2024, 5 pages. DOI : 10/hbcfgs .[HWKK23] H UANG , Z., W ITSCHARD , D., K UCHER , K., and K ERREN ,A. “VA + embeddings STAR: A state-of-the-art report on the use of embeddings in visual analytics”. Comput Graph Forum 42.3 (2023), 539– 571. DOI : 10/gsd7k8 .[HYZ*25] H ONG , Z IJIN , Y UAN , Z HENG , Z HANG , Q INGGANG , et al. “Next-generation database interfaces: A survey of LLM-based text-to-SQL”. IEEE Trans Knowledge Data Eng 37.12 (2025), 7328–7345. DOI :

10/hbjqnp .© 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 24 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

[HZC*23] H ONG , Y INING , Z HEN , H AOYU , C HEN , P EIHAO , et al. “3D-LLM: Injecting the 3D world into large language models”. Proc. NIPS .Red Hook: Curran Associates Inc., 2023, 20482–20494. URL : https: //proceedings.neurips.cc/paper_files/paper/2023/file/ 413885e70482b95dcbeeddc1daf39177-Paper-Conference.pdf .[IIJ*13] I SENBERG , T OBIAS , I SENBERG , P ETRA , J IAN C HEN , et al. “A systematic review on the practice of evaluating visualization”. IEEE Trans Vis Comput Graph 19.12 (2013), 2818–2827. DOI : 10/f5h29z .[Ise14] I SENBERG , T OBIAS . “An interaction continuum for visualization”. 

Proc. VIS Workshop “Death of the Desktop” . 2014. URL : https://hal. science/hal-01095454 .[Ise24] I SENBERG , T OBIAS . “The state of reproducibility stamps for visual-ization research papers”. Proc. BELIV . Los Alamitos: IEEE CS, 2024, 97– 105. DOI : 10/nt3t .[JA24] J IN , S ICHEN and A BHYANKAR , S HRIRANG . “ChatGrid: Power grid visualization empowered by a large language model”. Proc. EnergyVis .Los Alamitos: IEEE CS, 2024, 12–17. DOI : 10/hbcfgt .[Jah12] JAHNCKE , H ELENA . “Open-plan of fi ce noise: The susceptibility and suitability of different cognitive tasks for work in the presence of ir-relevant speech”. Noise Health 14.61 (2012), 315–320. DOI : 10/f232zz .[JGL*24] JAVAHERI , H AMRAZ , G HAMARNEJAD , O MID , L UKOWICZ ,PAUL , et al. “ARAS: LLM-supported augmented reality assistance system for pancreatic surgery”. Proc. UbiComp . New York: ACM, 2024, 176– 180. DOI : 10/g8qpdd .[JIB*25] J IA , D ONGGANG , I RGER , A LEXANDRA , B ESANÇON , L ONNI , et al. “VOICE: Visual oracle for interaction, conversation, and explanation”. 

IEEE Trans Vis Comput Graph 31.10 (2025), 8828–8845. DOI : 10 / g9r73s .[JLF*23] J I , Z IWEI , L EE , N AYEON , F RIESKE , R ITA , et al. “Survey of hallucination in natural language generation”. ACM Comput Surv 55.12 (2023), art. 248, 38 pp. DOI : 10/grvgw5 .[JRDX23] J IANG , P., R AYAN , J., D OW , S.P., and X IA , H. “Graphologue: Exploring large language model responses with interactive diagrams”. 

Proc. UIST . New York: ACM, 2023, art. 3, 20 pp. DOI : 10/gs3pvf .[ JXL*25] J IN , C HUANYANG , X U , J ING , L IU , B O , et al. The era of real-world human interaction: RL from user conversations . arXiv preprint 2509.25137. 2025. DOI : 10/qmtt .[KAMB25] K IM , N AM W OOK , A HN , Y ONGSU , M YERS , G RACE , and B ACH , B ENJAMIN . “How good is ChatGPT in giving advice on your visualization design?”: ACM Trans Comput-Hum Interact 32.5 (2025), art. 46, 33 pp. DOI : 10/g94xx4 .[ KBSL23] K OUTS , A RI , B ESANÇON , L ONNI , S EDLMAIR , M ICHAEL ,and L EE , B ENJAMIN . LSDvis: Hallucinatory data visualisations in real world environments . arXiv preprint 2312.11144. 2023. DOI : 10/hbjqpd .[KCM25] K HAN , S AADIQ R AUF , C HANDAK , V INIT , and M UKHERJEA ,S OUGATA . “Evaluating LLMs for visualization generation and under-standing”. Discov Data 3.1 (2025), art. 15, 26 pp. DOI : 10/hbjqnn .[KDAH19] K IM , Y EA -S EUL , D ONTCHEVA , M IRA , A DAR , E YTAN , and H ULLMAN , J ESSICA . “Vocal shortcuts for creative experts”. Proc. CHI .New York: ACM, 2019, art. 332, 14 pp. DOI : 10/gk9mqk .[KDPB24] K OSTIC , Z ONA , D UMAS , C ATHERINE , P RATT , S ARAH , and B EYER , J OHANNA . “Exploring mid-air hand interaction in data visual-ization”. IEEE Trans Vis Comput Graph 30.9 (2024), 6347–6364. DOI :

10/g8858b .[ KEBB25] K AUFMAN , Z HANNA , E NDRES , M ADELINE , B EARFIELD ,C INDY X IONG , and B RUN , Y URIY . Your model is unfair, are you even aware? Inverse relationship between comprehension and trust in explain-ability visualizations of biased ML models . arXiv preprint 2508.00140. 2025. DOI : 10/hbjqn7 .[KHC23] K AMATH , A MITA , H ESSEL , J ACK , and C HANG , K AI -W EI .“What’s “up” with vision-language models? Investigating their struggle with spatial reasoning”. Proc. EMNLP . Stroudsburg: ACL, 2023, 9161– 9175. DOI : 10/hbjqnv .[KI13] K EEFE , D ANIEL F. and I SENBERG , T OBIAS . “Reimagining the scienti fi c visualization interaction paradigm”. Comput 46.5 (2013), 51– 57. DOI : 10/kt4j .[KLH*23] K IM , J OONG H OON , L EE , S ANGMIN , H UN H AN , S EUNG , et al. “Which is better? Exploring prompting strategy for LLM-based metrics”. 

Proc. Eval4NLP . Stroudsburg: ACL, 2023, 164–183. DOI : 10/hbjqpn .[KLL*24] K IM , H YUNWOO , L E , K HANH D UY , L IM , G IONNIEVE , et al. “DataDive: Supporting readers’ contextualization of statistical statements with data exploration”. Proc. IUI . New York: ACM, 2024, 623–639. DOI :

10/gtt6qm .[KLSC21] K IM , Y OUNG -H O , L EE , B ONGSHIN , S RINIVASAN , A RJUN ,and C HOE , E UN K YOUNG . “Data@Hand: Fostering visual exploration of personal data on smartphones leveraging speech and touch interaction”. 

Proc. CHI . New York: ACM, 2021, art. 462, 17 pp. DOI : 10/gksk7k .[ KMA*17] K AHOU , S AMIRA E BRAHIMI , M ICHALSKI , V INCENT ,ATKINSON , A DAM , et al. FigureQA: An annotated fi gure dataset for visual reasoning . arXiv preprint 1710.07300. 2017. DOI : 10/mcr7 .[KMS*25] K ARANIKOLAS , N IKITAS N., M ANGA , E IRINI , S AMARIDI ,N IKOLETTA , et al. “Strengths and weaknesses of LLM-based and rule-based NLP technologies and their potential synergies”. Electron 14.15 (2025), art. 3064, 34 pp. DOI : 10/hbjqp3 .[KO17] K AUR , P AWANDEEP and O WONIBI , M ICHAEL . “A review on visualization recommendation strategies”. Proc. VISIGRAPP . Porto: SCITEPRESS, 2017, 266–273. DOI : 10/nm2h .[ KPCK18] K AFLE , K USHAL , P RICE , B RIAN , C OHEN , S COTT , and K ANAN , C HRISTOPHER . DVQA: Understanding data visualizations via question answering . arXiv preprint 1801.08163. 2018. DOI : 10/hbjqn5 .[KPR25] K OONCHANOK , R ATANOND , P APKA , M ICHAEL E., and R EDA ,K HAIRI . “Trust your gut: Comparing human and machine inference from noisy visualizations”. IEEE Trans Vis Comput Graph 31.1 (2025), 754– 764. DOI : 10/hbjqn6 .[KSR*22] K O ´ C -J ANUCHTA , M ARTA M, S CHÖNBORN , K ONRAD J, R OEHRIG , C ASEY , et al. ““Connecting concepts helps put main ideas together”: Cognitive load and usability in learning biology with an AI-enriched textbook”. Int J Educ Technol Higher Educ 19.1 (2022), art. 11, 22 pp. DOI : 10/gpkksr .[ KUK*25] K UMAR , C HARAKA V INAYAK , U RLANA , A SHOK , K ANU -MOLU , G OPICHAND , et al. No LLM is free from bias: A comprehen-sive study of bias evaluation in large language models . arXiv preprint 2503.11985. 2025. DOI : 10/qmtw .[LBGI25] L EÓN , G ABRIELA M OLINA , B EZERIANOS , A NASTASIA ,G LADIN , O LIVIER , and I SENBERG , P ETRA . “Talk to the wall: The role of speech interaction in collaborative visual analytics”. IEEE Trans Vis Com-put Graph 31.1 (2025), 941–951. ISSN : 1077-2626. DOI : 10/g88nds .[LBI*12] L AM , H EIDI , B ERTINI , E NRICO , I SENBERG , P ETRA , et al. “Em-pirical studies in information visualization: Seven scenarios”. IEEE Trans Vis Comput Graph 18.9 (2012), 1520–1536. DOI : 10/drrh6j .[LCI*20] L EE , B ONGSHIN , C HOE , E UN K YOUNG , I SENBERG , P ETRA ,et al. “Reaching broader audiences with data visualization”. IEEE Comput Graph Appl 40.2 (2020), 82–90. DOI : 10/gpbhhm .© 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 25 of 28 [LCO20] L IU , Z HENGLIANG , C ROUSER , R. J ORDAN , and O TTLEY ,A LVITTA . “Survey on individual differences in visualization”. Comput Graph Forum 39.3 (2020), 693–712. DOI : 10/gg6cr5 .[LHM*23] L IU , Y IHENG , H AN , T IANLE , M A , S IYUAN , et al. “Summary of ChatGPT-related research and perspective towards the future of large language models”. Meta-Radiol 1.2 (2023), art. 100017, 14 pp. DOI :

10/gtv4kr .[ LHS*] L UO , T IANQI , H UANG , C HUHAN , S HEN , L EIXIAN , et al. 

nvBench 2.0: Resolving ambiguity in text-to-visualization through step-wise reasoning . arXiv preprint 2503.12880. DOI : 10/qmtz .[LIB23] L EÓN , G ABRIELA M OLINA , I SENBERG , P ETRA , and B REITER ,A NDREAS . “Talking to data visualizations: Opportunities and challenges”. 

Proc. MERCADO . 2023. DOI : 10/qmt2 .[LIX*23] L IU , Y ANG , I TER , D AN , X U , Y ICHONG , et al. “G-Eval: NLG evaluation using GPT-4 with better human alignment”. Proc. EMNLP .Stroudsburg: ACL, 2023, 2511–2522. DOI : 10/hbjqpm .[LKL*25] L I , H AOBO , K AM -K WAI , W ONG , L UO , Y AN , et al. “Save it for the “hot” day: An LLM-empowered visual analytics system for heat risk management”. IEEE Trans Vis Comput Graph 31.10 (2025), 8928–8943. DOI : 10/p86v .[LLT*25] L IU , R UYANG , L I , C HEN , T ANG , H AORAN , et al. “ST-LLM: Large language models are effective temporal learners”. Proc. ECCV .Cham: Springer, 2025, 1–18. DOI : 10/hbjqqd .[LLY*24] L IN , Y ANNA , L I , H AOTIAN , Y ANG , L ENI , et al. “InkSight: Leveraging sketch interaction for documenting chart fi ndings in com-putational notebooks”. IEEE Trans Vis Comput Graph 30.1 (2024), 944– 954. DOI : 10/g6r5xv .[LM25] L EE , S AM Y U -T E and M A , K WAN -L IU . “HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents”. IEEE Trans Vis Comput Graph 31.9 (2025), 5532– 5546. DOI : 10/g8nwg2 .[LML*23] L IU , S HUSEN , M IAO , H AICHAO , L I , Z HIMIN , et al. “AVA: To-wards autonomous visualization agents through visual perception-driven decision-making”. Comput Graph Forum 43.3 (2023), art. e15093, 12 pp. DOI : 10/nmzw .[LMS*25] L IU , T ONG , M C I NTOSH , T IMOTHY R., S USNJAK , T EO , et al. “Modeling the Chaotic Semantic States of Generative Arti fi cial Intelli-gence (AI): A Quantum Mechanics Analogy Approach”. ACM Trans Intell Syst Technol 16.6 (2025), art. 126, 36 pp. DOI : 10/hbjqpf .[LN23] L I , Z HENLONG and N ING , H UAN . “Autonomous GIS: The next-generation AI-powered GIS”. Int J Digital Earth 16.2 (2023), 4668–4686. DOI : 10/g83jcf .[ LNS*25] L ONG , D O X UAN , N GOC , H AI N GUYEN , S IM , T IVIATIS , et al. 

LLMs are biased towards output formats! Systematically evaluating and mitigating output format bias of LLMs . arXiv preprint 2408.08656. 2025. DOI : 10/hbjqqc .[LPP*06] L EE , B ONGSHIN , P LAISANT , C ATHERINE , P ARR , C YNTHIA S IMS , et al. “Task taxonomy for graph visualization”. Proc. BELIV .Venice Italy: ACM, 2006, art. 14, 5 pp. DOI : 10/b2pm7w .[ LPP*21] L EWIS , P ATRICK , P EREZ , E THAN , P IKTUS , A LEKSANDRA ,et al. Retrieval-augmented generation for knowledge-intensive NLP tasks .arXiv preprint 2005.11401. 2021. DOI : 10/gtj4jn .[LQ24] L O , L EO Y U -H O and Q U , H UAMIN . “How good (or bad) are LLMs at detecting misleading visualizations?”: IEEE Trans Vis Comput Graph 

31.1 (2024), 1116–1125. DOI : 10/hbjqn8 .[LTL*21] L UO , Y UYU , T ANG , N AN , L I , G UOLIANG , et al. “Synthesizing natural language to visualization (NL2VIS) benchmarks from NL2SQL benchmarks”. Proc. SIGMOD . New York: ACM, 2021, 1235–1247. DOI :

10/nm2k .[Lu26] L U , J ANNA . “Tacit knowledge in large language models”. Rev Aus-trian Econ (2026). DOI : 10/hbjqp2 . To appear. [LWX*25] L IU , M ENGYI , W ANG , X IEYANG , X U , J IANQIU , et al. “A sys-tematic review of natural language interfaces for databases”. Front Com-put Sci 20.11 (2025), art. 2011623, 27 pp. DOI : 10.1007/s11704-025-50592-w .[LXH*25] L IU , Z IAO , X IE , X IAO , H E , M OQI , et al. “Smartboard: Visual exploration of team tactics with LLM agent”. IEEE Trans Vis Comput Graph 31.1 (2025), 23–33. DOI : 10/g9qr26 .[LYZ*24] L IANG , P AN , Y E , D ANWEI , Z HU , Z IHAO , et al. “C 5 : Toward bet-ter conversation comprehension and contextual continuity for ChatGPT”. 

J Vis 27.4 (2024), 713–730. DOI : 10/g8nwhz .[McC16] M C C ALL , C ADE . “Mapping social interactions: The science of proxemics”. Social Behavior from Rodents to Humans: Neural Founda-tions and Clinical Implications . Ed. by W ÖHR , M ARKUS and K RACH ,S ÖREN . Cham: Springer, 2016, 295–308. DOI : 10/gg7wcm .[MGAN18] M ERINO , L., G HAFARI , M., A NSLOW , C., and N IERSTRASZ ,O. “A systematic literature review of software visualization evaluation”. 

J Syst Softw 144 (2018), 165–180. DOI : 10/ggfv5c .[ MGKK] M ETHANI , N ITESH , G ANGULY , P RITHA , K HAPRA , M ITESH M., and K UMAR , P RATYUSH . PlotQA: Reasoning over scienti fi c plots .arXiv preprint 1909.00997. DOI : 10/qmt5 .[MHWH17] M EWES , A NDRÉ , H ENSEN , B ENNET , W ACKER , F RANK , and H ANSEN , C HRISTIAN . “Touchless interaction with software in inter-ventional radiology and surgery: A systematic literature review”. Int J Comput Assist Radiol Surg 12.2 (2017), 291–305. DOI : 10/f9rwmn .[MJB*23] M ATHIAK , B RIGITTE , J UTY , N ICK , B ARDI , A LESSIA , et al. “What are researchers’ needs in data discovery? Analysis and ranking of a large-scale collection of crowdsourced use cases”. Data Sci J 22.1 (2023), art. 3, 8 pp. DOI : 10/hbjqnw .[ MKB*25a] M ENA , O MAR , K OUYOUMDJIAN , A LEXANDRE , B E -SANÇON , L ONNI , et al. Augmenting a large language model with a combination of text and visual data for conversational visualization of global geospatial data . arXiv preprint 2501.09521. 2025. DOI : 10/pcxd .[MKB*25b] M ENA , O MAR , K OUYOUMDJIAN , A LEXANDRE , B ESANÇON ,L ONNI , et al. “Enabling visually aware conversational data visualization through LLM augmentation”. SIGGRAPH Asia Posters . New York: ACM, 2025, art. 43, 2 pp. DOI : 10/qmt7 .[MMA*25] M ANZONI , M., M ASCETTI , S., A HMETOVIC , D., et al. “Ma-pIO: A gestural and conversational interface for tactile maps”. IEEE Access 13 (2025), 84038–84056. DOI : 10/g94xwm .[MMCV24] M ASSON , D AMIEN , M ALACRIA , S YLVAIN , C ASIEZ , G ÉRY ,and V OGEL , D ANIEL . “DirectGPT: A direct manipulation interface to interact with large language models”. Proc. CHI . New York: ACM, 2024, art. 975, 16 pp. DOI : 10/hbjqpt .[MST*25] M ASSA , R ICKY , S HAIK , A AQEL , T A , B RIAN , et al. “GENIUS: AI powered assistant for scienti fi c research”. Proc. eSci . Los Alamitos: IEEE CS, 2025, 347–348. DOI : 10/hbcfgv .[MTCC17] M C D UFF , D ANIEL , T HOMAS , P AUL , C ZERWINSKI , M ARY ,and C RASWELL , N ICK . “Multimodal analysis of vocal collaborative search: A public corpus and results”. Proc. ICMI . New York: ACM, 2017, 456–463. DOI : 10/qmwj .© 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 26 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

[Mun09] M UNZNER , T AMARA . “A nested model for visualization design and validation”. IEEE Trans Vis Comput Graph 15.6 (2009), 921–928. DOI : 10/dzqbc2 .[NMD*24] N ATH , A NINDITA , M WESIGWA , S AVANNAH , D AI , Y ULIN , et al. “GENEVIC: Genetic data exploration and visualization via intelligent interactive console”. Bioinf 40.10 (2024), art. btae500, 5 pp. DOI : 10/ nmzt .[ OAA*24] O PEN AI, A CHIAM , J OSH , A DLER , S TEVEN , et al. GPT-4 technical report . arXiv preprint 2303.08774. 2024. DOI : 10/grx4cb .[OQS*20] O ZCAN , F ATMA , Q UAMAR , A BDUL , S EN , J AYDEEP , et al. “State of the art and open challenges in natural language interfaces to data”. 

Proc. SIGMOD . New York: ACM, 2020, 2629–2636. DOI : 10/gmdkgf .[Ovi02] O VIATT , S HARON . “Multimodal interfaces”. The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies and Emerging Applications . Ed. by J ACKO , J ULIE A. USA: L. Erlbaum Associates Inc., 2002. Chap. 18, 286–304. DOI : 10/qmt9 .[PCB*23] P OORYOUSEF , VAHID , C ORDEIL , M AXIME , B ESANÇON ,L ONNI , et al. “Working with forensic practitioners to understand the opportunities and challenges for mixed-reality digital autopsy”. Proc. CHI . New York: ACM, 2023, art. 843, 15 pp. DOI : 10/gsw9z9 .[PCB*24] P OORYOUSEF , VAHID , C ORDEIL , M AXIME , B ESANÇON ,L ONNI , et al. “Collaborative forensic autopsy documentation and su-pervised report generation using a hybrid mixed-reality environment and generative AI”. IEEE Trans Vis Comput Graph 30.11 (2024), 7452–7462. DOI : 10/g8p63q .[ PCR*24] P IMENTEL , M ARCO AF, C HRISTOPHE , C LÉMENT , R AHA ,TATHAGATA , et al. Beyond metrics: A critical analysis of the variability in large language model evaluation frameworks . arXiv preprint 2407.21072. 2024. DOI : 10/qmvb .[PGO*24] PARK , P ETER S., G OLDSTEIN , S IMON , O’G ARA , A IDAN , et al. “AI deception: A survey of examples, risks, and potential solutions”. 

Patterns 5.5 (2024), art. 100988, 16 pp. DOI : 10/gtvhdw .[PMB*21] PAGE , M ATTHEW J., M C K ENZIE , J OANNE E., B OSSUYT ,PATRICK M., et al. “The PRISMA 2020 statement: An updated guideline for reporting systematic reviews”. Syst Rev 10.1 (2021), art. 89, 11 pp. DOI : 10/gjm5rn .[PO25] PANDEY , S AUGAT and O TTLEY , A LVITTA . “Benchmarking visual language models on standardized visualization literacy tests”. Comput Graph Forum 44.3 (2025), art. e70137, 12 pp. DOI : 10/qmvc .[PPV25] P ODO , L UCA , P RENKAJ , B ARDH , and V ELARDI , P AOLA . “Ag-nostic visual recommendation systems: Open challenges and future di-rections”. IEEE Trans Vis Comput Graph 31.3 (2025), 1902–1917. DOI :

10/nm2f .[PRI02] P IPER , B EN , R ATTI , C ARLO , and I SHII , H IROSHI . “Illuminating clay: A 3-D tangible interface for landscape analysis”. Proc. CHI . New York: ACM, 2002, 355–362. DOI : 10/fdbzs9 .[PV] P ODO , L UCA and V ELARDI , P AOLA . “Plotly.plus, an improved dataset for visualization recommendation”. Proc. CIKM . New York: ACM, 4384–4388. DOI : 10/hbjqn4 .[ PWL*24] PACKER , C HARLES , W OODERS , S ARAH , L IN , K EVIN , et al. MemGPT: Towards LLMs as operating systems . arXiv preprint 2310.08560. 2024. DOI : 10/qmvd .[PZL*25] P ENG , B OCI , Z HU , Y UN , L IU , Y ONGCHAO , et al. “Graph retrieval-augmented generation: A survey”. ACM Trans Inf Syst 44.2 (2025), art. 35, 52 pp. DOI : 10/hbjqnr .[QWH*25] Q IU , T IAN , W ANG , F EN , H UANG , S HAOHUA , et al. “SmartM-LVs: LLM-enabled multiple linked views generation for interactive visu-alization”. Los Alamitos: IEEE CS, 2025, 58–68. DOI : 10/hbcfgw .[ RAN*23] R UBENSTEIN , P AUL K., A SAWAROENGCHAI , C HULAYUTH ,N GUYEN , D UC D UNG , et al. AudioPaLM: A large language model that can speak and listen . arXiv preprint 2306.12925. 2023. DOI : 10/hbjqpw .[RAW*16] R IND , A LEXANDER , A IGNER , W OLFGANG , WAGNER ,M ARKUS , et al. “Task Cube: A three-dimensional conceptual space of user tasks in visualization design and evaluation”. Inf Vis 15.4 (2016), 288– 300. DOI : 10/f3szvq .[RCDD13] R EN , L EI , C UI , J IN , D U , Y I , and D AI , G UOZHONG . “Multilevel interaction model for hierarchical tasks in information visualization”. 

Proc. VINCI . New York: ACM, 2013, 11–16. DOI : 10/hbjqpj .[RD22] R OY , D EEPJYOTI and D UTTA , M ALA . “A systematic review and research perspective on recommender systems”. J Big Data 9.1 (2022), art. 59, 36 pp. DOI : 10/g9pg4k .[RKO*20] R HEINGANS , P ENNY , K OSTIS , H ELEN -N ICOLE , O EMIG ,PAULO A., et al. “Reaching broad audiences in an educational set-ting”. Foundations of Data Visualization . Ed. by C HEN , M IN , H AUSER ,H ELWIG , R HEINGANS , P ENNY , and S CHEUERMANN , G ERIK . Cham: Springer, 2020. Chap. 20, 365–380. DOI : 10/hbjqn2 .[ RKPA23] R ODRIGUEZ , F ABIO J. C ORTES , K RAPP , L UCIEN F., P ER -ARO , M ATTEO D AL , and A BRIATA , L UCIANO A. HandMol: Coupling WebXR, AI and HCI technologies for immersive, natural, collaborative and inclusive molecular modeling . bioRxiv preprint 2023.11.24.568613. 2023. DOI : 10/nmzs .[ RLB22] R AJKUMAR , N ITARSHAN , L I , R AYMOND , and B AHDANAU ,D ZMITRY . Evaluating the text-to-SQL capabilities of large language models . arXiv preprint 2204.00498. 2022. DOI : 10/gtdg59 .[ SCTS24] S CLAR , M ELANIE , C HOI , Y EJIN , T SVETKOV , Y ULIA , and S UHR , A LANE . Quantifying language models’ sensitivity to spurious features in prompt design or: How I learned to start worrying about prompt formatting . arXiv preprint 2310.11324. 2024. DOI : 10/hbjqqb .[SD14] S ANTOS , B EATRIZ S OUSA and D IAS , P AULO . “Evaluation in vi-sualization: Some issues and best practices”. Visualization and Data Analysis 2014 . Vol. 9017. SPIE, 2014. DOI : 10/hbjqpp .[SEG*21] S PERRLE , F., E L -A SSADY , M., G UO , G., et al. “A survey of human-centered evaluations in human-centered machine learning”. Com-put Graph Forum 40.3 (2021), 543–568. DOI : 10/gmbdx9 .[SGC25] S INGH , S ONAL , G UPTA , YADUNATH , and C HOWDHURY ,S OUDIP R OY . “MEQA – A multi-modal interactive enterprise query answering system using multi-agent LLM”. Proc. COMAD . New York: ACM, 2025, 422–426. DOI : 10/g94xzf .[SHE25] S HIN , S UNGBOK , H ONG , S ANGHYUN , and E LMQVIST , N IKLAS .“Visualizationary: Automating design feedback for visualization designers using LLMs”. IEEE Trans Vis Comput Graph 31.10 (2025), 8796–8813. DOI : 10/p86s .[Shn96] S HNEIDERMAN , B. “The eyes have it: A task by data type taxon-omy for information visualizations”. Proc. VL . Los Alamitos: IEEE CS, 1996, 336–343. DOI : 10/fwdq26 .[SHP16] S CHÖNBORN , K ONRAD J., H ÖST , G UNNAR E., and P ALMERIUS ,K ARLJOHAN E. L UNDIN . “Nano education with interactive visualiza-tion”. Nano Today 11.5 (2016), 543–546. DOI : 10/f3r264 .[ SISS25] S IDDIQ , M OHAMMED L ATIF , I SLAM -G OMES , A RVIN , S EK -ERAK , N ATALIE , and S ANTOS , J OANNA . Large language models for software engineering: A reproducibility crisis . arXiv preprint 2512.00651. 2025. DOI : 10/qmvf .[SLW*25] S HEN , L EIXIAN , L I , H AOTIAN , W ANG , Y IFANG , et al. “Prompt-ing generative AI with interaction-augmented instructions”. CHI Extended Abstracts . New York: ACM, 2025, art. 451, 9 pp. DOI : 10/hbjqpr .© 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 27 of 28 [SMKS24] S HIH , J ASMINE Y., M OHANTY , V ISHAL , K ATSIS , Y ANNIS ,and S UBRAMONYAM , H ARI . “Leveraging large language models to en-hance domain expert inclusion in data science work fl ows”. New York: ACM, 2024, art. 223, 11 pp. DOI : 10/g8nwh4 .[ SMN*24] S AH , S UBHAM , M ITRA , R ISHAB , N ARECHANIA , A RPIT ,et al. Generating analytic speci fi cations for data visualization from natural language queries using large language models . arXiv preprint 2408.13391. 2024. DOI : 10/g8nwg5 .[SMPX23] S UH , S ANGHO , M IN , B RYAN , P ALANI , S RISHTI , and X IA ,H AIJUN . “Sensecape: Enabling multilevel exploration and sensemaking with large language models”. New York: ACM, 2023, art. 1, 18 pp. DOI :

10/gs3pvd .[SMWH17] S ATYANARAYAN , A RVIND , M ORITZ , D OMINIK , W ONG -SUPHASAWAT , K ANIT , and H EER , J EFFREY . “Vega-Lite: A grammar of interactive graphics”. IEEE Trans Vis Comput Graph 23.1 (2017), 341– 350. DOI : 10/f92f32 .[SNHS13] S CHULZ , H ANS -J ÖRG , N OCKE , T HOMAS , H EITZLER , M AG -NUS , and S CHUMANN , H EIDRUN . “A design space of visualization tasks”. IEEE Trans Vis Comput Graph 19.12 (2013), 2366–2375. DOI :

10/f5h3mb .[SNL*21] S RINIVASAN , A RJUN , N YAPATHY , N IKHILA , L EE , B ONGSHIN ,et al. “Collecting and characterizing natural language utterances for spec-ifying data visualizations”. Proc. CHI . New York: ACM, 2021, art. 464, 10 pp. DOI : 10/gksk6s .[SS17] S RINIVASAN , A RJUN and S TASKO , J OHN . “Natural language inter-faces for data analysis with visualization: Considering what has and could be asked”. EuroVis Short Papers . Goslar, DEU: Eurographics Association, 2017, 55–59. DOI : 10/gh38sw .[SSCS24] S TOKES , C HASE , S ANKER , C HELSEA , C OGLEY , B RIDGET , and S ETLUR , V IDYA . “Voicing uncertainty: How speech, text, and visualiza-tions in fl uence decisions with data uncertainty”. Proc. IEEE Workshop on Uncertainty Visualization . Los Alamitos: IEEE CS, 2024, 17–27. DOI :

10/qmwh .[SSKT20] S RINIVASAN , A RJUN , S TASKO , J OHN , K EEFE , D ANIEL F., and T ORY , M ELANIE . “How to ask what to say?: Strategies for evaluating natural language interfaces for data visualization”. IEEE Comput Graph Appl 40.4 (2020), 96–103. DOI : 10/gh59fr .[SSL*23] S HEN , L EIXIAN , S HEN , E NYA , L UO , Y UYU , et al. “Towards natural language interfaces for data visualization: A survey”. IEEE Trans Vis Comput Graph 29.6 (2023), 3121–3144. DOI : 10/gr2vzx .[SSS*14] S ACHA , D OMINIK , S TOFFEL , A NDREAS , S TOFFEL , F LORIAN ,et al. “Knowledge generation model for visual analytics”. IEEE Trans Vis Comput Graph 20.12 (2014), 1604–1613. DOI : 10/f6qj6x .[STZ*25] S HI , L IANG , T ANG , Z HENGJU , Z HANG , N AN , et al. “A survey on employing large language models for text-to-SQL tasks”. ACM Comput Surv 58 (2025), art. 54, 37 pp. DOI : 10/hbjqnz .[SWB*22] S ERENO , M ICKAEL , W ANG , X IYAO , B ESANÇON , L ONNI , et al. “Collaborative work in augmented reality: A survey”. IEEE Trans Vis Comput Graph 28.6 (2022), 2530–2549. DOI : 10/gjkq7w .[SWLL25] S ONG , Y IFAN , W ANG , G UOYIN , L I , S UJIAN , and L IN , B ILL Y UCHEN . “The good, the bad, and the greedy: Evaluation of LLMs should not ignore non-determinism”. Proc. NAACL . Kerrville: ACL, 2025, 4195– 4206. DOI : 10/hbjqph .[SXC*23] S UN , Y., X U , Y., C HENG , C., et al. “Explore the future earth with Wander 2.0: AI chatbot driven by knowledge-base story generation and text-to-image model”. Proc. CHI . New York: ACM, 2023, art. 450, 5 pp. DOI : 10/g8nwjm .[SYC*21] S U , C HENGYU , Y ANG , C HAO , C HEN , Y ONGHUI , et al. “Natu-ral multimodal interaction in immersive fl ow visualization”. Vis Inf 5.4 (2021), 56–66. DOI : 10/gp54tf .[TBS23] TANG , B ENNY , B OGGUST , A NGIE , and S ATYANARAYAN ,A RVIND . “VisText: A benchmark for semantically rich chart caption-ing”. Proc. ACL . Vol. 1. Toronto: ACL, 2023, 7268–7298. DOI : 10/kz35 .[TCD*25] T IAN , Y UAN , C UI , W EIWEI , D ENG , D AZHEN , et al. “Chart-GPT: Leveraging LLMs to generate charts from abstract natural lan-guage”. IEEE Trans Vis Comput Graph 31.3 (2025), 1731–1745. DOI :

10/g8qccg .[TLT*25] TABALBA Roderick S, J R ., L EE , C HRISTOPHER J., T RAN ,G IORGIO , et al. “A pragmatics-based approach to proactive digital assis-tants for data exploration”. New York: ACM, 2025, art. 69, 14 pp. DOI :

10/p9c2 .[ TMS*23] T OUVRON , H UGO , M ARTIN , L OUIS , S TONE , K EVIN , et al. 

Llama 2: Open foundation and fi ne-tuned chat models . arXiv preprint 2307.09288. 2023. DOI : 10/ktkj .[TRP26] T HOMAS , L LEWELLYN DW, R OMASANTA , A NGELO K ENNETH G., and P RIEGO , L AIA P UJOL . “Jagged competencies: Measuring the reliability of generative AI in academic research”. J Bus Res 203 (2026), art. 115804, 14 pp. DOI : 10/hbjqpg .[Tuk77] T UKEY , J OHN . Exploratory Data Analysis . Reading, Mass.: Pear-son, 1977. URN : urn:oclc:record:1034668314 .[URQ*25] U HL , J. -F., R EYES , P., Q UIHUIS , C., et al. “XR for educational anatomy: An Immersive human atlas with chatbot”. Los Alamitos: IEEE CS, 2025, 1016–1020. DOI : 10/g94xwq .[VAM*22] V OIGT , H ENRIK , A LACAM , O ZGE , M EUSCHKE , M ONIQUE ,et al. “The why and the how: A survey on natural language interaction in visualization”. Proc. NAACL . Seattle: ACL, 2022, 348–374. DOI : 10/ gr549r .[VCM*23] V OIGT , H ENRIK , C ARVALHAIS , N UNO , M EUSCHKE ,M ONIQUE , et al. “VIST5: An adaptive, retrieval-augmented language model for visualization-oriented dialog”. Stroudsburg: ACL, 2023, 70–81. DOI : 10/g8p28w .[VHS*17] VARTAK , M ANASI , H UANG , S ILU , S IDDIQUI , T ARIQUE , et al. “Towards visualization recommendation systems”. ACM SIGMOD Rec 

45.4 (2017), 34–39. DOI : 10/gfxrs9 .[ VSP*17] VASWANI , A SHISH , S HAZEER , N OAM , P ARMAR , N IKI , et al. 

Attention is all you need . arXiv preprint 1706.03762. 2017. DOI : 10/ gpnmtv .[WBG*19] WANG , X IYAO , B ESANÇON , L ONNI , G UÉNIAT , F LORIMOND ,et al. “A vision of bringing immersive visualization to scienti fi c work-

fl ows”. Proc. CHI Workshop on Interaction Design & Prototyping for Immersive Analytics . 2019. URL : https : / / hal . science / hal -02053969 .[WCWQ22] WANG , Q IANWEN , C HEN , Z HUTIAN , W ANG , Y ONG , and Q U ,H UAMIN . “A survey on ML4VIS: Applying machine learning advances to data visualization”. IEEE Trans Vis Comput Graph 28.12 (2022), 5134– 5153. DOI : 10/gms3sb .[WG26] W U , R UOLING and G UO , D ANHUAI . “Do large language models have spatial cognitive abilities?”: ACM Trans Intell Syst Technol (2026). DOI : 10/hbjqnt . To appear. [WGFL25] WANG , M., G AO , Y., F ANG , M., and L YU , X. “ChatWeaver: Interactive knowledge graph for ef fi cient information retrieval and visual-ization empowered by LLM”. Los Alamitos: IEEE CS, 2025, 1978–1982. DOI : 10/g94xv9 .© 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd. 28 of 28 Brossier et al. / State of the Art of LLM-Enabled Interaction with Visualization 

[WGM*24] W U , Y IFAN , G UO , Z IYANG , M AMAKOS , M ICHALIS , et al. “The rational agent benchmark for data visualization”. IEEE Trans Vis Comput Graph 30.1 (2024), 338–347. DOI : 10/hbjqpb .[WHL*24] WANG , H AOYU , H U , G UOQIANG , L IN , G UODONG , et al. “Simul-Whisper: Attention-guided streaming whisper with truncation detection”. Proc. Interspeech . ISCA, 2024, 4483–4487. DOI : 10/hbjqqg .[WHT*25] WANG , H UICHEN W ILL , H OFFSWELL , J ANE , T HAZIN T HANE ,S AO M YAT , et al. “How aligned are human chart takeaways and LLM predictions? A case study on bar charts with varying layouts”. IEEE Trans Vis Comput Graph 31.1 (2025), 536–546. DOI : 10/n82w .[WHZ*24] WANG , Q UAN , H UANG , Y ILING , Z HAO , G UANLONG , et al. “DiarizationLM: Speaker diarization post-processing with large language models”. Proc. Interspeech . ISCA, 2024, 3754–3758. DOI : 10/qmv2 .[WJ25] W HEELER , S CHAUN and J EUNEN , O LIVIER . “Procedural memory is not all you need: Bridging cognitive gaps in LLM-based agents”. Ad-junct Proc. UMAP . New York: ACM, 2025, 360–364. DOI : 10/hbjqp9 .[ WLZ*25] WANG , X INYU , L IANG , C HENWEI , Z HENG , S HUNYUAN ,et al. Visualization generation with large language models: An evaluation .arXiv preprint 2401.11255. 2025. DOI : 10/m4fv .[WMW*25] WANG , Y UEQIAN , M ENG , X IAOJUN , W ANG , Y UXUAN , et al. “VideoLLM knows when to speak: Enhancing time-sensitive video comprehension with video-text duet interaction format”. Proc. EMNLP .Kerrville: ACL, 2025, 6338–6359. DOI : 10/hbjqqf .[WOG*22] W ILLIAMSON , J ULIE R., O’H AGAN , J OSEPH , G UERRA -G OMEZ , J OHN A LEXIS , et al. “Digital proxemics: Designing social and collaborative interaction in virtual environments”. Proc. CHI . New York: ACM, 2022, art. 423, 12 pp. DOI : 10/grp3kx .[Woo22] W OOD , J O . “Beyond the walled garden: A visual essay in 

fi ve chapters”. Proc. IEEE alt.VIS Workshop . 2022. URL : https : / / openreview.net/pdf?id=pCqgijmvLH4 .[WRB*20] WANG , X IYAO , R OUSSEAU , D AVID , B ESANÇON , L ONNI , et al. “Towards an understanding of augmented reality extensions for existing 3D data analysis tools”. Proc. CHI . New York: ACM, 2020, art. 528, 13 pp. DOI : 10/gm4jj8 .[WTL24] WANG , C., T HOMPSON , J., and L EE , B. “Data Formulator: AI-powered concept-driven visualization authoring”. IEEE Trans Vis Comput Graph 30.1 (2024), 1128–1138. DOI : 10/g8nz5f .[ WWA*24] WANG , J UNLIN , W ANG , J UE , A THIWARATKUN , B EN , et al. 

Mixture-of-agents enhances large language model capabilities . arXiv preprint 2406.04692. 2024. DOI : 10/hbjqp8 .[WWL*25] W ENG , L., W ANG , X., L U , J., et al. “InsightLens: Augmenting LLM-powered data analysis with interactive insight management and navigation”. IEEE Trans Vis Comput Graph 31.6 (2025), 3719–3732. DOI :

10/g94wwz .[ WWS*22a] W EI , J ASON , W ANG , X UEZHI , S CHUURMANS , D ALE , et al. 

Chain-of-thought prompting elicits reasoning in large language models .arXiv preprint 2201.11903. 2022. DOI : 10/gr263w .[WWS*22b] W U , A OYU , W ANG , Y UN , S HU , X INHUAN , et al. “AI4VIS: Survey on arti fi cial intelligence approaches for data visualization”. IEEE Trans Vis Comput Graph 28.12 (2022), 5049–5070. DOI : 10/grxn3d .[XMW23] X U , C ANWEN , M C A ULEY , J ULIAN , and W ANG , P ENGHAN .“Mirror: A natural language interface for data querying, summarization, and visualization”. New York: ACM, 2023, 49–52. DOI : 10/nmzz .[XZX*24] X IE , L IWENHAN , Z HENG , C HENGBO , X IA , H AIJUN , et al. “WaitGPT: Monitoring and steering conversational LLM agent in data analysis with on-the-fl y code visualization”. Proc. UIST . New York: ACM, 2024, art. 119, 14 pp. DOI : 10/g8nwhg .[YGZ*25] Y U , H AO , G AN , A ORAN , Z HANG , K AI , et al. “Evaluation of retrieval-augmented generation: A survey”. Proc. Big Data . Singapore: Springer, 2025, 102–120. DOI : 10/hbjqnq .[YHX*25] YAN , Y., H OU , Y., X IAO , Y., et al. “KNOWNET: Guided health information seeking from LLMs via knowledge graph integration”. IEEE Trans Vis Comput Graph 31.1 (2025), 547–557. DOI : 10/g8nwjh .[YKSJ07] Y I , J I S OO , K ANG , Y OUN AH , S TASKO , J OHN , and J ACKO , J.A. “Toward a deeper understanding of the role of interaction in information visualization”. IEEE Trans Vis Comput Graph 13.6 (2007), 1224–1231. DOI : 10/fmrs6r .[YLT18] Y NNERMAN , A NDERS , L ÖWGREN , J ONAS , and T IBELL , L ENA .“Exploranation: A new science communication paradigm”. IEEE Comput Graph Appl 38.3 (2018), 13–20. DOI : 10/gd8wmr .[YZE*24] YAN , L IXIANG , Z HAO , L INXUAN , E CHEVERRIA , V ANESSA ,et al. “VizChat: Enhancing learning analytics dashboards with contex-tualised explanations using multimodal generative AI chatbots”. Proc. AIED . Cham: Springer, 2024, 180–193. DOI : 10/g8nwg7 .[ ZBM*24] Z HANG , Z EYU , B O , X IAOHE , M A , C HEN , et al. A Survey on the memory mechanism of large language model based agents . arXiv preprint 2404.13501. 2024. DOI : 10/qmvx .[ZMD*22] Z ENG , Z EHUA , M OH , P HOEBE , D U , F AN , et al. “An evaluation-focused framework for visualization recommendation algorithms”. IEEE Trans Vis Comput Graph 28.1 (2022), 346–356. DOI : 10/gnhb8q .[ZMFS09] Z HU , S HAOJIAN , M A , Y AO , F ENG , J INJUAN , and S EARS , A N -DREW . “Speech-based navigation: Improving grid-based solutions”. Proc. INTERACT . Berlin: Springer, 2009, 50–62. DOI : 10/bcq5dt .[ZPM24] Z ULFIKAR , W AZEER D EEN , P IERCE , C AYDEN , and M AES , P AT -TIE . “MeMic: Towards social acceptability of user-only speech recording wearables”. CHI Extended Abstracts . New York: ACM, 2024, art. 238, 9 pp. DOI : 10/hbjqpc .[ ZRK*25] Z HANG , Z HEHAO , R OSSI , R YAN A., K VETON , B RANISLAV ,et al. Personalization of large language models: A survey . arXiv preprint 2411.00027. 2025. DOI : 10/qmvz .[ZSJ*20] Z HU , S UJIA , S UN , G UODAO , J IANG , Q I , et al. “A survey on automatic infographics and visualization recommendations”. Vis Inf 4.3 (2020), 24–40. DOI : 10/ghhxst .[ ZSWY25] Z HANG , Y ULIN , S HI , C HENG , W ANG , Y ANG , and Y ANG ,S IBEI . Eyes wide open: Ego proactive video-LLM for streaming video .arXiv preprint 2510.14560. 2025. DOI : 10/hbjqp6 .[ZWS*24] Z HANG , W EIXU , W ANG , Y IFEI , S ONG , Y UANFENG , et al. “Natural language interfaces for tabular data querying and visualization: A survey”. IEEE Trans Knowl Data Eng 36.11 (2024), 6699–6718. DOI :

10/hbjqnx .[ ZZL*25] Z HAO , W AYNE X IN , Z HOU , K UN , L I , J UNYI , et al. A survey of large language models . arXiv preprint 2303.18223. 2025. DOI : 10/ gsv9x2 .[ZZY*26] Z HAO , P ENGHAO , Z HANG , H AILIN , Y U , Q INHAN , et al. “Retrieval-augmented generation for AI-generated content: A survey”. 

Data Sci Eng (2026). DOI : 10/hbjqns . To appear. [ZZZ*25] Z HAO , Y UHENG , Z HANG , Y IXING , Z HANG , Y U , et al. “LEVA: Using large language models to enhance visual analytics”. IEEE Trans Vis Comput Graph 31.3 (2025), 1830–1847. DOI : 10/nmzr .© 2026 The Author(s). Computer Graphics Forum published by Eurographics and John Wiley & Sons Ltd.