# Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning
# 仅靠记忆保留不足以胜任强化学习中的记忆任务

**Authors**: Oleg Shchendrigin, Egor Cherepanov, Alexey K. Kovalev, Aleksandr I. Panov
**Date**: 2026-01-21
**PDF**: https://arxiv.org/pdf/2601.15086v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span>
**Score**: 8.0
**Evidence**: introduces a new benchmark for memory tasks in reinforcement learning

---

## Abstract
Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/

## 摘要
现实世界中的有效决策依赖于既稳定又具有适应

---

## 论文详细总结（自动生成）

这篇论文由来自 Innopolis University 和俄罗斯相关实验室的研究人员合作完成，发表于 AAMAS 2026。以下是对该论文的结构化深入总结：

### 1. 核心问题与整体含义（研究动机和背景）
在部分可观测（POMDP）的强化学习（RL）环境中，智能体必须依赖记忆来做出决策。现有的研究和基准测试（如 T-Maze）主要关注**记忆保留（Retention）**，即如何长时间存储一个线索。然而，现实世界往往是动态的，旧的信息会过时。
*   **核心问题**：智能体不仅需要记住信息，还需要具备**记忆重写（Memory Rewriting）**的能力——即在环境变化时，能够主动丢弃过时的信息并用新的相关信息进行覆盖。
*   **研究动机**：作者认为，现有的记忆增强型智能体在“重写”这一关键能力上缺乏系统性的评估，导致我们并不清楚哪些架构真正擅长处理动态更新的任务。

### 2. 论文提出的方法论
论文没有提出一种全新的算法，而是提出了一个**专门用于评估记忆重写能力的基准测试框架**，并对现有架构进行了形式化分析。

*   **核心思想**：将记忆更新过程分解为：$m_{t+1} = W_\phi(F_\phi(m_t), E_\phi(\eta_t))$。其中 $F_\phi$ 是选择性遗忘机制，$E_\phi$ 是新输入编码，$W_\phi$ 是整合过程。
*   **关键基准环境**：
    1.  **Endless T-Maze（无尽 T 迷宫）**：将经典的 T-Maze 串联起来。智能体在每个走廊开始时收到一个新线索（左或右），这个新线索会使之前的线索完全失效。这迫使智能体必须不断重写记忆。
    2.  **Color-Cubes（彩色方块）**：一个网格世界，方块会随机传送。智能体需要根据目标颜色寻找方块。当非目标方块位置变动时，智能体必须更新其内部的“环境地图”，测试其在不确定性下的选择性记忆更新能力。

### 3. 实验设计
*   **对比方法（Baselines）**：
    *   **循环神经网络**：PPO-LSTM, PPO-GRU, PPO-RNN。
    *   **Transformer 架构**：GTrXL（带门控的 Transformer-XL）。
    *   **结构化外部记忆**：FFM（快速遗忘记忆）、SHM（稳定哈达玛记忆）。
    *   **无记忆基准**：PPO-MLP。
*   **实验场景**：
    *   **Endless T-Maze**：分为固定长度（Fixed）和随机长度（Uniform）两种模式，测试智能体的插值（处理比训练时更短的任务）和外推（处理更长或更多次重写的任务）能力。
    *   **Color-Cubes**：分为简单（Trivial）、中等（Medium）、极端（Extreme）三个难度等级。

### 4. 资源与算力
*   **训练细节**：每个智能体在每个配置下训练 **200 万个时间步（2M timesteps）**。
*   **框架**：使用了 Ray RLlib 和 Stable-Baselines3。
*   **算力说明**：论文**未明确说明**具体的 GPU 型号或数量，但从 2M 时间步的训练量和所使用的模型规模（隐藏层维度 128-512）来看，这类实验通常在单张消费级 GPU（如 RTX 3090/4090）或中等规模的计算集群上即可完成。

### 5. 实验数量与充分性
*   **实验规模**：
    *   每个配置都进行了 **10 次独立运行（10 independent runs）**，并报告了平均成功率和标准误差（SEM），这在 RL 论文中属于较为严谨的做法。
    *   涵盖了多种环境参数（走廊长度 $l$、走廊数量 $n$、采样模式）。
    *   进行了**消融实验**（Ablation Study），对比了 RNN、GRU 和 LSTM，以验证“门控机制”和“遗忘门”的具体作用。
*   **充分性评价**：实验设计非常充分且具有针对性。通过插值和外推实验，客观地评价了模型的泛化能力，而非仅仅是过拟合训练环境。

### 6. 论文的主要结论与发现
1.  **LSTM 表现最优**：经典的 PPO-LSTM 在大多数重写任务中表现最强，具有最好的灵活性和泛化能力。
2.  **遗忘门是关键**：消融实验显示，拥有显式、可学习遗忘门（Forget Gate）的架构（如 LSTM）在重写任务中远好于没有遗忘门的架构（如 RNN 或某些结构化记忆）。
3.  **Transformer 的局限**：GTrXL 在稀疏奖励的重写任务中表现糟糕，往往只能处理简单的保留任务，无法有效应对频繁的记忆更新。
4.  **结构化记忆的僵化**：FFM 和 SHM 虽然在固定模式下表现尚可，但在随机（Uniform）模式下表现大幅下降，说明其重写机制缺乏对环境随机性的适应力。
5.  **集体失败**：在最复杂的 Color-Cubes（中等和极端模式）中，**所有**现有模型均告失败（成功率为 0），说明当前的 RL 记忆机制还远未达到处理复杂动态环境的要求