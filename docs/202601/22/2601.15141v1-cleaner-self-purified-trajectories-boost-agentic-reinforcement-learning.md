# CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning
# CLEANER：自净化轨迹助力智能体强化学习

**Authors**: Tianshi Xu, Yuteng Chen, Meng Li
**Date**: 2026-01-21
**PDF**: https://arxiv.org/pdf/2601.15141v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span> <span class="tag-label tag-pink">keyword:ppo</span>
**Score**: 8.0
**Evidence**: Agentic RL using LLMs for complex problem solving and policy optimization

---

## Abstract
Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub

## 摘要
智能体强化学习（Agentic RL）使大语言模型（

---

## 论文详细总结（自动生成）

这是一份关于论文 **《CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning》** 的结构化深度总结：

---

### 1. 论文的核心问题与整体含义（研究动机和背景）
*   **核心问题**：在智能体强化学习（Agentic RL）中，模型（尤其是 4B-7B 的中轻量级模型）在探索阶段会产生大量**执行失败**（如 Python 代码报错）。
*   **研究背景**：
    *   **信用分配难题**：在仅基于结果（Outcome-based）的奖励机制下，如果一个轨迹最终成功了，其中包含的所有错误尝试和报错信息也会被赋予正向奖励。这导致模型错误地强化了“报错-重试”的冗余循环，而非直接生成正确代码。
    *   **上下文污染**：冗余的报错 Traceback 占据了上下文窗口，干扰了模型的逻辑推理。
    *   **现有方案局限**：密集奖励（Dense Reward）易导致奖励作弊（Reward Hacking）；超采样（Supersampling）则会带来极高的计算成本。

### 2. 论文提出的方法论：CLEANER 与 SAAR
论文提出了 **CLEANER** 框架，其核心是 **SAAR（相似度感知自适应回滚）** 机制，旨在数据收集阶段实时净化轨迹。

*   **核心思想**：利用模型自身的纠错能力，在发现错误并修正成功后，通过“回滚”操作，将原始轨迹中的错误片段替换为修正后的正确片段，从而构建一条“看起来一次性成功”的自净化轨迹。
*   **关键技术细节 (SAAR)**：
    1.  **错误触发与修正**：当代码执行报错时，模型进入临时看回馈阶段，尝试生成修正代码 $c'_t$。
    2.  **相似度判定**：计算原始错误代码 $c_t$ 与修正代码 $c'_t$ 的语义相似度（使用 `difflib`）。
    3.  **自适应替换粒度**：
        *   **Case A（浅层替换）**：相似度高（如语法拼写错误）。仅替换代码部分，保留原有的推理过程（Thought）。
        *   **Case B（深层替换）**：相似度低（如逻辑策略错误）。将整个错误的推理+代码片段全部回滚，替换为修正时的辅助推理+正确代码。
    4.  **Logit 重新计算**：由于上下文发生了变化，使用 **RadixAttention**（基于 SGLang）高效地重新计算新轨迹的 Logits，确保策略更新的因果一致性。
*   **课程混合策略**：为了保持模型的鲁棒性，并非 100% 净化，而是保留 30% 的原始错误轨迹，让模型依然具备一定的纠错能力。

### 3. 实验设计
*   **基座模型**：Qwen3-4B-Instruct, Qwen2.5-7B-Instruct。
*   **数据集/场景**：
    *   **数学推理**：AIME 2024, AIME 2025。
    *   **科学问答**：GPQA。
    *   **代码生成**：LiveCodeBench (v6)。
*   **对比方法**：
    *   **纯推理模型**：DeepSeek-V3, DeepSeek-R1-Distill-32B, Qwen2.5-72B 等。
    *   **智能体 RL 模型**：DemyAgent-4B, rStar2-Agent-14B, ToRL-7B, ARPO-7B 等。
    *   **消融基准**：DAPO-baseline（不含 SAAR 的标准 RL 训练）。

### 4. 资源与算力
*   **硬件环境**：单节点 4×NVIDIA **H100** 或 4×NVIDIA **H200** GPU。
*   **训练时长**：
    *   **Qwen2.5-4B**：完整训练周期约 **4 天**。
    *   **Qwen2.5-7B**：约 **2 天**（时长较短是因为对训练数据进行了过滤，剔除了过易或过难的样本）。
*   **参数设置**：Rollout Batch Size 为 128，Group Size 为 16，最大响应长度 20,480 词元。

### 5. 实验数量与充分性
*   **实验规模**：在 4 个主流硬核 Benchmark 上进行了测试，涵盖了数学、代码和科学推理。
*   **消融实验**：
    *   验证了 SAAR 的必要性（对比有无 SAAR 的性能）。
    *   验证了学习率的影响。
    *   **内化验证**：在推理阶段关闭 SAAR，证明模型已将正确模式“内化”到参数中，而非依赖外部插件。
    *   **恢复实验**：从一个已经训练崩了的 Baseline 检查点开始加入 CLEANER，观察其纠偏能力。
*   **充分性评价**：实验设计较为全面，不仅对比了同规模模型，还跨规模对比了 14B 甚至 72B 的模型，证明了该方法在小模型上的卓越效率。

### 6. 论文的主要结论与发现
1.  **性能显著提升**：在 AIME 24/25 上平均提升 6%，GPQA 提升 3%，LiveCodeBench 提升 5%。
2.  **训练