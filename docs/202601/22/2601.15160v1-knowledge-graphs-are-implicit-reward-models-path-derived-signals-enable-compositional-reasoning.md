# Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning
# 知识图谱是隐式奖励模型：路径衍生信号赋能组合推理

**Authors**: Yuval Kansal, Niraj K. Jha
**Date**: 2026-01-21
**PDF**: https://arxiv.org/pdf/2601.15160v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 8.0
**Evidence**: Uses reinforcement learning and LLMs for compositional reasoning tasks

---

## Abstract
Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.

## 摘要
大语言模型在数学和编程等

---

## 论文详细总结（自动生成）

这篇论文由普林斯顿大学的研究团队发表，探讨了如何利用知识图谱（KG）作为隐式奖励模型，通过强化学习（RL）提升大语言模型（LLM）在复杂科学领域（如医学）的组合推理能力。

以下是对该论文的结构化深入总结：

### 1. 论文的核心问题与整体含义
*   **研究动机**：尽管 LLM 在数学和编程方面表现优异，但在需要多步、跨领域事实组合的科学推理（如医学诊断）中仍显不足。
*   **核心挑战**：现有的强化学习（如 RLHF 或 DPO）通常只优化最终答案（Outcome-based），而忽略了推理过程（Process-based），导致模型容易出现“奖励作弊”（Reward Hacking）或推理逻辑断裂。
*   **核心问题**：如何在不依赖昂贵人工标注的情况下，构建可扩展、可验证的奖励信号，引导模型学会将原子事实（Axiomatic Facts）组合成复杂的推理链？

### 2. 论文提出的方法论
论文提出了一种**自下而上的学习范式**，通过“Base Model → SFT → RL”的流水线实现：
*   **核心思想**：将知识图谱中的路径视为推理的“地面真理”（Ground Truth）。如果模型的推理轨迹（Reasoning Trace）与 KG 中的三元组路径一致，则给予奖励。
*   **关键技术细节**：
    *   **SFT 阶段**：使用 LoRA 对模型进行微调，使其掌握基本的领域知识和推理格式。
    *   **RL 阶段（GRPO）**：采用组相对策略优化（Group Relative Policy Optimization），无需批判者网络（Critic），通过组内归一化估计优势。
    *   **奖励函数设计 ($R_{total}$)**：
        1.  **路径对齐奖励 ($R_{path}$)**：计算模型推理文本中包含的 KG 实体/关系与标准路径的重合度（Coverage）。
        2.  **二元正确性奖励 ($R_{bin}$)**：采用**负强化**策略，对错误答案给予更高的惩罚（$\beta > \alpha$），以鼓励模型探索正确的推理路径。
    *   **组合桥梁**：训练时仅使用 1-3 跳（Hops）的简单任务，测试时评估其在 4-5 跳复杂任务上的零阶泛化能力。

### 3. 实验设计
*   **数据集**：
    *   **训练集**：基于 UMLS（统一医学语言系统）生成的 24,660 个多选推理题（1-3 跳）。
    *   **测试集**：ICD-Bench，包含 3,675 个涵盖 15 个 ICD-10 类别的题目（2-5 跳）。
*   **Benchmark 与对比方法**：
    *   **基座模型**：Qwen3-8B/14B。
    *   **对比模型**：GPT-5.2、Gemini 3 Pro（前沿通用模型）、QwQ-Med-3 (32B)（医学专家模型）。
    *   **消融实验**：对比了 Zero-RL（直接从基座模型做 RL）、仅 SFT、以及不同奖励组合（相似度、思考质量等）。

### 4. 资源与算力
*   **GPU 型号与数量**：
    *   8B 模型实验：使用 8 张 **NVIDIA H100** GPU。
    *   14B 模型实验：使用 8 张 **NVIDIA H200** GPU。
*   **训练时长**：
    *   5k 样本的 RL 训练约需 12 小时。
    *   24.66k 全量样本的 RL 训练约需 65 小时。
*   **推理框架**：使用 DeepSpeed 进行模型分片和内存管理。

### 5. 实验数量与充分性
*   **实验规模**：论文进行了多维度的评估，包括跳数泛化（1-3 to 4-5）、难度分级（Level 1-5）、医学类别分析（15 类）以及鲁棒性测试（选项随机打乱）。
*   **消融实验**：非常充分。作者详细对比了“SFT+RL”与“Zero-RL”的效果，并测试了四种不同的奖励函数组合，证明了路径对齐奖励的有效性。
*   **客观性**：通过严格的训练/测试集分离（路径和实体级去重）确保了实验的公平性，避免了数据泄露。

### 6. 论文的主要结论与发现
*   **RL 是组合的桥梁**：SFT 负责注入知识，而基于 KG 路径的 RL 真正教会了模型如何“组合”这些知识。
*   **零阶泛化能力**：仅在短路径（1-3 跳）上训练的模型，在长路径（4-5 跳）上的表现显著优于 SFT 模型，甚至在最难的任务上超过了参数量大得多的 GPT-5.2 和 Gemini 3 Pro。
*   **小模型胜大模型**：经过该流程训练的 14B 模型在复杂推理任务上优于 32B 的医学专家模型。
*   **鲁棒性**