Title: Knowledge Restoration-driven Prompt Optimization: Unlocking LLM Potential for Open-Domain Relational Triplet Extraction

URL Source: https://arxiv.org/pdf/2601.15037v1

Published Time: Thu, 22 Jan 2026 02:08:37 GMT

Number of Pages: 17

Markdown Content:
# Knowledge Restoration-driven Prompt Optimization: Unlocking LLM Potential for Open-Domain Relational Triplet Extraction 

## Xiaonan Jing 1,2, Gongqing Wu 1,2‚àó, Xingrui Zhuo 1,2‚àó, Lang Sun 3, Jiapu Wang 41The Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, China 

> 2

## School of Computer Science and Information Engineering, Hefei University of Technology, China 

> 3

## Anhui Zhongke Guojin Intelligent Technology Co., Ltd., China 

> 4

## Nanjing University of Science and Technology jxn@mail.hfut.edu.cn, wugq@hfut.edu.cn, zxr@mail.hfut.edu.cn, slang34@163.com, jiapu.wang@njust.edu.cn 

## Abstract 

Open-domain Relational Triplet Extraction (ORTE) is the foundation for mining structured knowledge without predefined schemas. Despite the impressive in-context learning capabilities of Large Language Models (LLMs), existing methods are hindered by their reliance on static, heuristic-driven prompting strategies. Due to the lack of reflection mechanisms required to internalize erroneous signals, these methods exhibit vulnerability in semantic ambiguity, often making erroneous extraction patterns permanent. To address this bottleneck, we propose a Knowl-edge Reconstruction-driven Prompt Optimization (KRPO 1) framework to assist LLMs in contin-uously improving their extraction capabilities for complex ORTE task flows. Specifically, we design a self-evaluation mechanism based on knowledge restoration, which provides intrinsic feedback signals by projecting structured triplets into semantic consistency scores. Subsequently, we propose a prompt optimizer based on a textual gradient that can internalize historical experiences to iteratively optimize prompts, which can better guide LLMs to handle subsequent extraction tasks. Furthermore, to alleviate relation redundancy, we design a relation canonicalization memory that collects representative relations and provides semantically distinct schemas for the triplets. Ex-tensive experiments across three datasets show that KRPO significantly outperforms strong baselines in the extraction F1 score. 

## 1 Introduction 

Knowledge Graphs (KGs) have become the semantic back-bone of modern artificial intelligence [Hogan et al. , 2021; 

> ‚àó

Corresponding author 

> 1

https://anonymous.4open.science/r/KRPO-B26W 0 200 400 600 800  

> 0.5
> 0.6
> 0.7
> 0.8
> F1 score
> Sample Index
> F1 Evolution over Samples with Iterative Prompt Optimization on WebNLG
> Ours (Dynamically Optimized Prompt) EDC (Static Prompt)

Figure 1: Comparison between KRPO (Ours) and EDC [Zhang and Soh, 2024]. KRPO dynamically optimizes prompts via self-reflection, releasing LLM‚Äôs potential for ORTE, whereas EDC relies on static prompts and suffers from performance stagnation. 

Pan et al. , 2017], providing structured knowledge for down-stream tasks such as information retrieval [Xiong et al. ,2017], logical reasoning [Zhuo et al. , 2024], and ques-tion answering systems [Zhang et al. , 2018]. The core of Knowledge Graph Construction (KGC) is Relational Triplet Extraction (RTE), which transforms unstructured text into 

(subject, relation, object ) triplets. However, traditional RTE methods are limited by predefined relations and su-pervised labels, which limit their applicability in the open world. Therefore, Open Domain Relational Triplet Extraction (ORTE) has emerged, aiming to extract triplets from unre-stricted language environments and serve as a bridge to com-prehensive open-world knowledge understanding [Banko et al. , 2007]. The emergence of Large Language Models (LLMs) [Pan 

et al. , 2024; Wang et al. , 2024b] fundamentally reshaped the landscape of ORTE. LLMs have rich knowledge and powerful in-context learning capabilities, which have trans-formed ORTE from a training-intensive supervised learning to a training-free, prompt-driven paradigm [Wei et al. , 2023; Xu et al. , 2024]. As a key interface, the prompt translates human intentions into machine execution. Through carefully crafted prompts, LLMs can achieve few-shot knowledge ex-traction, significantly reducing the threshold for constructing massive KGs without expensive manual annotation. However, there is a fundamental contradiction in cur-

> arXiv:2601.15037v1 [cs.CL] 21 Jan 2026

rent LLM-based ORTE methods: the conflict between im-mutable static prompts and the infinite variability of open-domain semantics . Existing methods mainly rely on manually designed static prompts to guide the LLM. These unified instructions cannot capture the long-tail distribution of natural language in the open domain, including com-plex syntactic structures and stylistic variations. When static prompts encounter the inherent ambiguity and diversity of the open world, their lack of adaptability leads LLMs to fall into suboptimal performance and be unable to fully unleash their ORTE potential. As shown in Figure 1, EDC [Zhang and Soh, 2024], which relies on static prompts, struggled to adapt to diverse contexts, resulting in stagnating performance. Furthermore, a critical limitation of these methods is their 

absence of reflective capability , which confines them to an open-loop inference mode and prevents self-evolution. Through a closed-loop of feedback and reflection, human cognition can continuously evolve from historical experi-ence. In contrast, existing ORTE operate as unidirectional pipelines, treating the output as final regardless of their faith-fulness to the source text. This deficiency is critically exac-erbated by the supervision deficit in open-domain settings, where the unavailability of the ground truth label prohibits the availability of feedback. Therefore, without an internal mechanism to distinguish valid knowledge from hallucina-tions, these methods remain blind to their own errors, unable to generate the feedback necessary for prompt optimization, thereby capping their performance on ORTE. Even when triplets are correctly extracted, ORTE is af-fected by semantic fragmentation in relation representa-tion . Relations extracted by LLMs and directly used for KGC often suffer from redundancy (e.g., ‚Äú birthplace ‚Äùand ‚Äú born in ‚Äù). Existing relation canonicalization meth-ods [Vashishth et al. , 2018; Zhang and Soh, 2024] mainly rely on shallow semantic clues, such as clustering or similarity over static embeddings, treating relations as isolated points in vector space and ignoring their complex semantic dependen-cies. Therefore, these methods cannot achieve fine-grained alignment in real-world scenarios, which damages the struc-tural quality and practical value of KGs. To address these challenges, we propose a Knowledge Restoration-driven Prompt Optimization (KRPO) frame-work that unleashes LLMs‚Äô potential for ORTE through a self-reflective mechanism. Firstly, to tackle the lack of re-flection, we introduce a Knowledge Restoration mechanism that uses the semantic consistency between triplet-restored text and the original input as a proxy feedback signal. Guided by a textual gradient transformed from this signal, our iter-ative Prompt Optimization module enables the LLM to re-flect on errors and dynamically optimize the prompt to adapt to complex linguistic environments. Finally, to resolve se-mantic ambiguity, we propose a Memory-augmented Rela-tion Canonicalizer, which aligns the extracted relations with the dynamic schema memory through a cross-encoder to en-sure the refinement of KGs. As shown in the orange track in Figure 1, KRPO continuously self-reflected on its historical outputs and optimized prompts during the iteration process, steadily improving ORTE performance. In summary, we have made the following contributions: ‚Ä¢ We propose KRPO for ORTE, enabling LLMs to itera-tively optimize prompts via proxy supervision informed by knowledge restoration, effectively overcoming the supervision deficit in open domains and breaking free from the constraints of static instructions. ‚Ä¢ We design a Relation Canonicalizer equipped with a dy-namic memory. It captures semantic interactions be-tween extracted relation schemas to resolve relational ambiguity and redundancy. ‚Ä¢ Extensive experiments on three widely used benchmarks show that KRPO consistently outperforms the state-of-the-art baselines. 

## 2 Related Work 

2.1 Open-domain Relational Triplet Extraction 

Early Relational Triplet Extraction for Knowledge Graph Construction (KGC) [Zelenko et al. , 2003] focused on closed-domains with predefined relations and supervised training, typically using pipeline architectures that split the task into Named Entity Recognition [Sundheim, 1995] and Relation Classification [Chinchor, 1998] via feature-based models [Chan and Roth, 2011; Gupta et al. , 2016]. Pretrained Language Models have enabled joint modeling [Shang et al. , 2022] to mitigate error propagation, yielding discrimina-tive [Wang et al. , 2020; Dixit and Al-Onaizan, 2019] and gen-erative [Cabot and Navigli, 2021a] methods, yet remain con-strained by fixed schemas and domain-specific supervision. To improve scalability, Open Information Extraction [Banko 

et al. , 2007] removes these constraints, and recent Large Lan-guage Models (LLMs) enable few-shot extraction in open-domain settings [Wei et al. , 2023]. Building on this paradigm, LLM-driven KGC frameworks have been proposed, includ-ing SAC-KG [Chen et al. , 2024a], KGGen [Mo et al. , 2025], GraphRAG [Edge et al. , 2024], and AutoKG [Zhu et al. ,2024]. Despite their flexibility, these methods often suf-fer from relation inconsistency and prompt sensitivity, un-derscoring the need for more controllable, quality-aware open-domain extraction frameworks, which are important for downstream tasks like KG reasoning [Zhuo et al. , 2025a; Huang et al. , 2026] and temporal knowledge graph comple-tion [Wang et al. , 2023; Wang et al. , 2024a]. 

2.2 Prompt Optimization 

Pretrained language models have motivated prompt optimiza-tion research [Liu et al. , 2023; Brown et al. , 2020], includ-ing discrete prompts [Shin et al. , 2020; Jiang et al. , 2020] that optimize fixed prompts with labeled data, and continuous prompts [Li and Liang, 2021; Lester et al. , 2021; Liu et al. ,2022] that learn task-specific prompt vectors. However, both paradigms either depend on supervision or require gradient-based training, limiting their interpretability and applicability in open-domain settings. Recent work extends optimization to black-box LLMs using only outputs and external feedback, including reinforcement learning or gradient-inspired sig-nals [Zhou et al. , 2023; Deng et al. , 2022; Yang et al. , 2023; Y¬® uksekg¬® on¬® ul et al. , 2025]. Despite their promise, most ap-proaches assume supervised data or task-specific rewards and are evaluated on closed or semi-closed tasks, leaving prompt optimization under ORTE underexplored. Prompt ÔºöYour  task  is  to  transform  the  given                        

> text into asemantic graph in the form of a
> list of triplets [Entity 1,Relation, Entity 2]‚Ä¶
> Sentence :
> West Lake is ascenic spot in
> Hangzhou, renowned for ....

West  Lake  lies 

in  Hangzhou .

Knowledge Restoration 

T1(s1, r1, o1)

NLI Evaluation           

> Premise :(Sentence)
> West Lake is ascenic spot in
> Hangzhou, renowned for ...

Evaluation 

Indicators 

Entailment  +1 

Contradiction  -0.5 

Neutral  0

‚ë° Self Evaluation 

> How to improve Evaluation indicators?
> 1. Correctness: Clarify Entity names to avoid ambiguity‚Ä¶
> 2. Completeness: Expand Relation for all connections‚Ä¶
> 3. Clarity: Encourage Clear expression by precise words‚Ä¶
> How to improve ORTE Prompt?
> 1. Review the Triplet for Accuracy to improve correctness‚Ä¶
> 2. Identify Missing Relations to improve completeness‚Ä¶
> 3. Use Clear and Precise Language to improve clarity‚Ä¶

‚ë¢ Prompt Optimization                     

> Optimize the ORTE Prompt.
> Transform the given text into asemantic graph ‚Ä¶
> [Entity 1,Relation, Entity 2].Ensure that the entities are
> accurate, contextually relevant, and clearly expressed ‚Ä¶

ùùè ùë¥ùíÜùíïùíìùíäùíÑùíî 

ùùèùë∫ùíÜùíèùíïùíÜùíèùíÑùíÜ 

ùùèùë¥ùíÜùíïùíìùíäùíÑùíî 

ùùèùëªùíìùíäùíëùíçùíÜùíïùíî 

ùùèùëªùíìùíäùíëùíçùíÜùíïùíî 

ùùèùë∫ùíÜùíèùíïùíÜùíèùíÑùíÜ 

Gradient -based Prompt Optimizer  RS (r1)‚®ÅRS(rc 1)

> RS(

r1)‚®ÅRS(rc 2)

> RS(

r1)‚®ÅRS(rc n)

> ¬∑ ¬∑ ¬∑

rc1

> ¬∑¬∑¬∑

rck       

> None of above.
> Which relation in [ùëü ùëê
> ùëò ]can
> replace r1in T1¬∑¬∑¬∑
> Relation
> Canonicalization
> Memory

rc1: location 

> RS( rc1):
> ‚Ä¶ is located in
> the place
> specified by ‚Ä¶

rc2: member of      

> RS( rc2):
> ‚Ä¶ is a member of
> the group or
> organization ‚Ä¶
> r1: lies in
> RS( r1):‚Ä¶lies in ‚Ä¶
> ¬∑¬∑¬∑¬∑¬∑¬∑

Relation  Canonicalizer   

> Alignment Score ->Sort
> location

T1 (s 1, r1:lies  in , o1)      

> RS(r 1):The subject lies in the object .

‚ë£ Knowledge Graph Storage 

‚ë†

RTE 

> Triplets :

Ti(si, ri, oi) {i‚àà[1,n]}     

> Hypothesis :
> West Lake lies in Hangzhou .

Metrics 

s2

s1

sn

¬∑¬∑¬∑

(s 1, r 1, o 1)(s 1,r c1 o1)Figure 2: Overview of KRPO. Comprises four modules: (1) Relational Triplet Extraction (RTE), extracting triplets by LLM with an opti-mizable prompt; (2) Self Evaluation, assessing consistency via NLI on restored text; (3) Prompt Optimization, optimizing the prompt using evaluation feedback as gradients; and (4) Relation Canonicalization, aligning relations with dynamic schemas for Knowledge Graph storage. 

2.3 Relation Canonicalization 

Relations in ORTE are schema-free [Banko et al. , 2007], yielding lexically diverse but semantically equivalent expres-sions that introduce redundancy and ambiguity, which is amplified in LLM-based generation [Wadhwa et al. , 2023]. Relation Canonicalization is thus used as a post-processing step to canonicalize such outputs [Gal¬¥ arraga et al. , 2014]. Early methods rely on external resources such as Word-Net [Miller, 1995] for definition matching or on Siamese models [Putri et al. , 2019], while clustering-based ap-proaches [Vashishth et al. , 2018] use embeddings from re-sources such as PPDB [Ganitkevitch et al. , 2013]. However, these often over-generalize and miss fine-grained distinctions. Recent work, such as EDC [Zhang and Soh, 2024], uses embedding-based similarity between extracted and canoni-cal relations, improving scalability but still relying on shal-low surface forms. Overall, existing methods struggle to pre-cisely canonicalize free-form open-domain relations, which are needed for deep, semantically driven canonicalization. 

## 3 Preliminaries 

This section formally defines ORTE, specifies the prompt op-timization objective for minimizing the potential of LLMs, and outlines the problem of relation canonicalization. 

3.1 Open-domain Relational Triplet Extraction 

Given an unstructured corpus X, ORTE aims to extract rela-tional triplets for KGC without a predefined relation ontology. For an input text x ‚àà X, the goal is to extract M triplets 

T = {(si, r i, o i)}M

> {i=1 }

, (1) 

where si and oi are subject and object entities in x, and ri

is a natural language phrase describing their semantic rela-tion. These triplets represent relational facts expressed in x.Unlike closed-domain extraction with fixed schemas, the re-lation set Rc = r1 

> c

, r 2 

> c

, . . . , r Nc in open-domain settings is incomplete or empty, and dynamically extensible. 

3.2 Prompt LLM-based ORTE 

Recent LLM-based ORTE methods frame the task as a conditional generation problem. Given input text x and prompt P, an LLM M generates triplets T by maximiz-ing ÀÜT = arg max œÑ PM(T | x, P). However, reliance on static prompts limits the model‚Äôs ability to extract knowl-edge adaptively across diverse contexts, capping triplet qual-ity by the prompt‚Äôs expressiveness. To better exploit LLMs in open-domain settings, we shift the optimization from the output space ( T ) to the input space ( P), seeking an improved prompt: 

P‚àó = arg max  

> P

PM(P | T , x ). (2) 

The prompt most likely to induce high-quality triplet gen-eration for a given x. See Appendix A.2 for justification. 

3.3 Semantic-based Relation Canonicalization 

Triplets from the prior stage often contain semantically re-dundant relations, where distinct phrases express the same meaning. Let Rraw = {r1, r 2, . . . , r n} be the extracted raw relations, relation canonicalization maps them to canonical schemas Rc via a function f : Rraw ‚Üí R c, such that similar relations ri and ric map to the same canonical form: 

f (ri) = f  ric

 ‚áê sem( ri) ‚âà sem  ric

, (3) 

where sem (¬∑) denotes semantic meaning. In our method, this similarity is measured in a Cross-Encoder semantic space to ensure consistency in the knowledge graph. 

3.4 Task Formulation 

ORTE can be viewed as a composite function that takes text 

x and an iteratively optimized prompt, leverages LLMs to generate triplets, and canonicalizes relations to high-quality KGC: 

KG ‚àó = RelCanon {ORTE [x, P-Opt (x, P)] }, (4) 

where P-Opt (¬∑) is the gradient-based prompt optimizer, RelCanon is the relation canonicalization module. 4 Methodology 

We present KRPO, comprising two core components: (1) a knowledge restoration-driven self-evaluative prompt opti-mization mechanism, which leverages textual gradients as feedback from the self-evaluation process to dynamically op-timize the prompt and boost open-domain extraction perfor-mance; and (2) a dynamic schema expansion-based relation canonicalization mechanism, which aims to eliminate seman-tic ambiguity and ensure consistency among open-domain re-lation schemas. As illustrated in Figure 2, our framework op-erates in two main stages. 

4.1 Gradient-based Prompt Optimizer 

This stage aims to optimize the relational triplet extraction prompt. Starting with a task-specific initial prompt, we itera-tively optimize it over iterations. 

LLM-based ORTE 

Let p(t) be the prompt at iteration t. The LLM extracts can-didate relational triplets from input text x:     

> ÀÜT=fORTE
> Œ∏
>   p(t);x ,(5)

where fŒ∏ (¬∑) represents an LLM-based operation, and 

f ORTE  

> Œ∏

(¬∑) specifically denotes the ORTE functionality. Each triplet is denoted as ÀÜt = (ÀÜ s, ÀÜr, ÀÜo), where ÀÜs, ÀÜr and ÀÜo represent-ing subject, relation, and object. 

Knowledge Restoration-based Self-Evaluation 

To assess the faithfulness of the triplets to the source text, we evaluate their semantic consistency with the original text through two steps. Firstly, each extracted triplet is converted into a natural lan-guage sentence by the knowledge restoration module:       

> hÀÜt=KR (ÀÜ t) = KR (ÀÜ s, ÀÜr, ÀÜo),(6)

where KR (¬∑) denotes knowledge restoration, which is imple-mented by an LLM or other equivalent tools. The correspond-ing prompt is provided in the appendix A.3. Secondly, the restored sentence hÀÜt and the original input 

x are treated as the hypothesis and premise. A natural lan-guage inference (NLI) model is introduced to evaluate their semantic consistency:    

> EÀÜt= NLI( x, h ÀÜt)‚àà { entailment, neutral, contradiction },(7)

we implement this via LLM prompting (see Appendix A.4), though standard NLI models are also viable. The outcome is mapped to a discrete consistency score:          

> Metrics (ÀÜ t) =
> Ô£±Ô£¥Ô£≤Ô£¥Ô£≥
> 1,EÀÜt=entailment ,
> 0,EÀÜt=neutral ,
> ‚àí0.5,EÀÜt=contradiction .
> (8)

For input x, the total self-evaluation score is computed as:   

> Metrics( ÀÜT) =
> MX
> i=1
> Metrics(ÀÜ ti),(9)

which serves as the self-evaluation signal for the backward prompt optimization process. 

Gradient-based Prompt Optimization 

After extracted T (T riplets ) from x(Sentence ) and self-evaluation score M etrics , we construct feedback for guid-ing prompt optimization, targeting arg max P PM(P | T , x ).Specifically, we form feedback prompts from them, guiding the LLM to generate textual gradients indicating how to opti-mize the prompt for improving extraction quality:           

> ‚àáP‚âú‚àáLLM (Sentence, T riplets, M etrics ),(10)
> ‚àáP‚àù‚àÇM etrics ‚àÇSentence =‚àÇM etrics ‚àÇT riplets ‚ó¶‚àÇT riplets ‚àÇSentence ,(11)

where ‚àáLLM (¬∑) is a gradient operator, generating textual feedback to optimize the prompt. Based on the extracted T riplets and their evaluation scores M etrics , we define instruction I1 to guide the LLM to generate feedback that improves the evaluation metric from correctness, completeness, and clarity. The feedback is treated as a textual gradient:           

> ‚àá(1)
> p=‚àÇM etrics ‚àÇT riplets =fgrad
> Œ∏1([ I1;T riplets ;M etrics ]) ,(12)

where f grad  

> Œ∏i

(¬∑) denotes an LLM-based gradient generator. Next, conditioned on the initial prompt p(t), the input 

Sentence , the extracted T riplets , and the feedback ‚àá(1)  

> p

,we define instruction I2 to generate optimization guidance, yielding the second textual gradient:           

> ‚àá(2)
> p=‚àÇT riplets ‚àÇSentence =fgrad
> Œ∏2([ I2;p(t);Sentence ;T riplets ;‚àá(1)
> p]) .
> (13)

I2 also emphasizes correctness, completeness, and clarity. 

I1. Prompt for Evaluation-driven Feedback You are an expert evaluator for Open-domain Relational Triplet Extraction (ORTE). Given the extracted triplets: {T riplets }, and their NLI-based evaluation {M etrics }, provide concise feedback on ‚Äú how to improve evaluation indicators ‚Äù to improve its quality in terms of 1. correctness, 2. completeness, and 3. clarity. 

I2. Prompt for Prompt Optimization Guidance You are an expert prompt engineer for Open-domain Re-lational Triplet Extraction (ORTE). Given the ORTE prompt {P rompt }, input sentence 

{Sentence }, extracted triplets {T riplets }, and the feed-back {‚àá (1)  

> p

}, provide concise guidance on ‚Äú how to im-prove ORTE prompt ‚Äù to improve extraction quality in terms of 1. correctness, 2. completeness, and 3. clarity. 

I3. Prompt for Prompt Optimization You are improving a structured system prompt used for the Open-domain Relational Triplet Extraction task. The variable to improve is: 

<VARIABLE >{P rompt } </VARIABLE >

Given the contextual feedback related to this prompt: 

<CONTEXT >{context } </CONTEXT >

Your task is to follow the improvement strategies in the 

<CONTEXT >to optimize the ORTE prompt to make it clearer, more effective, and more concise. Finally, based on the generated textual gradient, we update the prompt by defining instruction I3. Here, context serves as a feedback buffer that aggregates a batch of instances. For each instance, it includes the original Sentence , the extracted 

T riplets , and the derived textual gradient ‚àá(2)  

> p

.

4.2 Relation Canonicalization 

To align extracted relations with canonical schemas, we re-trieve the T op -K semantically closest candidates from the re-lation canonicalization memory and use an LLM as a seman-tic decision-maker for fine-grained alignment. As an instan-tiation of the relation canonicalization module, we fine-tune XLM-RoBERTa [Conneau et al. , 2020] with a Cross-Encoder architecture to jointly encode query and candidate schemas, capturing both semantic equivalence and subtle distinctions. Corresponding details are in Appendix B.1. Canonical relation schemas are critical for high-quality knowledge graphs. Given a restored text hÀÜt from an extracted triplet, we mask its subject and object to form the relation schema representation: 

RS (rq ) = MaskEntity (hÀÜt; ÀÜ s; ÀÜ o), (14) where rq is the raw relation to canonicalize. For canonical schemas RS (Rc) = RS (r1 

> c

), RS (r2 

> c

), . . . , RS (rNc ) in memory, we compute semantic relevance score: 

si = œÉ AŒ∏

 RS (rq ), RS (ric) , (15) where AŒ∏ (¬∑) is a Cross-Encoder scoring function and œÉ(¬∑) is a canonicalization function. Canonical schemas are ranked and the T op -K schemas R(K) 

> c

are selected. Finally, an LLM serves as the decision function f RC  

> Œ∏

(¬∑),selecting a canonical relation r‚Ä≤ from R(K) 

> c

to replace the raw relation rq , yielding the canonicalized triplet (ÀÜ s, r ‚Ä≤, ÀÜo):

r‚Ä≤ = f RC  

> Œ∏

(x, ÀÜt, r q , R(K) 

> c

‚à™ R N one ), (16) where the prompt is provided in Appendix A.5. If r‚Ä≤ ‚ààRN one , means that none of the candidate relations in R(K)

> c

could replace rq , then rq is added to memory as a new canon-ical schema: Rc ‚Üê Add( Rc, r q ), enabling future alignment of semantically similar relations schemas. 

4.3 Method Overview 

The overall KRPO workflow is divided into two distinct phases. Initially, LLM uses a prompt to extract triplets from sentences; these triplets are restored to natural text and self-evaluated against the input via NLI to quantify consistency. And then, this evaluation signal is converted into text gra-dients, guiding the optimizer to iteratively update the prompt based on error feedback, which helps LLMs to extract higher-quality triplets. Subsequently, the restored text is aligned with the existing schemas in the relation canonicalization memory. After the Cross-Encoder-based relation canonicalizer obtains the Top-K relations, an LLM determines whether to match an existing relation or expand the schema memory. Finally, a high-quality knowledge graph is obtained. The comprehen-sive algorithmic procedure is detailed in Algorithm 1 in Ap-pendix A.1. 

## 5 Experiments 

In this section, we conduct comprehensive experiments, in-cluding comparisons with multiple baselines and ablation studies, to report the effectiveness and impact of KRPO. 

5.1 Experimental Setups 

Datasets. Following EDC [Zhang and Soh, 2024], we use three processed datasets from their release: WebNLG [Fer-reira et al. , 2020], REBEL [Cabot and Navigli, 2021b], and Wiki-NRE [Trisedya et al. , 2019]. Dataset details can be found in Appendix C.1. 

Evaluation Metrics. We use the official WebNLG script [Ferreira et al. , 2020] to compute token-level Preci-sion, Recall, and F1 by matching extracted triplets against ground truth under three lexical settings: Exact (Full token-level match), Partial (Partial token overlap suffices), and Strict (Exact match requiring alignment of subject, relation, and object). 

Baselines. We compare the KRPO to the state-of-the-art (SOTA) baseline for all datasets: REGEN [Dognin et al. ,2021], GenIE [Josifoski et al. , 2022], and EDC [Zhang and Soh, 2024]. Baseline details see Appendix C.2. 

Implementation Details. Since the capability of the under-lying LLM is critical to knowledge graph construction, we evaluate our method with multiple LLM backbones across different datasets, including Mistral-7B, Qwen3-32B, GPT-4o-mini, GPT-5, and DeepSeek-V3. Detailed information and links to these models can be found in the Appendix B.2. Mistral-7B and Qwen3-32B are deployed locally using vLLM, while the remaining models are accessed via their respective APIs. For relation canonicalization, we fine-tune XLM-RoBERTa as a Cross-Encoder for semantic schema alignment and directly use the trained model for matching. In prompt optimization, the batch size is set to 5, and the default 

T op -K value for relation canonicalization is also 5. 

5.2 Main Results 

Table 1 reports the performance comparison between KRPO and representative baselines across three datasets under all evaluation protocols and LLM backbones. Overall, KRPO consistently outperforms the strongest baseline EDC on three datasets and across all matching settings, demonstrating sta-ble and universal performance gains. Notably, the improve-ments are more pronounced under the Strict evaluation proto-col, which imposes the most stringent requirements on triplet correctness, indicating that KRPO substantially enhances extraction fidelity rather than merely increasing surface-level matches. The consistent gains observed on WebNLG, REBEL, and Wiki-NRE further confirm the strong general-ization ability of the proposed framework across datasets with diverse relation distributions and linguistic characteristics. We further analyze the impact of LLM backbones with different model capacities. KRPO consistently outper-forms EDC across all backbones, with notably larger gains on smaller models such as Mistral-7B. This suggests that prompt optimization and relation canonicalization effectively mitigate the limited reasoning and alignment capacity of Method LLMs WebNLG REBEL Wiki-NRE Overall Avg. Partial Strict Exact Partial Strict Exact Partial Strict Exact REGEN 76.7 72.0 72.3 - - - - - - -GenIE - - - 38.5 35.6 36.4 48.4 46.3 47.8 -EDC Deepseek-V3 76.7 70.0 72.7 50.6 46.4 48.0 65.5 64.6 64.9 62.2 GPT-5 74.8 69.1 70.9 49.7 45.5 46.9 65.9 65.2 65.4 61.5 GPT-4o-mini 70.8 63.4 66.0 49.6 43.5 46.2 61.8 60.6 60.8 58.1 Qwen3-32B 67.2 59.2 61.4 50.2 45.6 47.4 59.4 58.3 58.6 56.4 Mistral-7B 64.1 56.1 58.7 47.2 40.4 43.4 55.0 53.9 54.1 52.5 KRPO Deepseek-V3 77.1 74.3 75.2 55.2 50.5 52.0 68.0 67.4 67.4 65.2 GPT-5 77.7 72.0 74.2 55.8 50.4 52.3 67.1 65.5 65.8 64.5 GPT-4o-mini 74.4 69.0 71.2 51.9 45.6 48.9 64.2 61.8 62.1 61.0 Qwen3-32B 74.7 70.1 71.8 51.7 47.4 48.8 60.3 59.2 59.3 60.4 Mistral-7B 70.6 65.4 67.2 51.7 44.1 47.9 59.4 58.4 58.7 58.2 Avg. Gain ( ‚Üë) +5.9% +10.4 % +9.1% +7.7% +7.5% +7.8% +3.7% +3.2% +3.1% +6.4% 

Table 1: Comparison with SOTA methods on three datasets in terms of F1 score. The best results are highlighted in bold , and the second-best results are underlined. The ‚ÄúAvg.‚Äù column denotes the average performance across all datasets across all evaluation settings. The bottom row ‚ÄúAvg. Gain ( ‚Üë)‚Äù reports the mean improvement of our method over EDC across all LLM backbones. 60      

> 62
> 64
> 66
> 68
> 70
> Strict-F1 Scores
> WebNLG
> 40
> 42
> 44
> 46
> REBEL
> 57
> 58
> 59
> 60
> 61
> 62 Wiki-NRE
> w.o. PO w.KR(cat) w.o.RC w.RC(eme) KRPO

Figure 3: Ablation study on WebNLG, REBEL, and Wiki-NRE. We report the Strict-F1 scores to analyze the contribution of different components (e.g., PO and RC modules) to the final performance. 

lightweight LLMs, narrowing the performance gap to larger models. See Appendix C.3 for the complete experimental re-port. 

5.3 Ablation Studies 

To assess the contribution of each component, we conduct ab-lation studies on two key modules, with all experiments using GPT-4o-mini as the LLM backbone for fair comparison. 

Ablation on Prompt Optimization 

We conduct ablation studies to assess the prompt optimization (PO) module and its knowledge restoration (KR) component. As shown in Figure 3, removing PO (w.o. PO) causes con-sistent performance drops across all datasets, confirming the importance of iterative prompt optimization. Replacing KR with naive triplet concatenation (w. KR(cat)) yields limited improvements but still lags behind the full model, indicat-ing that concatenation provides insufficient feedback. Over-all, the complete PO module achieves the best F1 scores, es-pecially under stricter evaluation settings, demonstrating the necessity of knowledge restoration for effective optimization. See Appendix C.4 for the complete ablation report for the prompt optimization module. 

Ablation on Relation Canonicalization 

Figure 3 presents the ablation results for the relation canon-icalization (RC) module. Removing RC (w.o. RC) leads to substantial performance drops across all datasets, especially under more strict settings, which shows the necessity of this 1 3 5 10          

> 50
> 60
> 70
> 80
> Hit@k
> WebNLG
> 13510
> 20
> 40
> 60
> 80 REBEL
> 13510
> 20
> 40
> 60
> 80 Wiki-NRE
> Embedding-based EDC Relation Canonicalizer in KRPO

Figure 4: Analysis of Relation Canonicalization Accuracy under dif-ferent k. Evaluated on triplets with correct ‚Äú(subject,object)‚Äù pair but incorrect ‚Äúrelation‚Äù. 

module. Using cosine-similarity-based embedding matching (w. RC(eme)) partially recovers limited performance, indi-cating the limitations of shallow semantic similarity. The full RC module achieves the best results, confirming its effective-ness in enforcing the semantic consistency of relations, es-pecially in more challenging settings. See Appendix C.5 for the complete ablation report for the relation canonicalization module. 

5.4 Analysis 

Analysis of Relation Canonicalization Accuracy 

To further evaluate relation canonicalization, we analyze samples with correctly predicted entity pairs but misclassi-fied relations. As shown in Figure 4, the Cross-Encoder-based semantic consistency scoring consistently outperforms embedding-based similarity across most datasets and Hit@k settings, especially at lower k, demonstrating its stronger dis-criminative power for fine-grained schema alignment, which enables the LLM to reliably standardize the quality of knowl-edge graph between relations with subtle differences. We provide an accuracy report of KRPO and the baseline EDC on three datasets in the Appendix C.6, with detailed statistics across subject, relation, object, and entity-pair dimensions. 

Impact of Self-evaluation in Prompt Optimization 

To evaluate the reliability of self-evaluation, we analyze triplets classified as entailment against a static prompt base-line EDC under partial matching. As shown in Table 2, Sentence  In the city of Detroit in the state of Michigan, the first Pontiac  Rageous  was produced on the assembly line in 1997.                                

> Prompt
> Initial :
> Your task is to transform the given text into asemantic graph in
> the form of acomprehensive list of triplets .
> The triplets must be in the form of [Entity 1,Relationship,
> Entity 2].
> In your answer,

please  strictly  only  include  the  triplets  list  and                                                                                                                   

> do not include any explanation or apologies .
> Updated :
> Your task is to transform the given text into asemantic graph represented as alist of relational triplets .
> -Extract all explicitly stated, text -supported relationships between entities, without omitting any valid relations
> mentioned in the input .
> -Ensure that each extracted triplet captures asingle, atomic, and contextually accurate relation, using concise and
> unambiguous relationship phrases faithful to the semantics expressed in the text .
> -Avoid inferred, explanatory, or implicit relations that are not explicitly stated .
> -Present the triplets in the format [Entity 1,Relationship (clear term), Entity 2].
> In your answer, output only the list of triplets and do not include any additional text .
> Triplets
> [Pontiac Rageous , assembly, Detroit]
> [Pontiac Rageous ,buildDate , 1997]
> [Pontiac Rageous , state, Michigan]
> [Detroit, state, Michigan] [Pontiac Rageous ,productionYear , 1997]
> [Pontiac Rageous ,productionLocation , Detroit]
> [Pontiac Rageous ,productionLocation , Michigan ]

Your  task  is  to  transform  the  given  text  into  a semantic 

graph  in  the  form  of  a comprehensive  list  of  triplets .

The  triplets  must  be  in  the  form  of  [Entity 1, Relationship, 

Entity 2].

In  your  answer,  please  strictly  only  include  the  triplets  list 

and  do  not  include  any  explanation  or  apologies .

Your  task  is  to  transform  the  given  text  into  a semantic  graph  represented  as  a list  of  relational  triplets .

- Extract  all  explicitly  stated,  text -supported  relationships  between  entities,  without  omitting  any  valid 

relations  mentioned  in  the  input .

- Ensure  that  each  extracted  triplet  captures  a single,  atomic,  and  contextually  accurate  relation,  using 

concise  and  unambiguous  relationship  phrases  faithful  to  the  semantics  expressed  in  the  text .

- Avoid  inferred,  explanatory,  or  implicit  relations  that  are  not  explicitly  stated .

- Present  the  triplets  in  the  format  [Entity 1, Relationship  (clear  term),  Entity 2].

In  your  answer,  output  only  the  list  of  triplets  and  do  not  include  any  additional  text .

> neutral
> entailment
> entailment
> entailment entailment
> entailment
> entailment

Figure 5: A case study comparing the extraction results between the Initial and Updated prompts. The optimized prompt (right) incorporates more detailed constraints (in blue), guiding the LLM to generate triplets that are more precise compared to the initial prompt (left). 

Entailment Eval(Partial) WebNLG REBEL Wiki-NRE Prop. F1 Prop. F1 Prop. F1 EDC 95% 68.5 96% 50.2 75% 59.7 KRPO 98% 72.7 98% 52.4 94% 63.0 

Table 2: Entailment Triplet Analysis. Best results are in bold. Prop. is the entailment proportion by self-evaluation. 0 10 20 30 40                      

> 0.6
> 0.8
> F1 score
> WebNLG Partial(KRPO) Strict(KRPO) Exact(KRPO) Partial(EDC) Strict(EDC) Exact(EDC) 0510 15 20 25 30 35 40
> 0.4
> 0.5
> 0.6
> F1 score
> REBEL 0510 15 20 25 30 35 40
> Sample Batches
> 0.5
> 0.6
> 0.7
> 0.8
> F1 score
> Wiki-NRE

Figure 6: F1 Evolution over Samples with Iterative Prompt Opti-mization. 

KRPO consistently retains more entailment triplets and achieves higher F1 scores across all datasets, whereas EDC shows limited gains, confirming that self-evaluation is better aligned with the optimized prompt, enabling more accurate discrimination between correct and incorrect triplets. While static prompts are difficult to guide the LLM to extract suf-ficiently precise triplets. Please refer to Appendix C.7 for a detailed experimental report. 

Effect of Iterative Prompt Optimization 

To validate the effectiveness of our iterative prompt optimiza-tion mechanism, we tracked the evolution of F1 scores as prompt optimization progressed, with each point aggregating performance across a consecutive batch of 25 samples. As shown in Figure 6, KRPO consistently outperforms the static baseline across all datasets and evaluation settings, with the performance gap widening as more samples are incorporated into prompt updates. This trend indicates that our gradient-based prompt optimization effectively accumulates task-level feedback and progressively optimizes the prompt. Notably, the improvement is particularly pronounced under Strict and Exact metrics, suggesting that continuous prompt optimiza-tion not only boosts recall but also enhances structural preci-sion and schema-level consistency in triplet generation. 

5.5 Case Study 

Figure 5 presents a representative case study comparing ex-tractions produced by the initial and optimized prompts on the same sentence. For the input describing the produc-tion context of ‚Äú Pontiac Rageous ‚Äù, the initial prompt gen-erates several triplets that are either semantically vague or schema-inconsistent. Moreover, the relation between the sub-ject and ‚Äú Michigan ‚Äù expressed as ‚Äú state ‚Äù is not clear enough. In contrast, after self-evaluation, the model could obtain er-ror experience from the previous samples, iteratively opti-mize the prompt, and provide clearer instructions to guide the LLM to extract triplets that are more fine-grained, schema-consistent, and faithful to the text. The optimized prompt cor-rectly isolates the geopolitical relation ‚Äú [Detroit, state, Michi-gan] ‚Äù, and consistently uses canonicalized relations ‚Äú produc-tionLocation ‚Äù and ‚Äú productionYear ‚Äù, thereby reducing ambi-guity and improving structural consistency, which enhances KG construction quality and enables reliable performance on downstream tasks such as KG reasoning [Zhuo et al. , 2025b]. We provide a more detailed KRPO case in the Appendix D. 

## 6 Conclusion 

In this paper, we analyze the challenges of prompt optimiza-tion and relation canonicalization in open-domain relational triplet extraction and propose KRPO, which leverages knowl-edge restoration-based self-evaluation to accumulate experi-ence from previous errors and iteratively derive more effec-tive prompts, thereby better unlocking the potential of large language models for extraction. In addition, KRPO employs a relation canonicalizer to align relations with deeper seman-tics, further improving the quality of constructed knowledge graphs. Experiments on multiple datasets with different LLM backbones demonstrate the effectiveness of KRPO. References 

[Agarwal et al. , 2021] Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based synthetic corpus generation for knowledge-enhanced lan-guage model pre-training. In NAACL , pages 3554‚Äì3565, 2021. [Banko et al. , 2007] Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Et-zioni. Open information extraction from the web. In IJ-CAI , pages 2670‚Äì2676, 2007. [Brown et al. , 2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, et al. Language models are few-shot learners. In NeurIPS ,2020. [Cabot and Navigli, 2021a] Pere-Llu¬¥ ƒ±s Huguet Cabot and Roberto Navigli. REBEL: relation extraction by end-to-end language generation. In EMNLP , pages 2370‚Äì2381, 2021. [Cabot and Navigli, 2021b] Pere-Llu¬¥ ƒ±s Huguet Cabot and Roberto Navigli. REBEL: relation extraction by end-to-end language generation. In EMNLP , pages 2370‚Äì2381, 2021. [Chan and Roth, 2011] Yee Seng Chan and Dan Roth. Ex-ploiting syntactico-semantic structures for relation extrac-tion. In ACL , pages 551‚Äì560, 2011. [Chen et al. , 2024a] Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, and Jieping Ye. SAC-KG: exploiting large language models as skilled automatic constructors for domain knowledge graph. In ACL , pages 4345‚Äì4360, 2024. [Chen et al. , 2024b] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. BGE m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge dis-tillation. CoRR , abs/2402.03216, 2024. [Chinchor, 1998] Nancy Chinchor. Overview of MUC-7. In Seventh Message Understanding Conference (MUC-7) ,1998. [Conneau et al. , 2020] Alexis Conneau, Kartikay Khandel-wal, Naman Goyal, Vishrav Chaudhary, Guillaume Wen-zek, Francisco Guzm¬¥ an, et al. Unsupervised cross-lingual representation learning at scale. In ACL , pages 8440‚Äì8451, 2020. [Deng et al. , 2022] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, et al. Rl-prompt: Optimizing discrete text prompts with reinforce-ment learning. In EMNLP , pages 3369‚Äì3391, 2022. [Dixit and Al-Onaizan, 2019] Kalpit Dixit and Yaser Al-Onaizan. Span-level model for relation extraction. In ACL ,pages 5308‚Äì5314, 2019. [Dognin et al. , 2021] Pierre L. Dognin, Inkit Padhi, Igor Melnyk, and Payel Das. Regen: Reinforcement learning for text and knowledge base generation using pretrained language models. In EMNLP , pages 1084‚Äì1099, 2021. [Edge et al. , 2024] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Tru-itt, and Jonathan Larson. From local to global: A graph RAG approach to query-focused summarization. CoRR ,abs/2404.16130, 2024. [Ferreira et al. , 2020] Thiago Castro Ferreira, Claire Gar-dent, Nikolai Ilinykh, Chris Van Der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. The 2020 bilingual, bi-directional webnlg+ shared task overview and evaluation results (webnlg+ 2020). In Proceedings of the 3rd International Workshop on Natural Language Gener-ation from the Semantic Web (WebNLG+) , 2020. [Gal¬¥ arraga et al. , 2014] Luis Gal¬¥ arraga, Geremy Heitz, Kevin Murphy, and Fabian M. Suchanek. Canonicalizing open knowledge bases. In CIKM , pages 1679‚Äì1688, 2014. [Ganitkevitch et al. , 2013] Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. PPDB: the paraphrase database. In NAACL , pages 758‚Äì764, 2013. [Gupta et al. , 2016] Pankaj Gupta, Hinrich Sch¬® utze, and Bernt Andrassy. Table filling multi-task recurrent neural network for joint entity and relation extraction. In COL-ING , pages 2537‚Äì2547, 2016. [Hogan et al. , 2021] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d‚ÄôAmato, Gerard de Melo, Claudio Gutierrez, et al. Knowledge Graphs . Synthesis Lectures on Data, Semantics, and Knowledge. 2021. [Huang et al. , 2026] Manzong Huang, Chenyang Bu, Yi He, Xingrui Zhuo, and Xindong Wu. Relink: Construct-ing query-driven evidence graph on-the-fly for graphrag. 

arXiv preprint arXiv:2601.07192 , 2026. [Jiang et al. , 2020] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know. Trans. Assoc. Comput. Linguistics , 8:423‚Äì 438, 2020. [Josifoski et al. , 2022] Martin Josifoski, Nicola De Cao, Maxime Peyrard, Fabio Petroni, and Robert West. Ge-nie: Generative information extraction. In NAACL , pages 4626‚Äì4643, 2022. [Lester et al. , 2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP , pages 3045‚Äì3059, 2021. [Li and Liang, 2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In 

ACL/IJCNLP , pages 4582‚Äì4597, 2021. [Liu et al. , 2022] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In ACL , pages 61‚Äì68, 2022. [Liu et al. , 2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv. , 55(9):195:1‚Äì195:35, 2023. [Miller, 1995] George A. Miller. Wordnet: A lexical database for english. Commun. ACM , 38(11):39‚Äì41, 1995. [Mo et al. , 2025] Belinda Mo, Kyssen Yu, Joshua Kazdan, Proud Mpala, Lisa Yu, Chris Cundy, Charilaos I. Kanat-soulis, and Sanmi Koyejo. Kggen: Extracting knowl-edge graphs from plain text with language models. CoRR ,abs/2502.09956, 2025. [Pan et al. , 2017] Jeff Z. Pan, Guido Vetere, Jos¬¥ e Manu¬¥ el G¬¥ omez-P¬¥ erez, and Honghan Wu. Exploiting Linked Data and Knowledge Graphs in Large Organisations . 2017. [Pan et al. , 2024] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large lan-guage models and knowledge graphs: A roadmap. IEEE Trans. Knowl. Data Eng. , 36(7):3580‚Äì3599, 2024. [Putri et al. , 2019] Rifki Afina Putri, Giwon Hong, and Sung-Hyon Myaeng. Aligning open IE relations and KB relations using a siamese network based on word embed-ding. In IWCS , pages 142‚Äì153, 2019. [Shang et al. , 2022] Yuming Shang, Heyan Huang, and Xi-anling Mao. Onerel: Joint entity and relation extraction with one module in one step. In AAAI , pages 11285‚Äì 11293, 2022. [Shin et al. , 2020] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. Auto-prompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP , pages 4222‚Äì 4235, 2020. [Sundheim, 1995] Beth M Sundheim. Overview of results of the MUC-6 evaluation. In Sixth Message Understanding Conference (MUC-6) , 1995. [Trisedya et al. , 2019] Bayu Distiawan Trisedya, Gerhard Weikum, Jianzhong Qi, and Rui Zhang. Neural relation extraction for knowledge base enrichment. In ACL , pages 229‚Äì240, 2019. [Vashishth et al. , 2018] Shikhar Vashishth, Prince Jain, and Partha P. Talukdar. CESI: canonicalizing open knowledge bases using embeddings and side information. In WWW ,pages 1317‚Äì1327, 2018. [Wadhwa et al. , 2023] Somin Wadhwa, Silvio Amir, and By-ron C. Wallace. Revisiting relation extraction in the era of large language models. In ACL , pages 15566‚Äì15589, 2023. [Wang et al. , 2020] Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu, and Limin Sun. Tplinker: Single-stage joint extraction of entities and rela-tions through token pair linking. In COLING , pages 1572‚Äì 1582, 2020. [Wang et al. , 2023] Jiapu Wang, Boyue Wang, Meikang Qiu, Shirui Pan, Bo Xiong, Heng Liu, Linhao Luo, Tengfei Liu, Yongli Hu, Baocai Yin, et al. A survey on temporal knowledge graph completion: Taxonomy, progress, and prospects. arXiv preprint arXiv:2308.02457 , 2023. [Wang et al. , 2024a] Jiapu Wang, Zheng Cui, Boyue Wang, Shirui Pan, Junbin Gao, Baocai Yin, and Wen Gao. Ime: Integrating multi-curvature shared and specific embedding for temporal knowledge graph completion. In WWW ,pages 1954‚Äì1962, 2024. [Wang et al. , 2024b] Jiapu Wang, Kai Sun, Linhao Luo, Wei Wei, Yongli Hu, Alan W Liew, Shirui Pan, and Bao-cai Yin. Large language models-guided dynamic adap-tation for temporal knowledge graph reasoning. NeurIPS ,37:8384‚Äì8410, 2024. [Wei et al. , 2023] Xiang Wei, Xingyu Cui, Ning Cheng, Xi-aobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wen-juan Han. Zero-shot information extraction via chatting with chatgpt. CoRR , abs/2302.10205, 2023. [Xiong et al. , 2017] Chenyan Xiong, Russell Power, and Jamie Callan. Explicit semantic ranking for academic search via knowledge graph embedding. In WWW , pages 1271‚Äì1279, 2017. [Xu et al. , 2024] Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, et al. Large language models for generative information extraction: a survey. 

Frontiers Comput. Sci. , 18(6):186357, 2024. [Yang et al. , 2023] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. CoRR ,abs/2309.03409, 2023. [Y¬® uksekg¬® on¬® ul et al. , 2025] Mert Y¬® uksekg¬® on¬® ul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative AI by backpropagating language model feedback. Nat. ,639(8055):609‚Äì616, 2025. [Zelenko et al. , 2003] Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. Kernel methods for relation extrac-tion. J. Mach. Learn. Res. , 3:1083‚Äì1106, 2003. [Zhang and Soh, 2024] Bowen Zhang and Harold Soh. Ex-tract, define, canonicalize: An llm-based framework for knowledge graph construction. In EMNLP , pages 9820‚Äì 9836, 2024. [Zhang et al. , 2018] Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song. Variational reasoning for question answering with knowledge graph. In AAAI , pages 6069‚Äì6076, 2018. [Zhou et al. , 2023] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In ICLR , 2023. [Zhu et al. , 2024] Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Hua-jun Chen, and Ningyu Zhang. Llms for knowledge graph construction and reasoning: recent capabilities and future opportunities. World Wide Web (WWW) , 27(5):58, 2024. [Zhuo et al. , 2024] Xingrui Zhuo, Gongqing Wu, Zan Zhang, and Xindong Wu. Geometric-contextual mutual infomax path aggregation for relation reasoning on knowl-edge graph. IEEE Trans. Knowl. Data Eng. , 36(7):3076‚Äì 3090, 2024. [Zhuo et al. , 2025a] Xingrui Zhuo, Shirui Pan, Jiapu Wang, Gongqing Wu, Zan Zhang, Rui Li, Zizhong Wei, and Xin-dong Wu. Progressive prefix-memory tuning for complex logical query answering on knowledge graphs. In IJCAI ,pages 3716‚Äì3724, 2025. [Zhuo et al. , 2025b] Xingrui Zhuo, Jiapu Wang, Gongqing Wu, Shirui Pan, and Xindong Wu. Effective instruction parsing plugin for complex logical query answering on knowledge graphs. In WWW , pages 4780‚Äì4792, 2025. Appendix 

## A Method details 

A.1 The KRPO Algorithm 

We provide the comprehensive algorithmic procedure of our proposed KRPO framework in Algorithm 1. The workflow is structured into two sequential phases. 

Algorithm 1 KRPO: Knowledge Restoration-driven Prompt Optimization 

Input : Dataset X , Initial Prompt p, Relation Memory Rc

Parameter : Batch Size B, Instructions I1, I 2, I 3

Output : Optimized Prompt p‚àó, Knowledge Graph 

G 

> 1:

Phase 1: Gradient-based Prompt Optimization  

> 2:

for each batch B ‚äÇ X do  

> 3:

Initialize feedback buffer F ‚Üê ‚àÖ . 

> 4:

for each sentence x ‚àà B do  

> 5:

ÀÜT ‚Üê LLM (x, p ) // Individual Extraction  

> 6:

S ‚Üê PÀÜt‚àà ÀÜT NLI (x, Restore (ÀÜ t)) // Self-Evaluation per sample  

> 7:

// Step 1 & 2: Generate Sample-level Gradients  

> 8:

‚àá(1)  

> p

‚Üê LLM (I1, ÀÜT , S ) // 1. Evaluation Feedback  

> 9:

‚àá(2)  

> p

‚Üê LLM (I2, p, x, ÀÜT , ‚àá(1)  

> p

) // 2. Optimization Guidance  

> 10:

Append (x, ‚àá(2)  

> p

) to F. // Buffer gradients  

> 11:

end for  

> 12:

// Step 3: Batch-level Prompt Update  

> 13:

p ‚Üê LLM (I3, p, F) // 3. Update Prompt with aggregated feedback  

> 14:

end for  

> 15:

Set p‚àó ‚Üê p. 

> 16:

Phase 2: Extraction & Relation Canonicalization  

> 17:

Initialize G ‚Üê ‚àÖ . 

> 18:

for each sentence x ‚àà X do  

> 19:

ÀÜTraw ‚Üê LLM (x, p ‚àó) 

> 20:

for each triplet ÀÜt = (ÀÜ s, r q , ÀÜo) ‚àà ÀÜTraw do  

> 21:

hÀÜt ‚Üê MaskEntity (Restore (ÀÜ t))  

> 22:

RtopK ‚Üê CrossEncoder (hÀÜt, Rc) 

> 23:

r‚Ä≤ ‚Üê LLM-Decision (rq , RtopK ) 

> 24:

if r‚Ä≤ is N one then  

> 25:

Rc ‚Üê R c ‚à™ { rq } // Dynamic Schema Expansion  

> 26:

r‚Ä≤ ‚Üê rq 

> 27:

end if  

> 28:

Add (ÀÜ s, r ‚Ä≤, ÀÜo) to G. 

> 29:

end for  

> 30:

end for  

> 31:

return p‚àó, G

In Phase 1, the system performs iterative prompt optimiza-tion. It processes the unlabelled dataset X in batches. For each sample within a batch, the system performs extraction and self-evaluation to generate hierarchical textual gradients (Instructions I1 and I2). These gradients are aggregated into a feedback buffer, which is utilized to update the prompt via Instruction I3 at the end of each batch step. In Phase 2, the converged prompt p‚àó is deployed for open-domain ex-traction. The extracted triplets undergo relation canonical-ization, where a Cross-Encoder and an LLM decision maker dynamically align raw relations with the schema memory Rc

or expand it when necessary, ultimately constructing the final knowledge graph G.

A.2 Optimization Objective Transfer Validation 

In this section, we validate the feasibility of transferring the optimization objective from the output space to the input space through a Bayesian perspective. Specifically, accord-ing to Bayes‚Äô theorem, for arbitrary prompt P, triplet T , and input text x, we have 

PM(P | T , x ) = PM(T | x, P)PM(P | x)

PM(T | x) , (17) rearranging terms yields the triplet generation probability 

PM(T | x, P) = PM(P | T , x )PM(T | x)

PM(P | x) , (18) since PM(T | x) is independent of the prompt 

P, optimizing the original objective is equivalent to 

arg max P PM(T | P , x ) < arg max P PM(P|T ,x ) 

> PM(P| x)

. Taking the logarithm, we obtain arg max P log PM(T | P, x ) =arg max P [log PM(P | T , x ) ‚àí log PM (P | x)] .Furthermore, in our feedback-driven prompt optimization process, prompt updates are dominated by task-level feed-back induced by the generated triplets. Consequently, the optimization dynamics are primarily driven by the gradient 

‚àáP log PM (P | T , x ), while the language prior log PM(P | 

x) only serves as a weak regularizer that maintains linguis-tic fluency and structural validity. Under this assumption, the following approximation holds: 

arg max  

> P

PM(T | P , x ) ‚âà arg max  

> P

PM(P | T , x ). (19) Therefore, by optimizing the reverse probability PM(T |

x) , we can effectively substitute the optimization of the orig-inal objective PM(T | x). This reformulation enables us to build higher-quality KGs by optimizing the input space to un-lock the potential of LLMs. 

A.3 Prompt for Knowledge Restoration 

In this section, we introduce a prompt designed for knowl-edge restoration. The task is to convert a given factual triplet, consisting of a subject, relation, and object, into a natural lan-guage sentence. The prompt ensures that no information not present in the triplet is added, while retaining the semantics of the relation. The generated sentence should accurately reflect the meaning of the triplet, maintaining the original context and factual integrity. Prompt for Knowledge Restoration 

You are given a factual triplet in the form (subject, re-lation, object). Your task is to convert the triplet into a natural lan-guage sentence, ensuring the following: - Do not add any information not present in the triplet. - Retain the semantics of the relation. - The sentence should accurately reflect the meaning of the triplet. - Just need to give one sentence. 

A.4 Prompt for NLI 

In this section, we present a prompt designed for Natural Lan-guage Inference (NLI). The task is to determine the logical relation between a given premise (a long text) and a hypoth-esis (a short text). The prompt provides three possible labels for the relation: ‚Äòentailment‚Äô, where the hypothesis logically follows from the premise; ‚Äòcontradiction‚Äô, where the hypoth-esis is logically inconsistent with the premise; and ‚Äòneutral‚Äô, where the hypothesis is neither entailed nor contradicted by the premise. The result is returned in a structured JSON for-mat, including the label, confidence score, and reasoning be-hind the decision. Other NLP tools with NLI capabilities can also be used to evaluate text relations when LLM is not avail-able. Prompt for NLI You are an expert in Natural Language Inference (NLI). Your task is to determine the logical relation between a given premise (a long text) and a hypothe-sis (a short text). There are three possible labels: 1. ‚Äúentailment‚Äù - The hypothesis logically follows from the premise. 2. ‚Äúcontradiction‚Äù - The hypothesis is logically in-consistent with the premise. 3. ‚Äúneutral‚Äù - The hypothesis is neither entailed nor contradicted by the premise. Return the result in the following JSON format: 

{

‚Äúlabel‚Äù: ‚Äúentailment‚Äù / ‚Äúcontradiction‚Äù / ‚Äúneutral‚Äù, ‚Äúconfidence‚Äù: float between 0 and 1, ‚Äúreasoning‚Äù: a brief explanation of your decision. 

}

Premise: Hypothesis: 

A.5 Prompt for Relation Canonicalization Decision 

In this section, we present a prompt designed for the Relation Canonicalization Decision task. The goal of this task is to de-termine whether a raw relation extracted from a given text can be replaced by an existing canonical relation schema. Given an input text and its extracted relational triplet, the prompt explicitly provides the definition of the query relation and a set of candidate canonical relations with their corresponding schemas. The model is required to assess semantic equiv-alence and select the most appropriate canonical relation to replace the query relation. If none of the candidates are suit-able, the model is expected to indicate that no replacement is applicable. This prompt helps fine-grained semantic align-ment of relations and supports the dynamic expansion of the canonical relation set. Prompt for Relation Canonicalization Decision Given the following text and a relational triplet ex-tracted from it: Text: Triplet: The relation ‚Äú query relation ‚Äù in the triplet is defined as ‚Äú query relation schema ‚Äù In this context, is there any one relation appropriate to replace it? Choices: Relation choices with schemas. 

Output: 

## B Model details 

B.1 Fine-tuning Details of the Relation Canonicalizer 

We construct the training data from the TEKGEN [Agarwal 

et al. , 2021] dataset by filtering samples whose token length is less than 20, resulting in approximately 199,621 instances. Each instance consists of a short text pair (St, S x), where 

St denotes a linearized triplet sequence and Sx is the corre-sponding natural language sentence. To construct negative samples, we adopt a sentence re-placement strategy. For each positive pair (Sx, S t, y = 1) ,we randomly sample another instance whose triplet relation differs from that of St, and use its sentence as a negative sen-tence Sx. This yields a negative pair (S‚àí 

> x

, S t, y = 0) , where 

r(S‚àí 

> x

)Ã∏ = r(St). The number of negative samples is set to be equal to that of positive samples. Our objective is to learn a semantic consistency scoring function AŒ∏ (St, S x) ‚àà R. Considering the directional na-ture of Relation Canonicalization, we concatenate the input sequences and feed them into an XLM-RoBERTa encoder to obtain the joint representation based on the [CLS ] token: 

h = Encoder XLM-RoBERTa ([ St ‚äï Sx]) , (20) where ‚äï denotes concatenation using the [SEP ] token. A linear layer is then applied to compute the match-ing score s = W ‚ä§h + b. Following the training strat-egy of BGE-M3 [Chen et al. , 2024b], we introduce a bi-nary cross-entropy loss to guide optimization LBCE =

‚àí [y log œÉ(s) + (1 ‚àí y) log (1 ‚àí œÉ(s))] , where œÉ(¬∑) denotes the sigmoid function. In addition, for positive and negative samples with scores s+ = AŒ∏ (St, S x), s‚àí = AŒ∏ (St, S ‚àí 

> x

), we in-troduce a ranking loss to explicitly enlarge the relative score margin between positive and negative pairs: Lrank =max (0 , m ‚àí (s+ ‚àí s‚àí)) , where m > 0 is a margin hyper-parameter. The final training objective is defined as L =

LBCE + Lrank .B.2 LLMs Details 

To ensure fair comparison, we evaluated KRPO based on dif-ferent Large Language Models (LLMs) from the baseline. This includes a range of models of varying sizes and architec-tures to evaluate the robustness of our approach on different LLM backbones. The specific model we used in our experi-ment is as follows: ‚Ä¢ Mistral-7B: https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.3 ‚Ä¢ Qwen3-32B: https://huggingface.co/Qwen/Qwen3-32B ‚Ä¢ GPT-4o-mini: https://platform.openai.com/docs/ models/gpt-4o-mini ‚Ä¢ GPT-5: https://platform.openai.com/docs/models/gpt-5 ‚Ä¢ DeepSeek-V3: https://api-docs.deepseek.com/api/ deepseek-api 

## C Experimental Details 

C.1 Dataset Details 

To comprehensively evaluate the performance of KRPO in open-domain scenarios, we employ three distinct bench-marks: WebNLG [Ferreira et al. , 2020], REBEL [Cabot and Navigli, 2021b], and Wiki-NRE [Trisedya et al. , 2019]. Cru-cially, we utilize the versions processed by EDC [Zhang and Soh, 2024]. The statistics of these processed datasets are summarized in Table 3. 

WebNLG. Originally designed for Natural Language Gen-eration (NLG), the WebNLG+2020 (v3.0) dataset maps DB-pedia triplets to text. Following the EDC setting, we use a refined subset containing 1,165 instances that cover 159 unique relations . This dataset serves as a gold-standard benchmark for precise extraction capabilities due to its high-quality human annotations. 

REBEL. The REBEL dataset is a large-scale silver-standard corpus derived from Wikipedia via a BART-based extraction pipeline. To ensure experimental consistency, we adopt the EDC-processed version, which consists of 1,000 sampled instances (filtered from the original 105k+ entries). This subset encompasses 200 distinct relations , providing a diverse and semantically complex testbed. 

Wiki-NRE. Derived from Wikipedia for neural relation ex-traction, Wiki-NRE is widely used for evaluating relational generalization. Similarly, we utilize the version processed by EDC, which contains 1,000 instances sampled from the orig-inal corpus. With 45 relation types , it evaluates the model‚Äôs stability on standard relational distributions. 

C.2 Baselines Details 

We compare KRPO against three representative state-of-the-art methods, encompassing both fully supervised models and LLM-based frameworks. REGEN [Dognin et al. , 2021] (Reinforcement Learning for Text-to-Graph Generation) represents the state-of-the-art among fully supervised approaches on the WebNLG dataset. It is a sequence-to-sequence framework built upon pre-trained language models (such as T5 or BART). A key innovation of                 

> Table 3: Statistics of the experimental datasets. All datasets are pro-cessed using the EDC framework [Zhang and Soh, 2024] to ensure high quality and consistency. ‚ÄúOriginal Size‚Äù refers to the raw cor-pus size, while ‚ÄúUsed Size‚Äù indicates the refined subset used in this work.
> Dataset Source Original Size Used Size # Relations WebNLG DBpedia -1,165 159 REBEL Wikipedia 105,516 1,000 200 Wiki-NRE Wikipedia 29,619 1,000 45

REGEN is its integration of Reinforcement Learning (RL) to address the exposure bias problem in standard teacher forc-ing. By optimizing directly for non-differentiable graph eval-uation metrics (e.g., SPICE, METEOR) during training, it achieves robust bidirectional performance for both text-to-graph and graph-to-text tasks. GenIE [Josifoski et al. , 2022] (Generative Information Ex-traction) is a fully supervised autoregressive formulation that achieves SOTA performance on large-scale relation extrac-tion benchmarks like REBEL and Wiki-NRE. Unlike tradi-tional classification-based methods, GenIE treats extraction as a sequence generation task. Its core contribution is a constrained decoding mechanism (trie-based beam search), which forces the model to generate only valid entity and re-lation identifiers defined in the schema. This ensures struc-tural validity and schema compliance, making it a strong up-per bound for supervised closed-domain extraction. EDC [Zhang and Soh, 2024] (Extract-Define-Canonicalize) serves as the strongest baseline for open-domain extraction without fine-tuning, reporting SOTA results across all three datasets (WebNLG, REBEL, Wiki-NRE) in zero-shot/few-shot settings. It operates in athree-stage pipeline: (1) Extract : prompting a frozen LLM to generate open information extraction (OpenIE) triplets; (2) Define : dynamically defining relation candidates; and (3) 

Canonicalize : utilizing an embedding-based retriever to map extracted raw relations to canonical schemas via semantic similarity. As a direct predecessor to our work, it represents the current best practice for LLM-driven knowledge graph construction. 

C.3 Full version of the main experiment 

The experimental results, as shown in Tables 4, 5, and 6, demonstrate that our proposed method consistently outper-forms baseline models across all three datasets on various metrics, including Precision, Recall, and F1-score. Notably, the performance improvements are especially evident in the ‚ÄúStrict‚Äù and ‚ÄúExact‚Äù settings, where our method achieves the highest F1 scores when compared to others like GenIE and EDC. One key observation is that our approach shows a particularly pronounced performance advantage on smaller models, such as Mistral-7B. This suggests that our method is particularly effective in leveraging the capabilities of smaller models, which may otherwise struggle to achieve high accu-racy. The results underscore the robustness and adaptabil-ity of our method across different datasets and model sizes, highlighting its potential for enhancing knowledge extraction tasks, especially in resource-constrained environments. WebNLG Method LLM Partial Strict Exact Precision Recall F1 Precision Recall F1 Precision Recall F1 REGEN 75.5 78.8 76.7 71.3 73.5 72.0 71.4 73.8 72.3 GenIE - - - - - - - - -EDC Deepseek-V3 75.6 78.5 76.7 69.2 71.2 70.0 71.9 73.9 72.7 GPT-5 73.4 77.2 74.8 68.2 70.7 69.1 69.9 72.6 70.9 GPT-4o-mini 69.5 73.3 70.8 62.5 65.0 63.4 65.0 67.7 66.0 Qwen3-32B 65.4 70.4 67.2 58.0 61.4 59.2 60.2 63.7 61.4 Mistral-7B 62.4 67.2 64.1 54.8 58.3 56.1 57.4 60.9 58.7 my Deepseek-V3 77.0 77.5 77.1 74.1 74.6 74.3 75.0 75.4 75.2 

GPT-5 77.0 79.2 77.7 71.5 73.0 72.0 73.6 75.2 74.2 GPT-4o-mini 73.9 75.5 74.4 68.6 69.7 69.0 70.8 71.9 71.2 Qwen3-32B 74.3 75.6 74.7 69.7 70.7 70.1 71.5 72.4 71.8 Mistral-7B 70.0 71.7 70.6 65.0 66.2 65.4 66.8 68.0 67.2  

> Table 4: Comparison with baseline methods on WebNLG. The best results are highlighted in bold , and the second-best results are underlined

REBEL Method LLM Partial Strict Exact Precision Recall F1 Precision Recall F1 Precision Recall F1 REGEN - - - - - - - - -GenIE 38.1 39.1 38.5 35.3 36.1 35.6 36.2 36.9 36.4 EDC Deepseek-V3 50.3 51.2 50.6 46.2 46.8 46.4 47.8 48.4 48.0 GPT-5 49.4 50.4 49.7 45.3 45.9 45.5 46.6 47.4 46.9 GPT-4o-mini 49.1 50.4 49.6 43.2 44.0 43.5 45.9 46.7 46.2 Qwen3-32B 49.7 50.9 50.2 45.3 46.1 45.6 47.1 48.0 47.4 Mistral-7B 46.8 47.9 47.2 40.1 40.8 40.4 43.2 43.9 43.4 my Deepseek-V3 54.8 56.1 55.2 50.2 51.0 50.5 51.7 52.7 52.0 GPT-5 55.4 56.6 55.8 50.1 50.8 50.4 52.0 52.7 52.3 

GPT-4o-mini 51.5 52.7 51.9 45.3 46.1 45.6 48.6 49.5 48.9 Qwen3-32B 51.3 52.2 51.7 47.2 47.8 47.4 48.6 49.1 48.8 Mistral-7B 51.1 52.6 51.7 43.8 44.7 44.1 47.5 48.5 47.9  

> Table 5: Comparison with baseline methods on REBEL. The best results are highlighted in bold , and the second-best results are underlined

C.4 Ablation on Prompt Optimizer 

To validate the effectiveness of the prompt optimizer, we con-duct ablation studies by removing the prompt optimization module (w.o. PO) and replacing the knowledge restoration component with naive subject-relation-object concatenation (w.KR(cat)). As shown in Table 7, removing the prompt opti-mizer leads to performance drops across all datasets, confirm-ing its critical role. Replacing knowledge restoration with simple concatenation also degrades results, highlighting the importance of knowledge restoration for effective prompt op-timization. 

C.5 Ablation on Relation Canonicalization 

To validate the effectiveness of relation canonicalization, we conduct ablation studies by removing the relation canonical-ization module (w.o. RC). As shown in Table 8, removing the relation canonicalization module leads to performance drops across all datasets, confirming its critical role. Replacing the module with a cosine-similarity-based embedding matching strategy (w. RC(eme)) also degrades results, highlighting the importance of our designed relation canonicalization for ef-fective knowledge graph construction. 

C.6 Analysis of Triplet elements 

To further analyze where the performance gains originate, we report Strict-F1 scores at different granularities. Table 9 shows that KRPO consistently outperforms EDC across all datasets, with the gains on the relations indicating the effec-tiveness of relation canonicalization. Improvements in sub-jects and objects suggest that prompt optimization enhances extraction stability. And the gap at the triplet level demon-strates better structural consistency under strict evaluation. Wiki-NRE Method LLM Partial Strict Exact Precision Recall F1 Precision Recall F1 Precision Recall F1 REGEN - - - - - - - - -GenIE 48.2 48.6 48.4 46.2 46.4 46.3 47.7 47.9 47.8 EDC Deepseek-V3 65.3 65.8 65.5 64.4 64.9 64.6 64.7 65.1 64.9 GPT-5 65.8 66.3 65.9 65.0 65.5 65.2 65.2 65.6 65.4 GPT-4o-mini 61.5 62.4 61.8 60.4 61.0 60.6 60.5 61.2 60.8 Qwen3-32B 59.0 60.0 59.4 58.0 58.8 58.3 58.3 59.0 58.6 Mistral-7B 54.6 55.7 55.0 53.5 54.4 53.9 53.8 54.6 54.1 my Deepseek-V3 67.8 68.3 68.0 67.3 67.7 67.4 67.3 67.7 67.4 

GPT-5 66.7 67.4 67.1 65.4 65.9 65.5 65.5 66.1 65.8 GPT-4o-mini 64.0 64.4 64.2 61.7 62.0 61.8 62.1 62.4 62.1 Qwen3-32B 60.0 60.6 60.3 59.0 59.5 59.2 59.1 59.5 59.3 Mistral-7B 59.2 59.7 59.4 58.2 58.7 58.4 58.6 59.0 58.7  

> Table 6: Comparison with baseline methods on Wiki-NRE. The best results are highlighted in bold , and the second-best results are underlined

F1 WebNLG REBEL Wiki-NRE Partial Strict Exact Partial Strict Exact Partial Strict Exact w.o. PO 72.7 66.7 69.1 50.0 44.1 46.3 62.5 61.2 61.4 w.KR(cat) 73.9 68.5 70.7 50.3 43.8 47.3 63.3 61.8 61.9 KRPO 74.4 69.0 71.2 51.9 45.6 48.9 64.2 61.8 62.1 

> Table 7: Ablation study of prompt optimizer. The best results are in bold. ‚Äúw.o.PO‚Äù denotes removing the prompt optimization module, and ‚Äúw.KR(cat)‚Äù replaces the knowledge restoration component with naive subject-relation-object concatenation.

C.7 Performance analysis of self-evaluation mechanism in KRPO 

To further validate the effectiveness of our proposed self-evaluation mechanism, we conduct an in-depth analysis of the entailment triplets generated during the extraction pro-cess. As shown in Table 10, KRPO achieves a higher entail-ment proportion across all datasets compared to EDC. This indicates that the triplets generated by KRPO are more se-mantically consistent with the source text, demonstrating the effectiveness of our self-evaluation strategy in enhancing the quality of extracted information. 

## D Case study of detailed KRPO process 

To show our algorithm flow in more detail, we use an example in this chapter to go step-by-step through the method details. 

ORTE. Before the task starts, we need an initial Prompt to initialize ORTE. The initial prompt p is as follows: 

p: Initial Prompt for ORTE Your task is to transform the given text into a semantic graph in the form of a comprehensive list of triplets. The triplets must be in the form of [Entity1, Relation-ship, Entity2]. In your answer, please strictly only include the triplets list and do not include any explanation or apologies. Given the input sample sentence: ‚Äú In the city of Detroit in the state of Michigan, the first Pontiac Rageous was produced on the assembly line in 1997. ‚Äù, we initially used the initial prompt p to extract the following relation triplets: ‚Ä¢ [Pontiac Rageous, assembly, Detroit] 

‚Ä¢ [Pontiac Rageous, buildDate, 1997] 

‚Ä¢ [Pontiac Rageous, state, Michigan] 

Self Evaluation. In the second stage, we applied the prompt for Knowledge Restoration to guide the LLM in reconstruct-ing these triplets into natural language. The reconstructed sentences are as follows: ‚Ä¢ ‚Äú The Pontiac Rageous was assembled in Detroit. ‚Äù‚Ä¢ ‚Äú The Pontiac Rageous was built in 1997. ‚Äù‚Ä¢ ‚Äú The Pontiac Rageous is associated with the state of Michigan. ‚ÄùNext, we evaluated the semantic consistency between the reconstructed sentences and the original input text using the NLI evaluation prompt. Detailed evaluation results are as fol-lows: 

{

‚Äúlabel ‚Äù: ‚Äú entailment ‚Äù, ‚Äúconfidence ‚Äù: 0.95, ‚Äúreasoning ‚Äù: ‚Äú The premise explicitly states that the first Pontiac Rageous was produced on the as-sembly line in Detroit, thus confirming that the Pon-tiac Rageous was assembled in Detroit. ‚Äù

}F1 WebNLG REBEL Wiki-NRE Partial Strict Exact Partial Strict Exact Partial Strict Exact w.o. RC 67.8 60.5 62.8 46.9 40.3 43.8 59.3 57.7 57.9 w. RC(eme) 73.0 67.4 69.6 49.5 43.5 46.8 62.9 61.4 61.6 KRPO 74.4 69.0 71.2 51.9 45.6 48.9 64.2 61.8 62.1 

> Table 8: Ablation study of relation canonicalization. The best results are in bold. ‚Äúw.o. RC‚Äù means without the relation canonicalization module; ‚Äúw. RC(eme)‚Äù indicates replacing the module with a cosine-similarity-based embedding matching strategy.

Strict-F1 WebNLG REBEL Wiki-NRE s r o (s,o) tri s r o (s,o) tri s r o (s,o) tri EDC 72.2 64.3 67.5 66.7 63.4 47.4 44.7 45.3 45.1 43.5 65.1 61.3 62.9 61.5 60.8 KRPO 78.2 70.2 73.7 72.3 69.0 49.6 48.5 48.5 48.2 45.6 67.5 63.2 65.6 64.6 62.1 

> Table 9: Analysis of Triplet elements. The best results are in bold. ‚Äús‚Äù, ‚Äúr‚Äù, ‚Äúo‚Äù, ‚Äú(s,o)‚Äù, and ‚Äútri‚Äù denote subject, relation, object, subject-object pair, and complete triplet, respectively.

{

‚Äúlabel ‚Äù: ‚Äú entailment ‚Äù, ‚Äúconfidence ‚Äù: 0.9, ‚Äúreasoning ‚Äù: ‚ÄúThe premise states that the first Pontiac Rageous was produced on the assembly line in 1997 in Detroit. This supports the hypothesis that the Pontiac Rageous was built in 1997, making the hypothesis logically follow from the premise. ‚Äù

}{

‚Äúlabel ‚Äù: ‚Äú neutral ‚Äù, ‚Äúconfidence ‚Äù: 0.85, ‚Äúreasoning ‚Äù: ‚Äú The premise states that the Pon-tiac Rageous was produced in Detroit, Michigan, in 1997, but it does not provide information about the current location of the Pontiac Rageous. Hence, the hypothesis that asks if the Pontiac Rageous is cur-rently in Michigan is neither confirmed nor denied by the premise. ‚Äù

}

Prompt Optimization. Next, we perform text gradient-based prompt optimization based on these evaluation results. We apply the instructions I1 and I2 in the optimizer to each triplet separately. Taking these triplets, we obtain the evaluation-driven feedback on ‚Äú how to improve evaluation in-dicators ‚Äù by I1 as follows: ‚Äú Correctness: Ensure that all elements of the triplet are accurately derived from the source text. In this case, verify if the relationship ‚Äòstate‚Äô truly describes the con-nection between ‚ÄòPontiac Rageous‚Äô and ‚ÄòMichigan.‚Äô If the extracted term is not reflective of the actual relation depicted in the source, it may require re-evaluation. Consider revising the relation to some-thing more precise if applicable. Completeness: Ensure that all relevant context is captured within the triplet. If ‚ÄòDetroit, Michigan‚Äô is the specific lo-cation mentioned, it might be beneficial to capture the city for more precision. Clarity: Clearly define the nature of the relation so that it is unambiguous. If ‚Äòstate‚Äô lacks clarity, replace it with a more explicit term that reflects the nature of the relationship. Ensure that end-users understand what the rela-tionship describes. If the triplet is part of a larger dataset, maintain consistent terminology across en-tries. Re-evaluation of the Context: Review the source context to verify that the ex-tracted triplet holds true under likely interpreta-tions. If the premise doesn‚Äôt support the hypothesis concretely, consider rephrasing the triplet to match the available evidence in the text. ‚ÄùBased on this feedback, we obtained the following prompt optimization guidance based on the prompts obtained by I2:‚Äú To enhance the system prompt for Relational Triplet Extraction, several targeted modifications are necessary: Clarify Relationship Extraction: Encourage accu-racy by specifying that the relationship term must directly reflect the semantic link in the context. Up-date the prompt to include examples of explicit rela-tionships to guide the model toward correct inter-pretations, especially when multiple relationships might be inferred. Example: ‚ÄòTransform the given text into a semantic graph of triplets, with relationships reflecting direct context links‚Äô Emphasize Completeness: Instruct the model to prioritize capturing all contextually relevant de-tails, ensuring precise location-based relationships are extracted. Add guidelines for interpreting nested or compound locations like ‚Äòcity, state‚Äô in-stead of only considering individual elements. Example: ‚ÄòEnsure completeness by evaluating compound locations and specific contexts thor-oughly, e.g., ‚ÄòDetroit, Michigan‚Äô. Entailment Eval(Partial) WebNLG REBEL Wiki-NRE Prop. Precision Recall F1 Prop. Precision Recall F1 Prop. Precision Recall F1 EDC 95% 67.5 70.5 68.5 96% 49.7 51.1 50.2 75% 59.2 60.4 59.7 KRPO 98% 72.1 73.7 72.7 98% 52.0 53.2 52.4 94% 62.7 63.4 63.0 

> Table 10: Entailment Triplet Analysis. Best results are in bold. Prop. is the entailment proportion by self-evaluation.

Enhance Clarity: Define the expected structure and context of the triplets more clearly. Suggest us-ing clear and standardized relationship terminol-ogy within the dataset‚Äôs context, ensuring reliable consistency. Example: ‚ÄòMaintain clarity and consistency in re-lationships, preferring standard terms within the dataset‚Äôs context.‚Äô Re-evaluate Contextually: Encourage a thorough re-evaluation of the text to confirm that extracted relationships are contextually supported, prevent-ing assumptions not rooted in the provided infor-mation. ‚ÄùAfter obtaining the prompt guidance strategies for all sam-ples in a batch, we integrate them into I3 and guide the LLM to update the prompts. The updated prompt is as follows: 

p‚àó: Updated Prompt for ORTE Your task is to transform the given text into a seman-tic graph represented as a list of relational triplets. - Extract all explicitly stated, text-supported relation-ships between entities, without omitting any valid re-lations mentioned in the input. - Ensure that each extracted triplet captures a single, atomic, and contextually accurate relation, using con-cise and unambiguous relationship phrases faithful to the semantics expressed in the text. - Avoid inferred, explanatory, or implicit relations that are not explicitly stated. - Present the triplets in the format [Entity1, Relation-ship (clear term), Entity2]. In your answer, output only the list of triplets and do not include any additional text. 

ORTE after Prompt Optimization. Based on the opti-mized prompts, we guide LLM to perform ORTE again, and the newly extracted triplets are as follows: ‚Ä¢ [Detroit, state, Michigan] 

‚Ä¢ [Pontiac Rageous, productionYear, 1997] 

‚Ä¢ [Pontiac Rageous, productionLocation, Detroit] 

‚Ä¢ [Pontiac Rageous, productionLocation, Michigan] 

Self Evaluation after Prompt Optimization. We again apply the prompt for Knowledge Restoration to guide the LLM in reconstructing these new triplets into natural lan-guage and evaluate their semantic consistency with the orig-inal input text using the NLI evaluation prompt. The evalua-tion results corresponding to the four extracted triplets above are all ‚Äú entailment ‚Äù with high confidence scores. 

Relation Canonicalization. Finally, we perform relation canonicalization on the extracted triplets. By calculating the semantic similarity between the extracted relations and schemas in memory, we map them to their canonical forms or update the memory. For instance, ‚Äú productionYear ‚Äù is mapped to ‚Äú buildDate ‚Äù, and ‚Äú productionLocation ‚Äù is added in memory as a new relation. The final canonical triplets are as follows: ‚Ä¢ [Detroit, state, Michigan] 

‚Ä¢ [Pontiac Rageous, buildDate, 1997] 

‚Ä¢ [Pontiac Rageous, productionLocation, Detroit] 

‚Ä¢ [Pontiac Rageous, productionLocation, Michigan] 

Ultimately, these triplets are integrated into the knowledge graph.