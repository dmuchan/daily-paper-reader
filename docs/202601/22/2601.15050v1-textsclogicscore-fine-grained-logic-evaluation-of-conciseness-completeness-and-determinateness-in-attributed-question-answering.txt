Title: \textsc{LogicScore}: Fine-grained Logic Evaluation of Conciseness, Completeness, and Determinateness in Attributed Question Answering

URL Source: https://arxiv.org/pdf/2601.15050v1

Published Time: Thu, 22 Jan 2026 02:10:24 GMT

Number of Pages: 21

Markdown Content:
# LOGIC SCORE : Fine-grained Logic Evaluation of Conciseness, Completeness, and Determinateness in Attributed Question Answering 

# Zhichao Yan 1 , Yunxiao Zhao 1 , Jiapu Wang 2 , Jiaoyan Chen 3 , Shaoru Guo 1 ,

# Xiaoli Li 4 , Ru Li 1 and Jeff Z. Pan 51Shanxi University, Taiyuan, China 

> 2

# Nanjing University of Science and Technology, Nanjing, China 

> 3

# University of Manchester, Manchester, England 

> 4

# Singapore University of Technology and Design, Singapore 

> 5

# University of Edinburgh, Edinburgh, England 

# {yanzhichao, zhaoyunxiao, liru, gsr }@sxu.edu.cn, xiaoli li@sutd.edu.sg,j.z.pan@ed.ac.uk 

# Abstract 

Current evaluation methods for Attributed Ques-tion Answering (AQA) suffer from attribution my-opia : they emphasize verification of isolated state-ments and their attributions but overlook the global logical integrity of long-form answers. Conse-quently, Large Language Models (LLMs) often produce factually grounded yet logically incoherent responses with elusive deductive gaps. To mitigate this limitation, we present L OGIC SCORE , a uni-fied evaluation framework that shifts the paradigm from local assessment to global reasoning scrutiny. Grounded in Horn Rules, our approach integrates a backward verification mechanism to systematically evaluate three key reasoning dimensions: Com-pleteness (logically sound deduction), Conciseness 

(non-redundancy), and Determinateness (consis-tent answer entailment). Extensive experiments across three multi-hop QA datasets (HotpotQA, MusiQue, and 2WikiMultiHopQA) and over 20 LLMs (including GPT-5, Gemini-3-Pro, LLaMA3, and task-specific tuned models) reveal a critical ca-pability gap: leading models often achieve high at-tribution scores (e.g., 92.85% precision for Gemini-3 Pro) but struggle with global reasoning quality (e.g., 35.11% Conciseness for Gemini-3 Pro). Our work establishes a robust standard for logical eval-uation, highlighting the need to prioritize reasoning coherence alongside factual grounding in LLM de-velopment. Codes are available at: https://github. com/zhichaoyan11/LogicScore. 

# 1 Introduction 

Large Language Models (LLMs), pre-trained on massive cor-pora, have demonstrated remarkable reasoning and general-ization capabilities across a broad spectrum of natural lan-guage processing tasks [Pan et al. , 2023, 2024; Wang et al. ,2024b]. Yet, their inherent susceptibility to hallucinations severely undermines reliability, posing a critical barrier to real-world deployment. A promising mitigation strategy lies 

Figure 1: Motivation of L OGIC SCORE . Traditional methods (pink area) yield high Factual Quality by focusing on local evidence, over-looking reasoning flaws. Our framework (blue area) evaluates Logic Quality via Completeness, Conciseness, and Determinateness, ex-posing logical deficits in the long-form answer. 

in grounding model outputs in reliable evidence. Attributed Question Answering (AQA) [Bohnet et al. , 2022] enhances trustworthiness and factual accuracy by providing explicit at-tributions to underlying real-world knowledge sources, en-abling users to verify the provenance of generated content. Previous AQA research generally focuses on detecting [Bohnet et al. , 2022] and revising [Gao et al. , 2023a; Yan 

et al. , 2025] factual errors in LLM outputs, which quantify the factual consistency between each statement in a response and its cited sources through a Natural Language Inference model [Rashkin et al. , 2023; Gao et al. , 2023a] or a LLM to judge[Hirsch et al. , 2025; Zhang et al. , 2025; Wang et al. ,2023a, 2024a]. However, they suffer from attribution my-opia , which only considering factual accuracy (factual qual-                                

> arXiv:2601.15050v1 [cs.CL] 21 Jan 2026 Method Scope Verification Unit Structure Awareness Logical Fallacy Detection
> FACTSCORE [Min et al. , 2023] Local Atomic Fact ✗(Flat) ✗(Hallucination Only) AutoAIS [Gao et al. , 2023a] Local Sentence/Attribution ✗(Flat) ✗(Hallucination Only) CoT Evaluation Global Unstructured Text ≈(Implicit) Weak (Often misses circularity)
> LOGIC SCORE (Ours) Global Reasoning Chain ✓(Horn Clauses) ✓(Circular, Broken, Deviated)
> Table 1: L OGIC SCORE is different from current evaluation paradigms by providing a way to analyze the logical structure of the information provided. L OGIC SCORE provides an analysis of how the information is constructed, as opposed to verifying that information is true or false.

ity metrics in pink in Figure 1) while neglecting global logi-cal integrity. As a result, LLMs often generate outputs where each statement is factually grounded, yet the overall reason-ing chain remains incoherent due to unaddressed deductive gaps, ambiguous logical links, or redundant premises. Fig-ure 1 exemplifies this pitfall: despite the high precision of the attribution (0.9), the reasoning chain is broken, failing to link Steve Stevens to Warner Bros. Records .This issue is exacerbated by the complex interplay of cross-document facts, which obscures intrinsic logical relation-ships. Consequently, there is an urgent need to shift from fact-by-fact assessments to frameworks that capture global reasoning logic. Inspired by Hegelian Logic [Hegel and Wallace, 1975], we formalize global evaluation beyond ad-hoc heuristics using three principles of ideal reasoning: Essence (the core of logi-cal structure), Totality (gap-free deductive coverage), and De-terminateness (strict entailment of conclusions). To opera-tionalize these principles into measurable technical metrics, we adopt Horn Rules [Levy and Rousset, 1998], which are tractable, interpretable logical structures that formalize natu-ral language reasoning into deterministic proof trees, elimi-nating semantic ambiguity while ensuring linear-time verifia-bility [Ferrand et al. , 2005]. Building on this foundation, we propose L OGIC SCORE ,a unified logic evaluation framework that systematically as-sesses three complementary dimensions of global reason-ing quality (Figure 1, blue area), involving: (1) Complete-ness ensures complete deductive paths from question to an-swer, aligning with Hegel’s “Totality”; (2) Conciseness quan-tifies logical density by penalizing redundant or irrelevant premises, reflecting “Essence”; (3) Determinateness veri-fies that premises consistently entail the final conclusion, em-bodying “Determinateness”. Specifically, Completeness is calculated by checking if a valid reasoning path can be con-structed from the gold answer to the question (scored as 1 for success, 0 for failure). Built on Completeness , we further cal-culate the Conciseness as the ratio of the number of steps in the valid path to the total number of steps in the long-form answer. Determinateness is computed by testing whether the long-form answer successfully entails the short answer. Equipped with these quantitative measures, we validate LOGIC SCORE via extensive experiments across three chal-lenging multi-hop QA benchmarks (HotpotQA, MusiQue, and 2WikiMultiHopQA) over 20 LLMs, encompassing pro-prietary models (e.g., GPT-5.1, Gemini-3-Pro), open-source models (e.g., LLaMA-3, Qwen3), and task-specific Super-vised Fine-Tuning (SFT) models. Our results reveal that lead-ing models that can achieve exceptional attribution scores often struggle with global reasoning quality; for example, Genini-3-pro achieves 92.85% Precision for attribution but only 35.11% Conciseness. We further identify a “scaling paradox”: increasing model parameters enhances factual re-call but exacerbates evidence over-provisioning, failing to im-prove logical density (conciseness), the ratio of essential de-ductive steps to total content. This work establishes a robust standard for logical evalu-ation in long-form answer generation for AQA, highlighting the need to increase reasoning coherence alongside factual grounding in LLM development. By addressing attribution myopia, L OGIC SCORE provides a diagnostic tool to uncover systemic reasoning failures, guiding the development of next-generation LLMs towards greater rigorous responses. 

# 2 Related Work 

Attributed Question Answering. Recent works in AQA can be categorized into two paradigms, each prioritizing differ-ent stages of evidence integration and verification. Retrieval-then-Read first retrieves top-k documents via a query and then lets LLMs to generate citation tokens (attribution) via Chain-of-thought (CoT) prompting [Gao et al. , 2023b], Supervised Fine-Tuning (SFT) [Aly et al. , 2024], or Preference Opti-mization (*PO) learning [Huang et al. , 2024; Li et al. , 2024]. Parallel work has explored using Knowledge Graph for AQA benchmark construction [Hu et al. , 2025]. 2) Post-hoc re-trieval generates answers first and subsequently retrieves ev-idence for verification. This is often achieved by decompos-ing answers into sub-questions or atomic facts to ground the generation in retrieved sources [Gao et al. , 2023a; Chen et al. , 2024; Slobodkin et al. , 2024; Yan et al. , 2025]. Unlike prior frameworks that prioritize local verification while ne-glecting logical dependencies, L OGIC SCORE explicitly mod-els deductive structure to assess generation through the dual dimensions of factual grounding and logical integrity. 

Factual Evaluation. Scientific evaluation of factual quality in LLM outputs has garnered widespread attention. [Rashkin et al. , 2023] first proposes Attributable to Identified Sources, which is a human evaluation framework that uses a binary attribution concept. In the era of LLMs, AutoAIS is proposed to automatically evaluate the attribution of the LLMs’ responses [Gao et al. , 2023a] with an Natural Lan-guage Inference (NLI) model. ALCE [Gao et al. , 2023b] also uses this NLI model to evaluate the Recall and Precision of the attribution quality. With the development of LLM-as-a-judge [Li et al. , 2025], many works try to use LLMs to eval-uate attribution [Zhang et al. , 2025; Xu et al. , 2025; Rawte Figure 2: Overview of the L OGIC SCORE evaluation framework, consisting of three phases: (1) Answer Generation, where the model pro-duces a Long-form Answer ( LA ) and a Short Answer ( SA ) based on the question and top-k documents; (2) Logic Transformation, which decomposes the LA into a set of atomic propositions ( P) structured as Horn clauses; and (3) Logic Evaluation, which assesses the reasoning quality across three dimensions: Completeness, Conciseness and Determinateness. 

et al. , 2025]. Although attribution evaluation has progressed, single-dimensional evaluation remain insufficient for explain-ing the quality of LLMs’ outputs. To this end, L OGIC SCORE 

complements standard metrics with logic-aware assessments, providing a comprehensive view of model behavior. 

Chain-of-Thought Evaluation. As a primary strategy for eliciting LLMs’ reasoning abilities via logical steps [Nguyen 

et al. , 2024], CoT has attracted increasing research interest in automatic evaluation methods. [Nguyen et al. , 2024] utilize a ground truth KG to evaluate factual errors and reasoning co-herence. [He et al. , 2024] propose a reference-free evaluation framework based on the Socratic. [Mao et al. , 2025] evaluate CoT by modeling stepwise confidence as a temporal signal and using temporal logic. [Zhou et al. , 2025] employ descrip-tion of image and reasoning steps to evaluate the multimodal CoT’s quality. These methods focus more on the factuality and overlook the importance of logic quality of CoT. 

# 3 Preliminaries 

In this section, we formally define the task of AQA, introduce three metrics for evaluating logical quality, and explain the Horn rules involved in this paper. 

Attributed Question Answering is a task that provides a long-form answer for a given question. Formally, given a question Q and a corpus of text passages D, the process can be defined as follows: 

LA ←− M AQA (Q, D),

where LA represents the long-form answer and MAQA de-notes a model used for AQA. To simplify the problem, we use attribution as a unified term that encompasses both the citations generated in the retrieve-then-read [Gao et al. ,2023b] and the verification evidence used in post-hoc re-trieval [Yan et al. , 2025]. In particular, the LA is followed by a short answer ( SA , Figure 2 ( )) as the final answer. 

Logic Quality Evaluation comprises three metrics as fol-lows: (i) Completeness evaluates logical continuity by deter-mining whether a gap-free deductive path exists from a ques-tion to its final answer; (ii) Conciseness measures information density by assessing whether the reasoning chain contains re-dundant or irrelevant content; (iii) Determinateness assesses inferential certainty by verifying if the reasoning process ex-plicitly and strictly entails the derived short answer. 

Definite Horn Rules. As a specialized subset of first-order logic, Definite Horn Rules can be efficiently extracted via au-tomated rule mining systems. Structurally, a Horn Rule con-sists of a body of conjunctive predicates and a single positive head predicate [Cheng et al. , 2021]. A Horn rule is often for-mally expressed as an implication: 

r0(x, y ) ← r1(x, z 1) ∧ · · · ∧ rn(zn−1, y ), (1) where x and y are two variables, r0(x, y ) denotes the rule 

head , and the conjunction sequence of atoms ri(·) is the rule 

body . Drawing from this formalism, we define a Reason-ing Path as a deterministic logical chain where a sequence of atomic propositions entails a final answer: 

F ← P1 ∧ P2 ∧ · · · ∧ Pn, (2) where Pi is a reasoning step, F represents the final answer grounded in an answer entity, and the ∧ is realized through sequential entity alignment. For example, given the proposi-tions “Steve Stevens played guitar on the Top Gun theme” (P1) and “Steve Stevens released an album titled Atomic Playboys” ( P2), a reasoning path involving them can be de-fined as follows: Top Gun theme performed by 

−−−−−−−−−→ Steve Stevens released 

−−−−−−→ Atomic Playboys. This formal-ize natural language into proof trees, ensuring inherent ex-plainability and strict verifiability [Ferrand et al. , 2005]. 4 Methodology 

This section outlines the proposed L OGIC SCORE framework, which includes three stages: Stage 1: Answer Generation 

prompts LLMs to generate a long-form answer and a short an-swer via CoT; Stage 2: Logic Transformation transforms the long-form answer into propositional logic expression; Stage 3: Logic Quality Evaluation utilizes Horn Rule to evaluate the logic quality of long-form answer with three dimensions. 

Answer Generation. Given a query Q and retrieved doc-uments D, our objective is to produce a detailed long-form response ( LA ) with citations, followed by a concise short an-swer ( SA ). To fully harness the reasoning potential of LLMs, we employ a CoT strategy [Ji et al. , 2024], that compels the model to explicitly articulate a step-by-step rationale (serving as the LA ) interleaved with specific attribution symbols, be-fore deriving the final SA . Formally, the whole process can be represented as: 

Who is the owner of the record label of the guitarist who performed on the Top Gun theme? 

Long-form Answer LA : The Top Gun theme’s gui-tar was performed by Steve Stevens, ... [12]. Steve Stevens released an album titled “Atomic Playboys” [17]. Warner Bros. Records ... located Burbank, Cal-ifornia [2]. Warner Bros. Records is a subsidiary of Warner Music Group ... Founded by James Conkling Distributor [2]. 

Short Answer SA : Warner Music Group 

Note we use some ellipses to indicate that words have been omitted due to space limitations, and [num ] denotes a citation where num represents the document number in D.

Logic Transformation. Long-form answers often exhibit intricate semantic structures with intertwined facts, which ob-scures the inferential logic required to derive the short answer, thereby making verification difficult. Inspired by [Acharya 

et al. , 2024], we employ an LLM-based logic transforma-tion to disentangle the unstructured text into a structured log-ical chain. Specifically, we prompt LLMs to decompose the reasoning process into step-by-step atomic propositions orga-nized as Horn clauses [Levy and Rousset, 1998]. In this for-mulation, the reasoning path is structured as a definite clause 

SA ← P1 ∧P2 ∧· · ·∧ Pn according to Eq.2, where the gener-ated propositions P = {Pi} serve as the premises supporting the final short answer (consequent). This linearization reveals the deductive path explicitly, enabling precise logical evalua-tion. This transformation T can be formalized as: 

T (LA , SA ) : SA ← P1 ∧ P2 ∧ · · · ∧ Pn. (3) 

Logic Quality Evaluation. Inspired by Hegelian dialec-tics, we formalize the structural integrity of reasoning into three distinct dimensions: (1) Completeness. This dimension is executed as a reverse-chaining algorithm designed to ground the Definite Horn Clause (as defined in Eq. 2) by iteratively linking the gold answer entity egold back to the question entity eq . The proce-dure consists of three stages: • Candidate Initialization : Given the set of n atomic propo-sitions P = {Pk}nk=1 , we partition it into two subsets based on the explicit presence of the gold answer entity egold . Let 

Pu = {Pi}mi=1 denote the subset of m propositions contain-ing egold , where each Pi is treated as a potential reasoning result. The remaining n − m propositions form the context set Pc, serving as the search space for the rule body. • Backward Chain Construction : Starting from each Pi,we perform a recursive search to build the antecedent chain. We use an LLM to parse Pi into a triple (esub , rel, e obj ).Since egold is fixed as one of these entities, the other serves as a logical bridge (ebridge ). We then iteratively scan Pc for any proposition that shares ebridge as an argument, thereby resolving the variable binding ( ∧ in Horn Rule) for the next hop. This process continues by updating ebridge with the newly discovered entity in the matched proposition. • Verification and Termination : The reconstruction ter-minates successfully if the backward traversal reaches a proposition containing the query entity eq , forming a con-tinuous path ξ = {Pξ1 , . . . , P ξk }. We define this sequence as the minimal sufficient set Pmin . If the search exhausts 

Pc without linking to eq , the branch is pruned. Finally, the Completeness is calculated as: 

Completeness = 

1 if Pmin ̸ = ∅,

0 otherwise . (4) In Figure 2, P1 links to P2 via “Steve Stevens” but fails to connect further ( Pmin = ∅), resulting in a Completeness of 0. (2) Conciseness. This metric is implemented via is struc-turally coupled with the Completeness evaluation. If P fails to form a connected path from Q to SA (i.e., Completeness 

failure), the effective information is considered null. Thus, 

Pmin is defined conditionally: 

Pmin =

ξ if a valid ξ exists ,

∅ otherwise . (5) Finally, the Conciseness score is calculated as the ratio of the effective logical steps to the total generated steps: 

Conciseness = |Pmin ||P| . (6) For instance in Figure 2, Pf lacks a connected path ( Pmin is 

∅), the Conciseness is 0. (3) Determinateness. This metric is implemented via a 

Re-Inference Verification mechanism, drawing on the prin-ciples of Self-Consistency [Wang et al. , 2023b; Lanham et al. , 2023]. Given the Horn Rule constraint where Body must strictly entail Head ( Body → Head ), we verify whether the generated LA (acting as the Body) deterministically yields the SA (acting as the Head) in a closed-world setting. Specifically, we treat the generated LA as the sole premise context C and prompt LLMs to generate a new short answer 

ˆSA conditioned exclusively on Q and C. This experimental setup freezes external knowledge access to mitigate memory retrieval bias. The Determinateness score is then calculated Models Size HotpotQA MusiQue 2WikiMultiHopQA                                                                                                                                                                                                                    

> Conc. Comp. Dete. Conc. Comp. Dete. Conc. Comp. Dete.
> Proprietary LLMs
> GPT-4o UNK 44.90 75.57 97.36 32.54 57.10 93.66 46.55 80.19 93.13 GPT-o3 UNK 47.44 73.24 98.92 37.30 60.83 96.48 50.00 82.37 95.80
> GPT-5.1 UNK 48.37 72.61 99.15 27.27 60.11 87.79 37.96 81.00 92.45 Gemini-3-pro UNK 39.69 78.65 98.56 38.98 59.21 94.10 53.93 85.78 94.69 Claude-4.5 UNK 36.40 78.33 98.07 27.13 63.88 89.66 40.13 85.86 92.03 DeepSeek-V3.2 671B 44.81 76.34 97.96 30.34 59.50 86.33 44.32 79.49 90.89 DeepSeek-R1 671B 49.60 73.91 98.56 34.33 55.82 91.12 56.41 84.16 93.13
> Open-source LLMs
> 1B 10.39 17.45 46.45 2.37 3.82 26.03 12.73 18.55 44.85 LLaMA-3.2 3B 10.39 19.02 70.41 3.46 7.43 55.42 28.37 42.00 58.45 8B 26.46 54.94 84.28 12.35 28.55 65.96 28.65 60.22 69.70 LLaMA-3.1 70B 39.40 71.40 96.04 27.72 51.92 86.06 43.79 75.62 90.48 0.6B 33.11 52.16 81.52 6.92 11.32 58.12 31.53 49.78 64.03 1.7B 50.07 66.17 92.90 21.56 30.39 73.05 52.06 71.64 82.53 4B 50.39 75.58 96.87 29.83 46.88 84.40 53.85 81.53 91.23 8B 47.47 75.36 97.68 31.33 52.06 86.71 52.88 81.33 90.95 14B 52.01 73.18 97.31 31.78 48.81 85.60 54.56 81.20 90.55 30B-A3B 50.97 77.09 97.77 32.41 53.69 85.06 55.99 82.73 91.86 Qwen3 235B-A22B 45.62 73.80 98.87 35.25 60.94 95.62 53.25 86.00 96.74 Supervised Fine-tuning LLMs
> FRONT 13B 27.76 57.96 82.86 6.34 13.70 61.19 26.08 50.23 69.94 LongCite 8B 38.85 64.56 91.75 17.55 32.73 76.04 40.13 67.88 72.30
> SelfCite 8B 39.23 66.74 91.04 19.02 35.16 75.59 39.22 66.47 72.17 Table 2: Evaluation results on three multi-hop QA datasets across three types of LLMs. “Conc.”, “Comp.” and “Dete.” represent “Concise-ness”, “Completeness” and “Determinateness”, respectively. The best two results within each LLM type are bold and underlined, respectively.

based on the logical equivalence between the re-inferred con-clusion and the original answer: 

Determinateness = I( ˆSA ≡ SA ), (7) where ˆSA ∼ M (· | Q , LA ) and I(·) is the indicator func-tion. The score of 1 implies the reasoning path is structurally robust, whereas 0 indicates logical hallucination or inconsis-tency. In Figure 2, using LA , the LLM derives ˆSA “James Conkling” from LA , yielding a Determinateness of 0. Un-like open-ended scoring, L OGIC SCORE mitigates subjective bias by constraining the evaluator to structured logical veri-fications rather than arbitrary quality ratings. We provide a detailed analysis of this robust in Appendix E. 

# 5 Experiments 

5.1 Experiment Setups 

This part mainly introduces the datasets, and LLMs evaluated in L OGIC SCORE evaluation. 

Datasets. Motivated by the need to assess complex logical reasoning processes, we conduct experiments on three chal-lenging multi-hop datasets: HotpotQA [Yang et al. , 2018], 

MusiQue [Trivedi et al. , 2022], and 2WikiMultiHopQA [Ho 

et al. , 2020]. In order to purely evaluate the logic of the gen-erated answers without the interference of retrieval errors, we specifically utilize the “distractor” setting in HotpotQA and the “answerable” setting in MusiQue. 

LLMs. We cover a diverse set of LLMs to ensure a com-prehensive evaluation. We include proprietary LLMs ( GPT 

series, Gemini-3-Pro , Claude-4.5 and DeepSeek 

series), open-source LLMs (LLaMA3 and Qwen3 se-ries), and task-specific Supervised Fine-tuning (SFT) LLMs (FRONT (LLaMA2-13B) [Huang et al. , 2024], LongCite 

(LLaMA3-8B) [Zhang et al. , 2025], and SelfCite 

(LLaMA3-8B) [Chuang et al. , 2025] fine-tuned on synthetic data). Details of implementation, factual evaluation setup and all prompt templates can be found in the Appendix B and F. 

5.2 Main Results and Analysis 

According to the experimental results in Table 2, we have the following observations. (1) Our results reveal a notable decoupling between in-ferential certainty and logical rigor in proprietary models. As shown in Table 2, while top-tier models like Gemini-3-Pro and GPT-o3 exhibit high Determinateness (94.10% and 96.48%, respectively), this confidence does not translate into high logic quality on MusiQue. The low Conciseness scores (e.g., 37.30% for GPT-o3) indicate that models engage in ev-idential over-provisioning by piling on redundant premises to artificially bolster their confidence. Meanwhile, moderate Completeness scores (e.g., 59.21% for Gemini-3-pro) suggest that despite this extra volume, models still frequently miss es-sential logical steps. (2) Among open-source LLMs, we observe substantial lim-itations in smaller models like LLaMA-3.2, which struggle to maintain the logical relevance, with Conciseness dropping as low as 2.37% on MusiQue. More interestingly, the Qwen3 series exhibits a “scaling paradox”, while increasing parame-ters from 14B to 235B drives Determinateness to near-perfect levels (98.87% on HotpotQA), it simultaneously degrades reasoning efficiency. Specifically, the Qwen-235B achieves 45.62% Conciseness score and lags behind its 14B counter-part (52.01%) on HotpotQA, a trend mirrored on MusiQue where high Determinateness score (95.62%) is coupled with severe verbosity (35.25% Conciseness score). This indicates that while scaling effectively solidifies the model’s inferen-tial consistency, it does not inherently cultivate the selective attention necessary for logical distillation. (3) When it comes to Supervised Fine-tuning LLMs (SFT LLMs), we see a clear trade-off: they align better with user domains, yet their reasoning remains structurally brittle. Take SelfCite (8B) as an example, on HotpotQA, it boosts 

Completeness to 66.74%, leaving the vanilla LLaMA-3.1-8B (54.94%) far behind. However, this performance proves frag-ile. Once we switch to the more complex MusiQue bench-mark, SelfCite’s Completeness collapses to 35.16%, failing to make a dent in the lead held by proprietary giants like GPT-5.1 (60.11%) or Claude-4.5 (63.88%). This sharp drop hints at a deeper issue: SFT seems to primarily polish the “surface form” of reasoning, such as citation formatting, rather than building the deep deductive robustness needed for multi-hop synthesis. Consequently, smaller models still lack the cross-domain logical resilience we see in larger proprietary LLMs. More experimental results and analyses are provided in Ap-pendix A. 

5.3 Robustness of L OGIC SCORE across Different Backbone Models                              

> Models GPT-5-mini GPT-5-nano Conc. Comp. Dete. Conc. Comp. Dete. GPT-4o 79.91 82.75 91.24 74.08 78.14 93.22 Gemini-3-pro 70.47 75.10 92.35 74.14 70.27 94.15 Claude-4.5 79.63 82.11 93.44 75.02 79.22 94.35 Table 3: Cohen’s Kappa scores across backbone LLMs. Ranges 0.61–0.80 and >0.80 indicate substantial and perfect agreement [McHugh, 2012]. We replace the original LLM used (GPT-4o-mini for Conc. and Comp. , GPT-o4-mini for Dete.) in Logic Quality evaluation with GPT-5-mini and GPT-5-nano.

Table 3 demonstrates the robustness of our evaluation frame-work. Most Cohen’s Kappa scores exceed 70%, indicating substantial to almost perfect agreement. Determinateness 

(Dete.) shows exceptional stability, surpassing 90% across all models. Additionally, the variance between different judge pairs (e.g., GPT-5-mini replaces GPT-4o-mini) is minimal. This confirms that our metrics are robust and independent of the specific evaluator model used. 

5.4 Logic Quality across Reasoning Depths 

> Figure 4: Impact of reasoning depth on logic quality metrics. The plots illustrate the performance of Proprietary, Open-source, and SFT LLMs across varying hops (2, 3, and 4).

We also calculate the mean scores and standard errors for models grouped by LLM types as shown in Figure 4. The re-sults reveal that the performance of Conciseness drops from nearly 40% to approximately 20%, while the other metrics experience varying degrees of decline when the hops increase beyond 3. See Appendix A.1 for detail results and discussion. 

5.5 Impact of Prompts in Answer Generation                        

> Methods Models Conc. Comp. Dete. Vanilla GPT-5.1 24.16 51.79 88.25 Gemini-3-Pro 32.33 57.96 95.30 Qwen3-235B-A22B 23.24 54.21 92.71 CoT GPT-5.1 27.27 ↑3.11 60.11 ↑8.32 87.79 ↓0.46
> Gemini-3-Pro 38.98 ↑6.75 59.21 ↑1.24 94.10 ↓1.20
> Qwen3-235B-A22B 35.25 ↑12 .01 60.94 ↑6.73 95.62 ↑3.91
> Table 4: Influence of different prompt strategies for long-form an-swer generation on MusiQue dataset. We use [Gao et al. , 2023b] proposed prompt template as Vanilla method.

As shown in Table 4, our optimized CoT prompt consistently outperforms the Vanilla baseline. Notably, Qwen3-235B-A22B raises Conciseness from 23.24 to 35.25 on MusiQue, accompanied by a 6.73% gain in Completeness , validating the superior efficacy of the proposed method. This simultane-ous improvement suggests that the optimized structure filters some redundant content and improves the coverage of essen-tial reasoning steps. 

5.6 Human Evaluation                         

> Datasets Pearson-r ↑Jaccard ↑Stage 2 Conc. Comp. Dete. Accuracy ↑
> HotpotQA 92.43 93.82 91.84 98.22 MusiQue 86.32 91.48 90.81 94.14 2Wiki 93.77 97.87 93.39 97.89
> Average 90.84 94.39 92.01 96.75 Table 5: Assessment of metric correlations and transformation ac-curacy. Pearson-r measures Conciseness, whereas Jaccard applies to the binary Completeness and Determinateness. Stage 2 Accuracy reflects the human-evaluated correctness of Logic Transformation. Figure 3: Case study. We observe three logic error types when prompting LLMs to generate attributed long-form answers: Circular denotes self-referential reasoning; Deviated represents a fundamental divergence in the reasoning trajectory; Broken signifies a logical discontinuity in the deductive chain. In contrast, Connected marks a complete reasoning chain, while the icon symbolizes the ideal reasoning paradigm.

To validate the effectiveness of L OGIC SCORE , we conduct a human evaluation on 100 random instances focusing on logic transformation quality and metric alignment. As shown in Table 5, annotators confirmed that the Logic Transformation maintains high semantic fidelity, achieving an average Ac-curacy (defined as the correctness of logic expressions) of 96.75% across three datasets. Furthermore, L OGIC SCORE 

demonstrates strong alignment with human judgments, with correlation scores of 90.84% ( Conciseness ), 94.39% ( Com-pleteness ), and 92.01% ( Determinateness ). (See Appendix C for Pearson-r and Jaccard calculate and other details). 

# 6 Case Study 

As depicted in Figure 3, we first categorize three distinct types of logic errors observed in evaluation datasets. Subse-quently, we contrast these with cases of structurally redundant reasoning, culminating in the illustration of the ideal reason-ing paradigm. We evaluate the factual grounding of LLM re-sponses using two established frameworks: AutoAIS [Zhang 

et al. , 2025] and FACTSCORE [Min et al. , 2023]. We report the F1 score (AAIS) calculated from the precision and recall of AutoAIS. The analysis are as follows: 

Logic Failures. The first instance highlights a Circular 

logic failure. For the Question 1 “What is the deepest part of the ocean by the state where Main Street Station is lo-cated?”, GPT-4o falls into a self-referential loop. As shown by the backward chain ( Milwaukee Deep → Puerto Rico Trench → Atlantic Ocean → Milwaukee Deep ), the model circles back to its premise instead of ad-vancing towards a conclusion. Moving to the Question 2 about Ulrich Walter’s employer, we observe two distinct fail-ures: Gemini-3-Pro commits a Deviated error by retrieving only partial information as irrelevant steps and failing to per-form the necessary multi-document reasoning. Conversely, GPT-4o presents a Broken chain on the same question, where the critical logical link between the “German Aerospace Cen-ter” and the “Lander Control Center” is ambiguous. Collec-tively, these cases illustrate a systemic issue: LLMs tend to “over-provision” evidence and stack facts rather than con-structing the logical bridges required for complex reasoning. 

Completeness vs Conciseness. While some models avoid fatal errors, they still struggle with efficiency. Claude-4.5 achieves notable logical completeness on Question 1 but this comes at the expense of conciseness. Although it captures every necessary step, it dilutes the answer with extraneous supplementary details (such as the description of “The Puerto Rico Trench...”). This verbosity results in a significant gap between its FACT score (91%) and AAIS score (44%), re-vealing that high factual density does not equate to the strict attribution formatting required for trustworthiness. A simi-lar pattern persists in Question 2, where redundant steps drag the Conciseness score down to 33.3%. This indicates that the model relies on an information dumping strategy, exhaus-tively enumerating evidence to ensure coverage rather than performing precise logic reasoning. 

Ideal Paradigm. As shown in Figure 3 ( ), Gemini-3-Pro exemplifies the ideal reasoning paradigm on Question 1. It exhibits the optimal deductive path: Milwaukee Deep 

→ Virginia → Atlantic Ocean → Main Street Station . By generating the minimum necessary reason-ing steps alongside precise attribution formatting, the model achieves superior factual accuracy without redundancy. 

# 7 Conclusion 

In this paper, we introduce L OGIC SCORE , a unified eval-uation framework that shifts the paradigm from local fac-tual verification to global reasoning scrutiny. Our extensive analysis of over 20 LLMs reveals that high factual attribu-tion often masks systemic reasoning failures, a phenomenon termed attribution myopia . By formalizing long-form an-swers into Horn Rules, we establish that resolving this my-opia requires models to prioritize both deductive integrity (Completeness ) and expression efficiency ( Conciseness ). Ul-timately, we identify the optimization of logical density as a critical frontier for next-generation reasoning models. Future research should move beyond surface-level alignment toward synthesizing compact, deterministic reasoning chains that en-sure global logical coherence alongside factual grounding. 

# Ethical Statement 

This research is conducted with careful consideration of ethi-cal implications. All data used in this study are collected from publicly available sources with appropriate permissions. For the generative AI model, we only use Gemini to polish the manuscript, and we are responsible for all the materials. 

# References 

Kamal Acharya, Alvaro Velasquez, and Houbing Herbert Song. A survey on symbolic knowledge distillation of large language models. IEEE Transactions on Artificial Intelli-gence , 1(01):1–21, 2024. Rami Aly, Zhiqiang Tang, Samson Tan, and George Karypis. Learning to generate answers with citations via factual consistency models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 11876–11896, 2024. Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Massimiliano Ciaramita, Ja-cob Eisenstein, Kuzman Ganchev, Jonathan Herzig, et al. Attributed question answering: Evaluation and model-ing for attributed large language models. arXiv preprint arXiv:2212.08037 , 2022. Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett, and Eunsol Choi. Complex claim verification with evidence retrieved in the wild. In Proceedings of the 2024 Confer-ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-gies (Volume 1: Long Papers) . Association for Computa-tional Linguistics, 2024. Kewei Cheng, Ziqing Yang, Ming Zhang, and Yizhou Sun. Uniker: A unified framework for combining embedding and definite horn rule reasoning for knowledge graph in-ference. In Proceedings of the 2021 Conference on Em-pirical Methods in Natural Language Processing , pages 9753–9771, 2021. Yung-Sung Chuang, Benjamin Cohen-Wang, Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James R Glass, Shang-Wen Li, and Wen-tau Yih. Selfcite: Self-supervised alignment for context attribution in large language mod-els. In Forty-second International Conference on Machine Learning , 2025. G´ erard Ferrand, Willy Lesaint, and Alexandre Tessier. Ex-planations and proof trees. In International Symposium on Explanation-Aware Computing, ExaCt 2005 , pages 76–85. AAAI Press, 2005. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Re-searching and revising what language models say, using language models. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics (Vol-ume 1: Long Papers) , pages 16477–16508, 2023. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. En-abling large language models to generate text with cita-tions. In Proceedings of the 2023 Conference on Empiri-cal Methods in Natural Language Processing , pages 6465– 6488, Singapore, December 2023. Association for Compu-tational Linguistics. Hangfeng He, Hongming Zhang, and Dan Roth. Socreval: Large language models with the socratic method for reference-free reasoning evaluation. In Findings of the Association for Computational Linguistics: NAACL 2024 ,pages 2736–2764, 2024. Georg Wilhelm Friedrich Hegel and William Wallace. 

Hegel’s logic . Oxford University Press Oxford, 1975. Eran Hirsch, Aviv Slobodkin, David Wan, Elias Stengel-Eskin, Mohit Bansal, and Ido Dagan. Laquer: Localized attribution queries in content-grounded generation, 2025. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceed-ings of the 28th International Conference on Computa-tional Linguistics , pages 6609–6625, 2020. Nan Hu, Jiaoyan Chen, Yike Wu, Guilin Qi, Hongru Wang, Sheng Bi, Yongrui Chen, Tongtong Wu, and Jeff Z. Pan. Can LLMs evaluate complex attribution in QA? automatic benchmarking using knowledge graphs. In Proceedings of the 63rd Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers) , pages 17096– 17118. Association for Computational Linguistics, 2025. Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu, Wei-hong Zhong, Xiachong Feng, Weijiang Yu, Weihua Peng, Duyu Tang, Dandan Tu, et al. Learning fine-grained grounded citations for attributed large language models. In 

Findings of the Association for Computational Linguistics ACL 2024 , pages 14095–14113, 2024. Bin Ji, Huijun Liu, Mingzhe Du, and See-Kiong Ng. Chain-of-thought improves text generation with citations in large language models. Proceedings of the AAAI Conference on Artificial Intelligence , 38(16):18345–18353, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Pro-ceedings of the ACM SIGOPS 29th Symposium on Operat-ing Systems Principles , 2023. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Mea-suring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702 , 2023. Alon Y Levy and Marie-Christine Rousset. Combining horn rules and description logics in carin. Artificial intelligence ,104(1-2):165–209, 1998. Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, and Min Zhang. Improving attributed text generation of large language models via preference learn-ing. In ACL (Findings) , 2024. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattachar-jee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing , pages 2757–2791, 2025. Zhenjiang Mao, Artem Bisliouk, Rohith Reddy Nama, and Ivan Ruchkin. Temporalizing confidence: Evaluation of chain-of-thought reasoning with signal temporal logic. 

arXiv preprint arXiv:2506.08243 , 2025. Mary L McHugh. Interrater reliability: the kappa statistic. 

Biochemia medica , 22(3):276–282, 2012. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic eval-uation of factual precision in long form text generation. In 

Proceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing , pages 12076–12100, 2023. Minh-Vuong Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy Vu, and Gholamreza Haffari. Direct evaluation of chain-of-thought in multi-hop reason-ing with knowledge graphs. In Findings of the Association for Computational Linguistics: ACL 2024 , pages 2862– 2883, 2024. Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, et al. Large language models and knowledge graphs: Op-portunities and challenges. Transactions on Graph Data and Knowledge , 2023. Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large language mod-els and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data Engineering , 36(7):3580–3599, 2024. Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gau-rav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural language generation models. Compu-tational Linguistics , 49(4):777–840, 2023. Vipula Rawte, Ryan A Rossi, Franck Dernoncourt, and Nedim Lipka. Document attribution: Examining citation relationships using large language models. arXiv preprint arXiv:2505.06324 , 2025. Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, and Ido Dagan. Attribute first, then generate: Locally-attributable grounded text generation. In Proceedings of the 62nd Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers) , pages 3309– 3344, 2024. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multi-hop questions via single-hop question composition. Transactions of the Asso-ciation for Computational Linguistics , 10:539–554, 2022. Jiapu Wang, Boyue Wang, Meikang Qiu, Shirui Pan, Bo Xiong, Heng Liu, Linhao Luo, Tengfei Liu, Yongli Hu, Baocai Yin, et al. A survey on temporal knowledge graph completion: Taxonomy, progress, and prospects. arXiv preprint arXiv:2308.02457 , 2023. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh Interna-tional Conference on Learning Representations , 2023. Jiapu Wang, Zheng Cui, Boyue Wang, Shirui Pan, Jun-bin Gao, Baocai Yin, and Wen Gao. Ime: Integrating multi-curvature shared and specific embedding for tempo-ral knowledge graph completion. In Proceedings of the ACM Web Conference 2024 , pages 1954–1962, 2024. Jiapu Wang, Kai Sun, Linhao Luo, Wei Wei, Yongli Hu, Alan W Liew, Shirui Pan, and Baocai Yin. Large language models-guided dynamic adaptation for temporal knowl-edge graph reasoning. Advances in Neural Information Processing Systems , 37:8384–8410, 2024. Yumo Xu, Peng Qi, Jifan Chen, Kunlun Liu, Rujun Han, Lan Liu, Bonan Min, Vittorio Castelli, Arshit Gupta, and Zhiguo Wang. Citeeval: Principle-driven cita-tion evaluation for source attribution. arXiv preprint arXiv:2506.01829 , 2025. Zhichao Yan, Jiapu Wang, Jiaoyan Chen, Xiaoli Li, Jiye Liang, Ru Li, and Jeff Z Pan. Atomic fact decomposition helps attributed question answering. IEEE Transactions on Knowledge and Data Engineering , 2025. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language pro-cessing , pages 2369–2380, 2018. Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, et al. Longcite: Enabling llms to generate fine-grained citations in long-context qa. In Findings of the As-sociation for Computational Linguistics: ACL 2025 , pages 5098–5122, 2025. Xiongtao Zhou, Jie He, Lanyu Chen, Jingyu Li, Haojing Chen, V´ ıctor Guti´ errez-Basulto, Jeff Z Pan, and Hanjie Chen. Miceval: Unveiling multimodal chain of thought’s quality via image description and reasoning steps. In Pro-ceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , pages 10002–10039, 2025. Appendix A More Experimental Results 

Model variance The main experiments in Sect. 5 are based on a single run. We run automated evaluation on 3 random runs of L OGIC SCORE , using GPT-4o outputs on MusiQue as long-form answer. The standard deviations of Conciseness ,

Completeness and Determinateness are 0.3270, 0.4934 and 0.2719 respectively. 

A.1 Performance Compare across Different Hops 

Table 6 illustrates model performance across three metrics (Conciseness, Completeness, and Determinateness) as rea-soning complexity increases from 2-hops to 4-hops. We make the following observation: (1) Performance Degradation. All models show a down-ward trend as hop counts rise, which confirms the inherent difficulty of multi-hop reasoning. In particular, longer infer-ence chains increase the risk of error accumulation and dis-tract the model from the core context. (2) Robustness of Proprietary LLMs. Proprietary LLMs (red line) demonstrate the highest stability. Their perfor-mance decline is notably gradual compared to other groups. For instance, in Completeness, these models drop only ∼10% at 4-hops. This suggests that models like GPT-4o maintain better logical coherence over extended reasoning chains. (3) Brittleness of SFT Models. SFT LLMs (green line) show distinct structural fragility. In simple 2-hop scenar-ios, they perform well and often match open-source models. However, this advantage disappears at 4-hops. SFT models exhibit the steepest decay, particularly in Determinateness. This implies that SFT primarily improves surface-level for-matting rather than the deep deductive capabilities needed for complex synthesis. In addition, Figure 5 echoes the “scaling paradox” by high-lighting the disconnect between surface form and reasoning depth. Metrics like Conciseness and Determinateness satu-rate early (around 7B), suggesting that smaller models ef-fectively master output formatting. In contrast, Complete-ness on the complex MusiQue dataset scales continuously up to 235B. This confirms that while shallow alignment is effi-cient, deep multi-hop reasoning remains an emergent capa-bility strictly tied to model scale. 

# B Implementation Detail 

B.1 Experimental Setups 

To unlock the potential for multihop reasoning, we incorpo-rated CoT prompting into the generation process. Concretely, we prompt LLMs to structure the long-form answer as a se-quential reasoning chain, requiring explicit attribution tokens for every reasoning step. For factual accuracy evaluation, we adopted both AutoAIS [Zhang et al. , 2025] and FACTCORE (FACT) [Min et al. , 2023]. Specifically, for AutoAIS, we re-port the F1 score (denoted as AAIS); for FACTCORE, we calculated the score using the gold documents provided by the original datasets as evidence. To clearly describe the case in our paper, we only provide document-level attribution re-sults rather than sentence-level in the case study section. All experiments were conducted on four NVIDIA H100 (80GB) GPUs, utilizing vLLM [Kwon et al. , 2023] for infer-ence acceleration. We set the sampling temperature to 1.0, while keeping all other hyperparameters at their default val-ues. Regarding the specific implementation of L OGIC SCORE :for Conciseness , we employed GPT-4o-mini to identify re-dundancy. For Completeness , and Determinateness , we em-ployed the generator model itself to perform self-assessment. However, for SFT models incapable of generating short an-swers, we employ a reasoning LLM, GPT-o4-mini, to deduce the answer from the generated content and verify the entail-ment relationship. 

B.2 Details of factual evaluation 

Attribution Quality. The attribution evaluation aims to pro-vide a quantitative measure of the extent to which the gen-erated response is grounded in the retrieved evidence. To en-sure a fair comparison of attribution capabilities across differ-ent LLMs, we adopt the LLM-based evaluation method from [Zhang et al. , 2025], which has demonstrated a higher corre-lation with human judgment. This LLM-based method eval-uates both Precision and Recall .

Precision measures the reliability of the attributions. It is calculated as the average relevance score of all citations pro-vided in the response and can be defined as: Precision = 1

N

> m

X

> i=1
> |Ci|

X

> j=1

Rel (Q, s i, c i,j ), (8) where si represents the i-th statement in the set S decom-posed from the long-form answer LA , Ci denotes the set of attributions associated with si (containing attribution ci,j ), m

is the total number of statements and N is the total number of attributions in the response. The function Rel (·, ·, ·) is im-plemented by prompting an LLM with a specific template to generate a relevance score. 

Recall assesses the evidential completeness of the gener-ated statements. It is computed as the average support score across all statements in P and can be formulated as: Recall = 1

m

> m

X

> i=1

S(si), (9) where S(·) is calculated as: 

S(si) = 

Supp (Q, s i, C i) if Ci̸ = ∅

1 − Need (Q, LA, s i) if Ci = ∅, (10) since si may not contain the attribution Ci = ∅, we pe-nalize the score if an attribution is deemed necessary via Need (·, ·, ·), if si has the citation Ci̸ = ∅, we evaluate if 

Ci support the si through Supp (·, ·, ·). Both Need (·, ·, ·)

and Supp (·, ·, ·) are implemented by a specific prompt which mentioned in [Zhang et al. , 2025]. 

FACTSCORE. We utilize GPT-4o as a replacement for GPT-3.5, which was originally used in FACTSCORE but has since been deprecated. In the evidence retrieval phase, we employed the gold-labeled documents provided in the dataset to evaluate model factuality. All other settings, such as the prompt templates for atomic fact decomposition, remained consistent with the official open-source repository. Models Size 2-hops 3-hops 4-hops                                                                                                                                                                                                                        

> Conc. Comp. Dete. Conc. Comp. Dete. Conc. Comp. Dete.
> Proprietary LLMs
> GPT-4o UNK 38.73 62.70 93.87 25.71 54.74 94.58 20.72 39.51 91.22 GPT-o3 UNK 44.54 68.01 96.50 24.78 48.52 96.72 27.14 50.57 95.98
> GPT-5.1 UNK 33.12 65.44 89.49 22.23 59.84 90.93 15.75 42.49 76.82 Gemini-3-pro UNK 46.13 64.71 94.14 30.25 54.23 96.83 28.81 48.46 89.43 Claude-4.5 UNK 31.84 69.46 90.49 21.09 58.29 90.93 20.83 53.74 84.58 DeepSeek-V3.2 671B 36.20 63.00 87.84 24.10 33.41 87.24 21.22 48.50 79.83 DeepSeek-R1 671B 42.17 63.52 91.70 23.82 48.43 93.98 24.68 41.56 84.42
> Open-source LLMs
> 1B 2.57 4.43 26.39 3.08 4.06 21.77 0.18 0.71 32.62 LLaMA-3.2 3B 4.09 8.92 52.15 3.88 7.42 61.29 0.94 2.84 56.87 8B 15.01 32.23 68.44 11.64 29.94 68.21 5.58 15.35 54.95 LLaMA-3.1 70B 33.13 56.03 88.09 23.71 51.55 88.17 17.12 39.44 76.06 0.6B 8.42 13.45 60.60 5.65 9.52 56.25 3.92 7.05 52.86 1.7B 28.23 36.33 75.41 15.19 25.85 71.69 9.61 18.18 67.53 4B 37.21 53.16 85.49 24.00 46.53 88.22 15.44 27.80 75.34 8B 38.48 58.52 88.07 25.06 50.88 85.59 18.17 33.18 84.09 14B 40.48 56.32 88.06 23.39 45.00 83.53 16.02 29.72 80.66 30B-A3B 39.64 60.11 87.41 25.14 50.29 84.41 20.69 38.67 78.67 Qwen3 235B-A22B 42.86 68.05 96.60 24.05 52.84 95.20 21.98 43.64 91.82 Supervised Fine-tuning LLMs
> FRONT 13B 7.82 16.80 63.30 5.91 13.42 63.26 2.65 5.02 52.05 LongCite 8B 20.97 35.66 79.89 16.39 35.77 75.72 9.35 19.25 65.27
> SelfCite 8B 23.97 40.92 80.73 15.44 34.36 73.59 10.13 19.42 63.64 Table 6: Evaluation results on varying hops across three types LLMs on MusiQue dataset. “Conc.”, “Comp.” and “Dete.” represent “Conciseness”, “Completeness” and “Determinateness”, respectively. The best two results within each LLM type are bold and underlined, respectively.

# C Details of Human Evaluation 

We establish three evaluation criteria for human evaluation: (1) Conciseness (Necessity Check): Annotators assess the logical indispensability of each proposition. A step is marked as redundant if its removal does not disrupt the derivation path from the question to the answer. (2) Completeness (Path Connectivity): Annotators verify the existence of a seamless logical chain. The reasoning is considered complete only if the propositions form a gap-free path connecting the eq and egold .(3) Determinateness (Inference Validity): Annotators per-form a blind re-inference test. The reasoning is deemed defi-nite if the provided propositions (serving as the sole premise) strictly entail the model’s Short Answer SA without ambigu-ity or external knowledge. To effectively evaluate the samples, we develop a annota-tion platform with flask and Vue, and the screenshot of this system are shown in Figure 11 and 12. 

Pearson Correlation Coefficient . To quantify the linear dependence between the continuous Conciseness scores gen-erated by our model and human judgment, we utilize the Pear-son correlation coefficient ( r). Formally, given n samples where xi represents the automated score and yi represents the human-annotated score for the i-th instance, r is calcu-lated as: 

r =

Pni=1 (xi − ¯x)( yi − ¯y)

pP ni=1 (xi − ¯x)2pP ni=1 (yi − ¯y)2 , (11) where ¯x and ¯y denote the means of the automated and hu-man scores, respectively. The coefficient ranges from −1 to 

1, where a value closer to 1 indicates a stronger positive lin-ear correlation, confirming that our Conciseness metric aligns with human perception of redundancy. 

Jaccard Index . For the binary metrics Completeness and Determinateness, we evaluate alignment using the Jaccard In-dex, which measures the similarity between the set of positive instances identified by humans ( A) and those identified by the model ( B). The index is defined as the size of the intersection divided by the size of the union of the sample sets: 

J(A, B ) = |A ∩ B||A ∪ B| = TP TP + FP + FN , (12) where TP, FP, and FN represent true positives, false posi-tives, and false negatives, respectively. This metric is stricter than simple accuracy as it ignores true negatives, focusing solely on the correctness of the positive assertions made by the model relative to human ground truth. 

# D More Case Study 

As shown in Table 7, both open-source LLMs and SFT LLMs have the same logic failures that are described in Figure 3. The large-scale Thinking model, Qwen3-235B-A22B, ex-emplifies a “Broken” reasoning chain in the multi-hop query regarding USS Peril. Despite correctly identifying the U.S. Navy as the operator, the model exhibits significant semantic drift during intermediate reasoning. Rather than linking the Figure 5: scaling Influence in multi-hops. 

destroyer class to the specific acronym requested, the narra-tive deviates into a generalization about “destroyer classes” before making an unjustified leap to “Navy SEALs”. This discontinuity fractures the logical path, preventing a coherent derivation of the “SEAL” definition. Similarly, DeepSeek-R1, despite employing chain-of-thought reinforcement learning, displays flawed logic in the Vivaldi case. While the model accurately identifies “Antonio Vivaldi” as the composer, its derivation of his birthplace is tenuous. Instead of retrieving the biographical fact directly, the model infers “Venice” as the location merely based on the staging of the opera “Orlando” furioso. This reliance on weak associative correlations, rather than direct factual dependen-cies, constitutes a critical failure in the inferential process. These structural disconnects extend to the SFT model, SelfCite, illustrating that task-specific fine-tuning does not guarantee immunity to logical gaps. In the The Bruce Lee Band example, the model neglects explicit entity resolution. It retrieves a generic premise, that a “Person” is part of “The Bruce Lee Band” and immediately pivots to “Mike Park runs Asian Man Records” without formally establishing the iden-tity of that “Person”. This omission of an essential bridg-ing step renders the chain “Broken”, underscoring the persis-tent challenge of implicit entity linking in complex reasoning tasks. 

# E The reliability of L OGIC SCORE 

While human evaluation remains the gold standard, its scal-ability is limited. To ensure L OGIC SCORE serves as a re-liable proxy, we address the inherent stochasticity of LLM-as-a-judge through three structural constraints, distinguishing our approach from generic quality scoring: • Structured Verification vs. Open-ended Scoring. Tra-ditional LLM judges often output arbitrary scores based on abstract criteria (e.g., “helpfulness”). In contrast, LOGIC SCORE transforms the evaluation into a struc-tured Horn Clause verification task. The model is not asked to assess quality subjectively but to perform dis-crete logical operations: identifying whether a premise is structurally redundant ( Conciseness ) or logically dis-connected ( Completeness ). • Constrained Logical Tasks. We decompose global reasoning evaluation into atomic, verifiable steps. For instance, Determinateness does not ask “Is the logic sound?” but specifically validates “Does Premise A strictly entail Conclusion B via Modus Ponens?”. This reduces the cognitive load on the evaluator model and minimizes hallucination. • Reduction of Subjective Bias. By confining the LLM’s role to verifying explicit logical dependencies (Horn Rules), we minimize the influence of the model’s inter-nal preferences (e.g., length bias). As demonstrated in our Human Evaluation (Appendix C), this structurally constrained paradigm yields high agreement with human annotators (e.g., 97% for Determinateness ), confirming that L OGIC SCORE is a robust and objective metric for reasoning evaluation. 

# F Prompt Templates 

The prompt templates used in this paper include: long-form answer generation (Figure 6), logic transformation (Figure 7), triple extraction (Figure 8), entity detection (Figure 9), and short answer generation (Figure 10). Models Question Gold Answer Long-form Answer Error Type 

LLaMA3.1-8B Who voices the char-acter in Spongebob Squarepants who is named after a glow-ing species found in some beaches in Por-tugal? Mr.Lawrence The character in SpongeBob SquarePants is named after a glowing species found in Por-tuguese beaches [12]. Bioluminescent species are mentioned [12]. The character Plankton is voiced by Mr. Lawrence [7]. Mr. Krabs is voiced by Clancy Brown [7]. There are no other characters named after bioluminescent species [15]. 

Broken 

LLaMA3.1-70B Who is the child of the Victim of Ro-mance performer? Chynna Phillips Victim of Romance is an album by Michelle Phillips [16]. Michelle Phillips was with The Ma-mas and the Papas [16]. Chynna Phillips is a member of Wilson Phillips [9]. Wilson Phillips consists of Carnie Wilson, Wendy Wilson, and Chynna Phillips [9]. Carnie Wilson and Wendy Wilson are the daughters of John and Michelle Phillips [9]. 

Circular 

Qwen3-4B What other county does the county where Imperial is lo-cated share a border with? Crockett County Imperial is located in Pecos County, Texas [18]. Lancaster Crossing shares a border with Crockett County [13]. 

Broken 

Qwen3-8B What is the record label for the band which performed Pythons? Kanine Records The album Pythons is the second studio album by Surfer Blood [6]. The album was released in June 2013[6]. The album was released under Sire Records[6]. 

Deviated 

Qwen3-235B-A22B What does seal stand for in the seals of the service operating list of destroyer classes of the operator of USS Peril? Sea, Air, and Land USS Peril was operated by the United States Navy during its commission from 1944 to 1945 [8]. The United States Navy maintains an operational structure that includes various destroyer classes as part of its fleet [16]. The Navy SEALs represent a specialized special operations force [18]. The acronym SEAL stands for Sea, Air, and Land [18]. 

Broken 

DeepSeek-R1 What is the name of the famous bridge in the city where the composer of La costanza trionfante degl’amori e de gl’odii was born? Rialto Bridge The composer of ’La costanza trionfante degl’amori e de gl’odii’ is Antonio Vivaldi [4]. Vivaldi’s opera *Orlando furioso* was staged in Venice [5]. Venice is recognized as Vivaldi’s birthplace. The Rialto Bridge is the oldest bridge spanning the Grand Canal [19]. The Rialto Bridge is a major tourist attraction [19]. 

Broken 

SelfCite What record label did the person who is part of The Bruce Lee Band start? Asian Man Records Person is part of The Bruce Lee Band [1]. Mike Park runs Asian Man Records [1]. Mike Park started a record label in 1989 under the name Dill Records [1]. Asian Man label established May 1996 [1]. 

Broken 

> Table 7: Cases study across different LLMs.

Prompt for Long-form answer generation 

Answer the given question using only the provided documents, ensuring that each reasoning step and conclusion is supported by citation tokens that refer to the specific document ID and sentence number. Each document is structured with: - Document ID (e.g., [1]) - Title - Content in sentences marked with tags ‘ <S# >‘ (e.g., ‘ <S1 >Sentence text <S1 >‘). When forming the answer: 1. **Read all provided documents in full.** 2. **Identify relevant sentences** that help answer the question. 3. **Construct a logical reasoning passage** before presenting the final conclusion, using citation tokens in the form “[DocumentID] <S# >” immediately after the statement they support. 4. **Avoid including any information not supported by the provided documents.** 5. **Don’t copy the original sentence, but paraphrase it based on the meaning.** 6. **Use as many short sentences as possible while retaining the key information.** 7. **The final conclusion should appear strictly after the reasoning section.** # Steps 

1. Parse the documents and break them into discrete facts based on ‘ <S# >‘ markers. 2. Identify which sentences are relevant to answering the question. 3. Write a coherent reasoning chain that explains step-by-step how the evidence leads to the conclusion. Use citation tokens to indicate exact supporting sentences. 4. Always place every step between <STATEMENT > and </STATEMENT >, don’t copy the original sentence, but paraphrase it based on the meaning. 5. Present the final well-formed answer/conclusion **after** the reasoning section. # Output Format 

The response must be structured as follows in plain text: **Reasoning:** [A multi-sentence explanation in logical order, with citations like ‘[1] <S1 >‘ and ‘[2] <S3 > or [1] <S1 >< S2 >‘ immediately after referenced facts.] **Answer:** [A short, concise direct answer to the question.] # Examples 

**Example Input:** Question: Who discovered penicillin? Documents: Document [1] Title: History of Penicillin Content: ‘ <S1 >In 1928, Alexander Fleming made a groundbreaking discovery. <S1 > < S2 > He observed that mold killed bacteria in cultures. <S2 > < S3 >This discovery led to the development of penicillin. <S3 >‘Document [2] Title: Steven Spielberg’s Biography Content: ‘ <S1 >Steven Spielberg is a famous filmmaker. <S1 > < S2 >He directed Jurassic Park. <S2 ><S3 >He was born in 1946. <S3 >‘Document [3] Title: Mark Twain’s Biography Content: ‘ <S1 >Mark Twain was an American author. <S1 > < S2 >He wrote The Adventures of Tom Sawyer. <S2 > < S3 >He was born in 1835. <S3 >‘**Example Output:** Reasoning: <STATEMENT > Alexander Fleming made a discovery [1] <S1 > < /STATEMENT >.

<STATEMENT > He noticed that mold could kill bacteria in cultures, which directly led to the development of penicillin [1] <S2 >< S3 >.</STATEMENT >

Answer: Alexander Fleming. # Notes 

- Always place every step between <STATEMENT > and </STATEMENT >.- Always place all reasoning before the conclusion. - Each claim in reasoning must be directly tied to specific citation tokens from the documents provided. - Do not fabricate citations or information. - If multiple documents contribute, cite them all where relevant. Now, Please answer the following question: 

> Figure 6: Prompt for Long-form answer generation

Prompt for Logic Transformation 

Reconstruct a given complex natural language reasoning process into a formal logical expression using intersection ( ∧). Focus on accurately identifying distinct reasoning components (conditions, premises, assertions) and mapping them to logical conjunctions ( ∧) for “AND”. Preserve the original logical flow, but represent it in a symbolic form. # Steps 

1. **Read and comprehend** the full natural reasoning process provided and question. 2. **Break down** the text into minimal distinct logical statements or propositions. Assign each proposition a short label or variable (e.g., ‘P1‘, ‘P2‘). 3. **Determine relationships** between propositions: - Use ∧ for conditions that must all be true (logical AND). 4. **Reconstruct** the reasoning as a single logical expression combining ∧, preserving original precedence and grouping with parentheses as needed. 5. **Double-check** that the reconstructed expression faithfully mirrors the intent of the natural reasoning pro cess. 6. **Ensure** the logical expression represents by P* rather than a statement. # Output Format 

Output should be in **JSON** with the following structure: 

{

“propositions”: {

“P1”: “[first proposition in plain language]”, “P2”: “[second proposition in plain language]”, “...”: “...” 

},“logical expression”: “P1 ∧ P2 ...” 

}

- Use UTF-8 characters for ∧.- Ensure parentheses are placed to preserve intended order of operations. - Keep propositions short but sufficient for clarity. - Keep the [] and <> corresponding to the original reasoning process. - Ensure the logical expression represents by P* rather than a statement. # Examples 

**Example 1** **Input:** Question: Should we carry the unbrella? *Reasoning process:* ”Step1: If it rains and the ground is wet[1] <S1 >. Step2: if the forecast says there will be rain tomorrow[2] <S4 >. Step3: we should carry an umbrella.” **Output:** 

{

“propositions”: {

“P1”: “It rains[1] <S1 >”, “P2”: “The ground is wet[1] <S1 >”, “P3”: “The forecast says there will be rain tomorrow[2] <S4 >”, 

},“logical expression”: “P1 ∧ P2” 

}

... # Notes 

- Do not omit any condition from the original reasoning. - Maintain a consistent labeling convention for propositions. Now, please reconstruct the following reasoning process into a logical expression: 

> Figure 7: Prompt for Logic Transformation

Prompt for triple extraction in Completeness evaluation 

Perform Named Entity Recognition (NER) to identify and extract all entities from the given input question. Entities may include persons, organizations, locations, dates, times, events, products, etc. Clearly differentiate entity types. Reason through the text by first identifying potential entities, determining their type, then compiling the final extracted results. Conclusions (final extracted entity list) must appear last. # Steps 

1. **Read and Understand** the given question carefully. 2. **Identify Potential Entities** by scanning for proper nouns, temporal expressions, locations, etc. 3. **Determine Entity Types** (e.g., Person, Organization, Location, Date, Time, Event, Product). 4. **List the Entities** with their corresponding types in a structured format. 5. Ensure the output only contains the extracted entities and their types, no additional text. # Output Format 

Output the result as a JSON object without code block formatting, containing: - “entities”: an array of objects, each with: - “text”: the exact entity text from the question - “type”: the entity type 

Example :Input: “Where did Barack Obama give his Nobel Prize lecture in 2009? Output: 

{

“entities”: [ 

{ “text”: “Barack Obama”, “type”: “Person” },

{ “text”: “Nobel Prize”, “type”: “Event” },]

}

# Notes 

- Maintain exact text casing and spelling from the input question. - Do not infer or guess entities beyond what is provided in the text. - If no entities are found, return an empty “entities” array. Now, please extract the entities from the following question: 

> Figure 8: Prompt for triple extraction in Completeness evaluation

Prompt for entity detection in Completeness evaluation 

Extract structured Wikidata-style triples (subject, predicate, object) from the provided sentence. Focus on representing facts and relationships in a clear, machine-readable format. Each triple should follow the structure “(entity, relationship, value)” and be derived through reasoning about the sentence meaning before listing conclusions. # Steps 

1. **Understand the sentence**: Identify entities, relationships, and values explicitly or implicitly mentioned. 2. **Reasoning**: Break down how each relationship is identified, including disambiguation of terms. 3. **Triple extraction**: Formulate each triple in the “(subject, predicate, object)” structure. 4. **Validate**: Ensure all triples are supported by the provided sentence and do not introduce facts not present. # Output Format 

Respond in JSON format, with an array of objects, each containing: - “subject”: the entity being described. - “predicate”: the relationship between subject and object. - “object”: the value or entity connected to the subject. Example schema: [

{

“subject”: “[EntityName]”, “predicate”: “[PredicateName]”, “object”: “[ObjectName]” 

}

]# Examples 

**Example 1** Input: Marie Curie was born in Warsaw. Reasoning: - Identify subject: Marie Curie. - Identify relationship: “place of birth”. - Identify object: Warsaw. Output: [

{

“subject”: “Marie Curie”, “predicate”: “place of birth”, “object”: “Warsaw” 

}

]... # Notes 

- Be precise: only produce triples explicitly inferred from the provided sentence. - Use standardized predicates wherever possible (e.g., “place of birth”, “located in”, “capital of”). - If the sentence contains multiple facts, output each fact as a separate triple in the array. - Do not include any unrelated or speculative facts. Now, please extract the triples from the following reasoning step: 

> Figure 9: Prompt for entity detection in Completeness evaluation

Prompt for short answer generation in Determinateness evaluation 

Generate an answer to a natural question by first producing clear, logical reasoning steps before arriving at the final conclusion. Ensure the reasoning is explicitly stated before the answer, and avoid skipping directly to the conclusion. # Steps 

1. Read and fully understand the natural question provided. 2. Break down the problem or question into smaller parts. 3. Produce detailed, step-by-step reasoning explaining thought process and relevant facts. 4. Ensure reasoning flows logically toward a conclusion. 5. State the final answer only after the reasoning is complete. # Output Format 

Provide the output in the following structure: - Reasoning: [Multi-sentence logical explanation of the path to answer, including any inference steps and key facts] - Answer: [Single sentence or short paragraph conclusion that directly addresses the natural question] # Examples 

Example 1 - Question: [What is the capital of France?] - Reasoning: France is a country in Western Europe. Its most populous and historically significant city, known for landmarks like the Eiffel Tower, is Paris. Paris serves as the political, cultural, and economic center of the country and has been designated as its capital for centuries. - Answer: Paris. # Notes 

- Important: The reasoning section must precede the answer in every case. - Avoid omitting logical steps even for simple questions—always provide full reasoning. - Do not reproduce copyrighted text or song lyrics within reasoning or answer. Now, please generate the reasoning process and answer for the following question: 

> Figure 10: Prompt for short answer generation in Determinateness evaluation

Figure 11: Screen shot of logic transformation evaluation system. Figure 12: Screen shot of logic quality evaluation system.