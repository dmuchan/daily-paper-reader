Title: Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems

URL Source: https://arxiv.org/pdf/2601.15161v1

Published Time: Thu, 22 Jan 2026 02:04:58 GMT

Number of Pages: 21

Markdown Content:
# Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems 

Yinzhu Chen, Abdine Maiga, Hossein A. Rahmani, Emine Yilmaz 

AI Center, University College London, UK 

{yinzhu.chen.20,abdine.maiga.23,hossein.rahmani.22,emine.yilmaz}@ucl.ac.uk 

Abstract 

Large Language Models (LLMs) are increas-ingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework de-signed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authori-tative medical evidence by decomposing re-trieved content into atomic facts and synthe-sizing them with user interaction constraints to form verifiable, fine-grained evaluation cri-teria. Evaluated on HealthBench, our frame-work achieves a Clinical Intent Alignment (CIA) score of 60.12% , a statistically sig-nificant improvement over the GPT-4 O base-line (55.16%). In discriminative tests, our rubrics yield a mean score delta ( μ∆ = 8.658 )and an AUROC of 0.977 , nearly doubling the quality separation achieved by GPT-4 O

baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59 .0% to 

68 .2% ). This provides a scalable and trans-parent foundation for both evaluating and im-proving medical LLMs. The code is available at https://anonymous.4open.science/r/ Automated-Rubric-Generation-AF3C/ .

1 Introduction 

Large Language Models (LLMs) have demon-strated strong capabilities across a wide range of NLP tasks (Zhao et al., 2023; Bommasani et al., 2021). Recent advances in LLMs further expand their potential in medical applications, ranging from differential diagnosis (McDuff et al., 2023) and stepwise clinical reasoning (Brodeur et al., 2024; Savage et al., 2024) to empathetic patient communication (Maida et al., 2024). However, reliable and scalable evaluation of these systems has become a central challenge. Conventional approaches relying on surface-level metrics or multiple-choice benchmarks fail to capture clinical reasoning (Croxford et al., 2025). Expert human assessment better reflects clinical judgment, yet its high cost and limited inter-rater consistency hinder scalability (Arora et al., 2025). To address scalability, LLM-as-a-Judge has been proposed as an automated evaluation paradigm and has shown promising results in general domains (Zheng et al., 2023; Dubois et al., 2024; Rahmani et al., 2025a). However, prior studies show that when evaluation criteria are coarse, LLM-based judging can suffer from bias (Shi et al., 2024; Rahmani et al., 2024), limited reproducibility (Ya-mauchi et al., 2025), and insensitivity to subtle but important differences (Kim et al., 2025). This issue is particularly consequential in medical set-tings: analyses show that medical errors are of-ten embedded in clinically plausible language and seemingly coherent reasoning (Asgari et al., 2025). The detectability of such errors depends critically on the evaluator’s level of domain expertise and the quality of the prompt provided to the model, making them particularly difficult to identify for non-experts and automated evaluation systems (As-gari et al., 2025; Rahmani et al., 2025a; Liu et al., 2024). When undetected, errors in clinical rea-soning or treatment recommendations can delay appropriate care or lead to inappropriate interven-tions, substantially increasing the stakes of evalu-ation failures in medical applications (Mehta and Devarakonda, 2018; Miles-Jay et al., 2023; Xia et al., 2024). These findings highlight that medical LLM evaluation cannot rely solely on implicit or impression-based judgments. A natural mitigation is to adopt fine-grained eval-uation criteria that ground judgments in explicit, 1

> arXiv:2601.15161v1 [cs.CL] 21 Jan 2026 Authoritative Medical Knowledge Sources
> (e.g., CDC PubMed, WHO)
> User Medical Query
> Routing Agent
> (Smart/Fast Strategy)
> Evidence Synthesize Agent
> (De-duplication & Conflict Check)
> Interaction Intent Agent
> (Instruction & Emotion Cue Extraction)
> Medical Fact Agent
> (Atomic Fact Decomposition)
> Reference Board
> (Atomic Facts)
> Rubric Synthesis Agent
> (Initial Criteria Generation)
> Auditing Agent
> (Gap Analysis & Quality Control)
> Structured Medical Rubric
> Stage 1
> Retrieval & Evidence Preparation
> Stage 2
> Dual-Track Constraint Construction
> Stage 3
> Audit & Refinement
> Refinement Loop
> (Triggered by Gap Analysis)

Figure 1: Retrieval-augmented multi-agent framework for medical rubric generation. The pipeline consists of three stages: (1) Retrieval and Evidence Preparation, (2) Dual-Track Constraint Construction and (3) Audit and Refinement, transforming a medical user query into a structured evaluation rubric. 

verifiable clinical requirements. Instead of relying on abstract dimensions, rubric-based evaluation specifies what a high-quality response should in-clude or avoid in concrete clinical terms. Recent work has shown that structured or decomposed eval-uation schemes can improve interpretability and consistency of automated judgments (Liu et al., 2023; Arora et al., 2025). However, medical dia-logue is highly context-dependent: generic rubrics are often too coarse to capture instance-specific clinical priorities, while instance-level rubrics, though more precise, introduce substantial anno-tation cost and stability challenges, limiting their practicality for large-scale evaluation (Kim et al., 2025). To address this gap, we propose a retrieval-augmented multi-agent framework for automat-ically generating instance-specific evaluation rubrics in medical dialogue through three coordi-nated stages. First, Retrieval and Evidence Prepa-ration stage employs a routing strategy to gather and synthesize authoritative medical knowledge into a unified evidence block. Second, a Dual-Track Construction mechanism effectively de-composes this evidence into atomic medical facts (creating a ’Reference Board’) while in parallel ex-tracting interaction intents from the user query. Fi-nally, the Audit and Refinement stage synthesizes these inputs into structured criteria and enforces clinical coverage via an Auditing Agent, which performs a gap analysis against the atomic facts to trigger iterative refinement. This framework effectively combines the scalability of automated systems with the clinical rigor of expert verifica-tion. Our contributions: (1) a retrieval-augmented multi-agent framework for instance-specific medi-cal rubric generation, achieving 60.12% Clinical Intent Alignment (CIA) and significantly outper-forming GPT-4o baseline; (2) enhanced discrimi-native sensitivity, with a mean score delta of 8.658 

and an AUROC of 0.977 , enabling precise detec-tion of subtle, near-miss clinical errors; and (3) actionable rubric-based feedback for refinement, improving downstream response quality by 9.2% 

through controlled, rubric-guided edits. Together, these findings establish that automated, knowledge-grounded rubrics provide a scalable and transparent foundation for both evaluating and improving med-ical language model outputs. 

2 Related Work 

NLU Evaluation. Early work on medical NLP evaluation focused on NLU-style tasks such as MedQA (Jin et al., 2020), MedMCQA (Pal et al., 2021), PubMedQA (Jin et al., 2019), and MMLU (Hendrycks et al., 2021), which primarily test fac-tual medical knowledge through multiple-choice questions. These benchmarks played an important role in assessing domain knowledge, but fail to capture clinical reasoning, contextual understand-ing, or the quality of patient-facing communication 2(Croxford et al., 2025). 

NLG Evaluation. As medical generation tasks emerged, datasets such as MedDialog (Zeng et al., 2020) and COVID-QA (Möller et al., 2020) were evaluated using generic NLG metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Snover et al., 2006). Later embedding-based metrics such as BERTScore (Zhang et al., 2019) and Sentence-BERT (Reimers and Gurevych, 2019) attempted to improve se-mantic alignment. More recent benchmarks such as HealthSearchQA (Singhal et al., 2023), Multi-MedQA (Singhal et al., 2023) and Med-Eval (He et al., 2023) introduced reference-free and human-graded evaluation to better assess open-ended gen-eration. These benchmarks more closely reflect real-world clinical needs by enabling open-ended evaluation, but they are labour-intensive and costly. 

LLM-as-a-Judge. LLM-as-a-judge has emerged as a scalable alternative to human evaluation for open-ended generation tasks (Zheng et al., 2023; Dubois et al., 2024; Rahmani et al., 2025a). In general domains, strong language models correlate reasonably well with human preferences in pair-wise or ranking-based evaluation, as demonstrated by frameworks such as MT-Bench and Chatbot Arena (Zheng et al., 2023), which have become de facto standards for general-purpose LLM compar-ison. Relatedly, AlpacaEval (Dubois et al., 2024) provides a standardised preference-based pipeline that supports rapid benchmarking. To enhance the reliability and interpretabil-ity of automated evaluation, recent research has adopted more rigorous architectures. One signif-icant direction involves structured judging proto-cols that decompose evaluation into explicit crite-ria(e.g., G-Eval Liu et al., 2023, Prometheus 2 Kim et al., 2024). Retrieval-augmented generation (RAG) (Lewis et al., 2020) has been widely adopted to reduce hallucination and improve factual ground-ing. For instance, systems like MiniCheck (Tang et al., 2024) utilize retrieval to verify the fac-tual precision of model outputs against external documents. To further mitigate individual model bias, recent approaches have incorporated multi-agent collaboration strategies, employing mecha-nisms such as debate (Liang and et al., 2023), self-consistency(Wang et al., 2023) and ensemble aggre-gation (Rahmani et al., 2025b) to yield more robust judgments. However, most paradigms primarily uti-lize agents and retrieval to assess responses based on fixed or latent standards. Even when structurally explicit, these approaches remain content-generic, relying on prompts or model parameters, and re-main sensitive to instruction phrasing and framing, particularly in high-stakes settings (Arroyo et al., 2024; Thomas et al., 2024). This motivates ap-proaches that make evaluation criteria explicit and structured, rather than relying solely on latent judge preferences. 

Rubric-based LLM-as-a-Judge. To address opacity, recent work has introduced fine-grained rubrics to guide LLM-as-a-judge evaluation. LLMEval-Med (Zhang et al., 2025) employs checklist-style criteria specific to each dialogue, while HealthBench (Arora et al., 2025) pro-vides conversation-specific rubrics covering axes such as accuracy, completeness, communication, and safety. While these fain-grained rubric-based frameworks improve transparency and multi-dimensionality, they rely on costly, manual expert construction that fails to scale with evolving clin-ical knowledge. Although SedarEval (Fan et al., 2024) explores automated rubric generation, it fo-cuses on general domains and lacks the rigorous clinical grounding required for medical dialogue. Because it lacks the retrieval mechanisms necessary to access specific medical protocols, it is unsuitable for verifying clinical correctness, and thus we do not use it as a baseline. 

Summary and Positioning. Existing research on automated medical LLM evaluation either relies on expert-authored rubrics, which ensure accuracy but are costly and rigid, or on generic rubric-based judges, which scale easily but lack transparency and grounding. Our work integrates these direc-tions by introducing a knowledge-grounded, multi-agent RAG framework for generating instance-specific rubrics in medical dialogue evaluation, combining interpretability, factual grounding, and scalability within a unified paradigm. 

3 Methodology 

3.1 Problem Formulation 

We formalize medical rubric generation as a multi-stage mapping across information spaces, opti-mized via a multi-agent framework (Fig. 1). Given a user query Q and an authoritative medical knowl-edge base K, we aim to produce a structured evalu-ation rubric R as follows: 

R = {(cj , a j , w j )}nj=1 ,

3where cj is the criterion, aj the evaluation axis, and wj ∈ Z ∩ [−10 , 10] the clinical weight. Adetailed reference of all mathematical notations, data structures, and agent operators is provided in Table 5 and Table 6. The pipeline executes in three sequential stages: 

Stage 1: Retrieval & Evidence Preparation (R, S). This stage maps the user query Q to the evidence space E. First, the Routing Agent ( R)

transforms the user query Q into a set of optimized search queries: 

Qsearch = R(Q) (1) These queries are used to retrieve raw candidates from K. Then, the Evidence Synthesis Agent ( S)

aggregates the retrieved results, which are priori-tized by a reranker agent to ensure clinical author-ity, into a coherent evidence block: 

E = S(Qsearch , K) (2) 

Stage 2: Dual-Track Constraint Construction (D, T ). To ensure the rubric captures both factual accuracy and conversational quality, we decom-pose the evidence and the query into objective D

and subjective T dimensions. The Medical Fact Agent ( D) distills and filters evidence E into a set of atomic facts F (the Reference Board). In par-allel, the Interaction Intent Agent ( T ) extracts communication constraints I from user query con-text: 

F = D(E), I = T (Q, E ). (3) 

Stage 3: Audit & Refinement ( Φ, A). The Rubric Synthesis Agent ( Φ) maps facts F and intent I to an initial draft rubric: 

Rinit = Φ( F, I, Q ) (4) To ensure validity, the Auditing Agent ( A) per-forms a structured audit by cross-referencing Rinit 

against the ground truth facts F and I. It ex-ecutes a process that first identifies and supple-ments missing details (Gap Analysis), then filters out unsupported hallucinations or irrelevant con-straints (Quality Control), and finally merges to final rubrics ( R). 

R = A(Rinit , F, I ) (5) 

3.2 Stage 1: Retrieval & Evidence Preparation 

This stage implements the synthesis operator S,aiming to construct a hierarchical retrieval pipeline that balances reasoning depth and efficiency. 

Routing Agent (Smart–Fast Strategy). To opti-mize the balance between deep reasoning and com-putational efficiency within the retrieval pipeline, we adopt a Smart–Fast configuration. Motivated by MasRouter (Yue et al., 2025) and DiSRouter (Zheng et al., 2025), which show that delegating tasks across models with different capacities can balance efficiency and performance, we route clin-ically complex queries to a high-capacity model (smart) for intent identification and targeted query generation, while retrieved candidates are reranked by a lightweight model (fast) using authority-aware criteria. Retrieval is strictly constrained to author-itative medical domains K (see Table 4 in Ap-pendix), ensuring that non-professional or low-credibility content is filtered out at the source. 

Evidence Synthesis Agent. The Evidence Syn-thesis Agent consolidates multi-source retrieved content into a unified evidence block E. Through cross-checking and de-duplication, this stage re-solves conflicts across sources and explicitly ex-tracts safety-critical signals, such as clinical con-traindications and red-flag warnings. This process reduces hallucination risks for downstream compo-nents by establishing a reliable clinical grounding. 

3.3 Stage 2: Dual-Track Constraint Construction 

Inspired by prior medical multi-agent systems that emphasize functional role decomposition and co-ordinated collaboration (Yang et al., 2024; Zhang et al., 2024; Li et al., 2024a), we design a Dynamic Atomic Dual-Track scheme. This design indepen-dently constructs two complementary constraint views to avoid interference when handling complex clinical evidence with a single monolithic prompt. 

Medical Fact Agent (Atomic Fact Decomposi-tion). The Medical Fact Agent decomposes the synthesized evidence E into a dynamic set of atomic medical facts F (the Reference Board), in-cluding declarative assertions, contraindications, and safety-critical red flags. This claim-level de-composition is motivated by prior work on factual verification (Welleck et al., 2023; Li et al., 2024b), and provides a structured source of truth for subse-quent auditing. 

Interaction Intent Agent. In parallel, the Inter-action Intent Agent applies the operator T to ex-tract explicit instructions and implicit communica-tion cues from the user query. The system further 4identifies medically necessary but missing contex-tual variables, ensuring that the resulting rubric incorporates context awareness. This design fol-lows prior work showing that evaluation should be conditioned on user-defined criteria rather than inferred user profiles (Kim et al., 2024). 

3.4 Stage 3: Audit & Refinement 

The final stage compiles constraints from both tracks into a finalized structured medical rubric, emphasizing coverage enforcement through closed-loop auditing. 

Rubric Synthesis Agent. The Rubric Synthesis Agent applies the operator Φ to generate an initial rubric Rinit , mapping medical facts to the accu-racy and completeness dimensions, and interaction constraints to the communication quality dimen-sion. Rubric generation follows fixed structural constraints, assigning high penalty weights wj to safety violations. 

Auditing Agent & Refinement Loop. To miti-gate omissions introduced by single-pass genera-tion, the Auditing Agent performs a gap analysis over Rinit by aligning each rubric item against the atomic facts in the Reference Board F . Any uncov-ered clinical constraint triggers a Refinement Loop (illustrated by the red dashed arrow in Figure 1) to revise the rubric. This process is inspired by the Reflexion paradigm (Shinn et al., 2023), but is specialized to enforce medical safety and factual coverage rather than purely linguistic quality. The final output is a structured, clinically au-ditable rubric Rfinal that balances factual rigor with communication quality. A concrete illustration of a generated rubric is shown in Table 7 in Appendix. 

4 Experiments 

4.1 Datasets 

We evaluate our framework primarily on Health-Bench (Arora et al., 2025), a public benchmark of medical dialogues paired with physician-authored, instance-specific rubrics. Each dialogue consists of a patient query and an ideal medical response reviewed by physicians, along with a couple of fine-grained criteria that assess accuracy, completeness, communication quality, instruction following and context awareness. An example from HealthBench is shown in Figure 4. To ensure a consistent and rigorous evaluation setting, we curated a subset of 254 diverse medical queries by filtering the HealthBench dataset based on specific criteria: We selected English dialogues and each sample is paired with 8-10 physician-authored gold criteria ( cj ) to control for evaluation complexity. To ensure multi-dimensional assess-ment, we specifically select instances that encom-pass at least three distinct evaluation axes ( aj ). This resulted in a test set of ~2.5k rubric items. These physician-authored rubrics serve as the gold stan-dard G for calculating the Clinical Intent Align-ment (CIA) metric defined in Section 4.4. 

4.2 Implementation Detail 

All agent prompts are provided in the Appendix B. 

4.2.1 Generation Model Configuration. We use API-based inference with a tiered model configura-tion. Llama-3.3-70B-Instruct performs intent routing, evidence synthesis, atomic fact extraction/filtering, and rubric auditing. 

Llama-3.1-8B-Instant handles result reranking. Processing the full dataset took approximately two hours; per-instance took about 20 to 30 seconds with latency depends on the volume of retrieved evidence. For each query, 3–5 search queries are generated for the Tavily Search API. Raw text is extracted via the Trafilatura library, and the evidence block E is synthesized from the top-5 reranked snippets. 

4.2.2 Evaluation Near-Miss Construction. For discriminative evaluation, we adopt a near-miss pairwise setting. Each query is associated with a reference answer 

Xref and a candidate Xcand that differs by exactly one critical clinical fact, with all other content held constant. This controlled setup tests whether evalu-ation rubrics enable judge models to identify subtle yet clinically significant errors. 

Judging Protocol. We use 

Llama-3.3-70B-Instruct as the judge with temperature T = 0 .0. For each pair, we perform 

N = 3 trials with order swapping (6 runs total) and determine the final decision by majority vote. 

4.3 Baselines 

We compare our approach with two representa-tive rubric-based baselines that are commonly used in LLM evaluation settings, differing in whether rubrics are instance-specific and how they are con-structed. 5Generic Rubric. We include a generic rubric that applies a fixed set of high-level evaluation criteria across all medical queries (shown in Ta-ble 8 in Appendix). This rubric assesses responses along broad dimensions such as accuracy, com-pleteness, and communication quality, without in-corporating query-specific medical facts or safety considerations. Similar task-agnostic rubrics are widely adopted in prior benchmarking and evalua-tion work, where a single rubric is used to assess responses across diverse instances (Chiang et al., 2024; Singhal et al., 2025). This baseline serves to evaluate the benefit of generating instance-specific rubrics. 

GPT-4o Rubric. This baseline represents the “one-step generation” approach where a large lan-guage model (GPT-4o) is prompted to produce an evaluation rubric directly from the user query with-out external retrieval or intermediate decomposi-tion (Farzi and Dietz, 2024; Hashemi et al., 2024), reflecting a common practice in recent LLM-based evaluation pipelines where rubrics are produced end-to-end from the task description. 

No Rubrics (None). In addition, for experiments on discriminative ability, we consider a No-Rubric 

setting in which the judge model directly compares candidate responses without being provided with any explicit evaluation rubric. This setting is used solely as a reference point to contextualize the im-pact of rubric-based evaluation. 

4.4 Evaluation Metrics 

We evaluate the generated rubrics based on their clinical coverage and discriminative sensitivity. 

4.4.1 Scoring and Bias Mitigation 

Each generated rubric R induces a scoring function 

V (X) = 

> n

X

> j=1

wj · y(X, c j ),

where y(X, c j ) ∈ { 0, 1} is a binary indicator func-tion determining if response X satisfies criterion 

cj .The weights wj are discrete integers in the range 

[−10 , 10] which are assigned by the Rubric Syn-thesis Agent based on predefined clinical severity tiers (shown in Table 14). LLM judges have been found to exhibit position bias, meaning their judgments can depend on the order in which options are presented rather than               

> Rubric CIA (%) 95% CI p-value
> Generic 19.66 [18.05, 21.37] <0.001 ∗
> GPT-4o 55.16 [53.12, 57.18] Ref.
> Ours 60.12 [58.10, 62.11] <0.001 ∗
> Table 1: Clinical Intent Alignment (CIA) of different rubric generation methods on HealthBench. Statistical significance is assessed using McNemar’s test.

Rubric Win Tie Score ∆ AUROC 

None 0.185 0.787 0.832 0.794 Generic 0.278 0.704 1.878 0.920 GPT-4o 0.304 0.686 4.972 0.975 

Ours 0.382 0.618 8.658 0.977  

> Table 2: Discriminative performance of LLM-as-a-judge under different rubric settings on the micro-perturbed pair dataset.

just response quality (Shi et al., 2024). To eliminate this, we calculate the Average Score Delta ∆V over 

N trials with order swapping: 

∆V = 12N

> N

X

> k=1

h Vk(Xref | 1st ) − Vk(Xcand | 2nd )

+  Vk(Xref | 2nd ) − Vk(Xcand | 1st )i

where V (X|pos ) denotes the score assigned to re-sponse X when it appears at position ‘pos’ in trial 

k.

4.4.2 Clinical Intent Alignment (CIA) 

We assess clinical coverage by comparing R

against expert-authored gold keypoints G =

{gi}|G|

> i=1

. we employ an LLM-based judge to verify semantic presence. For each gold keypoint gi, the evaluator determines whether the underlying medi-cal intent is effectively captured by the generated criteria in R. The CIA score is defined as CIA = 1

|G|

> |G|

X

> i=1

⊮ V(gi, R ) → Detected ,

where V denotes the LLM verification function. The indicator function equals 1 if the judge con-firms that the rubric contains the specific clinical concept described in gi (regardless of phrasing vari-ations), and 0 otherwise. 

4.4.3 Discriminative Sensitivity 

Using a dataset of M response pairs (Xref , X cand ),where Xref is a high-quality reference and Xcand 

6None Generic GPT-4o Ours       

> 0
> 2
> 4
> 6
> 8
> 10
> Mean Score Delta ( Δ)
> (A) Evaluation Sensitivity
> None Generic GPT-4o Ours
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Proportion
> (B) Outcome Distribution
> Win (REF)
> Tie
> Loss
> None Generic GPT-4o Ours
> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> 1.05
> AUROC Score
> (C) Discrimination Ability

Figure 2: Discrimination analysis on the micro-perturbed dataset: (A) Mean score difference between reference and perturbed responses, (B) outcome distribution (win/tie/lose), and (C) AUROC across rubric settings. 

is a perturbed variant (4.2.2), we report three mea-sures. 

Outcome Distribution. For each pair 

(Xref , X cand ), the final decision D ∈{Win , Tie , Loss } is obtained by majority vote on the sign of ∆V , where "Win" indicates the reference Xref scored higher than the candidate 

Xcand . We report the empirical probability 

P (D = Win ).

Mean Score Delta ( μ∆). While win rate mea-sures binary preference, the Mean Score Delta quantifies the magnitude of the quality separation. It is calculated as the average score difference across all pairs: μ∆ = 1

> M

P ∆V i. A larger posi-tive μ∆ indicates that the rubric enables the judge to distinguish the superior response with a wider margin. 

Ranking Accuracy (AUROC). We calculate the AUROC over score deltas ∆V to estimate the prob-ability that the rubric correctly ranks Xref above 

Xcand :AUROC = P (∆ V > 0 | Xref ≻ Xcand ).

4.4.4 Statistical Significance 

We estimate metric variability using non-parametric bootstrapping with 1,000 resamples. The 95% confidence interval is defined by the percentiles of the resampled distribution: 

[θlow , θ high ] = Perc (B, α 

> 2

), Perc (B, 1 − α 

> 2

).

where B = {¯x∗ 

> j

}Mj=1 denote the bootstrap samples. 

5 Results and Analysis 

We report quantitative results on three aspects of rubric quality: (i) clinical coverage of key medical intents, (ii) discriminative ability under near-miss conditions, and (iii) downstream effectiveness for response refinement. 

5.1 Clinical Coverage 

Table 1 reports Clinical Intent Alignment (CIA), measuring the extent to which generated rubrics cover physician-authored medical key points. Generic task-agnostic rubrics achieve very low cov-erage, indicating that they fail to capture instance-specific clinical content. Direct LLM-generated rubrics substantially improve coverage, while our method achieves the highest CIA among all ap-proaches. Compared to GPT-4o-generated rubrics, our rubrics yield a consistent improvement of +4.96 CIA points. Although the absolute gain is moder-ate, McNemar’s test on paired coverage decisions shows statistically significant differences, indicat-ing that conditioning rubric generation on retrieved medical evidence improves coverage of clinically relevant information. 

5.2 Discriminative sensitivity under Near-Miss Conditions 

We next evaluate whether generated rubrics im-prove the discriminative sensitivity of LLM-as-a-judge under near-miss conditions, where paired responses differ by only a single critical clinical fact. Table 2 summarizes win rate, tie rate, mean score difference, and AUROC. Without rubrics, the judge exhibits a high tie rate and limited score separation. Providing rubrics consistently improves discriminative performance across all metrics. Among all methods, our rubrics achieve the largest mean score difference and the highest AUROC, indicating stronger separation be-7Method Base (%) Refined (%) ∆ ↑ Rel. Imp. p-value 

Reference Rubrics 58.9 74.1 +15.2 +54.8% < 0.001 ∗

Self-Critique 59.0 64.4 +5.5 +25.0% < 0.001 ∗

GPT-4o Rubrics 59.0 65.7 +6.7 +33.2% < 0.001 ∗

Our Rubrics 59.0 68.2 +9.2 +35.8% < 0.001 ∗

> Table 3: Downstream response refinement performance under different rubric guidance. Reference rubrics serve as an oracle upper bound.

tween reference and perturbed responses. Although absolute win rates remain below 0.4 due to the near-identical nature of paired responses, Figure 2 shows that rubric guidance primarily im-proves discrimination by amplifying subtle but clin-ically meaningful score differences, rather than forcing hard win–lose decisions. 

6 Rubric-Guided Response Refinement 

Beyond evaluation, we investigate whether instance-specific, fine-grained rubrics can serve as 

structured feedback to improve medical responses through controlled refinement. We study the fol-lowing question: Can instance-specific rubrics im-prove response quality via rubric-guided refine-ment? This setting reflects a realistic deployment scenario, where an initial response is refined with-out re-generation. 

6.1 Task Setup and Baselines 

We utilize a subset of 254 medical queries from HealthBench. For each query, we generate a fixed base response using Llama-3.1-8B-Instant 

(T = 0.7, top_p = 0.9). Base responses are frozen across all methods, and no re-sampling or re-generation is performed, ensuring that any im-provement arises solely from refinement. In addition to GPT-4o generated rubrics (see Sec-tion 4.3), we extend our comparison to include two critical control settings that establish the perfor-mance bounds: Self-Critique (No-Rubric Base-line) , which measures intrinsic self-correction ca-pability (Madaan et al., 2023). In Self-Critique ,the model is prompted to identify weaknesses and propose improvements based solely on its internal knowledge, without access to any external rubric. This serves as a lower-bound control to verify the necessity of explicit guidance. Reference Rubric (Oracle Upper Bound). , which utilizes the ex-pert physician-authored rubrics provided by Health-Bench to guide the refinement. Since these repre-sent the ground truth standard, this setting serves as an Oracle , indicating the theoretical maximum performance achievable when ideal guidance is pro-vided. 

6.2 Refinement Mechanism 

To transform a scoring rubric into an actionable editing tool, we employ a two-step Critique-then-Refine protocol: 

Rubric-to-Critique Transformation. Given a user query Q, base response Xbase , and rubric, we use an evaluator model (Llama-3.3-70B) reviews the base response Xbase against the provided rubric 

R to output a structured Edit Plan (JSON). This edit plan explicitly lists prioritized actions (e.g., "ADD warning about drug interaction", "REMOVE unsupported claim") while strictly adhering to the rubric’s criteria. 

Constraint-Guided Refinement. An editor model (Llama-3.1-8B) executes the Edit Plan to produce Xrefined . We enforce strict behavioral constraints: the editor must revise the response by applying only the instructions in the plan. It is explicitly prohibited from introducing new medical facts or definitive diagnoses not present in the original context, thereby preventing refinement-induced hallucinations. 

6.3 Evaluation Protocol 

Refinement is strictly decoupled from evaluation. Original and refined responses are assessed inde-pendently by an external LLM judge, ensuring that observed gains can be causally attributed to rubric-guided refinement. The judge assesses the responses based on the gold-standard physician-authored criteria provided by HealthBench, rather than the automatically generated rubrics used for refinement. 8Self-Critique GPT-4o rubric Our rubric Reference rubric  

> 30
> 40
> 50
> 60
> 70
> 80
> Score (%)  64.4 65.7
> 68.2
> 74.1

HealthBench Downstream Performance 

> Communication quality
> Instruction following
> Accuracy
> Context awareness
> Completeness

Figure 3: Dimension-wise analysis of downstream response refinement under different rubric settings, including overall performance trends and trade-offs across evaluation dimensions. 

6.4 Response Refinement Results 

Finally, we assess whether higher-quality rubrics translate into better downstream response refine-ment. Table 3 reports performance improvements when responses are revised under different rubric guidance. Rubric-guided refinement consistently outper-forms self-critique without rubrics. Our rubrics yield the largest improvement among automatic methods and substantially close the gap to physician-authored reference rubrics, which serve as an oracle upper bound. Figure 3 further illustrates dimension-wise ef-fects. Improvements are most pronounced in fac-tual dimensions such as accuracy and completeness, while gains in communication-related dimensions are more modest. Compared to reference rubrics, our rubrics achieve a better balance between factual improvement and communication quality, suggest-ing reduced trade-offs between information cover-age and readability. 

7 Conclusion 

We presented a retrieval-augmented, multi-agent framework for automatically generating instance-specific evaluation rubrics for medical dialogue. By grounding rubric construction in authoritative medical evidence and explicitly separating clinical constraints from interaction-level requirements, our approach produces structured, interpretable rubrics that better reflect case-specific clinical priorities. Empirical results on HealthBench show that the generated rubrics achieve stronger clinical cover-age of gold key points and improved discrimina-tive ability in distinguishing high-quality responses from minimally flawed alternatives, compared to generic or directly generated rubrics. Beyond evalu-ation, we further demonstrate that instance-specific rubrics can function as actionable feedback, en-abling controlled response refinement without re-generation. Together, these findings suggest that automatic rubric generation offers a scalable and transpar-ent foundation for medical LLM evaluation, bridg-ing the gap between fine-grained clinical assess-ment and large-scale automated judging. We hope this work encourages further exploration of rubric-centered evaluation and its role in both assessing and improving medical language models. 

Limitations 

Our study is subject to several limitations. First, experiments are conducted on HealthBench and focus on English medical dialogue, and further val-idation is needed to assess generalization across other datasets, languages, and clinical special-ties. Second, the framework relies on retrieval from a curated set of authoritative medical sources, which may limit coverage for emerging or less-documented clinical scenarios. Finally, while we demonstrate downstream response refinement in a controlled, single-step setting, more flexible or 9interactive refinement strategies remain to be ex-plored in future work. 

References 

Rahul K. Arora, Jason Wei, Robert S. Hicks, Peter Bowman, Joaquin Quiñonero-Candela, Fotios Tsim-pourlas, and Karan Singhal. 2025. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775 .A. Arroyo, R. Aggarwal, S. Mohapatra, A. Chia, and M. Ghassemi. 2024. Open (clinical) llms are sensi-tive to instruction phrasings. Computing Research Repository , arXiv:2407.09429. Elham Asgari, Nina Montaña-Brown, Magda Dubois, Saleh Khalil, Jasmine Balloch, Joshua Au Yeung, and Dominic Pimenta. 2025. A framework to assess clin-ical safety and hallucination rates of llms for medical text summarisation. npj Digital Medicine , 8(1):274. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Veronika Buch, Dal-las Card, Rodrigo Castellon, Niladri S. Chatterji, Anthony Chen, Kathleen A. Creel, Jared Q. Davis, Dorottya Demszky, and 3 others. 2021. On the oppor-tunities and risks of foundation models. Computing Research Repository , arXiv:2108.07258. Paul G. Brodeur, Thomas A. Buckley, Ziad Kanjee, Ee Goh, Edward B. Ling, Priyanka Jain, Steven Cabral, Rabih-E. Abdulnour, Alexander Haimovich, Joseph A. Freed, and 1 others. 2024. Superhuman performance of a large language model on the rea-soning tasks of a physician. Computing Research Repository , arXiv:2412.10849. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anasta-sios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E Gonzalez, and 1 others. 2024. Chatbot arena: An open platform for evaluating llms by human pref-erence. In Forty-first International Conference on Machine Learning .Emma Croxford, Yanjun Gao, Nicholas Pellegrino, Karen Wong, Graham Wills, Elliot First, Frank Liao, Cherodeep Goswami, Brian Patterson, and Majid Af-shar. 2025. Current and future state of evaluation of large language models for medical summarization tasks. Npj health systems , 2(1):6. Yann Dubois, Frank Xu, Zhen Li, Susan Wang, and Percy Liang. 2024. Alpacaeval-med: Automatic eval-uation of medical dialogue using LLM-as-a-judge. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL) .Zhiyuan Fan, Weinong Wang, Debing Zhang, and 1 others. 2024. Sedareval: Automated evaluation using self-adaptive rubrics. In Findings of the Association for Computational Linguistics: EMNLP 2024 , pages 16916–16930. Naghmeh Farzi and Laura Dietz. 2024. Pencils down! automatic rubric-based evaluation of retrieve/gener-ate systems. In Proceedings of the 2024 acm sigir international conference on theory of information retrieval , pages 175–184. Helia Hashemi, Jason Eisner, Corby Rosset, Benjamin Van Durme, and Chris Kedzie. 2024. Llm-rubric: A multidimensional, calibrated approach to automated evaluation of natural language texts. arXiv preprint arXiv:2501.00274 .Zexue He, Yu Wang, An Yan, Yao Liu, Eric Y. Chang, Amilcare Gentili, Julian McAuley, and Chun-Nan Hsu. 2023. Medeval: A multi-level, multi-task, and multi-domain medical benchmark for language model evaluation. Computing Research Repository ,arXiv:2310.14088. Dan Hendrycks and 1 others. 2021. Measuring massive multitask language understanding. In Proceedings of ICLR .Qiao Jin, Bhuwan Dhingra, William W. Cohen, and Xinghua Lu. 2019. Pubmedqa: A dataset for biomed-ical research question answering. In Proceedings of EMNLP .Qiao Jin and 1 others. 2020. What disease does this patient have? a large-scale open-domain question an-swering dataset from medical exams. In Proceedings of ACL .Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024. Prometheus 2: An open source language model specialized in evaluating other language mod-els. In Proceedings of the 2024 Conference on Em-pirical Methods in Natural Language Processing .Yubin Kim, Hojin Jeong, Shiqi Chen, Stephen S. Li, Ming Lu, Khaled Alhamoud, and 1 others. 2025. Medical hallucinations in foundation models and their impact on healthcare. Computing Research Repository , arXiv:2503.05777. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-täschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Infor-mation Processing Systems (NeurIPS) , volume 33, pages 9459–9474. Ming Li, Rui Zhang, and Yifan Wang. 2024a. Triageagent: A multi-agent framework for clinical triage. In Findings of the Conference on Empirical Methods in Natural Language Processing .

10 Xinyu Li and 1 others. 2024b. Minicheck: Efficient fact-checking of llms on grounding documents. In 

Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing .Percy Liang and et al. 2023. Let’s debate! a multi-agent framework for evaluating llm reasoning. In Advances in Neural Information Processing Systems .Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out , pages 74–81. Jie Liu, Wenxuan Wang, Zizhan Ma, Guolin Huang, Yi-hang SU, Kao-Jung Chang, Wenting Chen, Haoliang Li, Linlin Shen, and Michael Lyu. 2024. Medchain: Bridging the gap between llm agents and clinical practice through interactive sequential benchmarking. 

arXiv preprint arXiv:2412.01605 .Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human align-ment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing .Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, and 1 others. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems , 36:46534–46594. E. Maida, M. Moccia, R. Palladino, G. Borriello, G. Affinito, M. Clerico, A. M. Repice, A. Di Sapio, R. Iodice, A. L. Spiezia, and 1 others. 2024. Chatgpt vs. neurologists: A cross-sectional study investigat-ing preference, satisfaction ratings and perceived em-pathy in responses among people living with multiple sclerosis. Journal of Neurology , pages 1–10. Daniel McDuff, Mike Schaekermann, Tu Tu, and 1 oth-ers. 2023. Towards accurate differential diagnosis with large language models. Computing Research Repository , arXiv:2312.00164. Neil Mehta and Murthy V Devarakonda. 2018. Machine learning, natural language programming, and elec-tronic health records: The next step in the artificial intelligence journey? Journal of Allergy and Clinical Immunology , 141(6):2019–2021. Arianna Miles-Jay, Evan S Snitkin, Michael Y Lin, Teppei Shimasaki, Michael Schoeny, Christine Fukuda, Thelma Dangana, Nicholas Moore, Sarah E Sansom, Rachel D Yelin, and 1 others. 2023. Lon-gitudinal genomic surveillance of carriage and trans-mission of clostridioides difficile in an intensive care unit. Nature Medicine , 29(10):2526–2534. Timo Möller and 1 others. 2020. Covid-qa: A question answering dataset for covid-19. In Proceedings of the EMNLP Workshop on COVID-19 NLP .Ankit Pal and 1 others. 2021. Medmcqa: A large-scale multi-subject multi-choice dataset for medical do-main question answering. In Proceedings of NeurIPS Datasets and Benchmarks .Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evalu-ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu-tational Linguistics , pages 311–318. Hossein A. Rahmani, Nick Craswell, Emine Yilmaz, Bhaskar Mitra, and Daniel Campos. 2024. Synthetic test collections for retrieval evaluation. In Proceed-ings of the 47th International ACM SIGIR Confer-ence on Research and Development in Information Retrieval , pages 2647–2651. Hossein A. Rahmani, Varsha Ramineni, Emine Yilmaz, Nick Craswell, and Bhaskar Mitra. 2025a. Towards understanding bias in synthetic data for evaluation. In Proceedings of the 34th ACM International Con-ference on Information and Knowledge Management ,pages 5166–5170. Hossein A. Rahmani, Emine Yilmaz, Nick Craswell, and Bhaskar Mitra. 2025b. Judgeblender: Ensem-bling automatic relevance judgments. In Companion Proceedings of the ACM on Web Conference 2025 ,WWW ’25, page 1268–1272, New York, NY, USA. Association for Computing Machinery. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP) and the 9th International Joint Conference on Nat-ural Language Processing (IJCNLP) , pages 3982– 3992. Association for Computational Linguistics. Thomas Savage, A. Nayak, R. Gallo, E. Rangan, and J. H. Chen. 2024. Diagnostic reasoning prompts reveal the potential for large language model in-terpretability in medicine. NPJ Digital Medicine ,7(1):20. Lin Shi, Chiyu Ma, Wenhua Liang, Weicheng Ma, and Soroush Vosoughi. 2024. Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by LLMs. Noah Shinn, Benjamin Labash, Ashwin Gopinath, and Karthik Narasimhan. 2023. Reflexion: Language agents with verbal reinforcement learning. In Ad-vances in Neural Information Processing Systems .Karan Singhal, Shravya Azizi, Tu Tu, Shrimai S. Mah-davi, Jiahui Wei, Hye Won Chung, Neil Scales, Af-saneh Tanwani, Hilary Cole-Lewis, Scott Pfohl, and 1 others. 2023. Large language models encode clinical knowledge. Nature , 620(7972):172–180. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen R Pfohl, Heather Cole-Lewis, and 

11 1 others. 2025. Toward expert-level medical ques-tion answering with large language models. Nature Medicine , 31(3):943–950. Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of AMTA .Liyan Tang, Philippe Laban, and Greg Durrett. 2024. Minicheck: Efficient fact-checking of llms on ground-ing documents. arXiv preprint arXiv:2404.10774 .Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2024. Large language models can ac-curately predict searcher preferences. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval ,pages 1930–1940. Xuezhi Wang, Jason Wei, Dale Schuurmans, and Quoc Le. 2023. Self-consistency improves chain of thought reasoning in language models. In Interna-tional Conference on Learning Representations .Sean Welleck and 1 others. 2023. Fine-grained atomic evaluation of factual precision in long form text gen-eration. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing .Wei Xia, Dandan Li, Wenguang He, Perry J Pick-hardt, Junming Jian, Rui Zhang, Junjie Zhang, Ruirui Song, Tong Tong, Xiaotang Yang, and 1 others. 2024. Multicenter evaluation of a weakly supervised deep learning model for lymph node diagnosis in rectal cancer at mri. Radiology: Artificial Intelligence ,6(2):e230152. Yusuke Yamauchi, Taro Yano, and Masafumi Oyamada. 2025. An empirical study of llm-as-a-judge: How design choices impact evaluation reliability. arXiv preprint arXiv:2506.13639 .Zhen Yang, Yichi Zhang, Junjie Chen, and Zhiyuan Liu. 2024. Medagents: Large language models as collaborative medical experts. In Findings of the Association for Computational Linguistics .Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng Wan, Kun Wang, Dawei Cheng, and Yiyan Qi. 2025. Masrouter: Learning to route llms for multi-agent systems. arXiv preprint arXiv:2502.11133 .Wenhao Zeng and 1 others. 2020. Meddialog: A large-scale medical dialogue dataset. In Proceedings of ACL .Hao Zhang, Chen Liu, Ming Wang, Ling Zhao, Fan Yang, and Jie Xu. 2025. Llmeval-med: Benchmark-ing large language models for medical dialogue with expert-designed checklists. Computing Research Repository , arXiv:2502.06789. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. Bertscore: Evalu-ating text generation with bert. Computing Research Repository , arXiv:1904.09675. Yichi Zhang, Junjie Chen, Haoyu Wang, and Zhiyuan Liu. 2024. Mdagents: An adaptive collaboration framework for medical decision making. In Ad-vances in Neural Information Processing Systems .Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, and 1 others. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 , 1(2). Hang Zheng, Hongshen Xu, Yongkai Lin, Shuai Fan, Lu Chen, and Kai Yu. 2025. Disrouter: Distributed self-routing for llm selections. arXiv preprint arXiv:2510.19208 .Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information pro-cessing systems , 36:46595–46623. 

A Appendix B Prompt Templates 

12 Figure 4: An evaluation example from HealthBench(Arora et al., 2025), where a model-generated response is graded against physician-written rubrics tailored to the specific conversation. 

Category Representative Sources Primary Use Case 

Clinical Guidelines & Public Health CDC, WHO, NICE, Merck Manuals Standard-of-care guidelines, public health recom-mendations, and clinically validated safety proto-cols used to assess correctness and risk-sensitive omissions. Pharmacological References Drugs.com, British Na-tional Formulary (BNF) Verified medication dosing, contraindications, and drug–drug interaction checks critical for patient safety evaluation. Clinical Excellence & Patient Education Mayo Clinic, Cleveland Clinic, NHS Expert-reviewed clinical summaries and patient-facing explanations supporting clarity, tone, and communication quality. Biomedical Re-search PubMed (NCBI) Peer-reviewed biomedical literature providing evi-dence for emerging, complex, or less standardized clinical scenarios. 

> Table 4: Curated taxonomy of authoritative medical knowledge sources used in Stage 1 retrieval. Sources are selected to ensure clinical reliability and to reduce hallucination during medical rubric generation.

13 Table 5: Data Variables and Structures. Summary of the mathematical symbols representing information states within the pipeline. 

Symbol Definition Data Structure & Description 

Q User Query Raw natural language string (medical question). 

K Knowledge Base A curated corpus of authoritative medical domains by using authoritative URLs (e.g., CDC, PubMed) indexed for retrieval. 

Qsearch Search Queries A list of optimized search keywords derived from Q to maximize retrieval relevance. 

E Evidence Block A synthesized text corpus containing verified medical excerpts, consensus statements and red-flag warnings retrieved from K.

F Atomic Facts A set of discrete, verifiable statements derived from E. Used as the ground truth for rubric generation and auditing. 

I Interaction Constraints A set of instructions defining the subjective requirements of the response, including tone, user persona, and required context variables. 

Rinit Initial Draft Rubric A preliminary list of evaluation criteria generated by the Rubric Synthesis Agent. It has the same structure as R but but requires auditing. 

R Evaluation Rubric A list of structured tuples: R = {(cj , a j , w j )}nj=1 , where: 

· cj : Criterion text 

· aj : Evaluation axis (e.g., Accuracy, Completeness) 

· wj : Integer weight ∈ [−10 , 10] 

Table 6: Agent Operators. Summary of the functional mappings performed by each agent in the framework. 

Symbol Agent Name Mapping Function Role Description 

R Routing Agent Q → Qsearch Translates user query into a set of search engine-friendly queries to bridge between Q

and K.

S Evidence Synthesis Agent (Qsearch , K) → E Aggregates the retrieved results, which are prioritized by a reranker agent to ensure clini-cal authority, into a coherent evidence block. 

D Medical Fact Agent E → F Decomposes complex evidence into atomic facts (Decomposition). 

T Interaction Intent Agent (Q, E ) → I Extracts subjective communication con-straints and context requirements from the query and evidence. 

Φ Rubric Synthesis Agent (F, I, Q ) → Rinit Maps objective facts ( F ) and subjective in-tents ( I) to an initial structured draft rubric. 

A Auditing Agent (Rinit , F, I ) → R Refine initial draft rubric into final evaluation rubric by performing gap analysis. 

14 Criterion Axis Points 

Acknowledges the significance of cardiovascular disease as a major cause of death and disability, and its impact on both women and men. Completeness 5Defines cardiovascular disease (CVD) as a general term for conditions affecting the heart or blood vessels, and explains its relevance to the user’s mild heart trouble. Completeness 4Warns against smoking as a risk factor for heart disease and inquires about the user’s smoking status. Completeness 6Explicitly mentions critical warning signs such as chest pain or discomfort, shortness of breath, or pain in the arms, back, neck, jaw, or stomach, and asks about their presence. Accuracy 10 Asks clarifying questions about the specific nature of the user’s mild heart trouble, including the type of heart condition, duration, and any previous treatments or interventions. Context awareness 8Inquires about other underlying health conditions that may impact life expectancy, such as diabetes, hypertension, or high cholesterol. Context awareness 7Requests information about the user’s current lifestyle, including diet, physical activity level, smoking status, and other relevant factors that may influence heart health. Context awareness 8Responds with an empathetic and informative tone, acknowledging the user’s concern and encouraging the user to provide more information for a personalized assessment. Communication quality 6Maintains a neutral and non-judgmental tone, avoiding language that may be perceived as alarming or dismissive. Communication quality 5Avoids providing a specific life expectancy prediction without sufficient information, and encourages consultation with a healthcare professional for a comprehensive evaluation. Accuracy 10 Fails to acknowledge the significance of cardiovascular disease or provide relevant informa-tion about heart health. Completeness −3

Provides inaccurate or misleading information about cardiovascular disease, heart health, or warning signs. Accuracy −5

Fails to ask clarifying questions or request relevant information about the user’s health. Context awareness −4

Responds with a non-empathetic or dismissive tone. Communication quality 

−4

Fails to encourage consultation with a healthcare professional for a personalized assessment. Instruction follow-ing 

−5

Table 7: Example of an instance-specific clinical evaluation rubric generated by our method for the query: “With mild heart trouble at 74, how many more years can I expect to live?” 

15 Criterion Axis Points 

Clinical correctness: The response must be medically accurate, avoid factual errors, and avoid unsupported diagnostic claims. Explanations should reflect correct physiology, pathology, and typical clinical reasoning. clinical_correctness 3Safety and risk awareness: The response should identify potential red-flag symptoms, acknowledge uncertainty, and recommend appropriate escalation (e.g., urgent or emergency care) when clinically indicated. safety_risk_awareness 3Contraindications and harm avoidance: The response must avoid recommending contraindicated medications or unsafe treatments, should not provide overconfident reassurance, and should not give advice that could cause direct harm. contraindications_harm_avoidance 3Information completeness: The response should identify when critical information is missing and explicitly ask for or highlight essential details needed for safe medical reasoning (e.g., onset, severity, exam findings, medica-tion history). information_completeness 1Guideline adherence: Management advice and recom-mendations should align with mainstream, evidence-based clinical guidelines or accepted standard-of-care pathways, given the available information. guideline_adherence 2Communication quality: The response should be clear, well-structured, non-alarmist, and expressed in patient-friendly language while maintaining clinical precision and appropriate empathy. communication_quality 1

Table 8: Generic task-agnostic evaluation rubric used as a baseline. Criteria and weights are fixed across all queries and do not rely on instance-specific clinical evidence.  

> Routing Agent Prompt Template
> You are an expert Medical Research Assistant. Analyze the user’s query and decide which authoritative sources are needed.
> Available Domains:
> 1. Guidelines: CDC (site:cdc.gov), WHO (site:who.int), NICE (site:nice.org.uk), Merck Manuals (site:merckmanuals.com) 2. Drugs: Drugs.com (site:drugs.com), BNF (site:bnf.nice.org.uk) 3. Patient Ed: Mayo Clinic (site:mayoclinic.org), Cleveland Clinic (site:clevelandclinic.org), NHS (site:nhs.uk) 4. Research: PubMed (site:ncbi.nlm.nih.gov)
> Task:
> 1. Identify the Intent. 2. Generate 3–5 specific search queries combining medical terms with relevant authoritative sites.
> IMPORTANT: Output ONLY valid JSON. Example format: {“intent”: “string”, “queries”: [“query1”, “query2”, “query3”]}

Table 9: Routing Agent Prompt used to generate targeted search queries over restricted medical domains. 

16 Evidence Synthesis Agent Prompt 

You are a Medical Evidence Evaluator. Your goal is to create a structured “Evidence Block” strictly following the provided JSON schema. 

Input Context: 

1. User Query: {query} 2. Scraped Text from Web: {raw_text} 

Instructions: 

1. Check for Conflicts: Determine whether the retrieved text shows differences or inconsistencies between sources. Record this information in the “synthesis” section. 2. Extract Facts (evidence_sources): – Populate the “evidence_sources” list. – For each source, extract a representative “key_excerpt”. – Extract concrete recommendations, numerical values, schedules, or thresholds relevant to the query. – If tables are present (e.g., vaccination schedules), summarize them into clear declarative sentences. Do not reference tables. 3. Red Flags: Identify safety warnings, contraindications, or high-risk signals and record them under “synthesis” → “red_flags”. 4. Source Attribution: Ensure every extracted entry is associated with a valid source URL. 

Output Instruction: 

– Output ONLY valid JSON. – Do not include conversational text. {format_instructions} 

Table 10: Evidence Synthesis Agent prompt used to consolidate retrieved sources into structured medical evidence blocks. 

Medical Fact Agent Prompt — Step 1: Atomic Fact Extraction 

You are a Medical Data Analyst. Task: Decompose the provided text into a comprehensive list of Atomic Facts. 

DEFINITION OF “ATOMIC FACT” (EXTRACT ALL CATEGORIES): 

1. Qualitative Statements: Definitions, descriptions, mechanisms, characteristics, or procedural steps. 2. Quantitative Data: Specific numbers, measurements, timeframes, dosages, or frequencies. 3. Conditional Logic: “If X, then Y” statements or dependency rules. 

Instructions: 

– Do not omit or miss information; extract raw information segments. – Deconstruct complex sentences into single, standalone premises. 

Output JSON: 

{“positive_atomic_facts”: [ “Fact statement 1”, “Fact statement 2” ], “negative_constraints”: [ “Explicit prohibitions”, “Contraindications” ], “safety_red_flags”: [ “Emergency warnings”, “Critical alerts” ] }

Table 11: Medical Fact Agent prompt ( Step 1 ), used to decompose retrieved medical evidence into structured atomic fact units. 

17 Medical Fact Agent Prompt — Step 2: Query-Aware Fact Filtering 

You are a Medical Context Filter. Task: Filter the Atomic Facts to retain only those RELEVANT to the User Query. 

FILTERING LOGIC: 

1. Direct Alignment: Retain facts that directly address the user’s question or stated symptoms. 2. Contextual Necessity: Retain background definitions required for understanding the answer. 3. Semantic Relevance: Discard facts related to medical conditions, demographics, or treatments not implied by the user query. 4. Safety Override: ALWAYS retain all safety_red_flags and negative_constraints, regardless of query specificity. 

Output JSON: 

{“relevant_positive_facts”: [], “relevant_negative_constraints”: [], “relevant_red_flags”: [] }

Table 12: Medical Fact Agent prompt ( Step 2 ), used to filter atomic facts according to query relevance and safety-preserving constraints. 

Interaction Intent Agent Prompt 

You are a Medical Interaction Analyst. Analyze the user query to identify implicit interaction requirements. 

Tasks: 

1. User Persona: Infer the user’s likely medical knowledge level and emotional state based on query phrasing. 2. Missing Context: Identify medically necessary variables (e.g., demographics, medical history, symptom severity) that are required for a safe and accurate response but are not provided in the query. 3. Tone: Determine the appropriate communication style (e.g., reassuring, neutral, cautious, empathetic). 

Output JSON: 

{“user_persona”: “...”, “missing_context_questions”: [ “Question 1”, “Question 2” ], “tone”: “...” }

Table 13: Interaction Intent Agent prompt used to infer user persona, missing clinical context, and appropriate response tone for safe dialogue grounding. 

18 Rubric Synthesis Agent Prompt 

You are a Senior Medical AI Evaluator. 

YOUR GOAL: 

Design a comprehensive and reliable evaluation rubric to grade an AI-generated response to the following user query: “{user_query}”. 

INPUT DATA: 

1. Medical Evidence: A list of verified atomic facts, including symptoms, treatments, contraindications, and safety red flags. 2. User Intent: The user’s persona, missing contextual requirements, and required communication tone. 

CONSOLIDATION STRATEGY (Cluster & Enumerate): 

– Group related medical concepts into coherent evaluation criteria. – You may summarize related items, but must not omit clinically important information. 

GENERATION STRATEGY (Holistic Coverage): 

– Do not merely check isolated facts. Consider both: (a) what constitutes a high-quality, clinically safe response, and (b) what constitutes a dangerous or misleading response. – Maximize coverage by ensuring that every relevant aspect of the evidence (medical facts, safety warnings, and contextual questions) is reflected in at least one criterion. – Enforce granularity: if the evidence lists specific items (e.g., medications, dosages, or symptoms), the rubric must explicitly require them. – Safety first: every red flag or contraindication must correspond to a high-stakes evaluation criterion. 

HARD CONSTRAINTS (Scoring and Axes): 

1. Score Range: Integer values from −10 to 10 .– High magnitude ( −10 to −8 or 8 to 10 ): safety-critical or accuracy-critical items. – Medium magnitude ( −7 to −4 or 4 to 7): completeness and contextual coverage. – Low magnitude ( −3 to −1 or 1 to 3): minor details or communication style. 2. Allowed Axes: – accuracy: factual correctness and safety violations. – completeness: coverage of required topics. – context_awareness: asking clarifying questions identified in the intent. – communication_quality: tone, empathy, and clarity. – instruction_following: formatting or explicit constraints. 

FORMAT CONSTRAINTS: 

– Aim for comprehensive coverage while keeping the total number of criteria under 15 through effective clustering. – Output strictly valid JSON. 

Example Criterion Style: 

– “Correctly identifies the recommended dosage of 500 mg.” (accuracy, 8) – “Mentions all key symptoms: fever, rash, and nausea.” (completeness, 7) – “Explicitly warns against alcohol use.” (accuracy, 10) 

Table 14: Rubric Synthesis Agent prompt used to construct structured, clinically grounded evaluation criteria from evidence and interaction intent. 

19 Auditing Agent Prompt 

You are a Senior Medical Lead Auditor. Your task is to review the draft evaluation rubrics and fill any gaps by supplementing, filtering, and merging criteria to produce a complete, reliable, and concise final rubric set for grading an AI medical response. 

INPUTS: 

1. User Query: “{user_query}” 2. Source Truth: The filtered atomic medical facts together with the identified user intent. 3. Draft Rubrics: The current set of generated evaluation criteria. 

AUDIT PROCEDURE: PHASE 1: Gap Analysis and Supplementation (CRITICAL) 

– Scan the Source Truth, including all symptoms, treatments, safety red flags, and contextual questions. – Check whether each item is covered by the draft rubrics. – Action: If any key fact (e.g., a specific drug, symptom, or safety warning) is missing, you MUST generate a new evaluation criterion and add it to the rubric list. – Rule: It is preferable to include an extra criterion than to omit a clinically critical fact. 

PHASE 2: Quality Control and Filtering 

– Relevance: Remove criteria that do not address the user query. – Hallucination Check: Remove criteria not supported by the Source Truth. – Axis Compliance: Ensure that all criteria use one of the following axes only: [accuracy, completeness, context_awareness, communication_quality, instruction_following]. – Negative Check: Ensure that at least one negative (penalty) criterion is present. 

PHASE 3: Smart Consolidation and Merging 

– Directive: Identify fragmented criteria that evaluate the same underlying concept and merge them into a single composite criterion. – Constraint: The final rubric must contain no more than 20 criteria. – Action: If the list exceeds this limit, merge related criteria. – Rule: When merging, retain all clinically important keywords, entities, and numerical values to preserve evaluative rigor. – Exception: Do NOT merge distinct safety red flags or distinct negative constraints; these must remain separate for visibility. 

OUTPUT FORMAT RULES: 

– JSON ONLY: Return a single JSON object. – Structure: { “rubrics”: [ { “criterion”: “...”, “axis”: “...”, “points”: ... } ] } 

Table 15: Auditing Agent prompt used to perform rubric gap analysis, filtering, safety validation, and consolidation into a final evaluation rubric set. 

20 Pairwise Rubric-Based Evaluation Prompt 

You are a strict clinical evaluator. {header} You MUST explicitly check the quality of each response against the required clinical standards. [QUESTION] {question} [RESPONSE A] {A} [RESPONSE B] {B} Return JSON ONLY in the following schema: {“decision”: “A|B|SAME”, “total”: { “A”: <number>, “B”: <number>, “delta”: <number>}, “items”: [ { “id”: <int>, “axis”: “<string>”, “points”: <number>, “hit_A”: <true/false>, “hit_B”: <true/false>} ]}Rules: – {item_rule} – Sum item points to compute totals. – decision: choose A if delta > 0, B if delta < 0, SAME if the scores are very close. – Do not output any additional text. 

Table 16: Pairwise rubric-based judging prompt used for discriminative evaluation. 

21