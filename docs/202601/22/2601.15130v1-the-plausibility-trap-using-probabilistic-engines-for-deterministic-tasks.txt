Title: The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks

URL Source: https://arxiv.org/pdf/2601.15130v1

Published Time: Thu, 22 Jan 2026 02:14:25 GMT

Number of Pages: 13

Markdown Content:
# The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks 

## Ivan Carrera 1 and Daniel Maldonado-Ruiz 21Laboratorio de Ciencia de Datos ADA, Departamento de Inform´ atica y Ciencias de la Computaci´ on, Escuela Polit´ ecnica Nacional, Quito, 170508, Ecuador 

> 2

## Facultad de Ingenieria en Sistemas, Electr´ onica e Industrial, Universidad T´ ecnica de Ambato, Ambato, 180206, Ecuador January 22, 2026 

# Abstract 

The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the “Plausibility Trap”: a phenomenon where individuals with access to Artificial Intelli-gence (AI) models deploy expensive probabilistic en-gines for simple deterministic tasks—such as Op-tical Character Recognition (OCR) or basic ver-ification—resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the ”efficiency tax”— demonstrating a 6.5x latency penalty—and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a frame-work to help developers determine when to use Gen-erative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Gener-ative AI, but also on knowing when not to use it. 

# 1 Introduction 

It happened last week during one of my lectures at 

Escuela Polit´ ecnica Nacional . I was projecting a snippet of Python code no more than 8 rows long on the screen—a standard exercise in data manipu-lation. A student in the back row stood up, took a photo of the projection with his smartphone, and sat back down. Curious about his workflow, I asked him how he planned to transfer that code to his IDE. I expected him to say he would use a standard OCR tool like Google Lens or perhaps type it manually to build muscle memory. Instead, he shrugged and replied, “I just send the photo to ChatGPT, and it gives me the code”. That brief interaction crystallized a phenomenon that I had been observing for months. To extract a few lines of deterministic text, this student—a future data scientist—had instinctively triggered a multi-modal chain of thought involving billions of parame-ters. He invoked a probabilistic engine running on a massive GPU cluster to perform a task that a lightweight, deterministic OCR algorithm could have handled locally on his device in milliseconds. This is the Maslow’s Hammer of the Generative AI era: when you have a chatbot that seems to understand everything, every problem looks like a conversation. The distinction between a creative task—where variance is a feature—and a logical task—where variance is a bug—vanishes behind the blinking cursor. We stop asking “what is the right 1

> arXiv:2601.15130v1 [cs.AI] 21 Jan 2026

tool?” and ask instead “how do I prompt this?”, ef-fectively flattening the diverse landscape of compu-tational problem-solving into a single, seductive text field. We are witnessing a profound shift in digital be-havior. The seamless User Experience (UX) of Large Language Models (LLMs) effectively cannibal-izes specialized tools. Users are trading computa-tional efficiency and deterministic precision for the convenience of a unified chat interface. But this con-venience comes with a hidden price tag: massive com-putational overhead and, as we will explore, danger-ous reliance on probabilistic outputs for binary tasks. This article argues that we are falling into the ”Plausibility Trap.” By deploying Generative AI for micro-tasks we are not just wasting energy; we are introducing unnecessary noise and the risk of halluci-nation into workflows that require absolute precision. It is time to ask: Why are we using a probabilistic cannon to kill a deterministic fly? 

# 2 THE ANATOMY OF THE TRAP 

The Plausibility Trap is the convergence of three sys-temic failures documented in the recent literature: computational inefficiency, algorithmic sycophancy, and cognitive erosion. 

## 2.1 The Efficiency Gap: Algorithmic Overkill 

The current literature has systematically begun to quantify the trade-offs between Generative AI and traditional methods for information extraction. A lit-erature review reveals a growing consensus: the cost of LLM flexibility includes a massive computational overhead for structured tasks. Several works developed in 2025 help to under-stand how LLMs introduce what the aforementioned consensus calls prohibitive computational overhead. Dennst¨ adt et al. [1], for example, quantified this gap in computational efficiency in clinical extrac-tion tasks, finding that a traditional approach based on the use of regular expressions was up to 28,120 times faster than the studied LLM while maintain-ing equivalent precision (89.2% for regular expres-sions vs. 87.7% for the LLM-based approach). In the same sense, Principe et al. [2] proved that a system based on rules created and managed by hu-mans achieved higher precision in real estate auction documents than LLMs (93-96 versus 85-89%, respec-tively). Both works prove that LLMs trade precision for flexibility, providing a quantitative baseline for the Plausibility Trap: the user sacrifices four orders of magnitude in speed for the convenience and hype of a chat interface. With the same idea, Busta & Oyler [3] demon-strated that Small Language Models (SLMs) con-sistently outperform general-purpose LLMs in la-tency for defined extraction, validating the argument that massive parameters are often an “algorithmic overkill”. The literature does identify the niche for LLMs: complex, unstructured, or low-resource domains where no schema exists. The works of Richter-Pechanski et al. [4] and Thakkar et al. [5] found that LLMs are a superior system for extracting clini-cal events from highly unstructured letters and com-plex structure fields. However, for the determinis-tic micro-tasks, like OCR or simple logic, traditional methods remain the efficiency gold standard. 

## 2.2 Sycophancy: The “Yes-Man” Problem 

The reliability of LLMs is further compromised by “Sycophancy”—the tendency to prioritize agreement over truth. This behavior is not an accident but a byproduct of Reinforcement Learning from Human Feedback (RLHF). Research by Humphreys [6] and Lindstr¨ om et al. [7] argues that because human an-notators prefer polite and agreeing responses, RLHF fine-tuning explicitly incentivizes models to mirror user biases. This phenomenon, described by Wen et al. [8] as “U-sophistry,” results in models that are persuasive but factually brittle. An LLM always needs to provide the idea that it has all the answers, whether the answers are correct or not. In high-stakes domains like medicine, Rosen et al. 210 0 10 1 10 2 10 3 10 4 10 5

Traditional Regex 

Small Language Model (SLM) 

Large Multimodal Model (LLM) 

0

5.01 

10 .24 

Computational Cost Factor (Log Scale) 

Figure 1: The Energy Efficiency Gap. A logarithmic comparison of computational cost. [9] found that LLMs frequently complied with log-ically flawed prompts, generating false information simply to satisfy the user’s premise. This creates a dangerous feedback loop where the tool validates the user’s misconceptions rather than challenging them. The existence of plausible but unverified or unreal outputs that are accepted as correct by users of the models creates a critical problem: those outputs re-shape user behavior, leaving the reasoning process totally to the AI. 

## 2.3 Cognitive Offloading: The Ero-sion of Skill 

Finally, the “trap” is psychological. Recent studies identify Generative AI as a form of “cognitive pros-thesis” that induces cognitive offloading —the delega-tion of mental processing to external tools. Although this reduces immediate mental effort, Gerlich [10] and Helal et al. [11] found that unstruc-tured reliance on these tools correlates with a sig-nificant reduction in critical thinking and analytical engagement, not to mention creative laziness. Neu-rophysiological evidence from Narayan [12] indicates that this reliance leads to reduced brain activation in areas of memory retention. The student in our initial example did not just choose a slower tool; he engaged in a pattern of “high-trust, passive use” that creates automation bias. 

# 3 THEORETICAL FRAME-WORK: LINGUISTIC vs. SCIENTIFIC INTELLI-GENCE 

To understand why individuals rely so much of their mental processes on LLMs, we must distinguish be-tween two types of machine intelligence that are often conflated: Linguistic Intelligence and Scientific Intel-ligence. 

• Linguistic Intelligence refers to the ability to manipulate symbols, generate coherent syntax, and mimic the stylistic tone of an expert, and 

• Scientific Intelligence , conversely, requires causal reasoning, hypothesis tracking, and the ability to revise one’s beliefs in the face of con-tradictory evidence. For the past few years, the “hype” surrounding Generative AI has been driven by its unparalleled Linguistic Intelligence. Because models like Gemini or GPT can write Python code that looks syntacti-cally perfect, users assume they possess the Scientific Intelligence to handle the logic behind that code with equal precision. 

## 3.1 The “Plausibility” Objective 

However, more recent and robust assessments suggest an alternative view. A landmark study recently re-leased by Song et al. [13], titled “Evaluating Large Language Models in Scientific Discovery”, exposes the fracture between these two capabilities. 3The researchers tested state-of-the-art LLMs not on trivial affairs, but on the full scientific discov-ery loop: hypothesis generation, experimental design, and result interpretation. The findings were sobering. While models are adept at suggesting initial hypothe-ses (a linguistic task), they are “brittle at everything that follows”. Crucially, the study found that LLMs “optimize for plausibility, not truth”. When an experiment fails, instead of revising the hypothesis, the model often hallucinates a convenient explanation to save face or doubles down on the error. They struggle to aban-don bad hypotheses even when presented with direct evidence to the contrary. 

## 3.2 The Trap in the Classroom 

This distinction explains the “Plausibility Trap” ob-served during the lecture we mentioned before. When the student takes the picture of the code on the board and asks the LLM to extract it, he is trusting a prob-abilistic engine designed to generate “plausible text completions” to perform a task that requires deter-ministic exactitude. The model does not “see” the code; it predicts the most likely token sequence that follows the image em-bedding. Sequence that the model previously knows as something close to the ‘correct pattern’. Typi-cally, the prediction aligns with reality. But because the model lacks true Scientific Intelligence (verifica-tion of truth), it can hallucinate a variable name or a digit with high confidence, purely because it fits the statistical pattern. By substituting a deterministic tool (OCR) or an internal intellectual process with a probabilistic scheme (LLM), the student is unknowingly gambling on the model’s linguistic fluency, mistaking it for log-ical reliability. When linguistic plausibility is eas-ily and frequently mistaken for scientific accuracy, it becomes measurable and shows that the problem goes beyond a laboratory or classroom perception, as shown in the following case studies. 

# 4 CASE STUDY A: THE HIGH COST OF CONVE-NIENCE 

To explore our assumption that the student’s behav-ior was not merely “different” but rather objectively inefficient, we must look under the hood of the com-putational processes involved. In the ADA Data Sci-ence Research Lab, at Escuela Polit´ ecnica Nacional ,we conducted a “Micro-Benchmark of Efficiency” to quantify the hidden cost of the chat-first workflow. 

## 4.1 The OCR Paradox: Extraction vs. Reconstruction 

When a user employs a dedicated tool to recognize the patterns on an image, like Google Lens or an on-device OCR engine (e.g., Tesseract), the process is linear and deterministic. The software identifies pixel patterns, maps them to Unicode characters, and outputs the string. It is a process of signal extraction. In contrast, sending an image to a multimodal LLM (like Gemini or GPT) triggers a process of signal reconstruction. The model does not simply “read” the pattern in the image; it tokenizes the visual in-put, passes it through billions of parameters to “un-derstand” the context, and then probabilistically pre-dicts the next text token to appear that should rep-resent the code that might (or might not) appear in the image. Figure 2 offers a comparison of these two processes, remarking on the necessary transformation layers in each LLM interaction. 

## 4.2 The Micro-Benchmark 

We compared the two workflows for the task of digi-tizing a 10-line Python script projected on a screen. The results illustrate a massive disparity in efficiency. 

• Workflow A (Deterministic): Open Google Lens 

→ Tap “Text” → Copy. 

• Workflow B (Probabilistic): Open Chatbot →

Upload Image → Wait for Upload/Tokenization 

→ Wait for Generation → Copy. 4Input Image 

Pre-processing (Binarization) 

Pattern Matching (Tesseract/Matrix) 

OUTPUT TEXT    

> DETERMINISTIC PATH
> Complexity: ∼O(N)

Input Image 

Visual En-coder (ViT) 

Tokenization & Emb. 

Transformer Layers 

(Attention 

N 2 × 96) 

Probabilistic Decoding 

OUTPUT TEXT 

(Hallucination Risk)    

> PROBABILISTIC PATH
> Complexity: ∼O(N2)

Figure 2: The Anatomy of Overkill. A struc-tural comparison showing why Generative AI incurs massive computational overhead for simple extrac-tion tasks compared to traditional methods. More critically, the computational overhead is vastly disproportionate. While exact energy met-rics for proprietary models are opaque, inference on a large multimodal model requires significant GPU Google Lens (OCR) Gemini (GenAI) 050 100 20 130 

> Time (seconds)

Figure 3: The Convenience Penalty. Comparing average time-to-task completion between a determin-istic OCR workflow (Google Lens) and a Generative AI workflow (Gemini). The probabilistic path intro-duces a ∼ 6.5x latency overhead for simple extraction [14]. activation in a specialized data center. In compari-son, standard OCR can often run efficiently on the edge (the smartphone’s NPU) or with minimal cloud overhead. We are essentially burning “computational calories” to perform a task that was solved efficiently decades ago. The quantitative results of our experiment are visu-alized in Figure 3 . As the data illustrates, the deter-ministic workflow (Google Lens) achieved task com-pletion in an average of 20 seconds. In stark contrast, the generative workflow in Gemini [15] required ap-proximately 2 minutes and 10 seconds—representing a ∼6.5x latency penalty .This disparity is not merely a matter of network speed but of architectural necessity. The probabilis-tic path incurs unavoidable sequential overheads—– image upload, vision encoding, tokenization, and au-toregressive generation—–that are structurally ab-sent in the deterministic approach. For a micro-task of this nature, the user is paying an “efficiency tax” of more than six times the time for the perceived con-venience of a chat interface. 5Figure 4: An example of how users were asking chat-bots to verify if large numbers were odd or even as a part of a viral trend. 

## 4.3 The Complexity Gap 

From an algorithmic perspective, the disparity is stark. A standard OCR algorithm (like Tesseract) operates largely on linear complexity relative to the input pixels. It identifies contours and matches pat-terns to a finite character set. In contrast, a Transformer-based model processes the image through a Vision Encoder (e.g., ViT) and then generates text autoregressively. This involves a quadratic attention mechanism, O(N 2), where every generated token attends to previous tokens. For a simple extraction task, we are essentially deploying a quadratic solution to a linear problem, as visualized in the structural comparison in Figure 2 . This is not just slower; it is algorithmic overkill that scales poorly as document length increases. 

## 4.4 The “Odd/Even” Fallacy 

The described inefficiency extends beyond vision to basic logic. Recently, a viral trend involved users asking chatbots to verify if large numbers were odd or even. Something like what is shown in Fig. 4 From a computer science perspective, this is baf-fling. In Python, checking x%2 == 0 is a constant-time operation (O(1)) requiring mere nanoseconds and negligible energy. An LLM, however, struggles with arithmetic on large numbers because of how it tokenizes digits. It does not “calculate”; it predicts. The main cause of this failure lies in tokenization. LLMs do not “see” numbers as mathematical quan-tities; they see them as tokens (sub-word units). For example, a number like “3824” might be tokenized as “38” and “24”, or any other combination based on the previous information the LLM manages. When a model attempts arithmetic, it is not calcu-lating; it is predicting the next statistically probable token based on training data text frequencies. Un-like a symbolic solver (e.g., Python’s Python REPL or Wolfram Alpha), the LLM lacks an internal logic unit (ALU). It is attempting to memorize the mul-tiplication table of the universe rather than learning the rule of multiplication. Relying on this architec-ture for deterministic logic is fundamentally architec-turally unsound. Asking an LLM to perform arithmetic is akin to asking a painter to draw a perfect circle freehand, rather than using a compass. They might get it right most of the time, but the effort is exorbitant, and the result is never guaranteed. By teaching students how to use LLMs by default, we are teaching them to ignore the fundamental computational properties of the problems they are trying to solve. 

# 5 CASE STUDY B: THE SYCOPHANCY TAX 

If the cost of using LLMs instead of OCR is mea-sured in seconds and Joules, the cost of using them for fact-checking is measured in trust and cognitive load. This brings us to the second dimension of the Plausibility Trap: the phenomenon of Sycophancy. LLMs are fine-tuned using Reinforcement Learn-ing from Human Feedback (RLHF). While this makes them helpful and conversational, it introduces a sub-tle alignment flaw: models are often rewarded for following the user’s instructions and matching their tone, even when the premise of the prompt is fac-tually incorrect. In short, the model wants to agree with you. 6Figure 5: Algorithmic Sycophancy: The model val-idates a false premise to satisfy the user’s prompt structure. 

## 5.1 The “Yes-Man” in the Machine 

Consider a test case performed in our lab using Grok [16]. We presented the model with a heavily biased, leading question: “Do you agree with Elon Musk when he says that Barcelona SC of Guayaquil is the biggest club in Ecuador?”, as shown in a translated screenshot in Figure 5 .A deterministic search engine would query its in-dex for “Elon Musk Barcelona Ecuador”, find zero matches, and return a null result. However, the LLM prioritized plausibility over retrieval. It hallucinated a detailed confirmation to provide a response that the user would consider satisfactory, inventing a scenario where Musk praised the team’s fanbase and stadium. This behavior reveals the danger of using a proba-bilistic generator as a knowledge base. The model did not “lie” per se , nor has it shown any malicious intent to misinform; it simply completed the pattern initi-ated by the user. It generated the most statistically probable continuation of a sentence where Musk does praise the team, even when that event never actually occurred. 

## 5.2 The Verification Tax 

The aforementioned behavior of the studied LLMs creates what we call the “Verification Tax”. The user turned to the AI to save the effort of a manual search. However, because the output is plausible but potentially fabricated, the user must now perform a secondary manual search to verify the AI’s claim. Instead of replacing the traditional workflow, the LLM has merely added a layer of noise that requires auditing. For students and junior professionals, who often lack the domain expertise to spot the halluci-nation immediately, this tax is dangerous. They may accept the sycophantic answer as truth, propagating misinformation under the guise of AI-validated data. The efficiency calculation is thus negative: using an LLM for factual verification often takes longer than the manual alternative, once the necessary auditing time is factored in. Altogether, both study cases re-veal the lack of a formal tool to guide the choice be-tween deterministic and probabilistic environments. 

# 6 TOWARDS TOOL SELEC-TION ENGINEERING 

The previous analysis shows a clear understanding: for deterministic micro-tasks, Generative AI is often the least efficient tool in the shed. Yet, the trend in Computer Science education has been to double down on “Prompt Engineering”—teaching students how to coerce a probabilistic model into doing everything, rather than teaching them how to choose the right tool for the job. We are raising a generation of “Hammer-First” de-velopers. If we do not correct this current course, we risk graduating engineers who can create com-plex multi-agent systems but who lack the instinct to write a simple script based on regular expressions or perform a database query. 76.1 The Proposal: Tool Selection En-gineering 

At the ADA Data Science Research Lab at Escuela Polit´ ecnica Nacional , we propose a curriculum shift. We argue that “Tool Selection Engineering” is a more critical skill for the future than Prompt Engineering. This discipline focuses on the metacognitive decision process before code is written. It asks: 

• Is the problem deterministic or probabilistic? 

• Does the solution require absolute truth or cre-ative plausibility? 

• What is the “computational cost of curiosity” for this query? 

## 6.2 Formalizing the Solution: The TSE Framework 

To operationalize this shift, we propose the Tool Se-lection Engineering (TSE) . Unlike Prompt Engi-neering, which optimizes the output of a model, TSE optimizes the architecture of the workflow before any code is written. As visualized in Figure 6 , the framework maps every engineering requirement onto a coordinate sys-tem defined by two critical axes: 

6.2.1 Axis X: Task Entropy (Outcome Deter-minism) 

The horizontal axis measures the rigidity of the solu-tion space. 

• On the Deterministic end (Low Entropy), problems have a single, verifiable ground truth (e.g., extracting a date, calculating a sum). The mapping from input to output is bijective; any deviation constitutes a failure. 

• On the Probabilistic end (High Entropy), problems accept a distribution of valid answers (e.g., “brainstorm marketing copy”). Here, vari-ability is a feature, and the model’s stochastic nature is an asset. 

6.2.2 Axis Y: Cost of Error (Verification La-tency) 

The vertical axis measures the Risk Asymmetry of the task. 

• High Stakes implies that a single hallucination causes system failure, financial loss, or safety risks. This introduces a prohibitive Verification Latency : if checking the AI’s work takes longer than doing the work manually, the tool is ineffi-cient regardless of its generation speed. 

• Low Stakes implies that errors are inconse-quential, easily reversible, or immediately obvi-ous to the human in the loop. 

## 6.3 The Deterministic-Probabilistic Decision Matrix (DPDM) 

As illustrated in Figure 6 , we categorize engineering tasks into four quadrants to guide strict tool selec-tion: 

• The Precision Quadrant (High Determin-ism, High Stakes): Tasks such as OCR, arith-metic, fact-checking, or syntax verification. 

Protocol: NO LLMs. Use symbolic algo-rithms, regular expression sentences, or special-ized APIs. Using Generative AI here consti-tutes “Algorithmic Malpractice” due to the doc-umented efficiency gap and hallucination risk. 

• The Augmented Quadrant (High Stakes, Probabilistic): Tasks requiring synthesis of facts, such as medical diagnosis support or le-gal research. 

Protocol: RAG + Human Loop. Deployment of Retrieval-Augmented Generation with strict citation constraints to mitigate sycophancy. 

• The Trivial Quadrant (Low Stakes, Deter-ministic): Simple utility tasks like unit conver-sion or date formatting. 

Protocol: Classical Tools. Use lightweight lo-cal tools (calculators, scripts) to minimize the processing load on used devices. 8Task Entropy  

> Cost of Error
> Deterministic Probabilistic
> High Stakes
> Low Stakes
> PRECISION QUADRANT
> Ex: OCR, Math, Fact-Checking
> PROTOCOL: NO LLMs
> Use Symbolic/Regex
> AUGMENTED QUADRANT
> Ex: Medical, Legal, Research
> PROTOCOL: RAG + Human
> Verify all outputs
> TRIVIAL QUADRANT
> Ex: Conversions, Sorting
> PROTOCOL: Classical Tools
> (Calculator, Excel)
> CREATIVE QUADRANT
> Ex: Brainstorming, Drafting
> PROTOCOL: LLM Native
> Use Generative AI

Figure 6: The Deterministic-Probabilistic Decision Matrix (DPDM). A framework for Tool Selection Engineering to mitigate the Plausibility Trap. 

• The Creative Quadrant (Low Determin-ism, Low Stakes): Tasks like ideation, draft-ing, or style transfer. 

Protocol: LLM Native. Here, the model’s lin-guistic intelligence and tendency to hallucinate (creativity) are features, not bugs. By enforcing this matrix, we move from “magical thinking” —where users expect the AI to do it all— to “modular engineering”. Under this framework, us-ing an LLM for OCR is not just a “quirk”; it is an engineering failure—a misallocation of resources akin to using a supercomputer to balance a checkbook. 

## 6.4 Distinguishing TSE from Prompt Engineering 

A common approach among AI enthusiasts is that better prompting can solve these issues. Our analy-sis shows that this line of thinking ignores the point. Unlike Prompt Engineering, which asks “How do I get the best answer from this model?” , Tool Selec-tion Engineering asks “Should I be using this model at all?” .While the former is a tactical optimization of a probabilistic engine, the latter is a strategic evalu-ation of computational fit. By formalizing this dis-tinction, we elevate the discussion from “improving outputs” to “architecting workflows.” 

## 6.5 Digital Sustainability as an Engi-neering Principle 

Furthermore, this is an issue of ethics and sustain-ability. In an era where data center energy consump-tion is skyrocketing, “Green AI” must move from a buzzword to a coding practice. A responsible data scientist understands that computational frugality is a virtue. By choosing deterministic tools for deterministic tasks, we reduce latency, eliminate hallucination risk, 91. THE TASK 

> Student faces a de-terministic problem (e.g., Code Extraction)
> 2. LOW-FRICTION ACTION
> Uses LLM to solve it (Cognitive Offloading)
> 3. IMMEDIATE REWARD
> Task done instantly. Zero mental effort.
> 4. SKILL DECAY
> Pattern recognition & syntax memory atrophy.
> triggers
> yields causes
> dependency increases

THE AT-ROPHY LOOP 

Figure 7: The Cognitive Atrophy Loop. Illustrating how removing cognitive friction via Generative AI creates a feedback loop of skill decay and increased dependency [10]. and significantly lower the carbon footprint of our workflows. True “AI Literacy” means knowing ex-actly when not to use AI. 

## 6.6 The Opportunity Cost of Trivial-ity 

Beyond the energy metrics, there is a strategic tragedy in the Plausibility Trap. We are currently liv-ing through the greatest hype cycle in the history of computer science, driven by the promise of Artificial General Intelligence (AGI) solving humanity’s hard-est problems—from protein folding to climate mod-eling. Yet, by normalizing the usage of these massive models for micro-tasks, we are squandering this mo-mentum. We are taking the most sophisticated com-putational engines ever built—capable of reasoning across billions of parameters—and reducing them to the role of a glorified clipboard or a slow calculator. This is the true waste: we are deploying Formula 1 engineering infrastructure to pizza delivery-level tasks. By allowing the hype to focus on trivial con-veniences (like summarizing a short email or extract-ing text from a photo), we distract individuals from pushing the models to do what they uniquely can do: complex reasoning and creative synthesis. We are not just wasting energy; we are wasting the technology’s potential. 

## 6.7 Educational Implementation: In-tentional Friction 

Finally, implementing TSE requires reintroducing 

Intentional Cognitive Friction . As shown in the Cognitive Atrophy Loop (Figure 7 ), low-friction 10 tools induce skill decay. The mechanism of this decay is cyclical, as illus-trated in Figure 7 . It begins when a student faces a deterministic task (Stage 1). Instead of engaging in the productive struggle by solving it, they choose the low-friction action of querying an LLM (Stage 2). The immediate reward—–a correct answer with zero mental effort (Stage 3)–—reinforces the behav-ior through a dopamine loop. Crucially, this bypasses the neural consolidation required for pattern recog-nition, leading to skill atrophy (Stage 4). As skills erode, the student becomes more dependent on the tool for even simpler tasks in the future, closing the feedback loop of dependency. We propose a curricular rule: “If the student can-not verify the output of the LLM (due to lack of foun-dational knowledge), they must not use the LLM to generate it” . This ensures AI acts as an exoskeleton for valid skills rather than a prosthesis for missing ones. The Plausibility Trap, as it is seen, is not a failure of technology or coding, but of engineering judgment. This problem can only be addressed by restoring in-tentionality, friction, and tool awareness for any AI user. 

# 7 CONCLUSION 

The Plausibility Trap is seductive at many levels. It offers us a world where a single chat interface can solve every problem, from coding to understanding math or learning history. But, as we have demon-strated, this convenience is an illusion that masks a regression in computational efficiency and scientific rigor. The recent findings presented by Song et al.[13] serve as a wake-up call: high benchmark scores in linguistic fluency do not equate to scientific capabil-ity. When we confuse the two, we do not just get inefficient workflows; we get “sycophantic” engineer-ing—solutions that look correct on the surface but crumble under scrutiny. Song’s findings are only the nearest iteration to the engineering of the ELIZA effect [17], and the ease with which humans tend to project themselves psychologically onto formal systems simply because the responses resemble those of a human being. In our case, LLMs come to be considered magic , in the most literal sense, rather than iterations with com-plex computational schemes that are not truly un-derstanding the user but only responding with the closest available information, whether correct or not. We are not arguing against the use of Generative AI; we are arguing against its indiscriminate use. The future of Data Science, Cybersecurity and other dis-ciplines among Computer Sciences does not belong to those who can prompt a model to do everything but to those who have the wisdom to know when to use a model and when to write a script. It is time to return to a fundamental engineering principle: use the right tool for the job. It is time to stop using probabilistic cannons to kill deterministic flies. 

# 8 ACKNOWLEDGMENTS 

The authors acknowledge the use of Gemini 1.5 Pro (Google) for assistance in refining the structure and clarity of the abstract and English phrasing. Atask which sits at the Creative Quadrant of the Deterministic-Probabilistic Decision Matrix. We also acknowledge the use of Grok (xAI) to generate the ex-perimental output shown in Figure 5. The authors re-viewed and edited all AI-generated content and take full responsibility for the content of the article. 

# REFERENCES 

[1] F. Dennst¨ adt, L. Lerch, M. Schmerder, N. Ci-horic, G. Cereghetti, R. Gaio, H. Bonel, I. Filchenko, J. Hastings, F. Dammann et al. ,“A comparative performance analysis of regular expressions and a large language model-based approach to extract the bi-rads score from ra-diological reports,” JAMIA Open , vol. 8, 2025. [2] R. A. Principe, M. Viviani, and N. Chiarini, “Enhancing information extraction with large language models: A comparison with human annotation and rule-based methods in a real 11 estate case study,” in Proceedings of the 5th Conference on Language, Data and Knowledge ,M. Alam, A. Tchechmedjiev, J. Gracia, D. Gro-mann, M. P. di Buono, J. Monti, and M. Ionov, Eds. Naples, Italy: Unior Press, Sep. 2025, pp. 243–254. [3] L. Busta and A. Oyler, “Small language models enable rapid and accurate extraction of struc-tured data from unstructured text: An example with plants and their specialized metabolites,” 

Quantitative Plant Biology , vol. 6, 2025. [4] P. Richter-Pechanski, M. Seiferling, C. Kiriakou, D. Schwab, N. Geis, C. Dieterich, and A. Frank, “Medication information extraction using local large language models,” Journal of Biomedical Informatics , vol. 169, p. 104898, 2025. [5] V. Thakkar, G. Silverman, A. Kc, N. Ingraham, E. Jones, S. King, G. Melton, R. Zhang, and C. Tignanelli, “A comparative analysis of large language models versus traditional information extraction methods for real-world evidence of patient symptomatology in acute and post-acute sequelae of sars-cov-2,” PLOS One , vol. 20, 05 2025. [6] D. Humphreys, “Ai’s epistemic harm: Reinforce-ment learning, collective bias, and the new ai culture war,” Philosophy & Technology , vol. 38, 2025. [7] A. Lindstr¨ om, L. Methnani, L. Krause, P. Er-icson, M. De Rituerto De Troya, D. Mollo, and R. Dobbe, “Helpful, harmless, honest? so-ciotechnical limits of ai alignment and safety through reinforcement learning from human feedback,” Ethics and Information Technology ,vol. 27, 2025. [8] J. Wen, R. Zhong, A. Khan, E. Perez, J. Steinhardt, M. Huang, S. Bowman, H. He, and S. Feng, “Language models learn to mislead humans via rlhf,” arXiv preprint arXiv:2409.12822 , 2024. [9] K. Rosen, M. Sui, K. Heydari, E. Enichen, and J. Kvedar, “The perils of politeness: how large language models may amplify medical misinfor-mation,” NPJ Digital Medicine , vol. 8, 2025. [10] M. Gerlich, “From offloading to engagement: An experimental study on structured prompting and critical reasoning with generative ai,” Data ,vol. 10, no. 11, 2025. [11] M. Helal, I. Elgendy, M. Albashrawi, Y. Dwivedi, M. Al-Ahmadi, and I. Jeon, “The impact of generative ai on critical think-ing skills: a systematic review, conceptual framework and future research directions,” 

Information Discovery and Delivery , 2025. [12] N. Narayan, “Ai, cognition, and the cost of con-venience,” Journal of Technology and Systems ,2025. [13] Z. Song et al. , “Evaluating large language models in scientific discovery,” arXiv preprint arXiv:2512.15567 , dec 2025. [14] I. Carrera, “Data science lab efficiency micro-benchmark,” Escuela Polit´ ecnica Nacional, Data Science Research Lab, Quito, Ecuador, Internal Report, 2025. [15] Google, “Gemini,” Large Language Model, 2026, version 1.5 Pro. [Online]. Available: https://gemini.google.com/ [16] xAI, “Grok,” Large Language Model, 2025, version 4.1. [Online]. Available: https://grok. com/ [17] D. Affsprung, “The eliza defect: Constructing the right users for generative ai,” Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society , 2023. 12 Ivan Carrera is a Professor and the Head of the ADA Data Science Research Laboratory at Escuela Polit´ ecnica Nacional , Ecuador. His current research interests include Data Science applications in Educa-tion, Psychology, Computational Chemistry and Bi-ology. Carrera received his Ph.D. degree in Computer Science from University of Porto, Portugal. Contact him at adalab@epn.edu.ec. 

Daniel Maldonado-Ruiz is a researcher at the 

Universidad T´ ecnica de Ambato , Ecuador. His cur-rent research interests include Cybersecurity, and Information and Identity Security Management. Maldonado-Ruiz received his Ph.D. degree in Com-puter Science from Escuela Polit´ ecnica Nacional ,Ecuador. He is a Fellow of the IEEE Computer So-ciety. Contact him at da.maldonado@uta.edu.ec. 13