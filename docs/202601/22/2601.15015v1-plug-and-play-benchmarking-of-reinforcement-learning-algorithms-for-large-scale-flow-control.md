# Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control
# 大规模流体控制强化学习算法的即插即用基准测试

**Authors**: Jannis Becktepe, Aleksandra Franz, Nils Thuerey, Sebastian Peitz
**Date**: 2026-01-21
**PDF**: https://arxiv.org/pdf/2601.15015v1
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:RL</span> <span class="tag-label tag-pink">keyword:ppo</span> <span class="tag-label tag-pink">query:sr-bench</span>
**Score**: 7.0
**Evidence**: Differentiable benchmark suite for Reinforcement Learning

---

## Abstract
Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.

## 摘要
强化学习（RL）在主动流

---

## 速览摘要（自动生成）

**问题**：现有强化学习流体控制研究缺乏统一基准，且高度依赖外部CFD求解器，难以进行标准化评估。

**方法**：推出FluidGym，这是首个基于PyTorch和GPU加速、完全可微且无需