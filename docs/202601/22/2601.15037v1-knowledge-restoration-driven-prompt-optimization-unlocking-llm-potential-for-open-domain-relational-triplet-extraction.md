# Knowledge Restoration-driven Prompt Optimization: Unlocking LLM Potential for Open-Domain Relational Triplet Extraction
# 知识恢复驱动的提示优化：释放大语言模型在开放域关系三元组抽取中的潜力

**Authors**: Xiaonan Jing, Gongqing Wu, Xingrui Zhuo, Lang Sun, Jiapu Wang
**Date**: 2026-01-21
**PDF**: https://arxiv.org/pdf/2601.15037v1
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 6.0
**Evidence**: Explores LLM prompt optimization for knowledge extraction tasks

---

## Abstract
Open-domain Relational Triplet Extraction (ORTE) is the foundation for mining structured knowledge without predefined schemas. Despite the impressive in-context learning capabilities of Large Language Models (LLMs), existing methods are hindered by their reliance on static, heuristic-driven prompting strategies. Due to the lack of reflection mechanisms required to internalize erroneous signals, these methods exhibit vulnerability in semantic ambiguity, often making erroneous extraction patterns permanent. To address this bottleneck, we propose a Knowledge Reconstruction-driven Prompt Optimization (KRPO) framework to assist LLMs in continuously improving their extraction capabilities for complex ORTE task flows. Specifically, we design a self-evaluation mechanism based on knowledge restoration, which provides intrinsic feedback signals by projecting structured triplets into semantic consistency scores. Subsequently, we propose a prompt optimizer based on a textual gradient that can internalize historical experiences to iteratively optimize prompts, which can better guide LLMs to handle subsequent extraction tasks. Furthermore, to alleviate relation redundancy, we design a relation canonicalization memory that collects representative relations and provides semantically distinct schemas for the triplets. Extensive experiments across three datasets show that KRPO significantly outperforms strong baselines in the extraction F1 score.

## 摘要
开放域

---

## 速览摘要（自动生成）

**问题**：LLM在开放域关系抽取中依赖静态提示词且缺乏反思