Title: Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning

URL Source: https://arxiv.org/pdf/2601.15086v1

Published Time: Thu, 22 Jan 2026 01:59:36 GMT

Number of Pages: 11

Markdown Content:
# Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning 

Oleg Shchendrigin 1,3 Egor Cherepanov 2,3 Alexey K. Kovalev 2,3 Aleksandr I. Panov 2,3  

> 1

Innopolis University, Innopolis, Russia 2 Cognitive AI Systems Lab, Moscow, Russia  

> 3

MIRIAI, Moscow, Russia 

{shchendrigin.o, cherepanov.e}@miriai.org 

Project Page: quartz-admirer.github.io/Memory-Rewriting/ 

## ABSTRACT 

Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circum-stances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leav-ing the equally critical ability of memory rewriting largely unex-plored. To address this gap, we introduce a benchmark that explic-itly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments re-veal that classic recurrent models, despite their simplicity, demon-strate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under nar-row conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamen-tal limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, intro-duces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/ 

## KEYWORDS 

Reinforcement Learning, POMDP, Memory, Benchmark 

ACM Reference Format: 

Oleg Shchendrigin, Egor Cherepanov, Alexey K. Kovalev, and Aleksandr I. Panov. 2026. Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning. In Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), Paphos, Cyprus, May 25 â€“ 29, 2026 , IFAAMAS, 11 pages. 

## 1 INTRODUCTION 

Many everyday tasks demand memory that can both preserve and revise information [ 4, 18 , 32 ]. A pedestrian following a sequence of directional signs must replace an earlier instruction ( â€œturn left at the parkâ€ ) when a new sign ahead indicates a detour, while a warehouse robot tracking object locations must update its internal map each time an item is moved. In both cases, previously stored information   

> Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), C. Amato, L. Dennis, V. Mascardi, J. Thangarajah (eds.), May 25 â€“ 29, 2026, Paphos, Cyprus .Â©2026 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). This work is licenced under the Creative Commons Attribution 4.0 International (CC-BY 4.0) licence.

Figure 1: Illustration of memory rewriting vs. retention. The agent first observes a cue, stores it in memory, and acts ac-cording to that cue (jump for â†‘). Later, a new cue appears indicating a change in required behavior ( â†“). In the rewriting case (top), the agent overwrites its previous memory with the new cue and successfully adapts its action. In the retention case (bottom), the agent fails to update its memory, continues to act based on the old cue, and consequently performs the wrong action, leading to failure. 

becomes misleading unless the agent rewrites its memory with current observations (see Figure 1 for a representative illustration). Reinforcement Learning (RL) agents must exhibit the same capac-ity to revise stored information in order to act reliably in dynamic and partially observable environments. In realistic settings, ob-servations are often incomplete or ambiguous: sensors are noisy, observations are occluded (limited field of view, temporary block-ages), and the latent state cannot be inferred from a single frame [ 19 ,21 , 23 , 43 ]. Under partial observability, effective decision-making re-quires agents to construct and maintain a memory that summarizes the relevant aspects of past experience [8, 9, 17, 22, 28, 37, 42]. Memory in RL is not a single ability but a collection of inter-dependent processes: what information to encode, how long to retain it, and how to update or discard it [ 2, 26 , 41 ]. Most existing work focuses on retention â€“ storing a cue and preserving it across time [ 13 , 27 ]. However, many decision-making problems depend equally on rewriting : selectively discarding outdated content and in-tegrating new evidence while avoiding interference from previously stored but now-irrelevant representations. Agents that only retain information risk acting on obsolete internal states. Despite its cen-trality in natural cognition, this ability to rewrite memory remains underexplored in artificial agents. Current benchmarks typically reward remembering a single cue until the episode ends, offering little incentive to assess continual, selective updating [6, 20]. 

> arXiv:2601.15086v1 [cs.LG] 21 Jan 2026

In this work, we isolate and systematically investigate memory rewriting in RL through four diagnostic tasks that enforce contin-ual memory updates under partial observability. Endless T-Maze 

consists of sequential corridors where each new cue invalidates the previous one, requiring the agent to actively overwrite outdated instructions. Color-Cubes (in Trivial , Medium , and Extreme vari-ants) is a grid-world task where colored cubes stochastically tele-port, forcing agents to continually update their internal map and avoid acting on stale information. We evaluate three representative families of memory-augmented RL agents â€“ recurrent policies, transformer-based architectures, and structured external memories â€“ providing a detailed characteriza-tion of their respective strengths, limitations, and generalization patterns. We focus on how each architecture adapts when memory rewriting becomes essential for success, exposing distinct strategies and failure modes across these paradigms. 

Our contributions are threefold: 

(1) Benchmarks for rewriting. We introduce Endless T-Maze and Color-Cubes , two families of environments, which together provide four different options for isolate the ability to perform continual, selective memory updates beyond simple cue retention. (2) Systematic evaluation. We compare recurrent, transformer, and structured-memory agents, identifying when rewriting mechanisms succeed or fail and how performance degrades with increasing memory update frequency. (3) Design principles. We analyze architectural factors linked to rewriting competence, highlighting the effectiveness of 

explicit, adaptive forgetting (e.g., learnable forget gates) over cached-state or rigidly structured memories, and outline practical guidelines for evaluating memory mechanisms in partially observable tasks. 

## 2 BACKGROUND 

Markov Decision Processes. A fully observable decision-making problem can be formalized as a Markov Decision Process 

(MDP), M = (S , A, ð‘ƒ, ð‘Ÿ, ðœ‡ 0, ð›¾ ), where S and A are the state and action spaces, ð‘ƒ (Â· | ð‘ , ð‘Ž ) is the transition kernel, ð‘Ÿ (ð‘ , ð‘Ž ) the reward, 

ðœ‡ 0 the initial-state distribution, and ð›¾ the discount factor [ 35 ]. The Markov property implies that the optimal policy depends only on the current state, ðœ‹ â˜…(ð‘Ž ð‘¡ | ð‘  ð‘¡ ), with the objective of maximizing the expected discounted return EÃ ð‘¡ ð›¾ ð‘¡ ð‘Ÿ (ð‘  ð‘¡ , ð‘Ž ð‘¡ ).

Partially Observable Markov Decision Processes. In many real-world settings, the state ð‘¥ ð‘¡ is not directly observable. A Par-tially Observable MDP (POMDP) extends the MDP by introduc-ing an observation space O and observation kernel ð‘‚ (ð‘œ | ð‘  â€², ð‘Ž ),defining the tuple P = (S , A, O, ð‘ƒ, ð‘‚, ð‘Ÿ, ðœ‡ 0, ð›¾ ) [ 16 ]. Here, policies condition on the full interaction history â„Žð‘¡ = (ð‘œ 0, ð‘Ž 0, . . . , ð‘œ ð‘¡ ), result-ing in ðœ‹ (ð‘Ž ð‘¡ | â„Žð‘¡ ). Because exact Bayesian filtering [ 1] is typically intractable, agents approximate it using a learned memory state 

ð‘š ð‘¡ = ð‘“ ðœ™ (â„Žð‘¡ ), which summarizes past information for decision-making via ðœ‹ ðœƒ (ð‘Ž ð‘¡ | ð‘š ð‘¡ ). In the fully observable limit, ð‘š ð‘¡ coincides with the true state, recovering the standard MDP. 

Dynamic Memory and Rewriting. Under partial observability, agents must not only retain but also rewrite memory as task-relevant information changes. Let ð‘š ð‘¡ âˆˆ M denote the memory state and 

ðœ‚ ð‘¡ = (ð‘Ž ð‘¡ , ð‘œ ð‘¡ +1) the new input. A general update rule is ð‘š ð‘¡ +1 =

ð‘ˆ ðœ™ (ð‘š ð‘¡ , ðœ‚ ð‘¡ ), which can be decomposed as 

ð‘š ð‘¡ +1 = ð‘Š ðœ™ 

 ð¹ ðœ™ (ð‘š ð‘¡ ), ð¸ ðœ™ (ðœ‚ ð‘¡ ), (1) where ð¹ ðœ™ selects or forgets parts of the old memory, ð¸ ðœ™ encodes new input, and ð‘Š ðœ™ integrates both. Rewriting thus represents a selective update process â€“ information is preserved, attenuated, or replaced depending on current relevance â€“ ensuring that the memory state evolves to maintain only the most decision-relevant features of experience. 

Memory-intensive environments and dynamic correlations. 

A partially observable environment can be described by a set of cor-relation horizons Îž = {ðœ‰ ð‘› }, where each ðœ‰ = ð‘¡ ð‘Ÿ âˆ’ ð‘¡ ð‘’ âˆ’ Î”ð‘¡ + 1 measures the temporal gap between an informative event ð›¼ Î”ð‘¡ ð‘¡ ð‘’ , which begins at time ð‘¡ ð‘’ and lasts for Î”ð‘¡ steps, and the later decision ð›½ ð‘¡ ð‘Ÿ made at time ð‘¡ ð‘Ÿ that depends on it [ 7 ]. An environment is memory-intensive 

when min Îž > 1, requiring the agent to recall information across multiple timesteps. In our tasks, the events ð›¼ Î”ð‘¡ ð‘¡ ð‘’ are dynamic and evolve over time ( ð›¼ Î”ð‘¡ ð‘¡ ð‘’ = ð›¼ Î”ð‘¡ ð‘¡ ð‘’ (ð‘¡ )), producing shifting and overlap-ping correlation horizons. This dynamic structure forces the agent to not only retain long-range dependencies but also to rewrite outdated information as conditions changeâ€”an ability explicitly probed by our Endless T-Maze and Color-Cubes environments. 

## 3 RELATED WORK 3.1 Memory-augmented RL agents. 

Partial observability is a defining property of many real-world decision-making problems, where agents must infer hidden state information from incomplete and noisy observations. To act effec-tively, such agents require mechanisms that can retain, integrate, and adapt information over time. This need has motivated a broad line of research on memory-augmented RL architectures, which dif-fer in how they represent, store, and update temporal dependencies. 

Recurrent architectures. A common approach introduces re-current neural networks into policy and value functions. The PPO-LSTM [ 33 ] extends Proximal Policy Optimization (PPO, [ 33 ]) with Long Short-Term Memory (LSTM, [ 15 ]) units, allowing agents to maintain internal states that summarize recent experience. The gating mechanisms of the LSTM regulate what information is pre-served or forgotten at each step, providing a learnable and adaptive form of memory that serves as a strong baseline for studying se-quential decision-making in RL. 

Transformer-based architectures. Transformers have recently been adapted to RL to leverage their ability to model long-range dependencies. The Gated Transformer-XL (GTrXL ) [ 28 ]extends the Transformer-XL architecture (TrXL) [ 12 ] for RL by introducing identity-map reordering and gating mechanisms that improve training stability. It preserves long-range temporal context through cached hidden states, enabling policies to leverage information beyond the immediate observation window. However, transformers lack an explicit mechanism for selective forgetting, which limits their stability in long-horizon or sparse-reward environments. Structured memory architectures. To achieve more robust and interpretable forms of temporal abstraction, recent work has introduced structured external memory systems. The Fast and Forgetful Memory (FFM , [ 25 ]) models memory as a set of expo-nentially decaying traces, enabling gradual forgetting of outdated information inspired by computational psychology. The Stable Hadamard Memory (SHM , [ 22 ]) extends this idea by learning a dynamic calibration matrix that adaptively regulates which compo-nents of memory are reinforced or suppressed, enhancing stability during online updates. Across these architectures â€“ recurrent, transformer-based, and structured â€“ most research has focused on improving memory reten-tion and stability. In contrast, the equally important ability to rewrite 

memory â€“ selectively discarding obsolete content and incorporating new, task-relevant information â€“ remains largely unexplored. Our work isolates this capability and provides a systematic framework for evaluating how existing memory mechanisms handle continual memory updating in partially observable settings. 

## 3.2 Existing benchmarks for memory testing 

A wide range of benchmarks has been developed to evaluate how RL agents store, retain, and use information under partial observability. These environments differ in sensory modality, task abstraction, and the specific aspects of memory they test. 

3D embodied environments. First-person view benchmarks such as DeepMind Lab [ 3] and MiniWorld [ 11 ] test navigation and spatial reasoning under occlusion, though agents often rely on perceptual shortcuts instead of true long-term memory. Memory Maze [ 29 ] enforces integration of temporally separated cues for localization, while ViZDoom-Two-Colors [ 34 ] presents 3D naviga-tion tasks where agents must remember previously observed color cues to make correct decisions later in the episode. 

2D grid-world and diagnostic tasks. Grid- and vector-based environments provide controlled settings for studying temporal dependencies under partial observability. MiniGrid [ 11 ] and Mem-ory Gym [ 31 ] test sequence recall and delayed reasoning, while POPGym [ 24 ] and POPGym Arcade [ 40 ] cover both vector and visual modalities, emphasizing puzzle-like memory and POMDP control tasks. Synthetic POMDPs [ 39 ] extend this idea by proce-durally generating tasks with tunable observability and horizon length. The T-Maze [ 27 ] provides a minimal diagnostic setup where the agent must recall an early cue to act correctly later, and Endless Memory Gym [ 31 ] introduces adaptive, unbounded variants that evaluate continual memory updating over extended horizons. 

Cognitive and multi-modal benchmarks. Beyond navigation and sequence recall, several benchmarks target higher-level cogni-tive abilities. DeepMindâ€™s Memory Tasks Suite [ 20 ], Animal-AI [ 38 ], and BabyAI [ 10 ] test reasoning, instruction following, and object permanence. The POBAX benchmark [ 36 ] evaluates memory in partially observable settings through memory improvability â€“ the benefit of adding memory to agents â€“ across diverse tasks that require recalling information over time. 

Memory benchmarks in robotics. Memory has also emerged as a key challenge in robotic control, where agents must reason over continuous, partially observable dynamics. Benchmarks such as MIKASA-Robo [ 6] and MemoryBench [ 14 ] introduce visuomotor tasks that require recalling and updating latent scene properties across multiple interactions, emphasizing the role of memory in embodied decision-making. Existing benchmarks mainly test memory retention â€“ preserving information over time â€“ but rarely the ability to rewrite it when cues become outdated. Our Endless T-Maze and Color-Cubes bench-marks fill this gap by requiring agents to continually discard ob-solete cues and integrate new evidence under partial observability. Unlike Endless Memory Gym [ 31 ], which evaluates how long agents can preserve growing sequences of information in unbounded tasks, our environments focus on the opposite challenge â€“ when and how agents should overwrite prior beliefs as task conditions change. This shift from testing memory capacity to testing memory adap-tivity provides a complementary perspective on continual learning and exposes limitations in current architecturesâ€™ ability to update internal representations. 

## 4 BENCHMARKING MEMORY REWRITE 

Evaluating memory in RL has traditionally focused on how well agents retain past information. However, many real-world problems demand not just stable retention but also the ability to update internal representations when previously relevant cues become obsolete. To isolate and measure this capability, we introduce a set of controlled diagnostic environments specifically designed to stress continual memory rewriting under partial observability. Our framework comprises two families of tasks, Endless T-Maze and Color-Cubes , instantiated at three difficulty levels 

Trivial , Medium , and Extreme . All environments require agents to detect when internal information have become outdated and must be replaced, shifting the evaluation focus from long-term memory retention to adaptive memory manipulation. Endless T-Maze constitutes a minimal diagnostic that isolates the ability to overwrite old information with new one. At every corridor, the agent receives a fresh left/right cue that completely overrides the previous one, and only the most recent cue is relevant for the subsequent junction. In contrast, Color-Cubes serves as a full-fledged stress test for selective and context-dependent forget-ting. The agent navigates a grid world to collect cubes of a target color, while non-target cubes teleport over time. State updates are revealed only after specific events, and information that was previously irrelevant can later become useful again. As a result, the agent must determine what to discard, what to retain, and what to reinstate, thereby demanding not only memory rewriting but selective maintenance under uncertainty. We evaluate a diverse range of memory-augmented RL archi-tectures â€“ including recurrent models, transformer-based agents, and structured external memory systems â€“ enabling systematic comparison of the strategies by which different mechanisms sup-port continual memory updating. Taken together, these environ-ments provide a principled platform for investigating when and how agents rewrite memory, a capacity that is essential for adaptive behavior in non-stationary and partially observable domains. 

Endless T-Maze. This environment extends the T-Maze [ 27 ]into an infinite sequence of interconnected corridors, forming a continual version of the cue-based navigation task. At the start of each corridor, the agent receives a binary cue indicating whether Figure 2: T-Maze vs. Endless T-Maze . In the classic T-Maze 

(left), the agent receives a cue at the start and must remember it until the junction. Our proposed Endless T-Maze (right) extends this setup by chaining junctions: at each corridor, a fresh cue overrides all previous ones, and continual rewriting rather than simple retention is necessary to success. 

to turn left or right at the upcoming junction. Once a turn is made, the cue changes, invalidating all previous information â€“ thereby re-quiring continual memory overwriting rather than static retention. The observation space is a compact vector encoding the current cue and the agentâ€™s normalized position within the corridor, while the action space includes MOVE_FORWARD , TURN_LEFT , and TURN_ RIGHT . Rewards are assigned as +1 for a correct turn, âˆ’1 for an incorrect turn, and âˆ’0.01 if the agent does not move forward to the junction. Although the task is theoretically unbounded, in ex-periments we fix either the number of corridors or their maximum length to control task difficulty and ensure stable training. We also evaluate two sampling regimes for corridor lengths: (i) Fixed , where all corridors share the same length ð‘™ max , and (ii) Uniform , where lengths are sampled from a uniform distribution ð‘™ âˆ¼ U [ 1, ð‘™ max ],introducing stochastic variation in cue timing and memory horizon. Further implementation details are provided in Appendix A.1. 

Color-Cubes. Color-Cubes (Figure 3) is a grid-based environ-ment that evaluates an agentâ€™s ability to maintain, detect, and up-date internal representations under partial observability. The agent acts on a ðº Ã—ðº grid containing ð‘ uniquely colored cubes, each as-signed a color identifier ð‘ âˆˆ { 0, 1, . . . , ð‘ âˆ’1}. Each episode consists of ð¾ consecutive interaction phases. At the beginning of each phase, the agent is assigned a new target color ð‘ target , indicating which cube it must locate and interact with. During this initialization step, the agent observes the color of every cube and the target color 

ð‘ target , along with its own (ð‘¥, ð‘¦ ) position and the (ð‘¥, ð‘¦ ) coordinates of all cubes. This color information is shown only at the start of the phase or when the cube is accidentally teleported while the agent is moving toward the target, forcing the agent to rely on its memory of the target color and the initial color-position mapping to act correctly. To maintain partial visibility and not give the agent too many clues, there is a teleportation probability parameter ð‘ teleport ,which should not be set too high. After the start of each phase, the agent must navigate to the cube with color ð‘ target and execute the INTERACT action. During the agentâ€™s movement, any cube other than the current target cube may perform a stochastic teleportation with a certain probability 

ð‘ teleport , forcing the agent to detect inconsistencies between its memory and the current environment and update its internal map accordingly. Following each successful interaction, the target color changes, and the lifted cube is teleported to a new location. The 

Figure 3: Visualization of key principles of the Color-Cubes environment, such as Initialization, Stochastic Teleportation, Successful Interaction. 

agent receives a reward of +1 for a successful interaction and a small negative reward when moving away from the target cube. An episode terminates after successfully completing a number of phases or when the time expires. We define three difficulty levels that progressively increase the demands on memory rewriting: 

â€¢ Trivial â€“ Single cube and single target ( ð‘ =ð¾ =1). The agent only needs to memorize the color of one cube and perform one interaction; no rewriting is required. 

â€¢ Medium â€“ Multiple cubes with complete state updates (po-sitions and colors). During the agentâ€™s movement, several cubes may teleport, and the agent must update its memory to track the new configuration. 

â€¢ Extreme â€“ Multiple cubes with incomplete teleportation up-dates (positions only, colors hidden). The agent must infer which cube moved based on prior knowledge, update its in-ternal colorâ€“position mapping, and act accordingly, testing both rewriting and inference under uncertainty. Further details on the environment configuration and dynamics are provided in Appendix A.2. 

## 5 EXPERIMENTS 

We empirically evaluate how different memory architectures cope with continual memory rewriting under partial observability. We organize our investigation around four key research questions (RQs) that collectively probe different aspects of memory rewriting: 

â€¢ RQ1 (Retention). How do different memory mechanisms behave in trivial settings where rewriting is unnecessary (e.g., ð‘› =1 Endless T-Maze and Trivial Color-Cubes )? 

â€¢ RQ2 (Rewriting). Which mechanisms can reliably rewrite (i.e., discard obsolete cues and integrate new ones) when tasks explicitly require continual updates? 

â€¢ RQ3 (Generalization). How well do these mechanisms interpolate and extrapolate across horizons and rewrite fre-quencies in the Endless T-Maze environment (varying cor-ridor lengths ð‘™ and counts ð‘› ; fixed vs. uniform regimes)? 

â€¢ RQ4 (Prioritization). When multiple updates accumulate, which mechanisms can re-rank and maintain a consistent, up-to-date memory (e.g., Color-Cubes Medium /Extreme ) rather than merely forgetting? 5.1 Baselines 

To identify which architectural principles support effective mem-ory rewriting, we evaluate a broad set of agents covering both established and state-of-the-art (SOTA) memory mechanisms. Our study includes recurrent ( PPO-LSTM and PPO-GRU [ 30 ]), transformer-based ( GTrXL [ 28 ]), and structured-memory architec-tures ( FFM [ 25 ], SHM [ 22 ]). In addition, we use a memory-free 

PPO-MLP baseline to isolate the effect of explicit memory mecha-nisms under partial observability. Together, these agents represent the principal design paradigms for sequential decision-making in RL â€“ recurrent dynamics, attention-based context, and structured long-term memory â€“ and enable a systematic analysis of how each handles continual memory updating. 

## 5.2 Training and Evaluation Protocol 

We adopt a unified training and evaluation protocol to ensure com-parability across agents and environments. 

Training configurations. For Endless T-Maze , agents are trained across multiple combinations of corridor length ð‘™ , num-ber of corridors ð‘› , and corridor-length sampling regime ( Fixed 

vs. Uniform ); all configurations are summarized in Table 1. For 

Color-Cubes , we train agents on the three predefined difficulty modes â€“ Trivial , Medium , and Extreme â€“ with detailed parameters provided in Appendix A.2. 

Evaluation setup. For Endless T-Maze , each checkpoint is evaluated on the validation configurations listed in Table 1. We include both matched settings (same ð‘™ , ð‘› , and sampling regime as training) and cross-setting evaluations to probe generalization through interpolation (shorter corridors or fewer rewrite opera-tions than in training) and extrapolation (longer corridors or more rewrites). For Color-Cubes , agents trained on each difficulty mode are evaluated within that mode, allowing us to measure how well memory mechanisms handle increasing demands on continual up-dating and partial observability. 

Metrics and aggregation. Performance is reported as the suc-cess rate , defined as the fraction of correctly completed trials over 100 independent evaluation episodes per checkpoint. For each train-ing run, we first compute the mean success rate across these 100 episodes. We then report the overall mean and standard error of the mean (mean Â± SEM) across the ten independent runs. This aggregation captures both episode-level variability and the stability of learning across random seeds. 

Reproducibility and configuration control. All environment parameters used for training and validation â€“ including ð‘™ , ð‘› , and the sampling regime for Endless T-Maze , as well as grid size, num-ber of cubes, and teleportation probability for Color-Cubes â€“ are detailed in Appendix A. Each agent was tuned individually for each environment, after which its hyperparameters were kept fixed across all training and validation configurations of that environ-ment to ensure consistency within comparisons. The full list of hyperparameters and implementation details is provided in Appen-dix B. 

Table 1: Performance of all evaluated agents on the Endless T-Maze . Each configuration is defined by the corridor length 

ð‘™ and the number of corridors ð‘› under two regimes: Fixed (constant length) and Uniform (randomized length). Top-1 and top-2 results are highlighted. Here, ð‘› =1 corresponds to the classic single-turn T-Maze (pure retention), while larger 

ð‘› values require continual memory rewriting.                                                                                                        

> Regime ð‘™ ð‘› PPO-LSTM GTrXL SHM FFM PPO-MLP Fixed
> 511.00 Â±0.00 0.50 Â±0.19 1.00 Â±0.00 1.00 Â±0.00 0.50 Â±0.00 531.00 Â±0.00 0.17 Â±0.06 0.67 Â±0.24 1.00 Â±0.00 0.24 Â±0.05 551.00 Â±0.00 0.04 Â±0.02 0.84 Â±0.13 1.00 Â±0.00 0.05 Â±0.02 510 1.00 Â±0.00 0.61 Â±0.17 0.23 Â±0.22 1.00 Â±0.00 0.00 Â±0.00 10 11.00 Â±0.00 0.43 Â±0.01 0.84 Â±0.16 1.00 Â±0.00 0.50 Â±0.00 10 31.00 Â±0.00 0.17 Â±0.01 0.93 Â±0.07 1.00 Â±0.00 0.16 Â±0.02 10 50.69 Â±0.31 0.19 Â±0.12 0.52 Â±0.24 1.00 Â±0.00 0.04 Â±0.00 10 10 0.83 Â±0.17 0.02 Â±0.02 0.02 Â±0.01 0.73 Â±0.27 0.00 Â±0.00
> Uniform
> 511.00 Â±0.00 0.47 Â±0.08 0.55 Â±0.23 0.92 Â±0.08 0.26 Â±0.02 531.00 Â±0.00 0.15 Â±0.09 0.04 Â±0.01 0.43 Â±0.23 0.01 Â±0.01 551.00 Â±0.00 0.04 Â±0.01 0.04 Â±0.02 0.03 Â±0.03 0.00 Â±0.00 510 1.00 Â±0.00 0.00 Â±0.00 0.00 Â±0.00 0.00 Â±0.00 0.00 Â±0.00 10 11.00 Â±0.00 0.57 Â±0.02 0.91 Â±0.09 1.00 Â±0.00 0.24 Â±0.12 10 30.98 Â±0.02 0.16 Â±0.03 0.06 Â±0.04 0.00 Â±0.00 0.00 Â±0.00 10 51.00 Â±0.00 0.06 Â±0.01 0.04 Â±0.01 0.00 Â±0.00 0.00 Â±0.00 10 10 1.00 Â±0.00 0.01 Â±0.01 0.00 Â±0.00 0.00 Â±0.00 0.00 Â±0.00

## 6 RESULTS 

In order to group meaningful results across different environments and parameters, we propose to consider four research questions. 

RQ1: How different memory mechanisms deal with the triv-ial cases, where rewriting is not necessary? In the simplest 

Endless T-Maze task where ð‘› = 1 (become classic T-Maze), which only requires an agent to write the cue once and retain it through the episode, we evaluated the baseline performance of considered models. The results are presented in Table 1 (rows where ð‘› = 1). The PPO-LSTM, SHM, and FFM agents demonstrated best perfor-mance, achieving a perfect success rate ( 1.00 Â± 0.00 ) in all scenarios. In contrast, the GTrXL agentâ€™s success rates hovering around 50% 

like MLPâ€™s, indicating it could not reliably solve the task. In the 

Trivial Color-Cubes case represented in the first row in Table 3, the situation is slightly different. PPO-LSTM showed a result of 

0.52 Â± 0.10 , while FFM, GTrXL, and SHM showed perfect success. Based on the results, we can assume that the memory mecha-nisms of the PPO-LSTM, SHM, and FFM architectures are capable of retaining the necessary information within the scope of our ex-perimental protocols. A possible reason for the low success rate of GTrXL in conditions where the context length exceeds the episode length on Endless T-Maze is its instability in sparse reward condi-tions, which is confirmed by its high success rate on Color-Cubes ,where the agent regularly receives penalties or rewards. The sta-bility challenges of Transformer-based RL agents under sparse rewards also have been noted in prior work [5]. 

RQ2: What memory mechanisms cope with rewriting? To try to identify key features of memory mechanisms that aid in rewriting we consider Endless T-Maze validation tasks in which the length of the corridor, the number of corridors, and the sampling were identical to the training configuration, and the length of the Figure 4: Intermediate progress of SHM and GTrXL agents in the Endless T-Maze . The figure show the number of successfully passed corridors over time for different train-ing configurations of corridors number ð‘› and length ð‘™ . De-spite low overall success rates, both agents can partially progress through several corridors before failing, indicat-ing limited but nonzero capacity for short-term memory rewriting. Dashed red lines denote the target number of cor-ridors for each validation configuration to reach success. 

corridor was greater than 1 for Endless T-Maze environment in Table 1. The PPO-LSTM agent demonstrated complete success in most 

Endless T-maze tasks considered. Meanwhile, FFM was only able to successfully cope with tasks in fixed sampling mode with 1.00 Â±

0.00 result. SHM also demonstrated success in fixed tasks, but within 5 corridors ( ð‘› = 5) and less than FFM in configurations with ð‘› = 5

(0.84 Â± 0.13 in ð‘™ = 5 and 0.52 Â± 0.24 in ð‘™ = 10 ). GTrXL and MLP were unable to produce meaningful results in tasks that clearly required memory rewriting. To better understand how GTrXL and SHM behave in the 

Endless T-Maze , we examine how many corridors each agent can successfully traverse within an episode despite their overall low success rates (Figure 4). This analysis reveals how long these architectures can maintain accurate cue following before their memory representations deteriorate. In tasks with ð‘› =3 and ð‘› =5,both agents typically manage to navigate one corridor fewer than required, as indicated by the interrupted curves showing partial completion of the maze. 

Figure 5: Baseline comparison on Endless T-Maze under inter-polation and extrapolation conditions, that is, where corridor lengths and fixed sampling are the same during training and validation, and in each validation task the parameter of the number of corridors varies in accordance with the experi-ment protocol. The result for Success Rate is mean Â±sem. 

These results reveal that the baselines ability to handle memory rewriting is highly dependent on environmental predictability. The mechanism in PPO-LSTM is robust enough to succeed in both predictable ( Fixed ) and stochastic ( Uniform ) settings. Conversely, the mechanisms in FFM and SHM, while effective for static patterns, are not sufficiently flexible to adapt to the variability of the uniform mode. This suggests their rewriting capability is effective but limited to predictable scenarios. Also, SHM results regarding the number of timesteps required to complete episodes may be indicative of brute force, although stable successful execution of ð‘› âˆ’ 1 corridors in some tasks may refute this. 

RQ3: What memory mechanisms can demonstrate general-ization in memory rewriting? When considering these research question, we consider the terms â€œinterpolationâ€ and â€œextrapolationâ€ in context of Endless T-maze results. Interpolation means that the agent was able to cope with the validation task where the number of corridors or their length was less than the training configuration, while Extrapolation means the opposite result â€“ the number of corridors or their length in the validation task was greater. The results in the Figure 5 show that PPO-LSTM demonstrated interpolation for all lengths in non-trivial training configurations with uniform mode, achieving complete success regardless of the number of corridors trained and validated. In addition, PPO-LSTM was able to demonstrate incomplete but successful extrapolation to a corridor length of 10, again regardless of the number of corridors with a non-trivial uniform 5 training task. FFM, SHM, and GTrXL performed successfully in non-trivial fixed training configurations, the models show generalization to other numbers of corridors. The FFM agent demonstrated both ex-trapolation and interpolation across all such configurations, while SHM and GTrXL demonstrated only interpolation, and the trans-former architecture performs better. It is worth considering how the attention mechanism performed under uniform conditions using the example of the GTrXL baseline results. Agent managed to interpolate one uniform training task to fixed configuration task by the number of corridors. In particular, the training configuration ð‘› = 3, ð‘™ = 5, regime = Uniform showed a slight extrapolation to the task with 5 corridors with success 

0.38 Â± 0.21 .The generalization results allow us to establish a clear hierarchy of flexibility among the memory mechanisms. At the top is PPO-LSTM, which learns an abstract and adaptable strategy, enabling it to generalize successfully even in uniform en-vironments. Below it lies FFM, which demonstrates a more limited and conditional form of flexibility. Its ability to both interpolate and extrapolate, but exclusively within the predictable confines of the Fixed mode, suggests it can generalize a learned pattern, but cannot adapt the pattern itself. At the bottom of this hierarchy is SHM, which proves to be the most rigid. Its ability is restricted to interpolation only, indicating that its learned strategy is tightly coupled with the specific parameters of the training environment and cannot be scaled even in predictable settings. This distinct ranking of adaptability provides the piece of ev-idence, suggesting that the key to success lies in a fundamental property of the memory mechanism that dictates how it acquires and applies knowledge in new situations. 

RQ4: Which memory mechanisms are adaptive to efficiently ranking of rewritten information? After the first sub-episode, a new random target is chosen in Medium and Extreme modes. To successfully complete the second and subsequent sub-episodes, the agent must remember and record all teleportations that have oc-curred, and in Extreme mode, it must also make logical conclusions in order to correctly recognize which colors correspond to which coordinates. In other words, we also test the ability not only to adaptively overwrite and forget old information, but also dynam-ically update the storage, reevaluating the relevance of all stored data in order to be able to focus on the current task. Under these conditions, all baselines showed zero success Table 2, which may indicate that their memory mechanisms are not sensitive to tasks where simply forgetting information does not help. 

## 7 ABLATION STUDY 

Since the PPO-LSTM model outperformed other baselines under considered scenarios, a hypothesis was put forward that its effec-tiveness may be due to the specifics of its internal architecture, namely the presence of gates that separate and control different aspects of calculations. 

Figure 6: Representatives of recurrent models comparison on Endless T-Maze under interpolation and extrapolation conditions, that is, where corridor lengths and fixed sampling are the same during training and validation, and in each validation task the parameter of the number of corridors varies in accordance with the experiment protocol. The result for Success Rate is mean Â±sem. 

To test this hypothesis, a series of comparative experiments was conducted, including modifications of PPO with alternative recur-rent architectures â€“ PPO-RNN and PPO-GRU. The PPO-RNN model, based on a classic recurrent layer without gating mechanisms, will help in analyzing the general role of gates in memory rewriting tasks. Also PPO-GRU is an intermediate option in which the ar-chitecture is simplified compared to LSTM: the cell and hidden states are combined, and the number of gates is reduced from three (forget, input, output) to two (reset and update). This comparison allows us to evaluate not only the usefulness of gating as such, but also the impact of specific memory and information flow control mechanisms on overall performance. For performance comparison, we used scenarios from exper-iments in the Endless T-Maze environment in fixed mode, with corridor lengths varying between 5 and 10, and the number of cor-ridors varying between 3, 5, and 10. These scenarios were chosen because the extrapolation and interpolation conditions in them was useful in a comparative analysis of the success of the previous baselines taken. The results on Figure 6 show that RNN succeeds only in a limited subset of interpolation and extrapolation settings precisely those Table 2: The results in the all Color-Cubes . Medium and Ex-treme mode core parameters are: cubes = 3, sub-episodes = 3, grid size = 5, teleport chance = 0.3.                         

> PPO-LSTM GTrXL SHM FFM MLP
> Trivial 0.52 Â±0.10 1.00 Â±0.00 1.00 Â±0.00 1.00 Â±0.00 0.00 Â±0.00 Medium 0.00 Â±0.00 0.00 Â±0.00 0.00 Â±0.00 0.00 Â±0.00 0.00 Â±0.00 Extreme 0.00 Â±0.00 0.00 Â±0.00 0.00 Â±0.00 0.00 Â±0.00 0.00 Â±0.00

where SHM and FFM (also without gates) previously achieved par-tial success. At the same time, PPO-GRU demonstrated the ability to interpolate and extrapolate not only when changing the number of corridors in scenarios where SHM and FFM showed results, but also demonstrated a limited ability to generalize when changing the corridor length. However, PPO-GRU was inferior to PPO-LSTM in terms of generalization success over lengths. A comparison of PPO-RNN as a representative of recurrent mod-els without gating with PPO-GRU and PPO-LSTM, which showed significantly higher results, indicates that gating mechanisms have a key influence on the success of rewriting tasks. The higher per-formance of PPO-GRU over SHM, FFM, and PPO-RNN in its ability to generalize the solution to length variation further confirms the usefulness of gates in such tasks. At the same time, the advantage of PPO-LSTM over PPO-GRU in length generalization scenarios demonstrates the importance of an adaptive forgetting mechanism. 

## 8 DISCUSSION 

The results from the preceding research questions establish a consis-tent performance hierarchy among the tested architectures. Across tasks involving information retention (RQ1), rewriting (RQ2), and generalization (RQ3), the observed ranking was: PPO-LSTM, FFM, SHM, GTrXL, and MLP. This ranking correlates with the architectural approach each model takes to managing outdated information â€“ specifically, with its mechanism for forgetting. An analysis of the architectures re-veals a trend: 

â€¢ PPO-LSTM , the top-performing model, utilizes a dedicated forget gate with learnable parameters. This allows for an adaptive, context-dependent approach to information reten-tion and removal. 

â€¢ FFM architecture implements a more defined, rule-based forgetting process where older information is systematically replaced. This approach was effective in predictable fixed environments but was associated with a significant perfor-mance degradation in uniform settings. 

â€¢ GTrXL as mentioned earlier, the model is vulnerable to sparse reward environments, but at the same time it was able to show generalization in some scenarios. Nevertheless, the stabilization of architecture learning and information writing to the cache occurs with the help of gating, which once again emphasizes their necessity. 

â€¢ SHM has a similar concept of matrix memory structuring to FFM, but lacks an explicit forgetting mechanism. SHM has a dynamic update of information significance. This de-sign correlated with lower performance compared to FFM, particularly in generalization tasks. Based on this observed correlation, we hypothesize that the nature and adaptability of the forgetting mechanism may be key factors determining an agentâ€™s effectiveness in performing tasks that require memory rewriting in dynamic environments. A comparison in an ablation study with PPO-RNN confirmed that PPO-LSTM gates contributed to its success, while a comparison of its gates with PPO-GRU gates highlighted the need for an adaptive forgetting mechanism. However, PPO-LSTM performed worse than other memory-based baselines in Trivial Color-Cubes. Since this case study does not involve rewriting, it cannot be said that SHM, FFM, and GTrXL are superior, but PPO-LSTM may not be robust to grid environ-ments. 

## 9 LIMITATIONS & FUTURE WORK 

One of the key limitations of this study is the small number of scientific papers devoted directly to the mechanisms of memory rewriting in RL. This complicates the comparison of the results obtained. In addition, a broader range of benchmarks and a larger number of diverse agents are needed to more accurately assess and isolate individual memory abilities and mechanisms. Finally, based on the analysis of the results obtained, a promising direction for further work is the development of a proprietary mechanism that specifically addresses the task of memory rewriting. 

## 10 CONCLUSION 

Our study reveals that memory retention alone is insufficient for solving RL tasks that require continual adaptation under partial observability. Through the proposed Endless T-Maze and Color -Cubes benchmarks, we isolate and evaluate the ability of RL agents to rewrite memory â€“ to selectively discard obsolete information and integrate new evidence as environments evolve. Across recur-rent, transformer-based, and structured-memory architectures, we observe a consistent hierarchy of adaptive competence: agents with explicit, learnable forgetting mechanisms (such as LSTM-based policies) exhibit strong performance and generalization, whereas attention- and cache-based models struggle when rewriting is es-sential rather than optional. These findings suggest that the core challenge in memory-intensive decision-making is not the preser-vation of information but its controlled transformation over time. Future memory systems should therefore move beyond static rep-resentations of history toward architectures that model memory as an active, continuously updated belief state â€“ capable of forgetting, rewriting, and reallocating representational capacity as conditions change. We view this as a step toward closing the gap between ar-tificial and biological memory, where adaptation and forgetting are not failure modes but fundamental features of intelligent behavior. 

## ACKNOWLEDGMENTS 

Some figures in this paper use Chrome Dinosaurâ€“style sprites ob-tained from Wikimedia Commons, which are licensed under their respective Creative Commons licenses (e.g., CC BY 4.0 / CC BY-SA 4.0). We gratefully acknowledge the creators and maintainers of these assets. 

## REFERENCES 

[1] Mohamadreza Ahmadi, Nils Jansen, Bo Wu, and Ufuk Topcu. 2020. Control theory meets pomdps: A hybrid systems approach. IEEE Trans. Automat. Control 

66, 11 (2020), 5191â€“5204. [2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. 2018. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European conference on computer vision (ECCV) . 139â€“154. [3] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich KÃ¼ttler, Andrew Lefrancq, Simon Green, VÃ­ctor ValdÃ©s, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. 2016. DeepMind Lab. arXiv:1612.03801 [cs.AI] https://arxiv.org/abs/ 1612.03801 [4] Zachary H Bretton, Hyojeong Kim, Marie T Banich, and Jarrod A Lewis-Peacock. 2024. Suppressing the maintenance of information in working memory alters long-term memory traces. Journal of Cognitive Neuroscience 36, 10 (2024), 2117â€“ 2136. [5] Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. 2024. TransDreamer: Re-inforcement Learning with Transformer World Models. arXiv:2202.09481 [cs.LG] https://arxiv.org/abs/2202.09481 [6] Egor Cherepanov, Nikita Kachaev, Alexey K Kovalev, and Aleksandr I Panov. 2025. Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning. arXiv preprint arXiv:2502.10550 (2025). [7] Egor Cherepanov, Nikita Kachaev, Artem Zholus, Alexey K Kovalev, and Alek-sandr I Panov. 2024. Unraveling the Complexity of Memory in RL Agents: an Approach for Classification and Evaluation. arXiv preprint arXiv:2412.06531 

(2024). [8] Egor Cherepanov, Alexey Kovalev, and Aleksandr Panov. 2025. ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL. In CoRL 2025 Workshop RemembeRL . https://openreview.net/forum?id=H2dvLYqlaa [9] Egor Cherepanov, Alexey Staroverov, Dmitry Yudin, Alexey K Kovalev, and Aleksandr I Panov. 2023. Recurrent action transformer with memory. arXiv preprint arXiv:2306.09459 (2023). [10] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2018. Babyai: A platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272 (2018). [11] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. 2023. Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks. arXiv:2306.13831 [cs.LG] https://arxiv.org/abs/2306.13831 [12] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. arXiv:1901.02860 [cs.LG] https://arxiv.org/abs/1901.02860 [13] Kevin Esslinger, Robert Platt, and Christopher Amato. 2022. Deep transformer q-networks for partially observable reinforcement learning. arXiv preprint arXiv:2206.01078 (2022). [14] Haoquan Fang, Markus Grotz, Wilbert Pumacay, Yi Ru Wang, Dieter Fox, Ranjay Krishna, and Jiafei Duan. 2025. Sam2act: Integrating visual foundation model with a memory architecture for robotic manipulation. arXiv preprint arXiv:2501.18564 

(2025). [15] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-Term Memory. Neu-ral Computation 9 (1997), 1735â€“1780. https://api.semanticscholar.org/CorpusID: 1915014 [16] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. 1998. Plan-ning and acting in partially observable stochastic domains. Artificial intelligence 

101, 1-2 (1998), 99â€“134. [17] Peter Koepernik, Ruo Yu Tao, Ronald Parr, George Konidaris, and Cameron Allen. 2025. General Value Discrepancies Mitigate Partial Observability in Reinforcement Learning. In Finding the Frame Workshop at RLC 2025 . https: //openreview.net/forum?id=IrpcqYhREq [18] Seth R Koslov, Arjun Mukerji, Katlyn R Hedgpeth, and Jarrod A Lewis-Peacock. 2019. Cognitive flexibility improves memory for delayed intentions. ENeuro 6, 6 (2019). [19] Hanna Kurniawati. 2022. Partially Observable Markov Decision Processes and Robotics. Annu. Rev. Control. Robotics Auton. Syst. 5 (2022), 253â€“277. https: //api.semanticscholar.org/CorpusID:245789500 [20] Andrew Lampinen, Stephanie Chan, Andrea Banino, and Felix Hill. 2021. Towards mental time travel: a hierarchical memory for reinforcement learning agents. 

Advances in Neural Information Processing Systems 34 (2021), 28182â€“28195. [21] Mikko Lauri, David Hsu, and Joni Pajarinen. 2022. Partially observable markov decision processes in robotics: A survey. IEEE Transactions on Robotics 39, 1 (2022), 21â€“40. [22] Hung Le, Kien Do, Dung Nguyen, Sunil Gupta, and Svetha Venkatesh. 2024. Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning. arXiv:2410.10132 [cs.LG] https://arxiv.org/abs/2410.10132 [23] Lingheng Meng, Rob Gorbet, and Dana KuliÄ‡. 2021. Memory-based deep re-inforcement learning for pomdps. In 2021 IEEE/RSJ international conference on intelligent robots and systems (IROS) . IEEE, 5619â€“5626. [24] Steven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, and Amanda Prorok. 2023. POPGym: Benchmarking Partially Observable Reinforcement Learning. arXiv:2303.01859 [cs.LG] https://arxiv.org/abs/2303.01859 [25] Steven Morad, Ryan Kortvelesy, Stephan Liwicki, and Amanda Prorok. 2023. Re-inforcement Learning with Fast and Forgetful Memory. arXiv:2310.04128 [cs.LG] https://arxiv.org/abs/2310.04128 [26] Swaleha Mujawar, Jaideep Patil, Bhushan Chaudhari, and Daniel Saldanha. 2021. Memory: Neurobiological mechanisms and assessment. Industrial psychiatry journal 30, Suppl 1 (2021), S311â€“S314. [27] Tianwei Ni, Michel Ma, Benjamin Eysenbach, and Pierre-Luc Bacon. 2023. When do transformers shine in rl? decoupling memory from credit assignment. Ad-vances in Neural Information Processing Systems 36 (2023), 50429â€“50452. [28] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Sid-dhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al . 2020. Stabilizing transformers for reinforcement learning. In Inter-national conference on machine learning . PMLR, 7487â€“7498. [29] Jurgis Pasukonis, Timothy Lillicrap, and Danijar Hafner. 2022. Evaluating Long-Term Memory in 3D Mazes. arXiv:2210.13383 [cs.AI] https://arxiv.org/abs/2210. 13383 [30] Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. 2022. Gen-eralization, Mayhems and Limits in Recurrent Proximal Policy Optimization. arXiv:2205.11104 [cs.LG] https://arxiv.org/abs/2205.11104 [31] Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. 2025. Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of Agents. 

Journal of Machine Learning Research 26, 6 (2025), 1â€“40. [32] John J Sakon and Roozbeh Kiani. 2022. Differences in memory for what, where, and when components of recently formed episodes. Journal of neurophysiology 

128, 2 (2022), 310â€“325. [33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG] https://arxiv.org/abs/1707.06347 [34] Artyom Sorokin, Nazar Buzun, Leonid Pugachev, and Mikhail Burtsev. 2022. Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Un-certain Outcomes. (07 2022). https://doi.org/10.48550/arXiv.2207.13649 [35] Richard S Sutton, Andrew G Barto, et al . 1998. Reinforcement learning: An intro-duction . Vol. 1. MIT press Cambridge. [36] Ruo Yu Tao, Kaicheng Guo, Cameron Allen, and George Konidaris. 2025. Bench-marking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains. arXiv preprint arXiv:2508.00046 (2025). [37] Ruo Yu Tao, Kaicheng Guo, Cameron S. Allen, and George Dimitri Konidaris. 2025. Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains. ArXiv abs/2508.00046 (2025). https: //api.semanticscholar.org/CorpusID:280297434 [38] Konstantinos Voudouris, Ibrahim Alhas, Wout Schellaert, Matteo G. Mecattaf, Ben Slater, Matthew Crosby, Joel Holmes, John Burden, Niharika Chaubey, Niall Donnelly, Matishalin Patel, Marta Halina, JosÃ© HernÃ¡ndez-Orallo, and Lucy G. Cheke. 2025. The Animal-AI Environment: A Virtual Laboratory For Comparative Cognition and Artificial Intelligence Research. arXiv:2312.11414 [cs.AI] https: //arxiv.org/abs/2312.11414 [39] Yongyi Wang, Lingfeng Li, Bozhou Chen, Ang Li, Hanyu Liu, Qirui Zheng, Xionghui Yang, and Wenxin Li. 2025. Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling. arXiv preprint arXiv:2508.04282 (2025). [40] Zekang Wang, Zhe He, Borong Zhang, Edan Toledo, and Steven Morad. 2025. POPGym Arcade: Parallel Pixelated POMDPs. arXiv:2503.01450 [cs.LG] https: //arxiv.org/abs/2503.01450 [41] Zhifang Ye, Liang Shi, Anqi Li, Chuansheng Chen, and Gui Xue. 2020. Retrieval practice facilitates memory updating by enhancing and differentiating medial prefrontal cortex representations. Elife 9 (2020), e57023. [42] Daniil Zelezetsky, Egor Cherepanov, Alexey K Kovalev, and Aleksandr I Panov. 2025. Re: Frameâ€“Retrieving Experience From Associative Memory. arXiv preprint arXiv:2508.19344 (2025). [43] Haoxu Zhang, Parham M Kebria, Shady Mohamed, Samson Yu, and Saeid Na-havandi. 2023. A Review on Robot Manipulation Methods in Human-Robot Interactions. arXiv preprint arXiv:2309.04687 (2023). A DETAILED ENVIRONMENTS DESCRIPTIONS A.1 Detailed Description: Endless T-Maze. 

The Endless T-Maze environment extends the classical T-Maze [ 27 ] into a continual, multi-stage task that explicitly tests an agentâ€™s ability to rewrite memory as new cues arrive. The environ-ment consists of a sequential chain of ð‘ T-shaped corridors, each ending in a leftâ€“right junction. At the beginning of every corridor, the agent receives a binary cue represented as a two-dimensional vector â€“ (1, 0) for â€œturn leftâ€ or (0, 1) for â€œturn rightâ€. This cue is displayed only at the corridorâ€™s start and becomes invalid once the next corridor begins, forcing the agent to overwrite previously stored information rather than accumulate it. During traversal, the agent moves forward along the corridor, observing its normalized position (a scalar between 0 and 1) and the cue vector, which becomes (0, 0) after the first step. The observa-tion space therefore consists of three values: (cue 1, cue 2, position ).The discrete action space includes MOVE_FORWARD , TURN_LEFT , and 

TURN_RIGHT . A correct turn at a junction yields a reward of +1,an incorrect turn results in a penalty of âˆ’1, and each idle or for-ward step incurs a small penalty of âˆ’0.01 to encourage efficient movement. Two corridor-length regimes are supported: 

â€¢ Fixed: all corridors have the same predefined length ð‘™ max .

â€¢ Uniform: corridor lengths are sampled from a uniform dis-tribution ð‘™ âˆ¼ U [ 1, ð‘™ max ], introducing variability in memory duration between cue presentation and decision. Although the environment is conceptually infinite, in practice episodes are truncated after a fixed number of corridors or a global step limit to maintain stable training and evaluation. This setup directly measures an agentâ€™s capacity to discard obsolete cues and replace them with new, task-relevant information in a continually evolving sequence of decisions. 

## A.2 Detailed Description: Color-Cubes. 

The environment consists of a two-dimensional grid of size ðº Ã—ðº 

with ð‘ cubes placed at random, unique positions. Each cube is assigned a unique color ð‘ âˆˆ { 0, 1, . . . , ð‘ âˆ’1}, and the agentâ€™s goal is to sequentially locate and interact with cubes of a specified target color. An episode is divided into ð¾ sub-episodes. At the start of each sub-episode, the agent receives the target color ð‘ target . The agent must navigate to the corresponding cube and execute the INTERACT 

action. A successful interaction concludes the sub-episode, grants a reward of +1.0, teleports the collected cube to a new random unoccupied cell, and assigns a new target color from the remaining set of cubes. The agent also receives a new full_state_update .At every timestep where no successful interaction occurs, a ran-dom non-target cube teleports with probability ð‘ teleport , triggering another full_state_update . If neither event happens, the update is withheld, forcing the agent to act using only its internal memory. Because the world state may change without notice, outdated mem-ory can lead the agent to an empty cell where a cube previously stood. 

Difficulty variants. 

â€¢ Trivial: One cube and one target ( ð‘ =ð¾ =1). The agent only needs to remember and interact with a single target cube. 

â€¢ Medium: The standard setup with multiple cubes and com-plete state updates (positions and colors). 

â€¢ Extreme: Teleportation updates omit color identifiers. The agent must recall the initial colorâ€“position mapping, identify which cube moved, and update its internal representation accordingly. An episode terminates after ð¾ successful interactions or upon reaching the time limit. This setup explicitly tests whether agents can maintain, detect, and rewrite memory representations when the environment changes unpredictably. 

## B TRAINING CONFIGURATION 

Table 3: Hyperparameters for SHM. Hyperparameter Value 

Hidden size (â„Ž) 512 Memory size (ð‘š ) 128 Post-processing size 1024 Learning rate 5 Ã— 10 âˆ’5

Discount (ð›¾ ) 0.99 GAE lambda (ðœ† ) 1.0 Gradient clipping 0.5 Entropy coefficient 0.001 Value loss coefficient 0.5 BPTT length 1024 Train batch size 65536 Minibatch size 8192 PPO epochs 6Number of workers 12 Total timesteps 2M Framework Ray RLlib Table 4: Hyperparameters for FFM. Hyperparameter Value 

Hidden size 128 Memory size (ð‘š ) 128 Post-processing size 1024 Learning rate 5 Ã— 10 âˆ’5

Discount (ð›¾ ) 0.99 GAE lambda (ðœ† ) 1.0 Gradient clipping 0.5 Entropy coefficient 0.001 Value loss coefficient 0.5 BPTT length 1024 Train batch size 65536 Minibatch size 8192 PPO epochs 6Number of workers 8Total timesteps 2M Framework Ray RLlib 

Table 5: Hyperparameters for MLP. Hyperparameter Value 

Hidden size 256 Memory None Learning rate 5 Ã— 10 âˆ’5

Discount (ð›¾ ) 0.99 GAE lambda (ðœ† ) 1.0 Gradient clipping 0.5 Entropy coefficient 0.001 Value loss coefficient 0.5 BPTT length 1024 Train batch size 65536 Minibatch size 8192 PPO epochs 6Number of workers 2Total timesteps 2M Framework Ray RLlib 

Table 6: Hyperparameters for GTrXL. Hyperparameter Value 

Hidden size 512 Memory length 100 Transformer blocks 4Attention heads 8Embedding dimension 512 Positional encoding Relative GTrXL gating Enabled Learning rate (initial) 2.75 Ã— 10 âˆ’4

Learning rate (final) 1 Ã— 10 âˆ’5

Discount (ð›¾ ) 0.995 GAE lambda (ðœ† ) 0.95 Gradient clipping 0.25 Entropy coeff (initial) 0.001 Entropy coeff (final) 1 Ã— 10 âˆ’6

Value loss coefficient 0.5 PPO epochs 3Number of workers 32 Steps per worker 512 Minibatches per epoch 8Total timesteps 2M Framework Custom PPO 

Table 7: Hyperparameters for PPO-LSTM. Hyperparameter Value 

LSTM hidden size 128 Actor network [128, 128] Critic network [128, 128] Learning rate (initial) 3 Ã— 10 âˆ’4

Learning rate (final) 1 Ã— 10 âˆ’5

LR schedule Linear Discount (ð›¾ ) 0.99 GAE lambda (ðœ† ) 0.98 Gradient clipping 0.5 Entropy coefficient 0.1 Value loss coefficient 0.5 Sequence length 128 Minibatch size 512 PPO epochs 10 Number of environments 16 Steps per environment 128 Total timesteps 2M Framework Stable-Baselines3