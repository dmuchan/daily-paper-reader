Title: Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks

URL Source: https://arxiv.org/pdf/2601.15094v1

Published Time: Thu, 22 Jan 2026 02:17:07 GMT

Number of Pages: 32

Markdown Content:
# Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 

## MD ZAHIDUL HAQUE, William & Mary, USA 

## SAIMA AFRIN, William & Mary, USA 

## ANTONIO MASTROPAOLO, William & Mary, USA 

Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis‚Äìan aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues. CCS Concepts: ‚Ä¢ Computing methodologies ‚Üí Machine learning ; ‚Ä¢ Software and its engineering ‚Üí Software verification and valida-tion .Additional Key Words and Phrases: Multi-task Learning, Code Generation, Code Summarization, Code Translation, QLoRA, Full Fine-tuning, Non-functional Requirements, Static Analysis 

ACM Reference Format: 

Md Zahidul Haque, Saima Afrin, and Antonio Mastropaolo. 2026. Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks. 1, 1 (Janu-ary 2026), 32 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn 

1 Introduction 

The emergence of Large Code Models ( LCMs )‚Äìspecialized architectures designed for code understanding and generation‚Äìhas fundamentally expanded the automation capabilities within software engineering (SE). As adaptations of Large Language Models (LLMs) for code-intensive applications, LCMs have redefined the boundaries of what practitioners and developers can accomplish when supported by intelligent coding assistants capable of automating a variety of activities, including the generation of various software artifacts, such as code and code documentation [6, 16, 20, 28]. By leveraging large-scale pre-training on source code and natural language corpora, LCMs capture latent aspects of program semantics that enable them to automate key software engineer-ing tasks and streamline workflows across the development lifecycle [73]. These capabilities have transformed LCMs into powerful tools for practitioners, facilitating effective knowledge transfer and fundamentally reshaping how developers understand, write, and reason about code. Facilitated by the introduction and widespread adoption of ‚Äúon-the-fly‚Äù adaptation techniques‚Äìsuch as prompt engineering methods that enable task-specific calibration without retraining or modifying the model‚Äôs internal parameters‚Äì LCMs have quickly established themselves as flexible and accessible tools for a broad range of software engineering applications. These methods allow practitioners to guide model behavior through natural language instructions, examples, or structured information ( i.e., prompts), reducing the need for costly fine-tuning cycles while maintaining strong performance across diverse tasks. 

Authors‚Äô Contact Information: Md Zahidul Haque, William & Mary, Computer Science, Williamsburg, VA, USA, mhaque@wm.edu; Saima Afrin, William & Mary, Computer Science, Williamsburg, VA, USA, safrin@wm.edu; Antonio Mastropaolo, William & Mary, Computer Science, Williamsburg, VA, USA, amastropaolo@wm. edu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ¬© 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM Manuscript submitted to ACM 1

> arXiv:2601.15094v1 [cs.SE] 21 Jan 2026

2 Haque, Afrin and Mastropaolo However, despite the impressive adaptability offered by prompt-based interaction, recent studies [3, 6, 7, 32, 79] have shown that fine-tuning remains essential for achieving consistent, high-quality performance‚Äìparticularly in specialized downstream code-related tasks such as code summarization and code generation. In this regard, fine-tuned LCMs not only exhibit greater robustness and contextual precision but, when optimized efficiently, can attain these improvements with substantially lower resource demands through techniques such as Parameter-Efficient Fine-Tuning (PEFT) [9, 37, 58, 63, 80]. PEFT methods achieve targeted model adaptation by updating only a small subset of parameters, thus removing the need for complete model retraining. This strategy offers a highly favorable balance between efficiency and performance, achieving substantial reductions in computational cost while maintaining‚Äìor even improving‚Äìaccuracy on downstream code-related tasks [9, 14, 15, 22, 33, 57, 71, 72, 75, 91]. If, on the one hand, this behavior represents a clear advantage in terms of efficiency and scalability, on the other hand, it raises important questions about the extent to which these improvements hold across heterogeneous tasks. Most existing evaluations of PEFT methods‚Äìincluding QLoRA[19], which has been consistently identified as one of the most effective fine-tuning approaches for software engineering automation [4]‚Äìhave been conducted in Single-task settings, typically focusing on code generation or summarization. In such cases, task boundaries are clearly defined, and model objectives remain uniform. However, real-world software engineering workflows often involve multiple, interdependent tasks, such as generating, translating, and summarizing code, which demand that models generalize effectively across different modalities and reasoning processes. On top of that, introducing additional parameters through QLoRA optimization may disrupt the delicate balance between pa-rameter efficiency and representational stability, particularly during joint fine-tuning across diverse tasks. This concern aligns with the findings of Afrin et al. [5], who demonstrated that quantization can reshape a model‚Äôs parameter space by altering its topological structure, potentially affecting output quality. Since QLoRA inherently couples learnable parameter injection with quantization, it becomes crucial to examine whether these combined processes compromise output quality, consistency, and reliability‚Äìan aspect particularly relevant to modern software engineering practices, where models must handle heterogeneous and interdependent tasks within a unified framework. Understanding this interaction is essential to determine whether QLoRA maintains both efficiency and stability across diverse code-related applications. In practice, this requires models to perform ef-fectively on tasks such as natural language to code generation (NL-to-Code), code summarization (Code-to-NL), and code trans-formations (Code-to-Code) ( e.g., code translation). Ensuring consistent performance across these modalities demands a shared representational space that captures the semantic and syntactic relationships between programming and natural languages. Eval-uating QLoRA‚Äôs capacity to preserve this coherence under Multi-task fine-tuning is therefore key to developing scalable, resource-efficient LCMs capable of supporting real-world software engineering workflows. Motivated by these considerations, this paper investigates the effect of Multi-task QLoRA fine-tuning across the full spectrum of code-related tasks, evaluating code generation , code summarization , and code translation under a unified framework. Specifically, we adopt Qwen2.5-Coder‚Äìa model family widely recognized for its performance in code-oriented applications‚Äìas the base CLM .To ensure a controlled and balanced evaluation, we employ two task-aligned datasets (one for Python and one for Java ) for the Code-to-NL (code summarization) and NL-to-Code tasks (code generation), thereby strengthening the generalizability of our findings. For Code-to-Code applications, we focus on code translation, following prior research [9, 15, 33, 70, 71, 75] by selecting 

Java and C# as the source and target languages. Our experimental design comprises three distinct training configurations applied across three model scales from the Qwen2.5-Coder family (0.5B, 1.5B, and 3B parameters). For comprehensive evaluation, we implement: (i) QLoRA fine-tuning for both Multi-task and Single-task scenarios, (ii) full-parameter fine-tuning (FFT) for Multi-task scenarios. This results in the specialization ( i.e., 

training) of 15 models in total: 3 Multi-task QLoRA models (one per size trained on all three tasks), 9 Single-task QLoRA models (3 tasks √ó 3 sizes), and 3 Multi-task FFT models (one per size trained on all three tasks). We systematically analyze differences in functional correctness and non-functional quality, while controlling for potential confounding factors to ensure a rigorous and fair comparison across all configurations. Our comprehensive evaluation reveals nuanced trade-offs between correctness and quality in Multi-task QLoRA configurations. While Multi-task QLoRA demonstrates comparable functional correctness to Single-task variants, it achieves this with substan-tially reduced memory footprint. When contrasted with full-parameter Multi-task fine-tuning, QLoRA maintains competitive 

> Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 3performance while requiring significantly fewer computational resources. Perhaps most surprisingly, our non-functional anal-ysis indicates that Multi-task QLoRA does not systematically degrade code quality‚Äìcontradicting assumptions about inevitable quality compromises in parameter-efficient methods. To the best of our knowledge, this constitutes the first comprehensive investigation examining both functional correctness and non-functional quality attributes of Multi-task QLoRA across the full spectrum of code-related tasks. Specifically, our contributions encompass: 

‚Ä¢ A systematic empirical comparison of Multi-task versus Single-task QLoRA configurations across code generation, sum-marization, and translation, quantifying differences in functional correctness and resource utilization across the Qwen2.5-Coder model family. 

‚Ä¢ A rigorous evaluation contrasting Multi-task QLoRA with Multi-task full fine-tuning , establishing the practical trade-offs between parameter efficiency and functional performance across varying model scales. 

‚Ä¢ The first comprehensive non-functional quality assessment of Multi-task QLoRA outputs, employing: (i) static analyzers (Pylint, PMD, Roslyn, Lizard, SonarCloud) for generation and translation tasks, and (ii) LLM-as-judge evaluation for summa-rization to assess summary quality across multiple dimensions. 

‚Ä¢ A publicly available replication package [2] containing curated datasets, fine-tuned models spanning the Qwen2.5-Coder family, evaluation harnesses, and comprehensive documentation to facilitate reproducibility and extension of our findings. 

2 Background and Related Work 

This section provides an overview of recent advancements that form the foundation of our investigation. We begin by examining large code models and their role in automating software engineering tasks. We then discuss parameter-efficient fine-tuning (PEFT) techniques, with particular emphasis on QLoRA, which enable efficient adaptation of large models to specialized domains. Finally, we review recent findings on the quality of LLM-generated software artifacts, demonstrating that functional correctness alone provides an insufficient evaluation framework. While our work focuses on Multi-task QLoRA optimization, we note that Multi-task learning approaches for code models have evolved substantially since the introduction of text-to-text transfer transformers (T5) [40, 42, 54, 76]; readers seeking a comprehensive synthesis of this evolution are directed to the systematic literature review by Watson et al. [77]. 

2.1 Large Code Models 

Recent advances in large language models have substantially expanded capabilities for code automation. Specialized large code models ( LCMs ) including CodeGen [44], AlphaCode [29], PolyCoder [82], CodeLlama [56], DeepSeek-Coder [89], and Qwen2.5-Coder [24] demonstrate proficiency across diverse software engineering tasks [23], including code generation, summarization, translation, bug fixing, and test generation. General-purpose language models such as GPT-5 [45], Claude [8], and Gemini [66] similarly exhibit strong performance on code-related tasks, establishing their utility as benchmarks for evaluating model reason-ing, cross-language generalization, and coding capabilities. Although LCMs are often used without fine-tuning‚Äìadapting instead through instruction-based prompting that dynamically shapes model behavior rather than adjusting weights via backpropagation‚Äìthis approach typically achieves only superficial adap-tation with limited specialization for complex or domain-specific tasks [52, 78]. Fine-tuning remains the most effective strategy for optimal performance and reliability, as it enables models to internalize domain-specific representations and capture nuanced task dependencies that prompting alone cannot achieve [60, 80]. 

2.2 Parameter-Efficient Fine-Tuning 

The substantial computational demands of fine-tuning billion parameter code models have motivated research into optimization strategies that reduce resource requirements while preserving model effectiveness. In a recent literature mapping, Shi et al. [59] identified four key areas for optimizing large code models: data reduction, model-centric approaches, system-centric methods, and program-centric techniques. Parameter-Efficient Fine-Tuning (PEFT) falls within the model-centric category, optimizing model adaptation by updating only a subset of parameters rather than the entire model. The success of QLoRA for various coding tasks‚Äì as demonstrated in the systematic literature review by Afrin et al. [4] can be attributed to its ability to achieve both performance 

> Manuscript submitted to ACM

4 Haque, Afrin and Mastropaolo improvements and efficiency gains in almost all evaluated applications across Automated Program Repair, Code Completion, Code Generation, and Code Summarization. This reliability stems from QLoRA‚Äôs integration of 4-bit quantization with low-rank adaptation, enabling effective fine-tuning of large models in resource-constrained settings. Among the various methods, the prevalence of QLoRA Recent research has applied PEFT methods to various code automation tasks, though primarily in Single-task configurations. Studies have explored Adapter tuning [74], LoRA, and Prompt Tuning for tasks including code search, summarization, clone detection, defect detection, and code translation [3, 9, 14, 15, 33, 35, 64, 71, 72]. These investigations demonstrate that parameter-efficient methods can achieve performance comparable to or exceeding full fine-tuning while updating only a fraction of model parameters. However, existing PEFT research for code focuses exclusively on Single-task scenarios where models are optimized for one specific objective. Whether parameter-efficient methods can maintain effectiveness when learning multiple code-related tasks simultaneously remains unexplored. 

2.3 Quantized Low-Rank Adaptation (QLoRA) of Large Code Models 

Dettmers et al. [19] proposed QLoRA, an approach combining LoRA with LLM quantization 1. QLoRA introduces key innova-tions including (i) the 4-bit NormalFloat (NF4) data type, (ii) Double Quantization (DQ), and (iii) Paged Optimizers, enabling efficient fine-tuning that reduces memory usage while maintaining high performance [19]. The method quantizes pre-trained model weights to 4-bit precision using NF4, a data type optimized for neural network weight distributions. Double quantization further reduces memory footprint by quantizing both model weights and quantization constants, while Paged Optimizers man-age memory spikes during gradient checkpointing to prevent out-of-memory errors. A detailed explanation of QLoRA and the fine-tuning process is provided in Section 3.3 and Section 3.4. 

2.3.1 QLoRA-based Optimization of LCMs. Limited research has investigated QLoRA‚Äôs efficiency for code language models. Yang 

et al. [83] applied QLoRA to models including CodeLlama [1], StarChat-alpha [67], and Mistral-Instruct-7B [25] for automatic program repair (APR), demonstrating its effectiveness in defect repair tasks. Weyssow et al. [79] compared PEFT techniques to In-Context Learning (ICL) for code generation, finding PEFT methods superior, and investigated QLoRA applicability across CodeLlama 7B, 13B, and 34B Python models using 8-bit and 4-bit quantization. On the other hand, Afrin et al. [3] explored QLoRA for code summarization tasks, demonstrating proficient performance when models are required to process code as input, capture its intent, and project that understanding into natural language output. Liu et al. [31] introduced MFTCoder, a Multi-task fine-tuning framework that applies LoRA and QLoRA across five code-related tasks: code completion, text-to-code generation, code comment generation, code translation, and unit test case generation. The framework incorporates multiple loss functions to address data imbalance, task difficulty variations, and convergence speed inconsistencies across tasks. Experimental results on CodeLlama-13B-Python demonstrated that their Multi-task approach out-performed both Single-task fine-tuning and mixed-task data training across multiple benchmarks, achieving 74.4% pass@1 on HumanEval with their CodeFuse-CodeLlama-34B model. Despite these advances, several limitations persist in the current state of research. First, MFTCoder relies on synthetically generated training data through Self-Instruct and GPT-based generation for specific tasks, particularly code exercises datasets, raising questions about generalizability to real-world code distributions. Second, evaluation focuses exclusively on functional cor-rectness metrics‚Äìpass@1 for generation tasks, BLEU scores for translation and summarization, and GPT-4-based assessment for code comments‚Äìwithout examining the quality attributes of code produced by generation and translation tasks, or the linguistic quality of natural language summaries beyond similarity scores. To address these gaps, our work extends the evaluation of Multi-task QLoRA in three key directions. First, we conduct a systematic comparison across model scales (0.5B, 1.5B, 3B) to understand how capacity interacts with Multi-task learning un-der parameter-efficient constraints. Second, we provide the first comprehensive non-functional quality assessment of Multi-task 

QLoRA outputs, employing static analyzers (Pylint, PMD, Roslyn, Lizard, SonarCloud) for code generation and translation tasks, and LLM-as-judge evaluation for summarization quality beyond lexical similarity. Third, we contrast Multi-task QLoRA against  

> 1Quantization is a model compression technique that reduces the precision of numerical representations ( e.g., from 16-bit floating point to 8-bit or 4-bit formats) to decrease memory usage and computational cost. By representing model weights with fewer bits, quantization enables the fine-tuning and deployment of large models on resource-constrained hardware while maintaining near-original performance [18, 21]. Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 5                                                 

> Table 1. Comparison of parameter-efficient fine-tuning studies for code-related tasks. CG = Code Generation, CS = Code Summarization, CT = Code Translation, APR = Automatic Program Repair, CC = Code Completion, UT = Unit Test Generation.
> Study Method Multi-task Tasks Functional Quality
> CG CS CT APR CC UT Weyssow et al. [79] QLoRA 7337
> Yang et al. [83] QLoRA 7337
> Afrin et al. [3] QLoRA 7337
> Liu et al. [31] LoRA/QLoRA 33333337
> Our work QLoRA 333333

Multi-task full fine-tuning to establish the trade-offs between parameter efficiency and model effectiveness across different code-related tasks and model scales. Table 1 summarizes the scope of existing QLoRA-based parameter-efficient fine-tuning studies for code-related tasks and highlights the gaps addressed by our work. 

3 Study Methodology 

The primary objective of this study is to comprehensively examine the impact of Multi-task QLoRA fine-tuning on the func-tional correctness and non-functional quality attributes of LCMs outputs, compared to Single-task approaches. While QLoRA has demonstrated promise in reducing computational overhead for Single-task scenarios, it remains crucial to understand whether these efficiency gains persist and at what cost when models must balance multiple, potentially competing objectives within a shared parameter space. To systematically investigate these concerns, we formulate two research questions: 

RQ 1: How do Multi-task QLoRA-optimized models compare to Single-task QLoRA models in terms of functional correctness and overall output quality across code generation, summarization, and translation tasks? In RQ 1, we measure performance variations when specializing LCMs with QLoRA under Multi-task versus Single-task configura-tions, examining whether joint training on complementary code-related tasks enhances generalization or introduces performance trade-offs compared to task-specific optimization. 

RQ 2: How does Multi-task QLoRA fine-tuning compare to Multi-task full fine-tuning in terms of functional correctness and non-functional quality across code generation, summarization, and translation tasks? In RQ 2, we evaluate whether the parameter efficiency of Multi-task QLoRA comes at the cost of performance degradation com-pared to Multi-task full fine-tuning, assessing the trade-offs between computational efficiency and model effectiveness across different code-related tasks. 

3.1 Code Language Models 

To address the research questions outlined above, we employ the Qwen2.5-Coder-Instruct model family across three parameter scales: 0.5B, 1.5B, and 3B. Qwen2.5-Coder represents a code-specialized adaptation of the Qwen2.5 architecture, pre-trained on over 5.5 trillion tokens encompassing source code, code-grounded text, and synthetic data [24]. The model family implements a carefully balanced data mixture of 70% code, 20% text, and 10% mathematical content, designed to maintain strong coding ca-pabilities while preserving general language understanding. We select Qwen2.5-Coder over alternatives such as CodeLlama and DeepSeek-Coder based on its demonstrated effectiveness in recent software engineering automation research [53, 90]‚Äìwhere it has shown strong performance across diverse code understanding and generation tasks. On top of that, its robust instruction-following capabilities and competitive benchmark results make the Qwen2.5-Coder-Instruct family a reliable foundation for evaluating Multi-task scenarios where models must differentiate between task types‚Äìsuch as generation, summarization, and translation‚Äìand adapt their behavior according to natural language directives. Furthermore, its open availability, comprehensive documentation, and active community support facilitate reproducibility and enable rigorous experimental validation, aligning closely with the objectives of this study. 

> Manuscript submitted to ACM

6 Haque, Afrin and Mastropaolo 

3.2 Tasks and Datasets 

We ground our analysis on three fundamental code-related tasks that span different data modalities and represent core capabili-ties in modern software engineering workflows: code generation (NL-to-Code) , code summarization (Code-to-NL) , and code translation (Code-to-Code) . These tasks collectively assess a model‚Äôs ability to bridge natural language and code while trans-forming across programming languages, providing a comprehensive evaluation framework that captures the diverse reasoning patterns required for effective software engineering automation. 

3.2.1 Code Generation: For code generation, we evaluate performance across two programming languages‚Äì Python and Java ‚Äìto provide a comprehensive and balanced assessment. This dual-language approach enhances the empirical robustness of our analysis and strengthens the generalizability of our findings by capturing model behavior across syntactically and semantically distinct language paradigms commonly employed in code generation research [12, 85]. 

Training Data: We utilize the Code-to-Text dataset from the CodeXGLUE benchmark [38], adapting it for code generation through task inversion. CodeXGLUE was originally designed for code summarization and comprises pairs of code methods with corresponding natural language descriptions. We selected CodeXGLUE based on its established use in prior research on LLMs for code-related tasks [3, 75] and its comprehensive coverage of both Java and Python implementations. During this adaptation process, we filtered instances where: (i) the docstring field was empty or missing, (ii) the code field was empty or missing, or (iii) signature extraction failed to produce a valid result. The third condition applies when code does not conform to expected structural patterns‚Äìspecifically, Python code lacking lines beginning with def or async def , or Java code missing an opening brace to delimit the method signature from the body. These filtering criteria ensure that all retained instances contain valid input-output pairs with complete and parseable signatures. As shown in Table 2, this process yields 164,923 Java 

and 251,818 Python training instances, with corresponding validation sets of 5,183 and 13,914 instances, respectively. 

Evaluation Data: For evaluation, we employ CoderEval [85], a benchmark specifically designed for assessing code generation capabilities and widely used in the current body of research [48, 65, 75]. Originally, CoderEval featured 460 code generation problems (230 per language), where each problem comprises: (i) a natural language description specifying functional requirements, (ii) a reference implementation demonstrating a correct solution, and (iii) a test suite for evaluating functional correctness of generated code. Before adopting CoderEval, we conducted quality assurance following Crupi et al. [17]. Our final evaluation set contains 184 

Java and 190 Python problems with verified test suites (reported as Test instances in Table 2), ensuring that metrics capture genuine functional correctness rather than artifacts of test suite deficiencies. 

3.2.2 Code Translation: For code translation, we employ the benchmark dataset from CodeXGLUE [38], which provides paired code snippets for migration between Java and C#. 

Training Data: The CodeXGLUE code translation dataset comprises paired Java and C# code snippets that implement equiva-lent functionality, enabling bidirectional translation tasks (Java ‚ÜíC#, C# ‚ÜíJava). These tasks focus on preserving semantic equiv-alence while adapting syntax and idioms to the target language. Each pair serves as an input-output example where source code in one language corresponds to its functionally equivalent implementation in the other. As shown in Table 2, the dataset includes 10,300 training instances per direction (20,600 total) and 1,000 validation instances (500 per direction). 

Evaluation Data: For evaluation, we rely on the test split provided by CodeXGLUE, comprising 1,000 Java ‚ÜíC# and 1,000 C# ‚ÜíJava translation pairs (2,000 total instances). Each test instance consists of a source code snippet paired with its reference target implementation. 

3.2.3 Code Summarization: For code summarization, we employ the CodeXGLUE Code-to-Text dataset [38] for training and a curated subset from CoderEval [85] for evaluation. The CoderEval subset was created by inverting the input-output pairs from the code generation task, following the approach adopted in recent research [17, 68]. 

Training Data: For code summarization, we employ the same Code-to-Text dataset from the CodeXGLUE benchmark [38], which comprises paired examples of source code methods and their corresponding natural language summaries. The distinction from the code generation configuration lies in the input-output orientation: here, the input is the code snippet and the output is its semantically aligned natural language description‚Äìthe inverse of the configuration used for code generation. Consequently, 

> Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 7the dataset composition mirrors that of the code generation task in terms of size and language distribution, encompassing both 

Python and Java. Training, validation, and test splits are reported in Table 2. 

Evaluation Data: For evaluation, we employ the curated CoderEval subset from Crupi et al. [17], constructed by selecting the 100 longest functions per language based on statement count, as meaningful summaries are particularly valuable for com-plex code. While Crupi et al. reported 198 instances (99 per language) after deduplication, their replication package contains 199 instances: 99 Java and 100 Python functions. We utilize all 199 instances (Table 2), each comprising a function with its correspond-ing developer-written summary. The original CoderEval reference summaries contain documentation styles that differ from our training distribution. Java summaries consistently include Javadoc formatting with structured annotations ( e.g., , @param , @re-turn tags), inline markup ( e.g., , @code ), and HTML elements, while Python summaries frequently contain multi-line docstrings with parameter sections, return specifications, or verbose explanations spanning multiple sentences. Since our training data from CodeXGLUE comprises concise, single-sentence natural language summaries without such annotations or extended descriptions, evaluating against raw CoderEval summaries would introduce a systematic mismatch between training and evaluation distribu-tions. To ensure a fair and consistent evaluation, we preprocess the CoderEval reference summaries for both languages by extract-ing only the first sentence from multi-sentence docstrings (to match the single-sentence summary format), removing structured annotations, and condensing verbose explanations into concise summaries. The processed evaluation dataset is included in our replication package [2]. 

> Table 2. Dataset statistics across all three code-related tasks. Training and Validation instances are from CodeXGLUE [38].

Task Language Train Validation Test Code Summarization (CS) 

Java 164,923 5,183 99 *

Python 251,820 13,914 100 *

Total 416,743 19,097 199 

Code Generation (CG) 

Java 164,923 5,183 184 *

Python 251,818 13,914 190 *

Total 416,741 19,097 374 

Code Translation (CT) 

Java ‚Üí C# 10,300 500 1,000 C# ‚Üí Java 10,300 500 1,000 

Total 20,600 1,000 2,000 

> *

Test instances from CoderEval [85] (see Sections 3.2.1 and 3.2.3). 

3.3 QLoRA Fine-Tuning in a Nutshell 

We employ QLoRA [19], a parameter-efficient fine-tuning approach that combines low-rank adaptation (LoRA) with quantization to enable lightweight task-specific model specialization while keeping the base parameters frozen. LoRA modules are configured with rank ùëü = 8 , scaling factor ùõº = 16 , and dropout probability ùëù = 0.1 , as summarized in Table 3. These adapters are injected into seven projection layers within each transformer block: the attention projections ( q_proj , k_proj , v_proj , o_proj ) and the feed-forward projections ( gate_proj , up_proj , down_proj ). The scaling factor ùõº regulates the contribution of adapter updates, with an effective learning rate proportional to ùõº/ùëü . During training for code generation, summarization, and translation tasks, only the adapter parameters are updated, substantially reducing the number of trainable weights compared to full fine-tuning while preserving model performance. 

3.4 Training Procedure 

We conduct experiments under two primary training paradigms: Multi-task fine-tuning , where a single model learns across all three code-related tasks simultaneously, and Single-task fine-tuning , where separate models are trained independently for each task. For clarity, we adopt the following notation throughout this paper: MT denotes Multi-task configurations, ST denotes Single-task configurations, and FFT refers to full fine-tuning where all model parameters are updated. Consequently, MT-FFT represents 

> Manuscript submitted to ACM

8 Haque, Afrin and Mastropaolo           

> Table 3. QLoRA adapter configuration parameters.
> Parameter Description Value
> rRank of adapter matrices 8
> lora_alpha Adapter scaling factor 16
> lora_dropout Dropout rate for adapters 0.1
> target_modules Layers with adapter injection q/k/v/o_proj ,
> gate/up/down_proj

Multi-task full fine-tuning, ST-QLoRA represents Single-task QLoRA fine-tuning, and MT-QLoRA represents Multi-task QLoRA fine-tuning. 

Single-task Training Strategy. For ST-QLoRA , we train independent models for each of the experimented tasks. Each Single-task model is specialized with the same task-specific system prompt and data processing pipeline as its Multi-task counterpart, differing only in excluding training instances from other tasks during fine-tuning. 

Multi-task Training Strategy. For Multi-task configurations ( MT-FFT , MT-QLoRA ), we first create a unified training corpus that includes instances from CS Java,Python , CG Java,Python , and CT Java,C# . We concatenate the processed datasets and apply shuf-fling with a fixed random seed (seed= 42 ) to ensure randomized task exposure during training. This approach results in naturally balanced task sampling without explicit task weighting, as the model encounters examples from all tasks with probability pro-portional to their representation in the combined dataset‚Äìa well-known and widely used approach to tackle code-related tasks [16, 40, 42]. 

Training Configuration. We maintain consistent hyperparameters across all experiments (Table 4), following Afrin et al. [3]. Training employs a per-device batch size of 2 with gradient accumulation over 16 steps, yielding an effective batch size of 32. All models are trained for 5 epochs using the AdamW optimizer with a learning rate of 1 √ó 10 ‚àí4 , cosine learning rate scheduling, and 1,000 warmup steps. We set the maximum sequence length to 512 tokens with packing enabled, which concatenates short instances and truncates long ones to improve GPU utilization by minimizing padding overhead. For code translation, which comprises approximately 21K instances compared to approximately 416K instances for other tasks, we adjust the evaluation frequency to 625 steps and reduce warmup steps to 100 to maintain proportionality with the smaller dataset size (approximately one epoch given ‚àº644 steps per epoch). We evaluate on 100 samples randomly selected from the 1,000 validation instances, representing 5‚Äì10% proportional sampling across tasks. QLoRA fine-tuning employs 8-bit AdamW to reduce memory consumption and disables gradient checkpointing, as 4-bit quanti-zation provides sufficient memory savings. Full fine-tuning uses standard PyTorch AdamW with gradient checkpointing enabled to manage higher memory requirements. 

Early Stopping and Model Selection. We apply early stopping based on validation performance: SacreBLEU [50] for sum-marization and CodeBLEU [55] for generation and translation in Single-task models, or averaged combined metrics for Multi-task 

models. Training stops after three consecutive evaluations without improvement. Evaluation metrics are detailed in Section 3.5. All experiments ran on Ubuntu 22.04 LTS with 8 NVIDIA A40 GPUs (46GB each) and CUDA 12.2. 

3.5 Evaluation Procedure 

Having established our experimental setup, we now evaluate the outputs produced by different model configurations across all code-related tasks using established metrics from the literature. We structure this section by grouping tasks based on output modality: code generation and code translation produce source code and are evaluated for functional correctness and code quality, while code summarization produces natural language and is evaluated for linguistic and semantic quality. This organization allows us to apply task-appropriate metrics while maintaining coherence in evaluation methodology. 

3.5.1 Evaluating Correctness and Quality of Automatically Generated Code. The evaluation of AI-generated code encompasses two complementary dimensions: (i) correctness , which measures whether the code executes as intended and produces expected outputs, and (ii) quality , which assesses whether the code adheres to syntactic, semantic, and stylistic standards. Correctness cap-tures functional reliability and behavioral equivalence with respect to ground-truth implementations through execution-based testing. Quality encompasses attributes such as readability, maintainability, and adherence to coding standards‚Äìfactors that may 

> Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 9                            

> Table 4. Training hyperparameters for all experimental configurations.
> Parameter Description Value
> batch_size Per-device training batch 2
> gradient_accum Gradient accumulation steps 16
> learning_rate Initial learning rate 1 √ó 10 ‚àí4
> lr_scheduler Learning rate schedule cosine warmup_steps Linear warmup steps 1,000 ‚àó
> num_epochs Training epochs 5
> max_seq_length Maximum sequence length 512
> packing Sequence packing True patience Early stopping patience 3
> Method-Specific Settings
> optimizer QLoRA adamw_bnb_8bit optimizer FFT adamw_torch gradient_ckpt QLoRA False gradient_ckpt FFT True
> ‚àóTranslation uses 100 warmup steps and 625 evaluation steps due to smaller dataset size.

not affect immediate execution but are crucial for long-term software maintenance and evolution. Together, these dimensions provide a comprehensive assessment of a model‚Äôs ability to produce code that is both functionally accurate and practically valu-able in real-world software engineering contexts. 

Measuring Correctness via pass@k and CodeBLEU. We evaluate the functional correctness of automatically generated code using two complementary metrics: pass@k [13] and CodeBLEU [55]. The pass@k metric estimates the probability that at least one of ùëò generated solutions passes all unit tests for a given problem. We set ùëò = 1 , meaning only the model‚Äôs top-ranked output is evaluated. This strict configuration provides a conservative measure of the model‚Äôs ability to synthesize functionally valid programs that satisfy all task requirements. 

CodeBLEU extends the traditional BLEU metric [47] by incorporating code-aware components that capture the structural and semantic characteristics of programming languages. Specifically, it combines: (1) ùëõ -gram matching for surface-level similarity, (2) weighted ùëõ -gram matching based on syntax tokens derived from abstract syntax trees (ASTs), (3) data-flow matching to model variable dependencies, and (4) keyword matching for programming-specific operators and constructs. This metric provides deeper insights into the syntactic and semantic fidelity of generated code relative to reference implementations. For code generation , we use pass@1 as the primary metric to assess whether the top-ranked output passes all functional tests in the CoderEval benchmark [85]. We complement this with CodeBLEU scores to capture structural and semantic alignment with reference implementations. For code translation , the evaluation focuses on the model‚Äôs ability to accurately transform code from a source language into a target language while preserving its semantics and structure. Since executable test suites are unavailable for the CodeXGLUE translation dataset, we rely exclusively on CodeBLEU to assess functional correctness by measuring how well the translated code preserves the semantic intent and structural characteristics of the reference implementation across languages. 

Measuring Quality Attributes of Code. To assess the non-functional quality of the generated code, we rely on a set of well-established static analysis tools‚Äìfollowing the approach adopted in recent research [5, 26, 34, 36, 61]. These tools quantify struc-tural, stylistic, and maintainability attributes that influence the readability and robustness of model-generated code. For clarity, we report each indicator using its short form in parentheses ( e.g., Cyclomatic Complexity (CyC) ). Specifically, we employ LizaRd, PMD, Pylint, SonaRCloud, and Roslyn to evaluate different aspects of code complexity, style conformance, and maintainability across Java , Python , and C#. 

Lizard. [84] is a language-agnostic code complexity analyzer that processes source code without requiring compilation infras-tructure. The tool performs lexical analysis to extract structural metrics from function implementations. We apply Lizard to all three languages ( Java , Python , C#) in our study. 

PMD. [46] is a static analysis tool for Java that evaluates code quality using rule-based detection. We apply PMD to Java 

outputs generated in code generation and translation tasks. 

> Manuscript submitted to ACM

10 Haque, Afrin and Mastropaolo 

Pylint. [51] performs static analysis on Python code to enforce style guidelines and identify potential defects [36, 61]. 

SonarCloud. [62] performs cloud-based static analysis to detect code quality issues and technical debt across multiple program-ming languages. We apply SonarCloud to Java , Python , and C# outputs to obtain cross-language quality metrics that complement the language-specific insights from other tools. Since our code generation and translation tasks produce function-level outputs, we employ compilation wrappers to enable SonarCloud‚Äôs full analysis capabilities. 

Roslyn. [43] provides Microsoft‚Äôs open compiler infrastructure for C#, exposing compilation APIs that enable deep code in-spection. The platform performs syntactic, semantic, and structural analysis by leveraging the same logic used for compilation. This supports tasks including pattern detection, rule validation, and automated correction suggestions. We leverage Roslyn to categorize issues in C# outputs. Table 5 summarizes the metrics computed by each tool along with the supported programming languages. 

Table 5. Overview of static analysis tools and metrics used for non-functional code quality evaluation. 

Tool Metric Description Java Python C# Lizard [84] 

Detection Rate (DR) Quantifies the proportion of generated functions that Lizard can successfully parse. This indicates whether outputs are syntactically valid enough for automated anal-ysis. 

3 3 3

Cyclomatic Complexity (CyC) Quantifies structural complexity through control flow analysis. Given a control flow graph with ùê∏ edges, ùëÅ nodes, and ùëÑ connected components, the complexity 

ùëÄ is computed as ùëÄ = ùê∏ + 2ùëÑ ‚àí ùëÅ . Higher values correspond to more branching logic. 

3 3 3

Lines of Code (LoC) Counts non-whitespace, non-comment source lines. This size metric correlates with maintenance burden and development effort. 

3 3 3

Token Count (Tok) Enumerates lexical elements (keywords, identifiers, operators, literals) in the source. This language-neutral size measure is insensitive to formatting choices. 

3 3 3

PMD [46] 

Best Practices (BP) Detects code patterns that deviate from recommended design approaches or create obstacles for future maintenance activities. 

3

Code Style (CS) Verifies compliance with formatting standards, particularly regarding bracket placement and identifier naming conventions. 

3

Design Reports structural concerns such as excessive nesting of conditional logic that im-pact long-term maintainability. 

3

Error Prone (EP) Identifies problematic constructs that exhibit poor clarity or have high potential for causing failures during execution. 

3

Multi-threading (MT) Detects concurrency problems that arise when managing parallel execution con-texts, requiring appropriate coordination mechanisms. 

3

Performance (Perf.) Locates inefficient implementation choices that negatively affect execution speed and computational resource utilization. 

3

Pylint [51] 

Convention (Conv) Flags departures from PEP 8 style rules, encompassing spacing, indentation, iden-tifier naming, and line formatting. These affect uniformity and scanability. 

3

Refactor (Ref) Identifies poor structural choices including duplicated logic, excessive nesting depth, and oversized functions. These suggest refactoring opportunities. 

3

Warning (Warn) Surfaces suspicious patterns like unreferenced imports, unused bindings, and name shadowing that may cause unexpected behavior. 

3

Error (Err) Reports likely defects, including malformed syntax, reference to undefined names, incorrect function signatures, and type violations that cause runtime crashes. 

3

SonarCloud [62] 

Security Hotspots (Sec) Identifies security-sensitive code patterns requiring manual review. These include hard-coded credentials, weak cryptography usage, improper input validation, and unprotected resource access. 

3 3

Reliability (Rel) Counts the total number of issues impacting code robustness detected through static analysis rules. 

3 3

Maintainability (Main) Evaluates code quality by detecting issues that impact the ease of understanding, modifying, and extending code over time. 

3 3

Cognitive Complexity (CoC) Measures the difficulty of understanding code‚Äôs control flow structure by assigning penalties for nested constructs. 

3 3

Roslyn [43] 

Syntax Errors Reports malformed C# constructs that block compilation, including incorrect state-ment structure, incompatible type usage, and disallowed operations. 

3

Maintainability Issues Identifies structural and stylistic problems that complicate code comprehension, modification, and evolution over time. 

3

Together, these tools provide a multifaceted assessment of the non-functional quality of model-generated code. By combining metrics that capture structural complexity, stylistic consistency, and maintainability, we obtain an objective characterization of code quality across programming languages. To ensure the robustness of our findings, we apply non-parametric statistical analysis to evaluate whether observed differences between configurations are statistically significant: the McNemar test for code generation pass@1 , where outputs are evaluated as pass/fail, and the Wilcoxon signed-rank test [81] for code translation, where outputs are assessed using continuous metrics. When conducting multiple comparisons, we apply the Bonferroni correction procedure to adjust ùëù -values and account for multiple comparisons. This comprehensive evaluation framework allows us to assess how different fine-tuning strategies‚Äìsuch as Multi-task and Single-task QLoRA‚Äìimpact not only the correctness but also the practical usability and long-term maintainability of the generated software artifacts. 

Manuscript submitted to ACM Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 11 

3.5.2 Evaluating Natural Language in Software Engineering Application. We evaluate the performance of Multi-task and Single-task QLoRA-optimized models in producing meaningful summaries for source code, using a set of established metrics widely employed in prior code summarization research [3, 28, 39, 86, 87]: 

BLEU [47] measures n-gram precision between generated and reference summaries, producing scores from 0 to 1 where higher values indicate greater similarity. 

METEOR [10] combines unigram precision and recall through harmonic mean, incorporating stemming and synonym match-ing for better human judgment correlation. 

ROUGE-L [30] assesses sequence-level similarity by measuring the longest common subsequence between summaries. 

chrF [49] evaluates character-level similarity rather than token-level, offering robustness to minor word form variations. 

BERTScore [88] captures semantic similarity using contextualized BERT embeddings and token-level alignment, reporting F1-scores that go beyond surface-level lexical overlap. 

SIDE [39] is a code-specific metric that uses contrastive learning to measure alignment between Java methods and their summaries, producing scores from -1 to 1 where values closer to 1 indicate stronger code-comment alignment. We additionally employ the LLM-as-a-judge methodology to evaluate summary quality, following the approach presented by Crupi et al. [17], who demonstrated that LLMs can effectively assess code summary quality with performance comparable to human evaluation of code documentation. We replicate the experimental configuration that achieved the most reliable results in their study, which employs a zero-shot prompting approach to evaluate three dimensions of code summaries: (i) Content Adequacy , which quantifies how completely the summary captures implementation details present in the documented code; (ii) Conciseness , which assesses whether the summary contains redundant information beyond what is necessary for documentation; and (iii) Fluency , which evaluates the grammatical correctness and clarity of the summary. We instantiate this approach using GPT-5 Mini [45] with the evaluation prompt proposed by Crupi et al. [17]. 2 To mitigate the inherent variability of LLM-based evaluations, we score each generated summary independently five times for all three criteria 

‚Äîcontent adequacy , conciseness , and fluency ‚Äîand report the arithmetic mean for each dimension as the final score, improving the stability and reproducibility of the evaluation. Finally, we assess statistical significance using the Wilcoxon signed-rank test at ùõº = 0.05 , applying Holm-Bonferroni ùëù -value correction for multiple comparisons. 

4 Results 

To ensure a clear and well-structured discussion, we present the results of our investigation by research question, with each RQ examining performance across the three code-related tasks: (i) code generation, (ii) code summarization, and (iii) code translation. For each task, we report both functional correctness metrics and quality attributes of generated outputs i.e., code for generation and translation tasks, natural language for summarization. To facilitate comparisons across multiple configurations, we adopt the following visual conventions. For functional correctness metrics ( e.g., pass@1 , CodeBLEU, BLEU, etc.), the best-performing configuration within each model size is highlighted using 

bold font . For quality-related metrics , we apply color coding to indicate the relative performance of artifacts obtained by specializing the underlying code model in a Multi-task setting via QLoRA-based optimization, compared to Single-task QLoRA optimization. Specifically, light green denotes higher-quality artifacts under Multi-task QLoRA; light blue indicates higher-quality artifacts under Single-task QLoRA; and light yellow signifies no substantial difference between the two approaches. All results are available in our replication package [2]. 

4.1 RQ1: Multi-task QLoRA vs. Single-task QLoRA 

This research question investigates whether Multi-task QLoRA fine-tuning across code generation, summarization, and translation can match or outperform Single-task QLoRA training, focusing on whether cross-task knowledge transfer improves performance or whether competing objectives lead to degradation. 

> 2The full prompt is included in our replication package [2]. Manuscript submitted to ACM

12 Haque, Afrin and Mastropaolo 

Table 6. RQ1: Comparison of ST-QLoRA and MT-QLoRA for Python code generation on CoderEval.                                  

> Model Method Pass@1 Lizard Pylint SonarCloud LoC Tok DR CyC Err Warn Conv Ref Sec Rel Main CoC
> QwenCoder-0.5B ST-QLoRA 15.79% 1,931 12,904 100% 757 202 1,463 648 55 2353 547
> MT-QLoRA 15.79% 1,952

‚Üë 13,327 ‚Üë 100% 812 251 1,593 642 56 2 7 90 634                  

> QwenCoder-1.5B ST-QLoRA 16.84% 2,017 13,460 100% 748 308 1,664 670 70 111 45 641
> MT-QLoRA 18.95% 2,296

‚Üë 14,531 ‚Üë 100% 882 315 1,767 677 76 1 5 72 771                  

> QwenCoder-3B ST-QLoRA 20.53% 1,945 12,195 100% 709 226 1,773 675 56 10160 680
> MT-QLoRA 21.05% 1,759

‚Üì 11,773 ‚Üì 100% 677 269 1,521 673 56 2 5 106 549 

Table 7. RQ1: Comparison of ST-QLoRA and MT-QLoRA for Java code generation on CoderEval.                                                                                                                      

> Model Method Pass@1 Lizard PMD SonarCloud LoC Tok DR CyC Best Prac. CS Design EP MT Perf. Sec Rel Main CoC
> QwenCoder-0.5B ST-QLoRA 21.74% 1,523 10,820 96.70% 589 74 1,177 128 33 419 13525 497
> MT-QLoRA 20.11% 1,933 ‚Üë12,750 ‚Üë96.70% 697 76 1,123 139 33 310 00531 494
> QwenCoder-1.5B ST-QLoRA 25.54% 1,717 12,389 96.70% 697 82 1,227 125 32 4903541 526
> MT-QLoRA 19.02% 1,490 ‚Üì10,313 ‚Üì96.20% 589 75 1,183 127 31 511 02530 442
> QwenCoder-3B ST-QLoRA 29.89% 1,556 10,866 98.90% 589 79 1,185 128 40 311 03545 491
> MT-QLoRA 32.07% 1,580 ‚Üë10,920 ‚Üë98.40% 558 86 1,184 128 32 4702541 469

4.1.1 Code Generation. Before delving into specific patterns, we examine the overall behavior of Multi-task QLoRA for code gen-eration. From a general standpoint, we observe that Multi-task QLoRA demonstrates competitive pass@1 scores, with performance advantages becoming increasingly pronounced as model size increases. For Python code generation (Table 6), considering functional correctness as measured by pass@1 , Multi-task QLoRA achieves better performance in the 1.5B (18.95% vs. 16.84%, ‚Üë12.5%) and 3B (21.05% vs. 20.53%, ‚Üë2.5%) configurations, while main-taining identical performance in the 0.5B variant (15.79%). For Java code generation (Table 7), again focusing on pass@1 functional correctness , the 3B Multi-task model attains the highest score at 32.07%, outperforming the Single-task model (29.89%) by ‚Üë7.3%. However, smaller Multi-task models underperform in Java: the 1.5B variant achieves 19.02% compared to Single-task‚Äôs 25.54%, corresponding to a ‚Üì25.5% relative decline. While functional correctness is a fundamental requirement, it captures only one dimension of the practical value of automated techniques that generate code-related artifacts. Additional factors‚Äìincluding readability, maintainability, Conciseness , and adher-ence to coding standards‚Äìare equally important in determining real-world usability. Indeed, code that passes test cases may still suffer from excessive complexity, poor maintainability, or violations of established quality guidelines, all of which can negatively affect long-term software evolution and maintenance [5, 27, 41, 61]. To provide a more comprehensive evaluation, we complement functional correctness with aggregated metrics derived from static analysis tools. To give readers a clear and interpretable overview of code quality, we first aggregate results at the model level, which allows us to compare overall trends between Single-task and Multi-task QLoRA independently of specific configurations. This aggregation enables a high-level assessment of differences in structural complexity and coding standard compliance. We then examine whether variations in model size and parameter count affect these quality metrics. We begin by examining results for Python code generation. Aggregating across all model sizes (0.5B, 1.5B, and 3B), Single-task QLoRA produces a total of 2,214 Cyclomatic Complexity issues, 7,810 Pylint warnings, and 2,144 SonarCloud quality issues. In comparison, Multi-task QLoRA yields 2,371 Cyclomatic Complexity issues ( ‚Üë7.1%), 7,896 Pylint warnings ( ‚Üë1.1%), and 2,244 SonarCloud issues ( ‚Üë4.7%). For Java code generation, again aggregating across all model sizes, Single-task QLoRA produces 1,875 Cyclomatic Complexity issues, 4,360 PMD warnings, and 3,135 SonarCloud issues. In contrast, Multi-task QLoRA generates 1,844 Cyclomatic Complexity issues ( ‚Üì1.7%), 4,257 PMD warnings ( ‚Üì2.4%), and 3,011 SonarCloud issues ( ‚Üì4.0%). Overall, Python exhibits slightly higher aggre-gate complexity and quality issues under Multi-task fine-tuning, whereas Java shows modest improvements in both structural complexity and coding standard compliance. 

> Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 13 This observed behavior of Multi-task models across aggregate metrics prompts a deeper examination of specific quality dimen-sions. Turning to Cyclomatic Complexity patterns, Python Multi-task shows ‚Üë7.1% higher CyC than Single-task (2,371 vs. 2,214), indicating more complex control flow structures. However, this increase is not uniform across model sizes: the 0.5B model shows 812 vs. 757 CyC ( ‚Üë7.3%), the 1.5B shows 882 vs. 748 ( ‚Üë17.9%), but the 3B model reverses the trend with 677 vs. 709 ( ‚Üì4.5%). For Java, Multi-task consistently produces simpler code: 1,844 total CyC versus Single-task‚Äôs 1,875 ( ‚Üì1.7%). Breaking down by model size reveals distinct patterns: the 0.5B model shows 697 vs. 589 ( ‚Üë18.3%), indicating higher complexity in the smaller Multi-task 

variant, while the 1.5B shows 589 vs. 697 ( ‚Üì15.5%) and the 3B shows 558 vs. 589 ( ‚Üì5.3%), both demonstrating lower complexity. This suggests that joint training affects structural complexity differently across languages and model sizes, with larger models producing simpler implementations. The reversal in the 3B Python model is particularly noteworthy, as it suggests that sufficient model capacity enables Multi-task training to produce code that is not only more functionally correct, but also structurally simpler, aligning with recent findings that emphasize the role of model scale in enabling effective cross-task knowledge transfer without sacrificing code quality [31, 78]. For Pylint metrics in Python , the 3B Multi-task model produces fewer issues than the Single-task model (2,519 vs. 2,730, ‚Üì7.7%), aligning with its superior functional correctness and reduced complexity. While Errors increase slightly (269 vs. 226, ‚Üë19.0%), Warnings decrease substantially (1,521 vs. 1,773, ‚Üì14.2%). SonarCloud maintainability reinforces this pattern: the 3B Multi-task 

model achieves 106 issues compared to the Single-task model‚Äôs 160 ( ‚Üì33.8%), while smaller models show higher issue counts (0.5B: 90 vs. 53, ‚Üë69.8%; 1.5B: 72 vs. 45, ‚Üë60.0%). For Java, PMD analysis reveals that the Multi-task QLoRA-optimized Qwen-Coder models produce code with better style conformance and design patterns across most model sizes. In particular, the 0.5B model generates 1,384 total PMD issues compared to the Single-task model‚Äôs 1,435 ( ‚Üì3.6%), with improvements in Best Practices (76 vs. 74, ‚Üë2.7%), Code Style (1,123 vs. 1,177, ‚Üì4.6%), Multi-threading (3 vs. 4, ‚Üì25.0%), and Performance (10 vs. 19, ‚Üì47.4%), while Design and Error Prone remain comparable. The 1.5B model shows 1,432 issues compared to 1,479 ( ‚Üì3.2%), with reductions in Best Practices (75 vs. 82, ‚Üì8.5%), Code Style (1,183 vs. 

1,227, ‚Üì3.6%), Design (127 vs. 125, ‚Üë1.6%), Error Prone (31 vs. 32, ‚Üì3.1%), and Performance (11 vs. 9, ‚Üë22.2%), though Multi-threading increases slightly (5 vs. 4, ‚Üë25.0%). The 3B model demonstrates 1,441 issues compared to 1,446 ( ‚Üì0.3%), with improvements in Code Style (1,184 vs. 1,185, ‚Üì0.1%), Error Prone (32 vs. 40, ‚Üì20.0%), Multi-threading (4 vs. 3, ‚Üë33.3%), and Performance (7 vs. 11, ‚Üì36.4%), while Best Practices shows a slight increase (86 vs. 79, ‚Üë8.9%). SonarCloud maintainability results for Java show broadly similar patterns across both training variants. The Single-task QLoRA-optimized model generates code affected by 525, 541, and 545 issues for the 0.5B, 1.5B, and 3B models, respectively, while the Multi-task variant generates 531 ( ‚Üë1.1%), 530 ( ‚Üì2.0%), and 541 ( ‚Üì0.7%) issues. This pattern indicates that Multi-task training benefits increase with model capacity. Smaller models struggle to balance multiple objectives, producing code that violates more conventions and exhibits more code smells. In contrast, larger models successfully leverage knowledge gained from diverse tasks to generate cleaner, more maintainable code with better adherence to style guidelines and design patterns. Examining code size metrics from Lizard reveals that Multi-task models generate longer code in smaller configurations but more concise implementations in larger models. In the context of Python code generation, Multi-task QLoRA-based optimization with models featuring fewer than one billion parameters ( i.e., , Qwen-Coder 0.5B) produces 1,952 LoC compared to the Single-task variant‚Äôs 1,931 ( ‚Üë1.1%), while the Multi-task 1.5B model generates 2,296 LoC compared to 2,017 for its Single-task counterpart (‚Üë13.8%). However, Multi-task 3B reverses this trend with 1,759 LoC versus Single-task‚Äôs 1,945 ( ‚Üì9.6%), while simultaneously achieving lower complexity (677 vs. 709 CyC, ‚Üì4.5%), slightly higher pass@1 (21.05% vs. 20.53%, ‚Üë2.5%), fewer maintainability issues (106 vs. 160, ‚Üì33.8%), and reduced cognitive complexity (549 vs. 680, ‚Üì19.3%). For Java, model size plays a central role in how Multi-task training affects code length. The Multi-task 0.5B configuration generates longer implementations than its Single-task counterpart (1,933 vs. 1,523 LoC, ‚Üë26.9%), whereas the 1.5B and 3B Multi-task models produce more compact code (1,490 vs. 1,717, ‚Üì13.2% and 1,580 vs. 1,556 LoC, ‚Üë1.5%, respectively). A similar capacity-dependent pattern emerges when examining token counts. For Python , Multi-task models generate slightly more tokens at smaller model sizes (13,327 vs. 12,904 for 0.5B, ‚Üë3.3%; 14,531 

vs. 13,460 for 1.5B, ‚Üë8.0%), but fewer tokens in the 3B configuration (11,773 vs. 12,195, ‚Üì3.5%). Java exhibits an analogous trend: the Multi-task 0.5B model produces substantially more tokens (12,750 vs. 10,820, ‚Üë17.8%), the 1.5B configuration generates fewer tokens (10,313 vs. 12,389, ‚Üì16.8%), and the 3B models show nearly identical token counts (10,920 vs. 10,866, ‚Üë0.5%). 

> Manuscript submitted to ACM

14 Haque, Afrin and Mastropaolo To determine whether the empirically observed differences also translate into statistically significant effects, we compare Single-task and Multi-task QLoRA configurations using McNemar tests on pass@1 outcomes and Wilcoxon signed-rank tests on quality metrics, with Holm‚ÄìBonferroni correction applied ( ùõº = 0.05 ). To complement hypothesis testing, we report effect size measures to quantify the magnitude of observed differences. Specifically, odds ratios (ORs) from the McNemar tests capture the relative likelihood of Single-task QLoRA fine-tuning outperforming Multi-task QLoRA in generating more functionally correct code, while ClifÓÉ†‚Äôs Delta from the Wilcoxon tests measures the degree of separation between quality metric distributions. Across both Python and Java , no statistically significant differences are observed at any model scale, indicating that empiri-cal variations across configurations fall within the range expected from random fluctuation. Consequently, both Multi-task and Single-task QLoRA-based training equip the underlying models with comparable knowledge, resulting in generated code that does not differ substantially from either a quantitative or qualitative perspective. Although larger models may exhibit practical advantages along certain performance and code quality dimensions, these differences are not statistically distinguishable under our testing framework. For transparency and reproducibility, we provide the complete set of test statistics, effect sizes, and ad-justed ùëù -values in the replication package [2]. 

Key Findings 

Multi-task QLoRA is competitive with Single-task QLoRA for code generation, with benefits that generally increase with model capacity. For Python , Multi-task matches 0.5B and improves pass@1 at 1.5B and 3B. For Java , Multi-task peaks at 3B but underperforms at 1.5B, indicating scale-sensitive trade-offs. Static analysis suggests smaller Multi-task models can incur higher complexity or maintainability issues, while larger Multi-task models better balance correctness and quality. Statistical tests (McNemar for pass@1 ; Wilcoxon with Holm‚ÄìBonferroni for quality) find no significant differences across scales, implying practical but not statistically distinguishable gaps. 

4.1.2 Code Translation. We evaluate code translation performance using CodeBLEU across both Java ‚ÜíC# and C# ‚ÜíJava direc-tions. Tables 8 and 9 compare Multi-task QLoRA against Single-task QLoRA across all three model sizes (0.5B, 1.5B, and 3B), reporting both functional correctness and non-functional quality metrics. We begin by evaluating the ability of Single-task instruction-tuned code models to perform Java ‚ÜíC# and C# ‚ÜíJava translation, and then examine whether the empirical benefits previously observed for Multi-task training in code generation also carry over to translation tasks. In doing so, we find that, unlike code generation, translation performance varies substantially across both translation directions and model sizes, indicating a more complex interaction between task structure and model capacity. For Java ‚ÜíC#, this variability is immediately apparent. The 0.5B Multi-task model attains a CodeBLEU score of 68.92%, compared to 75.77% for the Single-task baseline, corresponding to a relative decrease of ‚Üì9.0%. A similar relative degradation is observed at the largest scale, where the 3B Multi-task configuration achieves 62.88% versus 66.83% for Single-task ( ‚Üì5.9%). In contrast, the intermediate 1.5B model exhibits a modest relative improvement under Multi-task QLoRA, reaching 73.08% compared to 72.27% for Single-task ( ‚Üë1.1%), suggesting a potential capacity sweet spot for this translation direction. This capacity-dependent pattern persists for C# ‚ÜíJava translation. The 0.5B Multi-task model again underperforms relative to the Single-task baseline (68.37% vs. 75.36%), corresponding to a relative decrease of ‚Üì9.3%. At the intermediate scale, however, 

Multi-task training yields a relative improvement of ‚Üë2.0% (70.72% vs. 69.35%), while at 3B the two approaches converge, with 

Multi-task achieving 71.22% compared to 72.81% for Single-task ( ‚Üì2.2%). To examine whether these functional trends are accompanied by systematic differences in code quality‚Äìas we did for code generation (Section 4.1.1)‚Äìwe aggregate metrics from static analysis tools. For Java ‚ÜíC#, where Lizard and Roslyn provide com-prehensive coverage, Single-task QLoRA produces 4,575 total Cyclomatic Complexity (CyC) issues and 4,919 Roslyn-reported issues across all model sizes. Multi-task QLoRA yields a marginal reduction in CyC issues (4,533; ‚Üì0.9%) and a lower number of Roslyn issues overall (4,183; ‚Üì15.0%). In contrast, for C# ‚ÜíJava translation‚Äìagain aggregating across all model sizes and evaluated using Lizard, PMD, and SonarCloud‚Äì Multi-task QLoRA-based fine-tuning consistently reduces aggregate quality issues, including fewer CyC issues ( ‚Üì1.0%), fewer PMD violations ( ‚Üì0.6%), and fewer SonarCloud-reported issues ( ‚Üì2.2%). The observed behavior across aggregate metrics motivates a closer examination of capacity-dependent effects. Disaggregating Cyclomatic Complexity by model size reveals indeed some latent patterns that could go unnoticed. For Java ‚ÜíC#, Multi-task 

> Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 15 QLoRA demonstrates progressive improvements with increased capacity: the 0.5B model shows 1,515 CyC compared to 1,528 under Single-task ( ‚Üì0.9%), the 1.5B configuration shows 1,563 versus 1,561 ( ‚Üë0.1%), and the 3B model achieves the largest reduction with 1,455 versus 1,486 ( ‚Üì2.1%). For C# ‚ÜíJava, the pattern differs: the 0.5B model shows 1,594 versus 1,586 CyC ( ‚Üë0.5%), the 1.5B model shows 1,566 versus 1,647 ( ‚Üì4.9%), and the 3B configuration shows 1,612 versus 1,588 ( ‚Üë1.5%). The particularly strong reduction observed for the 1.5B configuration aligns with its competitive CodeBLEU performance, suggesting that this capacity achieves an effective balance between similarity to reference implementations and structural simplicity. A closer inspection of language-specific diagnostics further highlights non-monotonic quality trends. For Roslyn metrics in Java ‚ÜíC# translation, Multi-task QLoRA exhibits substantial variability across model sizes. The 0.5B model produces 1,091 total Roslyn issues compared to 850 under Single-task, representing a ‚Üë28.4% increase despite lower cyclomatic complexity. While syn-tax errors decrease from 283 to 256 ( ‚Üì9.5%), maintainability issues increase substantially from 567 to 835 ( ‚Üë47.3%). In contrast, the 1.5B model shows pronounced improvements, producing 905 total issues versus 2,724 under Single-task ( ‚Üì66.8%), with reductions in both syntax errors (313 vs. 356, ‚Üì12.1%) and maintainability issues (592 vs. 2,368, ‚Üì75.0%). The 3B model reverses this trend, generating 3,187 total issues versus 1,345 ( ‚Üë137.0%), driven by increases in syntax errors (126 vs. 81, ‚Üë55.6%) and maintainabil-ity issues (3,061 vs. 1,264, ‚Üë142.2%). These results indicate that the 1.5B configuration achieves the most balanced outcome for Java ‚ÜíC# translation, while smaller and larger models exhibit different quality trade-offs. PMD analysis for C# ‚ÜíJava translation shows mixed results across different model sizes. For the smallest 0.5B model, Multi-task QLoRA generates code with 4,793 total PMD issues compared to Single-task‚Äôs 4,859 ( ‚Üì1.4%), showing improvements in all categories: Best Practices (400 vs. 416, ‚Üì3.8%), Code Style (3,953 vs. 3,973, ‚Üì0.5%), Design (234 vs. 247, ‚Üì5.3%), Error Prone (54 vs. 61, 

‚Üì11.5%), Multi-threading (12 vs. 15, ‚Üì20.0%), and Performance (140 vs. 147, ‚Üì4.8%). However, the 1.5B model shows the opposite trend, with Multi-task producing more issues (4,798 vs. 4,613, ‚Üë4.0%) across most categories, despite achieving higher CodeBLEU scores. This suggests a trade-off where the model prioritizes functional accuracy over code style. The largest 3B model demon-strates significant improvements with Multi-task training, reducing total issues from 1,358 to 1,171 ( ‚Üì13.8%), with particularly large gains in Code Style (1,072 vs. 1,214, ‚Üì11.7%), Design (45 vs. 83, ‚Üì45.8%), Multi-threading (5 vs. 9, ‚Üì44.4%), and Performance (1 

vs. 7, ‚Üì85.7%). Best Practices remain stable, while Error Prone violations increase slightly (11 vs. 8, ‚Üë37.5%). SonarCloud metrics confirm these patterns for C# ‚ÜíJava translation. The 3B Multi-task model produces code affected by a grand total of 435 total quality issues compared to Single-task‚Äôs 563 ( ‚Üì22.7%), with substantial reductions in maintainability (278 vs. 366, 

‚Üì24.0%) and cognitive complexity (157 vs. 192, ‚Üì18.2%). Notably, reliability issues are completely eliminated (0 vs. 5, ‚Üì100%). For smaller models, the results are less consistent: the 0.5B model shows modest improvement with 1,723 issues versus 1,760 ( ‚Üì2.1%), while the 1.5B model actually produces more issues (1,785 vs. 1,708, ‚Üë4.5%), aligning with the PMD findings. Finally, we examine code size metrics using Lizard. For Java ‚ÜíC#, Multi-task QLoRA consistently produces more compact implementations: 6,218 LoC versus 6,368 at 0.5B ( ‚Üì2.4%), 6,369 versus 6,519 at 1.5B ( ‚Üì2.3%), and 5,890 versus 6,189 at 3B ( ‚Üì4.8%). Token counts follow the same trend, decreasing from 40,746 to 39,509 at 0.5B ( ‚Üì3.0%), from 42,299 to 40,516 at 1.5B ( ‚Üì4.2%), and from 39,845 to 37,716 at 3B ( ‚Üì5.3%). For C# ‚ÜíJava, code size trends are mixed: 0.5B shows a slight increase in LoC (5,807 vs. 

5,777, ‚Üë0.5%), 1.5B shows a reduction (5,745 vs. 5,860, ‚Üì2.0%), and 3B shows a small increase (5,742 vs. 5,687, ‚Üë1.0%). Token counts similarly increase at 0.5B ( ‚Üë0.8%) but decrease at 1.5B ( ‚Üì3.3%) and remain nearly unchanged at 3B ( ‚Üë0.7%). Taken together, these results suggest that increased capacity allows Multi-task models to better leverage cross-task knowledge to produce concise and higher-quality translations, particularly for Java ‚ÜíC# and large-scale C# ‚ÜíJava settings. Ultimately, to reliably determine whether the empirical differences observed in code translation reflect statistically meaning-ful effects rather than random variation, we conduct Wilcoxon signed-rank tests with Holm‚ÄìBonferroni correction ( ùõº = 0.05 ), comparing Single-task QLoRA against Multi-task QLoRA across all model sizes and translation directions. Complete statistical results are available in our replication package [2]. Java ‚ÜíC# exhibits statistically significant CodeBLEU differences at all scales, whereas for C# ‚ÜíJava significance is limited to the 0.5B configuration. Quality metrics occasionally yield significant results, but effect sizes remain small, indicating that while some differences are statistically detectable, their practical impact is limited. 

> Manuscript submitted to ACM

16 Haque, Afrin and Mastropaolo 

Key Findings 

Code translation under Multi-task QLoRA is direction- and scale-dependent relative to Single-task QLoRA, with no consistent dominance. Across Java ‚ÜíC# and C# ‚ÜíJava, performance varies by model size, often favoring the intermediate configuration. Static analysis mirrors this non-monotonicity, where improvements in complexity or conciseness can co-occur with regres-sions in target-language diagnostics ( e.g., Roslyn for C#). Wilcoxon tests with Holm‚ÄìBonferroni correction detect significant CodeBLEU differences for Java ‚ÜíC# across scales, while effects for C# ‚ÜíJava are more limited and typically small in magni-tude. 

Table 8. RQ1: Comparison of ST-QLoRA and MT-QLoRA for Java ‚ÜíC# code translation on CodeXGLUE.                                                             

> Model Method CodeBLEU Lizard Roslyn LoC Tok DR CyC Syntax Errors Maintainability
> QwenCoder-0.5B ST-QLoRA 75.77% 6,368 40,746 99.1% 1,528 283 567
> MT-QLoRA 68.92% 6,218 ‚Üì39,509 ‚Üì98.7% 1,515 256 835
> QwenCoder-1.5B ST-QLoRA 72.27% 6,519 42,299 95.5% 1,561 356 2,368
> MT-QLoRA 73.08% 6,369 ‚Üì40,516 ‚Üì98.5% 1,563 313 592
> QwenCoder-3B ST-QLoRA 66.83% 6,189 39,845 97.7% 1,486 81 1,264
> MT-QLoRA 62.88% 5,890 ‚Üì37,716 ‚Üì93.6% 1,455 126 3,061

Table 9. RQ1: Comparison of ST-QLoRA and MT-QLoRA for C# ‚ÜíJava code translation on CodeXGLUE.                                                                                                                      

> Model Method CodeBLEU Lizard PMD SonarCloud LoC Tok DR CyC Best Prac. CS Design EP MT Perf. Sec Rel Main CoC
> QwenCoder-0.5B ST-QLoRA 75.36% 5,777 34,898 99.6% 1,586 416 3,973 247 61 15 147 022 1,071 667
> MT-QLoRA 68.37% 5,807 ‚Üë35,170 ‚Üë99.6% 1,594 400 3,953 234 54 12 140 013 1,056 654
> QwenCoder-1.5B ST-QLoRA 69.35% 5,860 36,265 97.8% 1,647 399 3,786 196 58 16 158 016 1,013 679
> MT-QLoRA 70.72% 5,745 ‚Üì35,074 ‚Üì99.6% 1,566 400 3,939 213 61 22 163 019 1,088 678
> QwenCoder-3B ST-QLoRA 72.81% 5,687 34,622 99.4% 1,588 37 1,214 83 89705366 192
> MT-QLoRA 71.22% 5,742 ‚Üë34,862 ‚Üë99.5% 1,612 37 1,072 45 11 5100278 157

4.1.3 Code Summarization. To provide a holistic view of the real capabilities of QLoRA-based optimization, we include results for the code-to ‚Äìnatural language task, completing the evaluation of the full code-related task spectrum, in the context of the RQ 1.Unlike the previous tasks, summarization outputs free-form text rather than executable code, necessitating evaluation metrics that capture both lexical overlap and semantic similarity. We therefore report a diverse set of similarity-based metrics, including BLEU, METEOR, Rouge-L, and chrF, which assess surface-level match to reference summaries, alongside contextual metrics such as BERTScore, which measures semantic similarity using contextualized embeddings. Additionally, for Java , we report SIDE, the novel code-aware metric presented by Mastropaolo et al. [39] that quantifies semantic alignment between code and summaries via contrastive learning. Table 10 reports the performance of Multi-task QLoRA and Single-task QLoRA across all three model sizes (0.5B, 1.5B, and 3B) for both Java and Python . Because CoderEval standardizes task requirements, the evaluated summarization instances exhibit com-parable complexity across languages, enabling controlled cross-language analysis. Observed performance differences therefore, cannot be attributed to variations in task difficulty, but instead could be the result of the interplay between the training strategy and intrinsic characteristics of each programming language. However, we emphasize that our objective is not to draw broad gen-eralizations from these observations, but rather to identify recurring patterns that shed light on subtle effects of QLoRA-based 

Multi-task training and motivate further investigation into language-specific training dynamics. We begin with Python method-level code summarization, where Multi-task QLoRA demonstrates clear and consistent advan-tages. Across all model sizes and evaluation metrics, the Qwen-Coder model fine-tuned via Multi-task QLoRA outperforms its Single-task counterpart, with performance gains that remain substantial across scales. All improvements are reported as relative gains over the Single-task baseline. At 0.5B parameters, Multi-task QLoRA achieves ‚Üë36.0% in BLEU (19.43% vs. 14.29%), ‚Üë18.0% in 

> Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 17 

Table 10. RQ1: Comparison of ST-QLoRA and MT-QLoRA for code summarization on CoderEval. SIDE applies to Java only.                                                                                       

> Language Model Method BLEU METEOR Rouge-L chrF BERTScore SIDE
> Java QwenCoder-0.5B ST-QLoRA 7.10% 21.91% 33.32% 28.66% 66.56% 92.38%
> MT-QLoRA 7.18% 21.80% 34.09% 27.71% 66.31% 91.75%
> QwenCoder-1.5B ST-QLoRA 8.48% 23.43% 35.60% 30.21% 67.01% 91.96%
> MT-QLoRA 8.12% 22.14% 34.15% 29.23% 66.78% 92.21%
> QwenCoder-3B ST-QLoRA 8.73% 23.95% 35.32% 30.58% 67.03% 91.91%
> MT-QLoRA 7.63% 22.94% 34.93% 29.31% 66.40% 91.58%
> Python QwenCoder-0.5B ST-QLoRA 14.29% 25.03% 37.16% 34.89% 63.83% ‚Äì
> MT-QLoRA 19.43% 29.53% 40.67% 39.16% 65.48% ‚Äì
> QwenCoder-1.5B ST-QLoRA 15.12% 26.84% 39.87% 36.88% 65.00% ‚Äì
> MT-QLoRA 22.90% 33.07% 44.50% 42.28% 66.84% ‚Äì
> QwenCoder-3B ST-QLoRA 23.31% 33.22% 44.93% 41.75% 67.07% ‚Äì
> MT-QLoRA 29.90% 40.06% 47.43% 48.20% 68.59% ‚Äì

METEOR (29.53% vs. 25.03%), ‚Üë9.4% in ROUGE-L (40.67% vs. 37.16%), ‚Üë12.2% in chrF (39.16% vs. 34.89%), and ‚Üë2.6% in BERTScore (65.48% vs. 63.83%). These advantages persist and strengthen at 1.5B, where Multi-task QLoRA attains ‚Üë51.5% in BLEU (22.90% vs. 

15.12%), ‚Üë23.2% in METEOR (33.07% vs. 26.84%), ‚Üë11.6% in ROUGE-L (44.50% vs. 39.87%), ‚Üë14.6% in chrF (42.28% vs. 36.88%), and 

‚Üë2.8% in BERTScore (66.84% vs. 65.00%). At 3B, gains remain consistent, with BLEU reaching ‚Üë28.3% (29.90% vs. 23.31%), METEOR 

‚Üë20.6% (40.06% vs. 33.22%), ROUGE-L ‚Üë5.6% (47.43% vs. 44.93%), chrF ‚Üë15.4% (48.20% vs. 41.75%), and BERTScore ‚Üë2.3% (68.59% vs. 

67.07%). 

Java summarization, in contrast, exhibits a distinctly different pattern under Multi-task training, with Single-task QLoRA demonstrating consistent advantages that become more pronounced at larger scales. At 0.5B parameters, results are mixed: BLEU shows a marginal gain of ‚Üë1.1% (7.18% vs. 7.10%) and ROUGE-L improves by ‚Üë2.3% (34.09% vs. 33.32%) under Multi-task training. However, declines emerge in METEOR ( ‚Üì0.5%), chrF ( ‚Üì3.3%), BERTScore ( ‚Üì0.4%), and SIDE ( ‚Üì0.7%), indicating no clear advantage for Multi-task QLoRA at this scale. At 1.5B parameters, Single-task QLoRA establishes a clearer advantage across most metrics. Relative to Multi-task QLoRA, Single-task achieves higher BLEU ( ‚Üë4.4%; 8.48% vs. 8.12%), METEOR ( ‚Üë5.8%; 23.43% vs. 22.14%), ROUGE-L ( ‚Üë4.2%; 35.60% vs. 34.15%), chrF ( ‚Üë3.4%; 30.21% vs. 29.23%), and BERTScore ( ‚Üë0.3%; 67.01% vs. 66.78%). The code-aware SIDE metric shows a modest improve-ment of ‚Üë0.3% (92.21% vs. 91.96%) under Multi-task training, representing the only metric where Multi-task QLoRA maintains an edge at this scale. At 3B parameters, the Single-task advantage becomes most pronounced. Single-task QLoRA outperforms Multi-task QLoRA across all metrics: BLEU by ‚Üë14.4% (8.73% vs. 7.63%), METEOR by ‚Üë4.4% (23.95% vs. 22.94%), ROUGE-L by ‚Üë1.1% (35.32% vs. 34.93%), chrF by ‚Üë4.3% (30.58% vs. 29.31%), BERTScore by ‚Üë0.9% (67.03% vs. 66.40%), and SIDE by ‚Üë0.4% (91.91% vs. 91.58%). Overall, the metric-based evaluation reveals clear language-dependent differences in how Multi-task training scales. For Python ,the benefits of Multi-task QLoRA are consistent across all model sizes, with substantial improvements in both lexical overlap and semantic similarity metrics. For Java , the pattern reverses: Single-task QLoRA maintains an advantage that strengthens with model capacity, suggesting that task-specific training may be more effective for Java summarization, particularly at larger scales. While these findings provide strong evidence based on automatic similarity metrics, such metrics are inherently limited in their ability to capture higher-level qualitative properties of summarization, including content adequacy, conciseness, and fluency. To address this limitation, we complement our analysis with an LLM-as-a-judge evaluation, in which a large language model is prompted to assess generated summaries along these qualitative dimensions. By using an LLM as a proxy for human judgment, this approach enables a more human-centric evaluation of summary quality that goes beyond surface-level similarity and has been shown to approximate expert assessments in prior work [17]. To motivate our choice of GPT-5 Mini as the LLM judge, we follow recent work showing that lightweight, well-calibrated models can reliably approximate human judgments when properly validated. In software engineering contexts, GPT-5 Mini has been adopted as an LLM-based evaluator under established LLM-as-a-judge paradigms, where it serves as a strong proxy for human assessment using zero-shot prompting [69]. Moreover, comparative validation studies across multiple candidate judge models report that GPT-5 Mini achieves the highest agreement with expert human judgments, outperforming larger models such as GPT-4.1 and GPT-4o [11]. Together, these findings indicate that GPT-5 

> Manuscript submitted to ACM

18 Haque, Afrin and Mastropaolo          

> Code Summary Evaluation Prompt
> You will be provided with a {lang} function (‚ÄúFunction‚Äù) and a textual summary of it (‚ÄúComment‚Äù). The goal of the Comment is to document the functionality implemented in the Function. Your role is to evaluate the Comment across three criteria, providing as output for each of them a rating and a rationale as described in the following.
> # Evaluation Criteria
> *Content adequacy : the extent to which the comment summarizes all information that can be inferred from the source code. *Conciseness : the extent to which the comment contains unnecessary information. *Fluency : the extent to which the comment is easy to understand. For each criterion, provide a score on a scale from 1 to 5: 1. Very poor 2. Poor 3. Fair 4. Good 5. Very good
> # Function
> {code}
> # Comment
> {summary}
> Please provide your ratings in this exact format:
> Content adequacy: [score] Conciseness: [score] Fluency: [score]
> Fig. 1. Zero-shot prompt used for LLM-as-a-judge evaluation of code summaries.

Mini offers a favorable balance between evaluation reliability and computational efficiency, making it well suited for scalable, human-centric assessment of generated software artifacts. Using the evaluation data described in Section 3.2.3, we use zero-shot prompting with GPT-5 Mini as the judge. The evaluator scores each generated summary along three complementary dimensions‚Äìcontent Adequacy, conciseness, and fluency‚Äìusing a five-point Likert scale ranging from 1 (Very poor) to 5 (Very good). The complete prompt used for this evaluation is shown in Fig. 1. We evaluate summaries generated by Qwen-Coder under two QLoRA fine-tuning strategies: (i) Multi-task training across multiple code-related tasks, and (ii) Single-task training exclusively for code summarization. We compare these model-generated summaries against human-written references curated by Crupi et al. [17] as reliable quality baselines. Complete distributions of GPT-5 mini quality scores across all configurations are presented in Appendix A (Fig. 5 and Fig. 6). Across both languages, Multi-task QLoRA achieves summary quality broadly comparable to Single-task QLoRA across all three evaluation dimensions. For Java , both configurations attain Content Adequacy scores corresponding to approximately 73‚Äì 78% of the human-written baseline, indicating that the generated summaries capture a substantial portion of the information conveyed by developer-authored documentation. Differences between Multi-task and Single-task QLoRA are minimal across all model sizes, typically within 0.1 points. Conciseness scores remain consistently high for both configurations, ranging from 4.92 to 4.99, and often surpass the human baseline of 4.62, suggesting that automated summaries tend to avoid unnecessary verbosity. Fluency scores are likewise close to human-written quality, with both configurations achieving values between 4.22 and 4.41, demonstrating that the generated summaries are generally clear, well-formed, and easy to understand. Consistent with these observations, Wilcoxon signed-rank tests with Holm‚ÄìBonferroni correction ( ùõº = 0.05 ) reveal no statis-tically significant differences between training strategies for Java across any metric or model size ( ùëù ‚â• 0.16 ). For Python , we observe similar qualitative trends, though with slightly lower Content Adequacy relative to Java . Human-written summaries establish a baseline of 2.64, while both QLoRA configurations achieve approximately 69‚Äì74% of this level across model sizes. Differences between Multi-task and Single-task QLoRA remain modest, with Multi-task models closely tracking Single-task performance and, in the 3B configuration, slightly exceeding it in content adequacy. Conciseness scores for Python 

summaries remain high for both configurations (4.59‚Äì4.73), approaching the human baseline (4.80), while Fluency scores (3.71‚Äì 3.81) fall somewhat below human quality (4.54) but remain consistent across training strategies. Statistical analysis supports these trends (complete results available in our replication package [2]): no significant differences are observed at 0.5B ( ùëù ‚â• 0.04 ), 

> Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 19 whereas at larger scales (1.5B and 3B), Multi-task QLoRA yields statistically significant improvements across all metrics ( ùëù < 0.01 ), indicating that Multi-task training provides measurable benefits for Python summarization at higher model capacities. 

Key Findings 

Code summarization exhibits strong language-dependent effects under Multi-task training. For Python , Multi-task QLoRA consistently improves similarity-based metrics over Single-task QLoRA across all model sizes, with gains ranging from 5.6% to 51.5% depending on metric and scale. For Java , the pattern reverses: Single-task QLoRA maintains an advantage that strengthens at larger scales, with Multi-task QLoRA showing declines of up to 14.4% relative to Single-task at 3B. LLM-as-a-judge results reveal no significant quality differences for Java , whereas Python shows significant Multi-task advantages at larger scales under Wilcoxon tests with Holm‚ÄìBonferroni correction. Overall, Multi-task QLoRA reliably benefits Python 

summarization while Single-task QLoRA proves more effective for Java .

4.2 RQ2: Multi-task QLoRA vs. Multi-task Full Fine-Tuning 

Having established the comparative trends between Single-task QLoRA and Multi-task QLoRA, we next examine whether the observed Multi-task advantages persist when Multi-task QLoRA is compared against Multi-task full fine-tuning ( MT-FFT ) across multiple code-related objectives. Unlike QLoRA, which updates only low-rank adaptation parameters, MT-FFT updates all model parameters and therefore provides a reference point for assessing the trade-offs between efficiency and overall effectiveness. We structure this analysis by task, considering code generation (Section 4.2.1), code translation (Section 4.2.2), and code summariza-tion (Section 4.2.3). 

4.2.1 Code Generation. We summarize the code generation results in Table 11 and Table 12 for Python and Java , respectively. Across both languages and all model scales, Multi-task QLoRA matches or exceeds MT-FFT in most configurations. For Python , Multi-task QLoRA achieves higher pass@1 scores across all model scales, increasing from 14.74% to 15.79% at 0.5B (‚Üë7.1%), from 15.79% to 18.95% at 1.5B ( ‚Üë20.0%), and from 17.37% to 21.05% at 3B ( ‚Üë21.2%). The performance gap widens as model capacity increases, with the largest relative improvement observed at 3B. For Java, Multi-task QLoRA similarly outperforms MT-FFT at 0.5B (20.11% vs. 19.02%, ‚Üë5.7%) and demonstrates substantial advantage at 3B (32.07% vs. 26.63%, ‚Üë20.4%). The 1.5B configuration represents the only case where MT-FFT slightly exceeds 

Multi-task QLoRA (20.11% vs. 19.02%, ‚Üì5.4%). Comparing Multi-task QLoRA performance across languages, we notice that the same model exhibits distinct behaviors when generating Java versus Python code under the same setting configuration. In particular, we notice that in terms of correctness (pass@1 ) gains are more pronounced and uniformly increasing with scale for Python , while Java exhibits a mild regression at 1.5B before recovering strongly at 3B. These differences may stem from intrinsic language characteristics, such as syntactic verbosity, type system rigidity, and the degree of structural constraint imposed during generation, which can interact differently with shared 

Multi-task representations. In addition to that, variations in the distribution of the training corpora may amplify scale-dependent effects.                                                                                                              

> Table 11. RQ2: Comparison of MT-FFT and MT-QLoRA for Python code generation on CoderEval.
> Model Method Pass@1 Lizard Pylint SonarCloud LoC Tok DR CyC Err Warn Conv Ref Sec Rel Main CoC
> QwenCoder-0.5B MT-FFT 14.74% 2,468 16,917 98.40% 928 352 1,809 725 68 328 133 569
> MT-QLoRA 15.79% 1,952 ‚Üì13,327 ‚Üì100% 812 251 1,593 642 56 2790 634
> QwenCoder-1.5B MT-FFT 15.79% 2,374 16,385 100% 928 350 1,596 650 66 1651 762
> MT-QLoRA 18.95% 2,296 ‚Üì14,531 ‚Üì100% 882 315 1,767 677 76 1572 771
> QwenCoder-3B MT-FFT 17.37% 2,706 18,038 100% 996 351 1,762 579 75 20135 686
> MT-QLoRA 21.05% 1,759 ‚Üì11,773 ‚Üì100% 677 269 1,521 673 56 25106 549

We move forward presenting the results of automated qualitative code analysis we conducted. 

> Manuscript submitted to ACM

20 Haque, Afrin and Mastropaolo 

Table 12. RQ2: Comparison of MT-FFT and MT-QLoRA for Java code generation on CoderEval.                                                                                                                      

> Model Method Pass@1 Lizard PMD SonarCloud LoC Tok DR CyC Best Prac. CS Design EP MT Perf. Sec Rel Main CoC
> QwenCoder-0.5B MT-FFT 19.02% 1,838 12,339 98.40% 649 90 1,183 131 58 415 05570 629
> MT-QLoRA 20.11% 1,933 ‚Üë12,750 ‚Üë96.70% 697 76 1,123 139 33 310 00531 494
> QwenCoder-1.5B MT-FFT 20.11% 1,828 13,029 98.90% 676 94 1,288 119 49 917 122 569 576
> MT-QLoRA 19.02% 1,490 ‚Üì10,313 ‚Üì96.20% 589 75 1,183 127 31 511 02530 442
> QwenCoder-3B MT-FFT 26.63% 1,727 11,565 99.50% 607 80 1,200 131 45 327 01548 501
> MT-QLoRA 32.07% 1,580 ‚Üì10,920 ‚Üì98.40% 558 86 1,184 128 32 4702541 469

Starting from Python , Multi-task QLoRA consistently produces code with lower complexity than MT-FFT , reducing CyC from 928 to 812 at 0.5B ( ‚Üì12.5%), from 928 to 882 at 1.5B ( ‚Üì5.0%), and from 996 to 677 at 3B ( ‚Üì32.0%). For Java, the pattern varies across scales: at 0.5B, Multi-task QLoRA exhibits slightly higher complexity than MT-FFT (697 vs. 649, ‚Üë7.4%), whereas at 1.5B and 3B it produces lower CyC values (589 vs. 676, ‚Üì12.9%; and 558 vs. 607, ‚Üì8.1%). These trends suggest that complexity-related benefits of parameter-efficient training become more consistent as model capacity increases. Static analysis results further contextualize the functional correctness findings by contrasting Multi-task QLoRA with MT-FFT 

across languages and scales. Overall, Multi-task QLoRA either matches or improves code quality and implementation efficiency relative to full fine-tuning, particularly at larger model sizes. For Python , Multi-task QLoRA reduces total Pylint violations by 5.8% overall (from 8,383 to 7,896), with clear improvements at 0.5B ( ‚Üì13.9%) and 3B ( ‚Üì9.0%), alongside a mild regression at 1.5B. SonarCloud maintainability metrics follow a similar trend, showing substantial reductions at 0.5B ( ‚Üì32.3%) and 3B ( ‚Üì21.5%). In parallel, Multi-task QLoRA consistently produces more compact Python implementations, reducing lines of code and token counts at all scales, with the largest gains observed at 3B (35.0% fewer lines of code and 34.7% fewer tokens). For Java , Multi-task QLoRA likewise improves static analysis outcomes relative to MT-FFT , though with smaller and more uniform gains. Total PMD violations decrease by 6.3% overall, with consistent reductions across all model sizes. Notably, improve-ments are concentrated in Error Prone and Performance categories, where violations drop by up to 43.1% and 74.1%, respectively, at larger scales. SonarCloud maintainability issues also decrease modestly but consistently across all configurations. Code size reductions for Java mirror the Python trend at medium and large scales‚Äìshowing decreases of 18.5% in lines of code at 1.5B and 8.5% at 3B‚Äìwhile exhibiting a slight increase at 0.5B. Overall, these results reinforce the emerging trend that QLoRA-based parameter-efficient Multi-task fine-tuning does not com-promise, and in many cases can improve code quality and implementation compactness relative to MT-FFT . Although both lan-guages benefit from Multi-task QLoRA, Python exhibits earlier and more pronounced improvements, whereas Java follows a similar trajectory with steadier and more conservative gains, particularly as model capacity increases. This contrast suggests that the effectiveness of Multi-task parameter-efficient fine-tuning is modulated by language-specific characteristics, with larger models better able to leverage shared representations across tasks. To assess statistical significance, we conduct McNemar tests for pass@1 and Wilcoxon signed-rank tests for quality metrics, applying Holm‚ÄìBonferroni correction ( ùõº = 0.05 ) to compare Multi-task QLoRA against MT-FFT . Across all model scales and both languages, Multi-task QLoRA achieves statistical parity in functional correctness, supporting the hypothesis that parameter-efficient Multi-task fine-tuning can preserve pass@1 performance relative to full fine-tuning. Beyond functional correctness, quality analyses reveal additional advantages of Multi-task QLoRA. For Python , larger model scales exhibit statistically significant improvements in code conciseness alongside reductions in complexity-related metrics. For 

Java , quality benefits are more pronounced at smaller scales, including significant reductions in code size and structural complex-ity. 

> Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 21 

Key Findings: 

Code generation under Multi-task QLoRA consistently matches or exceeds MT-FFT across both Python and Java , with gains becoming more pronounced as model capacity increases. For Python , Multi-task QLoRA yields steadily increasing im-provements in functional correctness and code quality across all scales, producing less complex, more compact, and cleaner implementations. For Java , improvements are more conservative and scale-dependent, with a mild regression at interme-diate scale followed by clear gains at larger models. Statistical analysis confirms parity in functional correctness across all configurations, while quality metrics reveal additional advantages of Multi-task QLoRA, indicating that parameter-efficient 

Multi-task fine-tuning preserves correctness and can enhance code quality, particularly at larger scales. 

4.2.2 Code Translation. We evaluate code translation performance using CodeBLEU across both Java ‚ÜíC# and C# ‚ÜíJava direc-tions. Table 13 and Table 14 compare Multi-task QLoRA against Multi-task FFT across all three model scales (0.5B, 1.5B, 3B). 

Table 13. RQ2: Comparison of MT-FFT and MT-QLoRA for Java ‚ÜíC# code translation on CodeXGLUE.                                                             

> Model Method CodeBLEU Lizard Roslyn LoC Tok DR CyC Syntax Errors Maintainability
> QwenCoder-0.5B MT-FFT 74.91% 6,230 40,104 99.1% 1,492 255 566
> MT-QLoRA 68.92% 6,218 ‚Üì39,509 ‚Üì98.7% 1,515 256 835
> QwenCoder-1.5B MT-FFT 78.53% 6,671 43,343 99.3% 1,688 302 494
> MT-QLoRA 73.08% 6,369 ‚Üì40,516 ‚Üì98.5% 1,563 313 592
> QwenCoder-3B MT-FFT 66.38% 5,986 38,418 92.1% 1,492 232 1,829
> MT-QLoRA 62.88% 5,890 ‚Üì37,716 ‚Üì93.6% 1,455 126 3,061

Table 14. RQ2: Comparison of MT-FFT and MT-QLoRA for C# ‚ÜíJava code translation on CodeXGLUE.                                                                                                                      

> Model Method CodeBLEU Lizard PMD SonarCloud LoC Tok DR CyC Best Prac. CS Design EP MT Perf. Sec Rel Main CoC
> QwenCoder-0.5B MT-FFT 73.82% 5,506 33,600 98.6% 1,536 398 3,862 188 57 14 141 017 1,049 604
> MT-QLoRA 68.37% 5,807 ‚Üë35,170 ‚Üë99.6% 1,594 400 3,953 234 54 12 140 013 1,056 654
> QwenCoder-1.5B MT-FFT 70.07% 5,784 35,926 98.9% 1,633 402 3,894 189 60 17 152 012 1,046 625
> MT-QLoRA 70.72% 5,745 ‚Üì35,074 ‚Üì99.6% 1,566 400 3,939 213 61 22 163 019 1,088 678
> QwenCoder-3B MT-FFT 76.98% 5,822 35,831 97.8% 1,614 378 3,060 204 42 12 162 012 777 641
> MT-QLoRA 71.22% 5,742 ‚Üì34,862 ‚Üì99.5% 1,612 37 1,072 45 11 5100278 157

Multi-task QLoRA shows mixed results compared to Multi-task FFT in code translation, with performance varying by direction and model size. In the Java ‚ÜíC# direction, Multi-task QLoRA falls short across the board: the 0.5B model achieves 68.92% versus FFT‚Äôs 74.91% ( ‚Üì8.0%), the 1.5B model reaches 73.08% versus 78.53% ( ‚Üì6.9%), and the 3B model attains 62.88% versus 66.38% ( ‚Üì5.3%). The C# ‚ÜíJava direction reveals a more nuanced picture: while the 0.5B model lags behind at 68.37% versus 73.82% ( ‚Üì7.4%) and the 3B model similarly underperforms at 71.22% versus 76.98% ( ‚Üì7.5%), the 1.5B configuration bucks this trend, slightly edging out Multi-task FFT with 70.72% versus 70.07% ( ‚Üë0.9%). This lone success at 1.5B stands in contrast to the otherwise consistent advantage of Multi-task FFT in translation tasks‚Äìa pattern that differs markedly from code generation, where Multi-task QLoRA matched or exceeded Multi-task FFT performance throughout. When we look at quality metrics across static analysis tools, the patterns echo what we observed for correctness. In the Java ‚ÜíC# direction, where Lizard and Roslyn assess code quality, Multi-task FFT generates 4,672 Cyclomatic Complexity issues and 3,678 Roslyn issues (combining syntax errors and maintainability concerns). Multi-task QLoRA produces slightly fewer CyC issues at 4,533 ( ‚Üì3.0%), but Roslyn flags jump considerably to 5,183 ( ‚Üë40.9%). The reverse direction, C# ‚ÜíJava, tells a different story. While Multi-task FFT produces 4,783 CyC issues, 13,232 PMD issues, and 4,783 SonarCloud quality issues, Multi-task QLoRA shows improvements across the board: 4,772 CyC issues ( ‚Üì0.2%), 10,762 PMD issues ( ‚Üì18.7%), and 3,943 SonarCloud quality issues (‚Üì17.6%). In essence, Multi-task QLoRA struggles with Roslyn-detected issues when translating to C#, but delivers notable quality gains in PMD and SonarCloud metrics when translating to Java. Cyclomatic Complexity trends shift depending on model size and translation direction. For Java ‚ÜíC#, the results are mixed: the 0.5B model produces 1,515 CyC issues versus FFT‚Äôs 1,492 ( ‚Üë1.5%), while the 1.5B model improves to 1,563 versus 1,688 ( ‚Üì7.4%), 

> Manuscript submitted to ACM

22 Haque, Afrin and Mastropaolo and the 3B model shows 1,455 versus 1,492 ( ‚Üì2.5%). The C# ‚ÜíJava direction follows a similar pattern: 0.5B generates 1,594 versus 1,536 ( ‚Üë3.8%), 1.5B improves to 1,566 versus 1,633 ( ‚Üì4.1%), and 3B achieves near parity at 1,612 versus 1,614 ( ‚Üì0.1%). Language-specific analysis reveal pronounced differences in quality profiles. For Roslyn metrics in Java ‚ÜíC# translation, Multi-task QLoRA shows an increasing numbers of detected issues across all scales. Breaking down by components: 0.5B produces 1,091 total issues (256 syntax errors + 835 maintainability) compared to FFT‚Äôs 821 (255 + 566), representing a ‚Üë32.9% increase. The 1.5B configuration generates 905 total issues (313 + 592) vs. 796 (302 + 494), yielding a ‚Üë13.7% increase. Notably, 3B produces 3,187 total issues (126 + 3,061) compared to 2,061 (232 + 1,829), representing a ‚Üë54.6% increase. While syntax errors decrease at 3B (126 vs. 

232, ‚Üì45.7%), maintainability issues increase substantially (3,061 vs. 1,829, ‚Üë67.4%). SonarCloud analysis for C# ‚ÜíJava reinforces these scale-dependent patterns. Aggregate quality issues decrease from 4,783 to 3,943 ( ‚Üì17.6%), with particularly pronounced improvements in the 3B configuration. Breaking down by scale: 0.5B shows mixed results with total issues at 1,723 vs. 1,670 ( ‚Üë3.2%), featuring increases in Maintainability (1,056 vs. 1,049, ‚Üë0.7%) and Cognitive Complexity (654 vs. 604, ‚Üë8.3%), while achieving reductions in Reliability (13 vs. 17, ‚Üì23.5%). The 1.5B configuration shows 1,785 

vs. 1,683 ( ‚Üë6.1%), with increases in Maintainability (1,088 vs. 1,046, ‚Üë4.0%), Cognitive Complexity (678 vs. 625, ‚Üë8.5%), and Reliability (19 vs. 12, ‚Üë58.3%). The 3B configuration achieves exceptional improvement with 435 vs. 1,430 ( ‚Üì69.6%), demonstrating substantial reductions in Maintainability (278 vs. 777, ‚Üì64.2%), Reliability (0 vs. 12, ‚Üì100%), and Cognitive Complexity (157 vs. 641, ‚Üì75.5%). Code size metrics reveal mixed patterns across translation directions and scales. For Java ‚ÜíC#, Multi-task QLoRA generates comparable or reduced code: 0.5B produces 6,218 LoC vs. FFT‚Äôs 6,230 ( ‚Üì0.2%), 1.5B generates 6,369 vs. 6,671 ( ‚Üì4.5%), and 3B produces 5,890 vs. 5,986 ( ‚Üì1.6%). Token counts demonstrate consistent reductions: 0.5B uses 39,509 vs. 40,104 ( ‚Üì1.5%), 1.5B uses 40,516 vs. 

43,343 ( ‚Üì6.5%), and 3B uses 37,716 vs. 38,418 ( ‚Üì1.8%). For C# ‚ÜíJava, patterns vary: 0.5B produces 5,807 LoC vs. 5,506 ( ‚Üë5.5%), 1.5B generates 5,745 vs. 5,784 ( ‚Üì0.7%), and 3B produces 5,742 vs. 5,822 ( ‚Üì1.4%). Token usage follows similar patterns: 0.5B uses 35,170 

vs. 33,600 ( ‚Üë4.7%), 1.5B uses 35,074 vs. 35,926 ( ‚Üì2.4%), and 3B uses 34,862 vs. 35,831 ( ‚Üì2.7%). Following the approach used for code generation, we apply Wilcoxon signed-rank tests with Holm‚ÄìBonferroni correction (ùõº = 0.05 ) to assess whether the quality of code produced by Multi-task QLoRA differs significantly from that produced by 

MT-FFT .For the Java ‚ÜíC# translation direction, the results are unambiguous: across all three model sizes, statistically significant dif-ferences in CodeBLEU scores are observed ( ùëù < 0.05 ), consistently favoring MT-FFT . These differences align with the previously observed gaps in functional correctness and static quality metrics, with corresponding effect sizes (ClifÓÉ†‚Äôs delta) indicating a 

negligible -magnitude effect. In contrast, the C# ‚ÜíJava direction exhibits a scale-dependent pattern. At 0.5B, MT-FFT maintains a statistically significant advantage ( ùëù < 0.01 ) with negligible effect size. This advantage disappears at 1.5B, where the two approaches achieve statis-tical parity, indicating no significant difference in performance. Notably, at 3B, the relationship reverses: Multi-task QLoRA demonstrates statistically significant superiority across both functional correctness and quality metrics ( ùëù < 0.01 ). This result is consistent with the substantial improvements observed at this scale in static analysis outcomes and code conciseness, with effect sizes (ClifÓÉ†‚Äôs delta) suggesting a small-to-medium -magnitude effect. 

Key Findings: 

Code translation under Multi-task QLoRA exhibits strong scale- and direction-dependent behavior. For Java ‚ÜíC#, quality improvements in syntax are offset by increased maintainability issues, whereas for C# ‚ÜíJava, larger models achieve clear gains in both correctness and quality. Overall, benefits of parameter-efficient Multi-task fine-tuning emerge primarily at larger scales, underscoring the role of model capacity in balancing translation quality and efficiency. 

4.2.3 Code Summarization. We evaluate code summarization performance by comparing Multi-task QLoRA against Multi-task 

Full Fine-Tuning ( MT-FFT ) using similarity-based metrics. Table 15 reports results for both Java and Python across all three model scales (0.5B, 1.5B, and 3B). 

Multi-task QLoRA exhibits clear language-dependent behavior when compared to Multi-task FFT. For Python , Multi-task 

QLoRA consistently outperforms MT-FFT across all metrics and model scales. At 0.5B, Multi-task QLoRA achieves 19.43% BLEU compared to MT-FFT ‚Äôs 9.53% ( ‚Üë103.9%), 29.53% METEOR vs. 20.28% ( ‚Üë45.6%), 40.67% Rouge-L vs. 31.55% ( ‚Üë28.9%), 39.16% chrF 

> Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 23 

Table 15. RQ2: Comparison of MT-FFT and MT-QLoRA for code summarization on CoderEval. SIDE applies to Java only.                                                                                       

> Lang Model Method BLEU METEOR Rouge-L chrF BERTScore SIDE
> Java QwenCoder-0.5B MT-FFT 7.70% 21.36% 33.29% 28.76% 71.25% 90.44%
> MT-QLoRA 7.18% 21.80% 34.09% 27.71% 66.31% 91.75%
> QwenCoder-1.5B MT-FFT 7.43% 21.86% 33.73% 27.66% 69.79% 89.68%
> MT-QLoRA 8.12% 22.14% 34.15% 29.23% 66.78% 92.21%
> QwenCoder-3B MT-FFT 7.90 %21.85% 33.59% 28.80% 65.90% 90.53%
> MT-QLoRA 7.63% 22.94% 34.93% 29.31% 66.40% 91.58%
> Python QwenCoder-0.5B MT-FFT 9.53% 20.28% 31.55% 29.14% 60.66% ‚Äì
> MT-QLoRA 19.43% 29.53% 40.67% 39.16% 65.48% ‚Äì
> QwenCoder-1.5B MT-FFT 12.67% 24.36% 34.72% 33.28% 62.01% ‚Äì
> MT-QLoRA 22.90% 33.07% 44.50% 42.28% 66.84% ‚Äì
> QwenCoder-3B MT-FFT 22.54% 32.91% 43.41% 42.27% 67.17% ‚Äì
> MT-QLoRA 29.90% 40.06% 47.43% 48.20% 68.59% ‚Äì

vs. 29.14% ( ‚Üë34.4%), and 65.48% BERTScore vs. 60.66% ( ‚Üë7.9%). These advantages persist at 1.5B, where Multi-task QLoRA attains 22.90% BLEU vs. 12.67% ( ‚Üë80.7%), 33.07% METEOR vs. 24.36% ( ‚Üë35.8%), 44.50% Rouge-L vs. 34.72% ( ‚Üë28.2%), 42.28% chrF vs. 33.28% (‚Üë27.0%), and 66.84% BERTScore vs. 62.01% ( ‚Üë7.8%). At 3B, Multi-task QLoRA continues to outperform, with BLEU reaching 29.90% 

vs. 22.54% ( ‚Üë32.7%), METEOR 40.06% vs. 32.91% ( ‚Üë21.7%), Rouge-L 47.43% vs. 43.41% ( ‚Üë9.3%), chrF 48.20% vs. 42.27% ( ‚Üë14.0%), and BERTScore 68.59% vs. 67.17% ( ‚Üë2.1%). For Java, differences between Multi-task QLoRA and Multi-task FFT are mixed across configurations, with each approach showing advantages on different metrics. At 0.5B, Multi-task QLoRA shows heterogeneous outcomes: METEOR improves by ‚Üë2.1% (21.80% vs. 21.36%), Rouge-L by ‚Üë2.4% (34.09% vs. 33.29%), and SIDE by ‚Üë1.4% (91.75% vs. 90.44%). However, MT-FFT achieves higher BLEU ( ‚Üë7.2%; 7.70% vs. 7.18%), chrF ( ‚Üë3.8%; 28.76% vs. 27.71%), and notably BERTScore ( ‚Üë7.4%; 71.25% vs. 66.31%). At 1.5B, Multi-task 

QLoRA exhibits improvements on most metrics: BLEU of 8.12% vs. 7.43% ( ‚Üë9.3%), METEOR of 22.14% vs. 21.86% ( ‚Üë1.3%), Rouge-L of 34.15% vs. 33.73% ( ‚Üë1.2%), chrF of 29.23% vs. 27.66% ( ‚Üë5.7%), and SIDE of 92.21% vs. 89.68% ( ‚Üë2.8%), while MT-FFT maintains an advantage in BERTScore (69.79% vs. 66.78%; ‚Üë4.5%). At 3B, the pattern continues with Multi-task QLoRA achieving higher METEOR ( ‚Üë5.0%; 22.94% vs. 21.85%), Rouge-L ( ‚Üë4.0%; 34.93% vs. 33.59%), chrF ( ‚Üë1.8%; 29.31% vs. 28.80%), BERTScore ( ‚Üë0.8%; 66.40% 

vs. 65.90%), and SIDE ( ‚Üë1.2%; 91.58% vs. 90.53%), while MT-FFT shows a slight edge in BLEU ( ‚Üë3.5%; 7.90% vs. 7.63%). Overall, while 

Multi-task QLoRA demonstrates consistent advantages on overlap-based metrics (METEOR, Rouge-L) and the code-aware SIDE metric for Java , MT-FFT tends to achieve comparable or better performance on lexical precision metrics, particularly BERTScore at smaller scales. Scaling behavior further highlights these language-dependent effects. For Python , Multi-task QLoRA benefits remain substantial across model capacities, though the relative advantage over MT-FFT narrows at larger scales as both approaches improve. From 0.5B to 3B under Multi-task QLoRA, BLEU improves from 19.43% to 29.90% ( ‚Üë53.9%), METEOR from 29.53% to 40.06% ( ‚Üë35.7%), Rouge-L from 40.67% to 47.43% ( ‚Üë16.6%), chrF from 39.16% to 48.20% ( ‚Üë23.1%), and BERTScore from 65.48% to 68.59% ( ‚Üë4.7%). Multi-task FFT also exhibits strong scaling, with BLEU improving from 9.53% to 22.54% ( ‚Üë136.5%), yet Multi-task QLoRA maintains its advantage across all scales. For Java, both approaches show relatively stable performance across model sizes, with differences between training strategies remaining within a narrow range and neither approach establishing consistent dominance across all metrics. To complement similarity-based evaluation with qualitative assessment, we employ the LLM-as-a-judge methodology de-scribed in RQ 1 (Section 4.1.3). Complete distributions of GPT-5 mini quality scores across all configurations are reported in the Appendix A. Statistical significance is assessed using Wilcoxon signed-rank tests with Holm‚ÄìBonferroni correction ( ùõº = 0.05 ), comparing Multi-task QLoRA against MT-FFT for both functional and quality metrics. For Java , Content Adequacy scores indicate that Multi-task QLoRA maintains summary quality comparable to MT-FFT across all model sizes. Multi-task QLoRA achieves scores of 2.178 vs. 2.139 at 0.5B ( ‚Üë1.8%), 2.224 vs. 2.198 at 1.5B ( ‚Üë1.2%), and 2.293 vs. 

2.228 at 3B ( ‚Üë2.9%), corresponding to approximately 73‚Äì77% of the human baseline (2.960). Conciseness scores remain consistently high for Multi-task QLoRA (4.925‚Äì4.980), marginally exceeding both MT-FFT (4.925‚Äì4.954) and the human baseline (4.620), while Fluency scores show small, scale-dependent variations. Importantly, none of these differences are statistically significant across metrics or model sizes, indicating performance parity between Multi-task QLoRA and full fine-tuning for Java summarization. 

> Manuscript submitted to ACM

24 Haque, Afrin and Mastropaolo For Python , qualitative assessment reveals a different pattern. Content Adequacy scores for Multi-task QLoRA and MT-FFT 

remain close across all model sizes (1.818 vs. 1.806 at 0.5B, 1.862 vs. 1.894 at 1.5B, and 1.938 vs. 1.962 at 3B), corresponding to 69‚Äì74% of the human baseline (2.636). Conciseness scores are consistently high for both approaches (4.624‚Äì4.730 for Multi-task 

QLoRA and 4.626‚Äì4.670 for MT-FFT ), approaching the human baseline (4.800), while Fluency scores remain comparable and below human quality but maintain acceptable readability. Despite the similarity in absolute scores, statistical testing reveals significant 

Multi-task QLoRA advantages across all qualitative metrics and model scales ( ùëù < 0.05 )3.Finally, an interesting pattern emerges: QLoRA-based Multi-task optimization can improve automatic metrics without pro-portionally changing human assessments. The 0.5B model illustrates this clearly‚Äì Multi-task QLoRA increases BLEU by 103.9% over MT-FFT (19.43% vs. 9.53%), while Content Adequacy remains flat (1.818 vs. 1.806). MT-QLoRA appears to enhance lexical alignment in ways that automatic metrics detect but human evaluators don‚Äôt perceive as proportional quality improvements. A detailed breakdown of statistical results and metric-level analyses is provided in the Appendix A. 

Key Findings: 

Code summarization under Multi-task QLoRA shows clear language-dependent effects. Multi-task QLoRA yields substantial improvements for Python across all similarity-based metrics, with gains ranging from 2.1% to 103.9% depending on metric and scale. For Java , results are more nuanced: Multi-task QLoRA demonstrates advantages on overlap-based metrics (METEOR, Rouge-L) and the code-aware SIDE metric, while MT-FFT achieves comparable or better performance on lexical precision metrics like BERTScore. LLM-as-a-judge evaluation confirms that Multi-task QLoRA preserves content adequacy, conciseness, and fluency for both languages. Overall, MT-QLoRA strongly benefits Python summarization while achieving competitive performance with full fine-tuning for Java. 

4.3 Qualitative Analysis of MT-QLoRA in Code-Related Tasks 

To better understand the quality characteristics behind the quantitative results, we conduct a qualitative analysis of model out-puts across code generation, summarization, and translation tasks. We perform an instance-level overlap analysis that partitions evaluation instances based on which configuration(s) succeed, enabling us to examine both shared successes and unique successes attributable to a specific training setup. For code generation , we use pass@1 to partition instances into those solved by both ST-QLoRA and MT-QLoRA and those uniquely solved by only one configuration. For code translation , we use Exact Match (EM) to isolate instances where only one configuration achieves a perfect prediction (EM = 1 ). We then inspect representative examples from these partitions to character-ize qualitative differences in correctness, completeness, and semantic preservation. For code summarization , we use Content Adequacy (CA) to identify instances where only ST-QLoRA , only MT-QLoRA , or only MT-FFT attains the highest CA score. 

4.3.1 Code Generation. Our analysis for code generation shows substantial agreement between ST-QLoRA and MT-QLoRA , but instances where one succeeds and the other fails reveal clear qualitative differences on multi-step specifications. Figure 2 presents a representative Python case where MT-QLoRA produces a correct, passing solution (green checkmark) while 

ST-QLoRA fails (red X). The prompt specifies a verification routine that conditionally checks iface.providedBy (skipped under 

tentative ), validates required methods (including signatures) and attributes, and raises Invalid on contract violations. 

MT-QLoRA generates a concise, specification-aligned implementation: it correctly gates the 

providedBy check ( if not iface.providedBy(candidate) and not tentative ), raises an informative Invalid exception on failure, and cleanly verifies methods and attributes via helper routines with early exits before returning True . In contrast, 

ST-QLoRA diverges from the intended control flow by using an incorrect gating condition ( iface.providedBy(candidate) != tentative ), introducing fragmented error accumulation, and devolving into incomplete branches (ellipsis placeholders), yielding a non-passing solution. As reported in Fig. 2, ST-QLoRA also produces a longer and more complex output (LoC: 32, Tok: 196, CyC: 9) than MT-QLoRA (LoC: 12, Tok: 100, CyC: 7), without improving correctness. This example reflects a recurring pattern: MT-QLoRA more consistently preserves the contract and control-flow structure implied by docstrings, whereas ST-QLoRA is more prone to semantic drift on multi-step behavioral requirements. 

> 3Complete statistical results are provided in the replication package [2]. Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 25 Pylint: 1 (Err: 1, Warn: 0, Conv: 4, Ref: 0) 

Lizard: LoC: 32, ToK: 196, CyC: 9

Pylint: 19 (Err: 5, Warn: 10, Conv: 4, Ref: 0) 

Lizard: LoC: 12, ToK: 100, CyC: 7

Model‚Äôs Input Single-Task QLoRA-based Optimization Multi-Task QLoRA-based Optimization 

def _verify (iface, candidate, tentative= False ,

vtype= None ): 

""" 

Verify that *candidate* might correctly 

provide *iface*. 

This involves: 

- Making sure the candidate claims that it 

provides the interface using 

``iface.providedBy`` (unless *tentative* 

is`True`, in which case this step is 

skipped). 

This means that the candidate's 

class declares that it `implements 

<zope.interface.implementer>` 

the interface, or the candidate itself 

declares that it 

`provides <zope.interface.provider>` 

the interface 

‚Ä¶‚Ä¶ 

:return bool: Returns a true value if 

everything that could be checked passed. 

:raises zope.interface.Invalid: If any of the 

previous conditions does not hold. 

‚Ä¶‚Ä¶ 

""" 

def _verify (iface, candidate, tentative= False ,

vtype= None ): 

""" 

‚Ä¶

""" 

if iface.providedBy(candidate) != tentative: 

raise Invalid( 

"The candidate %r does not claim to 

provide %r" 

% (candidate, iface) 

)

errors = [] 

for method_name in iface[ 'methods' ]: 

try :

method = getattr (candidate, method_name) 

except AttributeError: 

‚Ä¶

)

)

‚Ä¶

for attr_name in iface[ 'attributes' ]: 

try :

attr = getattr (candidate, attr_name) 

except AttributeError: 

‚Ä¶

def _verify (iface, candidate, tentative= False ,

vtype= None ): 

""" 

‚Ä¶

""" 

if not iface.providedBy(candidate) and not 

tentative: 

raise Invalid( 

"Object {!r} does 

not implement {!r}" .format (candidate, 

iface) 

)

for name, method in get_methods(candidate, iface): 

if not _verify_method(name, method, 

iface, vtype=vtype): 

return False 

for name, attr in get_attributes(candidate, iface): 

if not _verify_attribute(name, attr, 

iface, vtype=vtype): 

return False 

return True 

Fig. 2. Qualitative comparison of Python code generated by ST-QLoRA and MT-QLoRA for an interface verification routine. MT-QLoRA (right, green checkmark) produces a passing solution while ST-QLoRA (middle, red X) fails functional testing. 

4.3.2 Code Translation. To characterize how structural complexity relates to the observed success patterns in translation, we analyze the source inputs using Lizard [84]. Specifically, we focused on cyclomatic complexity alongside complementary indica-tors of code size and structure ( e.g., token count and nesting depth) of the source code. This analysis helps determine whether 

MT-QLoRA ‚Äôs gains are concentrated in particular complexity regimes and provides context for common qualitative failure modes, such as missing control-flow logic, broken API consistency, or semantic drift. Single-Task QLoRA-based Optimization Multi-Task QLoRA-based Optimization 

public static double devsq (double [] v) 

{

double r = double .NaN; 

if (v != null && v.Length >= 1)

{

double m = 0;

double s = 0;

int n = v.Length; 

for (int i = 0; i < n; i++) 

{

s += v[i]; 

}

m = s / n; 

s = 0;

for (int i = 0; i < n; i++) 

{

s += (v[i] - m) * (v[i] - m); 

}

r = (n == 1) ? 0 : s; 

}

return r; 

}

public static double sqdev (double [] v) 

{

double r = Double.NaN; 

if (v != null && v.length >= 1)

{

double m = 0;

double s = 0;

final int n = v.length; 

for (int i = 0;

i < n; 

i++) 

{

s += v[i]; 

}

m = s / n; 

s = 0;

for (int i = 0;

i < n; 

i++) 

{

s += Math.pow(v[i] - m, 2); 

}

}

return r; 

}

PMD: 12 (BP: 2, CS: 9, Design: 1, EP: 0, MT: 

0, Perf: 0) 

Lizard: LoC: 22, Tok: 131, CyC: 6

public static double devsq (double [] v) 

{

double r = Double.NaN; 

if (v != null && v.length >= 1)

{

double m = 0;

double s = 0;

int n = v.length; 

for (int i = 0; i < n; i++) 

{

s += v[i]; 

}

m = s / n; 

s = 0;

for (int i = 0; i < n; i++) 

{

s += (v[i] - m) * (v[i] - m); 

}

r = (n == 1) ? 0 : s; 

}

return r; 

}

EM = 0 EM = 1

Model‚Äôs Input 

PMD: 12 (BP: 2, CS: 9, Design: 1, EP: 0, MT: 

0, Perf: 0) 

Lizard: LoC: 21, Tok: 116, CyC: 5

Fig. 3. Qualitative comparison of C# ‚ÜíJava translations generated by ST-QLoRA and MT-QLoRA for a representative instance where MT-QLoRA 

achieves exact match (EM = 1 ) while ST-QLoRA fails (EM = 0 ). 

Our overlap analysis reveals distinct patterns in the structural complexity of code uniquely translated by different model configurations. For C# ‚ÜíJava translation, MT-QLoRA uniquely achieves perfect translations on structurally more complex code 

Manuscript submitted to ACM 26 Haque, Afrin and Mastropaolo Multi-Task FFT-based Optimization Multi-Task QLoRA-based Optimization                                                         

> static String[] toNoNullStringArray(Object[]
> array)
> {
> ArrayList<String> list =new
> ArrayList <String>(array.length);
> for (int i=0;i<array.length; i++)
> {
> Object e=array[i];
> if (e != null )
> {
> list.add(e.toString());
> }
> }
> return (String[])
> list.toArray(ArrayUtils.EMPTY_STRING_ARRAY);
> }
> Summary: Returns astring array with
> the elements of the given object
> array converted to strings.
> Summary: Converts the given array of
> objects into astring array. Null
> values are ignored.
> Model‚Äôs Input
> Content adequacy: 2.4
> Conciseness: 5
> Fluency: 4.4
> Mean: 3.6
> Content adequacy: 3.8
> Conciseness: 4.8
> Fluency: 4.2
> Mean: 4.27

Fig. 4. Qualitative comparison of Python code summaries generated by ST-QLoRA and MT-QLoRA . MT-QLoRA produces a higher quality sum-mary with better content adequacy, while ST-QLoRA generates a less accurate summary that misses key details. 

compared to both ST-QLoRA (RQ1) and MT-FFT (RQ2). Across the instances it solves uniquely, MT-QLoRA more often handles inputs with higher cyclomatic complexity ( ‚âà 39 ‚Äì51 %), larger code size ( ‚âà 30 ‚Äì33 %), and deeper nesting ( ‚âà 28 ‚Äì32 %), suggesting improved robustness to complex control flow and nested structures in this translation direction. In contrast, the Java ‚ÜíC# direction shows an opposite trend: ST-QLoRA and MT-FFT more often uniquely solve structurally complex inputs, whereas MT-QLoRA more often succeeds on comparatively simpler instances, suggesting direction-dependent translation difficulty and asymmetric transfer effects. Figure 3 shows a representative C# ‚ÜíJava case where MT-QLoRA produces a perfect translation (EM =1) while ST-QLoRA fails (EM =0). The source input exhibits moderate structural complexity (CyC =6), with nested control flow structures including mul-tiple for loops and conditional logic. Although the ST-QLoRA output remains syntactically plausible, it introduces semantic and interface-breaking changes: (1) it renames the method from devsq to sqdev , breaking API consistency; (2) it omits the assignment 

r = (n == 1) ? 0 : s , leaving r never updated from its initialization Double.NaN and thus returning NaN regardless of the computed accumulator s; and (3) it replaces the direct square (v[i] - m) * (v[i] - m) with Math.pow(v[i] - m, 2) , an unnecessary deviation from the original implementation style. In contrast, MT-QLoRA preserves the method signature, retains the complete conditional assignment logic, and maintains the original arithmetic structure. While both translations trigger com-parable static analysis warnings (PMD: 12 issues), the EM discrepancy is driven by substantive semantic preservation rather than superficial style differences, illustrating MT-QLoRA ‚Äôs stronger robustness on nested control-flow translation in C# ‚ÜíJava. 

4.3.3 Code Summarization. For summarization, we inspect instances where one configuration achieves higher Content Adequacy (CA) than the others, focusing on whether the generated summaries capture behaviorally salient details ( e.g., filtering conditions, edge cases, or implicit constraints) rather than only paraphrasing surface-level operations. Across the overlap partitions, MT-QLoRA more consistently produces summaries that reflect such conditional behaviors, whereas MT-FFT summaries are more likely to remain correct but underspecified, which reduces adequacy when key constraints are omitted. Figure 4 illustrates a representative Java method where MT-QLoRA achieves higher CA than MT-FFT . The method 

toNoNullStringArray constructs a String[] by iterating over an Object[] input, converting elements via toString() , and 

skipping null entries before returning the resulting array. MT-FFT produces a largely generic description (converting an object array to strings) but fails to state the null-filtering behavior, which is central to the method‚Äôs contract. In contrast, MT-QLoRA 

explicitly captures this constraint (‚ÄúNull values are ignored‚Äù), yielding a more semantically complete summary despite similar fluency and conciseness, and thereby improving content adequacy. 

5 Threats to Validity 

Internal Validity. Our experimental design may be influenced by several factors. First, hyperparameter selection follows con-figurations from prior work [3] rather than task-specific optimization, potentially underestimating the performance of certain 

> Manuscript submitted to ACM

Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 27 configurations. While we maintain consistent hyperparameters across all experiments to ensure fair comparison, task-specific tuning might yield different relative performance patterns. Second, the early stopping criterion based on validation metrics may favor certain model configurations over others, particularly when comparing Multi-task versus Single-task models with different convergence characteristics. 

External Validity. The generalizability of our findings faces several constraints. We evaluate using the Qwen2.5-Coder family (0.5B, 1.5B, 3B parameters), and results may not directly transfer to other model architectures. Our focus on three representative code-related tasks‚Äìgeneration, summarization, and translation‚Äìcovers the main spectrum of bi-modal and code-to-code trans-formations in software engineering, but does not include other downstream tasks such as bug fixing or program repair. The employed datasets (CodeXGLUE and CoderEval) predominantly contain function-level code snippets, which, while standard in prior research, may not fully reflect project-level dependencies observed in large industrial systems. Finally, our experiments cover Java , Python , and C#, offering diversity across paradigms, though future extensions to additional languages could further strengthen generalizability. 

Construct Validity. The evaluation design relies on well-established metrics and tools to assess both functional and non-functional performance. For correctness, we combine execution-based ( pass@1 ) and structural ‚Äìsemantic (CodeBLEU) measures; for summarization, we integrate lexical, semantic, and SIDE metrics, complemented by an LLM-as-a-judge protocol following a validated methodology [17]. For code quality, we employ multiple static analyzers ( e.g., Pylint, PMD, SonarCloud, Roslyn) to triangulate results across diverse rule sets. Although static analysis tools may differ slightly in sensitivity across languages, this multi-tool strategy minimizes tool-specific biases and provides a comprehensive perspective on quality dimensions. 

Conclusion Validity. Our statistical analyses employ robust non-parametric tests (Wilcoxon signed-rank and McNemar) with Holm-Bonferroni correction to control for multiple comparisons, complemented by effect size reporting to capture practical significance. These analyses are consistently applied across both RQ 1 (MT-QLoRA vs. ST-QLoRA ) and RQ 2 (MT-QLoRA vs. MT-FFT ), where model outputs allow for paired evaluation across configurations. Minor variations fall within expected stochastic bounds of LLM generation, and cross-task consistency supports the soundness and reproducibility of our findings. 

6 Conclusions and Future Work 

This study presents the first comprehensive evaluation of Multi-task QLoRA across code generation, translation, and summariza-tion, examining both functional correctness and code quality with Qwen2.5-Coder models (0.5B, 1.5B, 3B). Multi-task QLoRA achieves competitive performance versus Single-task QLoRA while training a unified model. The 3B configuration demonstrates language-dependent effectiveness: 32.07% Java pass@1 versus 29.89% Single-task (+2.18 points) and 29.90% Python summarization BLEU versus 23.31% (+28.3% relative). Quality improvements include 32.0% lower Python complexity and 6.3% fewer Java PMD vio-lations at 3B. Against Multi-task Full Fine-Tuning, Multi-task QLoRA shows superiority for generation and Python summarization (21.05% vs. 17.37% pass@1 ; 29.90% vs. 22.54% BLEU at 3B) while maintaining quality parity across methods. Our findings reveal that larger models consistently demonstrate superior ability to balance multiple objectives within QLoRA‚Äôs parameter-efficient framework. Translation exhibits scale and direction-dependent trade-offs, with Multi-task QLoRA underper-forming for Java‚ÜíC# across all scales but showing competitive or improved results for C#‚ÜíJava at specific configurations. Code summarization demonstrates clear language-dependent effects: Python summarization benefits substantially from Multi-task training across all scales, while Java summarization favors Single-task QLoRA, particularly at larger model capacities where task-specific training proves more effective. Future work should investigate task-specific adapters to address translation instability, dynamic task weighting for language-specific transfer, and attention-based routing to prevent destructive interference in parameter-efficient Multi-task scenarios. Ex-tension to additional programming languages and software engineering tasks (bug fixing, program repair) would further validate the generalizability of our findings. 

References   

> [1] [n. d.]. CodeLLama. https://github.com/meta-llama/codellama/tree/main. [2] [n. d.]. Replication Package. https://github.com/alvi75/MultiTask-QLoRA-NFAnalysis [3] Saima Afrin, Joseph Call, Khai-Nguyen Nguyen, Oscar Chaparro, and Antonio Mastropaolo. 2025. Resource-Efficient & Effective Code Summarization. In 2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge) . IEEE, 224‚Äì235. Manuscript submitted to ACM

28 Haque, Afrin and Mastropaolo 

[4] Saima Afrin, Md Zahidul Haque, and Antonio Mastropaolo. 2025. A systematic literature review of parameter-efficient fine-tuning for large code models. arXiv preprint arXiv:2504.21569 (2025). [5] Saima Afrin, Bowen Xu, and Antonio Mastropaolo. 2025. Is Quantization a Deal-breaker? Empirical Insights from Large Code Models. arXiv preprint arXiv:2507.09665 (2025). [6] Toufique Ahmed and Premkumar Devanbu. 2022. Few-shot training llms for project-specific code-summarization. In Proceedings of the 37th IEEE/ACM interna-tional conference on automated software engineering . 1‚Äì5. [7] Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, and Earl Barr. 2024. Automatic semantic augmentation of language model prompts (for code summa-rization). In Proceedings of the IEEE/ACM 46th international conference on software engineering . 1‚Äì13. [8] AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card 1, 1 (2024), 4. [9] Shamil Ayupov and Nadezhda Chirkova. 2022. Parameter-efficient finetuning of transformers for source code. arXiv preprint arXiv:2212.05901 (2022). [10] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization . 65‚Äì72. [11] Tyler Burleigh, Jenny Han, and Kristen DiCerbo. 2025. Beyond the Hint: Using Self-Critique to Constrain LLM Feedback in Conversation-Based Assessment. In 

Proceedings of the Artificial Intelligence in Measurement and Education Conference (AIME-Con): Coordinated Session Papers . 79‚Äì85. [12] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2023. Multipl-e: A scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering 

49, 7 (2023), 3675‚Äì3691. [13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [14] Nuo Chen, Qiushi Sun, Jianing Wang, Xiang Li, and Ming Gao. 2023. Pass-tuning: Towards structure-aware parameter-efficient tuning for code representation learning. In Findings of the Association for Computational Linguistics: EMNLP 2023 . 577‚Äì591. [15] YunSeok Choi and Jee-Hyong Lee. 2023. CodePrompt: Task-agnostic prefix tuning for program and language generation. In Findings of the Association for Computational Linguistics: ACL 2023 . 5282‚Äì5297. [16] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Antonio Mastropaolo, Emad Aghajani, Denys Poshyvanyk, Massimiliano Di Penta, and Gabriele Bavota. 2021. An empirical study on the usage of transformer models for code completion. IEEE Transactions on Software Engineering 48, 12 (2021), 4818‚Äì4837. [17] Giuseppe Crupi, Rosalia Tufano, Alejandro Velasco, Antonio Mastropaolo, Denys Poshyvanyk, and Gabriele Bavota. 2025. On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization. IEEE Transactions on Software Engineering (2025). [18] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in neural information processing systems 35 (2022), 30318‚Äì30332. [19] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems 36 (2024). [20] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. 2024. Evaluating large language models in class-level code generation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering . 1‚Äì13. [21] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 (2022). [22] Divyam Goel, Ramansh Grover, and Fatemeh H Fard. 2022. On the cross-modal transfer from natural language to code through adapter modules. In Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension . 71‚Äì81. [23] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. 2024. Large language models for software engineering: A systematic literature review. ACM Transactions on Software Engineering and Methodology 33, 8 (2024), 1‚Äì79. [24] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186 (2024). [25] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). [26] Mohammed Kharma, Soohyeon Choi, Mohammed AlKhanafseh, and David Mohaisen. 2025. Security and Quality in LLM-Generated Code: A Multi-Language, Multi-Model Analysis. arXiv preprint arXiv:2502.01853 (2025). [27] Rapha√´l Khoury, Anderson R Avila, Jacob Brunelle, and Baba Mamadou Camara. 2023. How secure is code generated by chatgpt?. In 2023 IEEE international conference on systems, man, and cybernetics (SMC) . IEEE, 2445‚Äì2451. [28] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Improved code summarization via a graph neural network. In Proceedings of the 28th international conference on program comprehension . 184‚Äì195. [29] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624 (2022), 1092‚Äì1097. [30] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out . 74‚Äì81. [31] Bingchang Liu, Chaoyu Chen, Zi Gong, Cong Liao, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen, Hailian Zhou, et al. 2024. Mftcoder: Boosting code llms with multitask fine-tuning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 5430‚Äì5441. [32] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems 35 (2022), 1950‚Äì1965. [33] Jiaxing Liu, Chaofeng Sha, and Xin Peng. 2023. An empirical study of parameter-efficient fine-tuning methods for pre-trained code models. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE) . IEEE, 397‚Äì408. [34] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems 36 (2023), 21558‚Äì21572. [35] Shuo Liu, Jacky Keung, Zhen Yang, Fang Liu, Qilin Zhou, and Yihan Liao. 2024. Delving into parameter-efficient fine-tuning in code change learning: An empirical study. In 2024 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER) . IEEE, 465‚Äì476. [36] Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn, Li Li, Xuan-Bach D Le, and David Lo. 2024. Refining chatgpt-generated code: Characterizing and mitigating code quality issues. ACM Transactions on Software Engineering and Methodology 33, 5 (2024), 1‚Äì26. Manuscript submitted to ACM Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 29 

[37] Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, and Chun Zuo. 2023. Llama-reviewer: Advancing code review automation with large language models through parameter-efficient fine-tuning. In 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE) . IEEE, 647‚Äì658. [38] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021). [39] Antonio Mastropaolo, Matteo Ciniselli, Massimiliano Di Penta, and Gabriele Bavota. 2024. Evaluating code summarization techniques: A new metric and an empirical characterization. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering . 1‚Äì13. [40] Antonio Mastropaolo, Nathan Cooper, David Nader Palacio, Simone Scalabrino, Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2022. Using transfer learning for code-related tasks. IEEE Transactions on Software Engineering 49, 4 (2022), 1580‚Äì1598. [41] Antonio Mastropaolo, Luca Pascarella, Emanuela Guglielmi, Matteo Ciniselli, Simone Scalabrino, Rocco Oliveto, and Gabriele Bavota. 2023. On the robustness of code generation techniques: An empirical study on github copilot. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE) . IEEE, 2149‚Äì2160. [42] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio, Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the usage of text-to-text transfer transformer to support code-related tasks. In 2021 IEEE/ACM 43rd International Conference on Software Engineering . IEEE, 336‚Äì347. [43] Microsoft .NET Team. 2025. The .NET Compiler Platform (Roslyn). https://github.com/dotnet/roslyn. Open-source compiler and analysis platform for C# and Visual Basic. Accessed: 2025-10-12. [44] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 (2022). [45] OpenAI. 2025. Introducing GPT-5. https://openai.com/index/introducing-gpt-5/. Accessed: 2025-10-12. [46] P. D. Team. 2025. PMD - Source Code Analyzer. https://pmd.github.io. Static code analysis tool for Java and other languages. [47] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics . 311‚Äì318. [48] Debalina Ghosh Paul, Hong Zhu, and Ian Bayley. 2024. Benchmarks and metrics for evaluations of code generation: A critical review. In 2024 IEEE International Conference on Artificial Intelligence Testing (AITest) . IEEE, 87‚Äì94. [49] Maja Popoviƒá. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the tenth workshop on statistical machine translation . 392‚Äì395. [50] Matt Post. 2018. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771 (2018). [51] Pylint Team. 2025. Pylint: Code Analysis for Python. https://www.pylint.org/. Version 3.3.8, released August 9, 2025. Initial release in 2001. Accessed: 2025-10-12. [52] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is ChatGPT a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476 (2023). [53] Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, et al. 2025. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. arXiv preprint arXiv:2501.01257 (2025). [54] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 1‚Äì67. [55] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020). [56] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023). [57] Iman Saberi, Fatemeh Fard, and Fuxiang Chen. 2024. Utilization of pre-trained language models for adapter-based knowledge transfer in software engineering. 

Empirical Software Engineering 29, 4 (2024), 94. [58] Ensheng Shi, Yanlin Wang, Hongyu Zhang, Lun Du, Shi Han, Dongmei Zhang, and Hongbin Sun. 2023. Towards efficient fine-tuning of pre-trained code models: An experimental study and beyond. In Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis . 39‚Äì51. [59] Jieke Shi, Zhou Yang, and David Lo. 2025. Efficient and Green Large Language Models for Software Engineering: Literature Review, Vision, and the Road Ahead. 

ACM Transactions on Software Engineering and Methodology 34, 5 (2025), 1‚Äì22. [60] Jiho Shin, Clark Tang, Tahmineh Mohati, Maleknaz Nayebi, Song Wang, and Hadi Hemmati. 2025. Prompt Engineering or Fine-Tuning: An Empirical Assessment of LLMs for Code. In 2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR) . IEEE, 490‚Äì502. [61] Mohammed Latif Siddiq, Lindsay Roney, Jiahao Zhang, and Joanna Cecilia Da Silva Santos. 2024. Quality assessment of chatgpt generated code and their use by developers. In Proceedings of the 21st international conference on mining software repositories . 152‚Äì156. [62] SonarSource. [n. d.]. SonarCloud. https://docs.sonarsource.com/sonarqube-cloud/. Accessed: 2025-10-18. [63] Chia-Yi Su and Collin McMillan. 2024. Distilled GPT for source code summarization. Automated Software Engineering 31, 1 (2024), 22. [64] Weisong Sun, Yun Miao, Yuekang Li, Hongyu Zhang, Chunrong Fang, Yi Liu, Gelei Deng, Yang Liu, and Zhenyu Chen. 2024. Source code summarization in the era of large language models. arXiv preprint arXiv:2407.07959 (2024). [65] Florian Tambon, Arghavan Moradi-Dakhel, Amin Nikanjam, Foutse Khomh, Michel C Desmarais, and Giuliano Antoniol. 2025. Bugs in large language models generated code: An empirical study. Empirical Software Engineering 30, 3 (2025), 65. [66] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [67] Lewis Tunstall, Nathan Lambert, Nazneen Rajani, Edward Beeching, Teven Le Scao, Leandro von Werra, Sheon Han, Philipp Schmid, and Alexander Rush. 2023. Creating a Coding Assistant with StarCoder. Hugging Face Blog (2023). https://huggingface.co/blog/starchat-alpha. [68] Antonio Vitale, Antonio Mastropaolo, Rocco Oliveto, Massimiliano Di Penta, and Simone Scalabrino. 2025. Optimizing Datasets for Code Summarization: Is Code-Comment Coherence Enough? arXiv preprint arXiv:2502.07611 (2025). [69] Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, and Antonio Mastropaolo. 2025. Toward Explaining Large Language Models in Software Engineering Tasks. arXiv preprint arXiv:2512.20328 (2025). [70] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, and Michael R Lyu. 2022. No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence. In Proceedings of the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering . 382‚Äì394. [71] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, and Michael R Lyu. 2023. Prompt tuning in code intelligence: An experimental evaluation. IEEE Transactions on Software Engineering 49, 11 (2023), 4869‚Äì4885. Manuscript submitted to ACM 30 Haque, Afrin and Mastropaolo 

[72] Deze Wang, Boxing Chen, Shanshan Li, Wei Luo, Shaoliang Peng, Wei Dong, and Xiangke Liao. 2023. One adapter for all programming languages? adapter tuning for code search and summarization. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE) . IEEE, 5‚Äì16. [73] Kaixin Wang, Tianlin Li, Xiaoyu Zhang, Chong Wang, Weisong Sun, Yang Liu, and Bin Shi. 2025. Software Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents. arXiv preprint arXiv:2505.05283 (2025). [74] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. 2022. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. arXiv preprint arXiv:2205.12410 (2022). [75] Yanlin Wang, Tianyue Jiang, Mingwei Liu, Jiachi Chen, Mingzhi Mao, Xilin Liu, Yuchi Ma, and Zibin Zheng. 2025. Beyond functional correctness: Investigating coding style inconsistencies in large language models. Proceedings of the ACM on Software Engineering 2, FSE (2025), 690‚Äì712. [76] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021). [77] Cody Watson, Nathan Cooper, David Nader Palacio, Kevin Moran, and Denys Poshyvanyk. 2022. A systematic literature review on the use of deep learning in software engineering research. ACM Transactions on Software Engineering and Methodology (TOSEM) 31, 2 (2022), 1‚Äì58. [78] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022). [79] Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, and Houari Sahraoui. 2023. Exploring parameter-efficient fine-tuning techniques for code generation with large language models. arXiv preprint arXiv:2308.10462 (2023). [80] Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, and Houari Sahraoui. 2025. Exploring parameter-efficient fine-tuning techniques for code generation with large language models. ACM Transactions on Software Engineering and Methodology 34, 7 (2025), 1‚Äì25. [81] Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics bulletin 1, 6 (1945), 80‚Äì83. [82] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming . 1‚Äì10. [83] Boyang Yang, Haoye Tian, Jiadong Ren, Hongyu Zhang, Jacques Klein, Tegawend√© F Bissyand√©, Claire Le Goues, and Shunfu Jin. 2024. Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs. arXiv preprint arXiv:2404.12636 (2024). [84] Terry Yin. 2019. Lizard: an extensible cyclomatic complexity analyzer. Astrophysics Source Code Library (2019), ascl‚Äì1906. [85] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie. 2024. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering . 1‚Äì12. [86] Chunyan Zhang, Junchao Wang, Qinglei Zhou, Ting Xu, Ke Tang, Hairen Gui, and Fudong Liu. 2022. A survey of automatic source code summarization. Symmetry 

14, 3 (2022), 471. [87] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020. Retrieval-based neural source code summarization. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering . 1385‚Äì1397. [88] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019). [89] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. 2024. DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. arXiv preprint arXiv:2406.11931 (2024). [90] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877 (2024). [91] Andrei Zlotchevski, Dawn Drain, Alexey Svyatkovskiy, Colin B Clement, Neel Sundaresan, and Michele Tufano. 2022. Exploring and evaluating personalized models for code generation. In Proceedings of the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering .1500‚Äì1508. 

A LLM-as-a-Judge Quality Assessment Visualizations 

This appendix presents the complete distribution of GPT-5 mini quality scores for code summarization across all experimental configurations. Fig. 5 and Fig. 6 display boxplots for three quality dimensions ‚Äì Content Adequacy, Conciseness, and Fluency ‚Äì comparing Human-written summaries against all training configurations. 

Manuscript submitted to ACM Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks 31 

(a) Content Adequacy 

(b) Conciseness 

(c) Fluency Fig. 5. Boxplots for Java code summarization across three quality dimensions. Distribution compares Human-written summaries against models at three scales (0.5B, 1.5B, 3B) trained with Full Fine-Tuning (FFT, solid) or QLoRA (faded) in SingleTask and MultiTask configurations, including parameter-matched variants and task-pair combinations. Red diamonds indicate mean scores, blue lines indicate medians, background shading distinguishes model sizes. 

> Manuscript submitted to ACM

32 Haque, Afrin and Mastropaolo 

(a) Content Adequacy 

(b) Conciseness 

(c) Fluency Fig. 6. Boxplots for Python code summarization across three quality dimensions. Distribution compares Human-written summaries against models at three scales (0.5B, 1.5B, 3B) trained with Full Fine-Tuning (FFT, solid) or QLoRA (faded) in SingleTask and MultiTask configurations, including parameter-matched variants and task-pair combinations. Red diamonds indicate mean scores, blue lines indicate medians, background shading distinguishes model sizes.