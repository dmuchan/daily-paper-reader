# Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks
# 代码相关任务中的参数高效多任务微调

**Authors**: Md Zahidul Haque, Saima Afrin, Antonio Mastropaolo
**Date**: 2026-01-21
**PDF**: https://arxiv.org/pdf/2601.15094v1
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 7.0
**Evidence**: Discusses parameter-efficient fine-tuning (QLoRA) for Large Code Models across multiple tasks

---

## Abstract
Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis--an aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues.

## 摘要
大语言模型（LLMs）在自动化软件工程任务中

---

## 速览摘要（自动生成）

**问题**：全量微调大模型处理多项代码任务成本极高，且 Q