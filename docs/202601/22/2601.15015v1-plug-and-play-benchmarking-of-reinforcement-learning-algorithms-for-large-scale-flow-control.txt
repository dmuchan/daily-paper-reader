Title: Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control

URL Source: https://arxiv.org/pdf/2601.15015v1

Published Time: Thu, 22 Jan 2026 02:06:35 GMT

Number of Pages: 41

Markdown Content:
# Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

Jannis Becktepe 1 2 Aleksandra Franz 3 Nils Thuerey 3 Sebastian Peitz 1 2 

## Abstract 

Reinforcement learning (RL) has shown promis-ing results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observa-tion and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, Flu-idGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable founda-tion for future research in learning-based flow control, and is available at https://github. com/safe-autonomous-systems/ fluidgym .

## 1. Introduction 

Active flow control (AFC) plays a central role in a wide range of real-world systems, such as aerodynamics (Batikh et al., 2017), energy harvesting (Barthelmie et al., 2009), nuclear fusion (Pironti & Walker, 2005), and reduction of turbulence (Jim ´enez, 2013). Europe, for instance, could save more than 20 × 10 6 tonnes of CO 2 per year by reducing drag on cars using AFC (Brunton & Noack, 2015). 

> 1

TU Dortmund University, Dortmund, Germany 2Lamarr In-stitute for Machine Learning and Artificial Intelligence, Dort-mund, Germany 3Technical University Munich, Munich, Ger-many. Correspondence to: Jannis Becktepe <jannis.becktepe@tu-dortmund.de >.

Preprint. January 22, 2026. 

Figure 1. The four uncontrolled environment classes in FluidGym. 

However, manually designing control strategies is challeng-ing due to the high dimensionality and inherent nonlinear-ities of such systems (Duriez et al., 2017). Recently, rein-forcement learning (RL) has demonstrated strong potential for advancing AFC in complex systems, e.g., stabilizing the plasma in a Tokamak reactor (Degrave et al., 2022). Despite its success, research in RL for flow control remains fragmented, and establishing a clear state of the art is diffi-cult for several reasons. Experimental setups vary widely across studies in terms of actuators, sensor placements, and physical parameter settings as well as RL algorithms and hyperparameters (Viquerat et al., 2022; Moslem et al., 2025). This results in inconsistent problem formulations that hinder direct comparisons. Moreover, insufficiently rigorous eval-uation and the use of few random seeds increase statistical variance (Henderson et al., 2018; Agarwal et al., 2021). Existing benchmarks (see Table 1) have seen limited adop-tion for two main reasons. First, most rely on external computational fluid dynamics (CFD) solvers that must be in-stalled, configured, and coupled to Python RL code through additional interfaces, which demands CFD expertise and creates brittle software stacks. Second, differentiability is either absent or limited to a small subset of scenarios, which prevents end-to-end use of differentiable predictive con-trol (DPC, Drgo ˇna et al. (2022)) and recent differentiable RL methods that can accelerate training and outperform classical RL (Xing et al., 2025; Lagemann et al., 2025b). 1

> arXiv:2601.15015v1 [cs.LG] 21 Jan 2026 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control

To address these limitations, we introduce FluidGym, the first standalone, fully differentiable RL benchmark for AFC in incompressible flows. Building entirely on Py-Torch (Ansel et al., 2024), FluidGym requires no external solver dependencies and seamlessly integrates with com-mon RL interfaces such as Gymnasium (Towers et al., 2024) or PettingZoo (Terry et al., 2021) and algorithm frameworks like Stable-Baselines3 (Raffin et al., 2021) or TorchRL (Bou et al., 2023). As all simulations and control interfaces live in one Python package, users can install FluidGym via pip 

and immediately run experiments with standard RL libraries, without compiling or coupling external CFD codes. Be-ing inherently end-to-end differentiable, FluidGym enables researchers to use gradient-based control methods along-side classical RL without any further modifications. Our benchmark provides diverse environments with consistent task definitions, supports single-agent (SARL) and multi-agent (MARL) settings, spans three difficulty levels in 2D and 3D, and enables transfer-learning studies. In summary, our main contributions are (1) the first stan-dalone, fully differentiable, plug-and-play benchmark for RL in AFC, implemented in a single PyTorch codebase without external solver dependencies; (2) a collection of standardized environment configurations spanning diverse 3D and MARL control tasks (see Figure 1); (3) an extensive experimental study covering all FluidGym environments and difficulty levels, including transfer-learning evaluations, amounting to over 16 k GPU hours, all publicly available. 

## 2. Background and Related Work 

RL for AFC Fluid flows are governed by the Navier-Stokes equations, a set of nonlinear partial differential equa-tions (PDEs) exhibiting highly complex behavior over a wide range of scales both in space and time. Due to their inherent complexity, analytical solutions are infeasible with-out substantial simplifications. Computational fluid dynam-ics (CFD) has become a standard approach to approximate solutions using spatial and temporal discretization (Ferziger et al., 2020). Such simulations, however, are computation-ally expensive and typically require specialized solvers such as OpenFOAM (Weller et al., 1998), FEniCS (Alnæs et al., 2015), or FLEXI (Krais et al., 2021). In many applications, the goal is not only to simulate the flow but to manipulate it. Active flow control (AFC) uses actuation to influence fluid motion, e.g., to reduce aerody-namic drag (Nair et al., 2019). Classical AFC approaches have demonstrated notable successes, ranging from the re-laminarization of turbulent channel flows using adjoint-based model-predictive control (MPC) (Bewley et al., 2001) to control of the separation bubble behind a bluff body using evolutionary optimization strategies (Gautier et al., 2015). However, these methods often rely on simplified models or require full-state information and expensive online optimiza-tion, which limits their scalability to complex, nonlinear, or high-dimensional flow configurations. Reinforcement learning (see Appendix A for an introduction to the basics and notation) has therefore emerged as a compelling alter-native and has been explored across a variety of AFC prob-lems, including drag reduction in bluff-body wakes (Rabault et al., 2019; Tokarev et al., 2020), turbulent channel-flow control (Guastoni et al., 2023), and heat-transfer enhance-ment (Beintema et al., 2020; Vignon et al., 2023). Several works have also studied multi-agent reinforcement learning (MARL, (Albrecht et al., 2024)) for wall turbulence mod-eling (Bae & Koumoutsakos, 2022), heat transfer enhance-ment (Beintema et al., 2020; Vasanth et al., 2024; Vignon et al., 2023; Markmann et al., 2025), and proposed convo-lutional RL for distributed control (Peitz et al., 2024). To avoid the high computational cost of CFD simulations, RL has also been used together with surrogate models (Werner & Peitz, 2024; Zolman et al., 2025). However, the research area faces challenges similar to those observed more broadly in machine learning for PDEs (Mc-Greivy & Hakim, 2024). Evaluation practices vary widely. Many works compare learned policies only to uncontrolled baselines (Tokarev et al., 2020; Ren et al., 2021; Wang et al., 2022b; Vignon et al., 2023; Vasanth et al., 2024; Ren et al., 2024; Zhao et al., 2024; Su ´arez et al., 2025; Montal `aet al., 2025). RL episodes often start from the same initial state (Rabault et al., 2019; Vignon et al., 2023; Ren et al., 2024; Sonoda et al., 2023; Garcia et al., 2025), even though this choice can substantially affect the performance of poli-cies (Guastoni et al., 2023). In several cases, test episodes reuse the same initial conditions used for training (Vignon et al., 2023; Vasanth et al., 2024), making generalization hard to assess. Reproducibility and statistical robustness is also limited: some works report a single run without seeds (Guastoni et al., 2023; Sonoda et al., 2023), despite the fact that this is a known source of variance in RL (Hen-derson et al., 2018; Agarwal et al., 2021). Finally, although the soft actor critic (SAC, Haarnoja et al. (2018)) algo-rithm often outperforms proximal policy optimization (PPO, Schulman et al. (2017)) on nonlinear continuous-control tasks (Abuduweili & Liu, 2023), more than 75% of AFC studies rely on PPO as surveyed by Moslem et al. (2025). 

RL for AFC Benchmarks Benchmark design is key to addressing the evaluation and reproducibility challenges outlined above. Several RL benchmarks for AFC exist, and their characteristics are stated in Table 1. However, existing efforts cover only parts of the AFC landscape and leave important gaps in accessibility, differentiability, RL methodologies, and dimensionality. 2Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control                                        

> Table 1. Overview of existing RL for AFC benchmarks in terms of external solver dependence, differentiability of all environments, multi-agent RL support, and 3D capabilities.
> BENCHMARK NOEXTERNAL SOLVER FULLY DIFFERENTIABLE MARL 3D
> DRL IN FLUIDS (W ANG ET AL ., 2022 A)××××
> DRLFOAM (W EINER & G EISE , 2022) ××××
> DRLF LUENT (M AO ET AL ., 2023) ××××
> GYM -PRE CICE (S HAMS & E LSHEIKH , 2023) ××××
> HYDRO GYM (L AGEMANN ET AL ., 2025 B)××✓✓
> FLUID GYM (O URS )✓✓✓✓

General PDE control benchmarks, such as those proposed by Bhan et al. (2024); Zhang et al. (2024); Mouchamps et al. (2025), focus on low-dimensional or non-fluid systems and do not address the complexities of high-dimensional fluid flows. Several frameworks have attempted to bridge the gap between CFD solvers and RL algorithms (Pawar & Maulik, 2021; Kurz et al., 2022; Xiao et al., 2025). However, they introduce additional software layers for the coupling rather than standardized benchmark environments. DRLinFluids (Wang et al., 2022a) and drlFoam (Weiner & Geise, 2022) interface with OpenFOAM but are limited to 2D cases (e.g., flow past a cylinder or fluidic pinball), while DRLFluent (Mao et al., 2023) couples RL with the commercial solver Fluent (ANSYS Inc., 2026), again fo-cusing on 2D cylinder flows. Gym-preCICE (Shams & Elsheikh, 2023) uses the preCICE coupling library (Chour-dakis et al., 2022) and includes a 2D flow past a cylinder. HydroGym (Lagemann et al., 2025b;a) provides a collection of 2D and 3D flow scenarios, with individual environments depending on different solver backends: FEniCS for 2D simulations, and m-AIA (Institute of Aerodynamics, 2024) for 3D simulations. Only the two environments based on JAX (Bradbury et al., 2018) are differentiable. 

Limitations of Existing Benchmarks Existing AFC benchmarks share several limitations (see Table 1): (i) they typically depend on external CFD solvers (e.g., OpenFOAM, Fluent, FEniCS, m-AIA), which require complex and of-ten brittle software pipelines and indirect coupling layers that hinder integration with Python RL libraries and com-plicate long-term maintenance; (ii) lack of differentiabil-ity, despite its potential for accelerating RL training (Xu et al., 2022; Xing et al., 2025; Lagemann et al., 2025b) and in DPC (Drgo ˇna et al., 2022), (iii) limited support for multi-agent RL, despite its natural alignment with spatially distributed actuation; and (iv) predominantly 2D environ-ments, which fail to capture essential 3D flow physics. To our knowledge, no existing benchmark simultaneously pro-vides a standalone implementation, uniform differentiability across all tasks, native multi-agent support, and high-fidelity 3D environments. 

## 3. FluidGym: Overview 

Motivated by the limitations of existing work on RL for AFC and related benchmarks, FluidGym is designed around the following desiderata: (i) a standardized, standalone, and easy-to-use RL–CFD interface that runs entirely in Python without external CFD software, (ii) an end-to-end differen-tiable framework suitable for various control methodologies, (iii) inherent support of multi-agent control, and (iv) high--fidelity 3D tasks. In the following, we outline the core design principles underlying our benchmark and describe how FluidGym fulfills these desiderata. 

3.1. Architecture and Interaction Interface 

Figure 2 summarizes the architecture of FluidGym, which unifies CFD simulation and control under a single, RL-centric interface. To meet desiderata (i) and (ii), FluidGym integrates the GPU-accelerated PICT solver (Franz et al. (2026); see Appendix B) with a modular PyTorch (Ansel et al., 2024) interaction layer. Because the design runs en-tirely in PyTorch, environment stepping and backprop use the same autograd mechanisms as standard deep networks. Consequently, no external CFD software or coupling code is required, and environments are compatible with common RL libraries through a lightweight API. The FluidEnv 

abstraction encapsulates all CFD computations and exposes standardized observation, action, and reward interfaces for both differentiable and classical RL methods. Finally, Flu-idGym scales to large experimental workloads via parallel execution of environments across multiple GPUs. Addressing desideratum (iii), the FluidEnv is imple-mented from the ground up with both single-agent and multi-agent RL in mind. Its interface provides standardized observation, action, and reward specifications for central-ized or decentralized control. All environments are modular, enabling new tasks to be defined by specifying domain con-figuration and control logic. This design makes FluidGym an extensible platform for future research on RL for AFC. Finally, addressing desideratum (iv), our environments built on top of FluidEnv focus on state-of-the-art, high-fidelity 3D flow simulations (see Section 3.2). 3Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control    

> Figure 2. Overview of FluidGym using the 2D Rayleigh–B ´enard Convection (RBC) environment. The framework provides three modes of interaction: single-agent RL (SARL), multi-agent RL (MARL), and gradient-based methods. The action space consists of 12 heater actuators along the lower boundary. In SARL, a single agent outputs the full action vector, whereas in MARL, each agent controls one actuator via a local action. Local actions are internally aggregated and mapped to boundary actuation values via the transformation function Γ. Gray dots indicate virtual sensor locations: in SARL, the agent receives all measurements, while in MARL, each agent observes only the local subset around its assigned actuator (denoted by the window framed in purple).

Modes of Interaction FluidGym supports three modes of interaction through its environment interfaces, which expose the FluidEnv via common RL environment interfaces, in-cluding Gymnasium (Towers et al., 2024), PettingZoo (Terry et al., 2021), Stable-Baselines3 (SB3, Raffin et al. (2021)), and TorchRL (Bou et al., 2023). First, in the single-agent RL (SARL) setting, a single RL agent applies a global action ⃗a t at each control step t and the environment returns a global observation ⃗o t+1 and scalar reward rt+1 . Secondly, in the multi-agent RL (MARL) configuration, multiple agents act simultaneously at different spatial locations in the domain. Each agent selects a local action ⃗a it and receives a local ob-servation ⃗o it and individual reward ⃗r it . Reward functions in these settings are typically constructed of a weighted sum of local and global properties of the domain. This interaction mode enables decentralized cooperation control strategies, where equivariance to translations allows us to deploy the same agent in all locations (Vasanth et al., 2024; Peitz et al., 2024). Lastly, in addition to standard RL, FluidGym sup-ports gradient-based control methods by providing end-to-end differentiability of the step() function with respect to the reward. This allows gradients to be backpropagated through FluidGym to the policy parameters. 

Example: 2D Rayleigh–B ´enard Convection Possible interaction modes are visualized in Figure 2 using the 2D Rayleigh-B ´enard Convection (RBC) environment as an ex-ample. Here, the action space consists of 12 scalar control inputs corresponding to heater elements placed along the lower boundary of the domain. In the SARL scenario, a single agent outputs the complete action vector, assigning temperature intensities to all actuators. In contrast, in the MARL configuration, each agent controls one individual actor via its local action. Internally, the environment in-terface aggregates local actions into a global action vector. The resulting action vector is then transformed into physi-cally meaningful boundary condition values via the control mapping function Γ, in this case normalization and spatial smoothing. Observations are constructed from virtual sensor measurements indicated by the gray dots in the figure. 

Training and Evaluation Protocol Many prior works lack standardized training and evaluation procedures for RL in AFC, with studies differing widely in how many and which initial conditions they use. FluidGym addresses this by providing a unified protocol based on three prede-fined splits ( train , val , and test ) each containing ten randomly generated initial domains. On first use, initial domains are automatically downloaded and cached locally. Each env.reset() applies random perturbations and ran-dom rollout steps; with consistent RNG seeding, this creates a standardized and reproducible train/val/test protocol. 

3.2. Benchmark Environments 

FluidGym provides a diverse set of environments, each in-troducing distinct challenges for learning well-performing RL policies. Formal SARL and MARL environment defini-4Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

Table 2. Overview of the FluidGym environments, listing control objectives, observation and action dimensions, SARL/MARL support, and mean per-step runtime across all difficulty levels on a single NVIDIA A100 GPU. SARL is omitted for environments with very large action spaces, where centralized control becomes impractical. For more details, see Table 4 in Appendix C and Table 7 in Appendix E. 

ID P REFIX OBJECTIVE #S ENSORS #A CTORS SARL MARL RUNTIME 

[SEC /STEP ]

CY L I N D E R RO T 2D 

DRAG REDUCTION 

302 1 ✓ × 1.95 

CY L I N D E R JE T 2D 302 1 ✓ × 2.01 

CY L I N D E R JE T 3D 4832 8 ✓ ✓ 9.52 

RBC2D 

HEAT TRANSFER ENHANCEMENT 

768 12 ✓ ✓ 1.92 

RBC2D-W I D E 1 536 24 ✓ ✓ 1.99 

RBC3D 221 184 64 × ✓ 1.17 

RBC3D-W I D E 884 736 256 × ✓ 1.71 

AI R F O I L 2D AERODYNAMIC EFFICIENCY ENHANCEMENT 

418 3 ✓ × 28 .76 

AI R F O I L 3D 2508 12 ✓ ✓ 52 .89 

TCFS M A L L 3D-B O T H 

DRAG REDUCTION 

1 024 1 024 × ✓ 0.33 

TCFS M A L L 3D-B O T T O M 512 512 × ✓ 0.29 

TCFL A R G E 3D-B O T H 4 096 4 096 × ✓ 0.56 

TCFL A R G E 3D-B O T T O M 2 048 2 048 × ✓ 0.52 

tions are stated in Appendix A. Each environment is offered in three difficulty levels to introduce increasing levels of turbulence and flow complexity. An overview of the environ-ments is shown in Figure 1 and summarized in Table 2. In the following, we outline four key flow scenarios, building the foundation of the 13 FluidGym environments. 

Flow Past a Cylinder The von K ´arm ´an vortex street is a canonical setup in which flow separation behind a cylinder induces periodic vortex shedding and fluctuating forces on the cylinder (Sch ¨afer et al., 1996). This configuration has consistently served as a benchmark for AFC using RL to reduce the drag acting on the cylinder (Koizumi et al., 2018; Rabault et al., 2019; Xu et al., 2020; Tang et al., 2020; Ren et al., 2021; Han et al., 2022; Su ´arez et al., 2025). The system is parametrized via the Reynolds number Re = U D ν

with mean incoming velocity U , cylinder diameter D, and kinematic viscosity ν. The objective is to reduce the drag coefficient CD while keeping the lift CL small, using the reward rt = CD, ref − ⟨ CD ⟩Tact − w⟨| CL|⟩ Tact , with lift regularization weight w ≥ 0 and ⟨·⟩ Tact referring to averag-ing over the actuation interval and reference uncontrolled drag coefficient CD, ref . We note that normalization with uncontrolled reference metrics is not essential in principle, but is used consistently across the benchmark. Actuation uses either (i) opposing synthetic jets on the top and bottom surfaces of the cylinder, or (ii) cylinder rotation. Difficulty levels, defined via Re , span different flow regimes in 2D/3D. 

Rayleigh-B ´enard Convection The Rayleigh-B ´enard Con-vection (RBC, B ´enard (1900); Rayleigh (1916)) models a buoyancy-driven flow between a heated bottom plate and a cooled top plate. This leads to convective fluid motion and the formation of thermal plumes with complex, potentially chaotic patterns (Pandey et al., 2018). The system is defined by two dimensionless parameters, the Prandtl number Pr 

and the Rayleigh number Ra . Pr is a material property of the fluid, while Ra controls the intensity of buoyancy-driven convection. Our setup follows Vignon et al. (2023), extended to 3D as in Vasanth et al. (2024), with the domain height reduced from 2 to 1 to match the standard dimension-less configuration (Pandey et al., 2018). The task aims to reduce convective heat transfer by minimizing the instanta-neous Nusselt number Nu instant = √RaPr ⟨uy T ⟩V , where 

uy denotes the vertical fluid velocity, T the temperature field, and ⟨·⟩ V a volume average (Pandey et al., 2018), resulting in the reward rt = Nu ref − Nu instant . Control is applied via bottom-boundary heaters whose temperatures are nor-malized, clipped, and spatially smoothed. The environment difficulty is varied by adjusting the Rayleigh number Ra ,with higher values in both 2D and 3D resulting in more turbulent convection. An additional wide-domain variant with aspect ratio 2π introduces richer spatial patterns. 

Flow Past an Airfoil The flow around an airfoil is a fun-damental configuration in aerodynamics and a common benchmark for AFC (Wang et al., 2022b; Garcia et al., 2025; Liu et al., 2025; Montal `a et al., 2025). Variations in Reynolds number and angle of attack influence flow sep-aration and vortex dynamics. Control aims to improve aero-dynamic efficiency by increasing the lift to drag ratio, i.e., 

rt = ⟨CL⟩Tact /⟨CD ⟩Tact −CL, ref /C D, ref . Actuation is pro-vided by zero net-mass-flux synthetic jet actuators mounted on the airfoil surface. Task difficulty is set by the Reynolds number, with higher values producing sharper separation and stronger turbulence. In this work, we only consider the easy 3D difficulty level due to the high computational cost. 5Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control  

> Figure 3. Final 3D flow fields at the end of test episodes for uncon-trolled and controlled cases across four FluidGym environments using PPO, SAC, or multi-agent variants. Transfer cases use poli-cies trained on corresponding 2D or smaller domains.

Turbulent Channel Flow The turbulent channel flow (TCF, the flow between two parallel, infinitely large plates) is a classic experiment for studying wall-bounded turbulence. Most AFC strategies aim to reduce the wall shear stress by imposing wall normal velocities (blowing or suction) via spatially distributed actuators at the walls (Bewley et al., 2001; Stroh et al., 2015; Guastoni et al., 2023; Sonoda et al., 2023; Zhao et al., 2025). The objective is captured through a reward based on the instantaneous reduction of shear stress τwall relative to the uncontrolled reference τwall ,ref , i.e., rt = 1 − τwall /τ wall ,ref . FluidGym provides both a small and a large channel variant, enabling evaluation under different spatial scales. Additionally, FluidGym provides a pre-computed opposition control baseline for this environment consistent with previous work (Guastoni et al., 2023). 

## 4. Experiments 

In the following, we present a comprehensive evaluation of FluidGym. All experimental results and trained models are publicly available at https://huggingface. co/datasets/safe-autonomous-systems/ fluidgym-experiments .

4.1. Experimental Setup 

In our experiments, we evaluate Proximal Policy Opti-mization (PPO, Schulman et al. (2017)) and Soft Ac-tor–Critic (SAC, Haarnoja et al. (2018)) using their Stable-Baselines3 (SB3, Raffin et al. (2021)) implementations, de-noted as MA-PPO and MA-SAC in the MARL setting. To enable the first large-scale evaluation of these algorithms on AFC, we use default SB3 hyperparameters (see Ap-pendix D). We conduct all experiments using five random seeds, with the exception of the 3D Airfoil and Cylinder environments, which are evaluated on three seeds due to computational constraints. Additionally, to study the utility of differentiable benchmarks, we evaluate a differentiable model predictive controller (D-MPC; see Appendix D), demonstrated on the CylinderJet2D environment. For each run, we collect ten evaluation episodes on the test set. We report mean reward per step rather than cumulative return to avoid confounding effects from episode length. Since episode lengths are constant within each environment, this choice does not affect relative or normalized metrics. 

4.2. Overall Benchmark Performance 

Before presenting quantitative results, we first show exem-plary final flow fields from controlled test set rollouts to illustrate the resulting flow states. Figure 3 displays four 3D environments with their uncontrolled and controlled cases at the end of test episodes, including transferred policies. Then, to assess the overall performance of RL algorithms on FluidGym, we consider their respective performance profiles following Agarwal et al. (2021), which depict the tail distribution of normalized rewards aggregated across all environments and random seeds. Figure 4 (left) shows the profiles of PPO, SAC, and their respective multi-agent variants. Notably, the performance profiles of PPO and SAC vary substantially. We attribute this to a slower overall learn-ing and convergence behavior (see Appendix E for detailed results). For the multi-agent variants, we observe similar performance profiles for both algorithms. MA-SAC exhibits marginally higher scores overall, though the differences partially lie within the associated confidence intervals. Inspecting performance across environment categories and difficulty levels (Figure 4, right) shows a consistent pat-tern: SAC achieves the highest normalized test set relative improvement over the baseflow across all levels, while MA-PPO performs slightly better on the TCF environments. Overall, two trends emerge: (i) SAC reliably outperforms PPO across all difficulty levels, while the multi-agent vari-ants are more comparable, likely because PPO benefits from increased sample counts, which reduces SAC’s usual sam-ple-efficiency advantage; and (ii) environments with similar flow structures (e.g., cylinder and airfoil) yield similar learn-ing dynamics and performance, despite differing reward definitions. These observations highlight the importance of algorithmic robustness and sample efficiency when scaling RL to turbulent AFC tasks. 

4.3. Results for Individual Environments 

Next, we discuss an individual test set episode using the final policies for CylinderJet2D-easy-v0 . Figure 5 shows the temporal evolution of applied actions at and the resulting drag coefficients CD . Both RL policies rapidly attenuate oscillations and reduce drag relative to the uncon-6Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 0.0 0.2 0.4 0.6 0.8 1.0      

> Min-Max Normalized Score τ(↑)
> 0.00
> 0.25
> 0.50
> 0.75
> Fraction of runs with score  > τ  FluidGym Score Distributions
> Airfoil
> Cylinder
> RBC
> TCF
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> easy
> medium hard
> 0.2
> 0.4
> 0.6
> 0.8
> PPO SAC MA-PPO MA-SAC

Figure 4. Left: Performance profiles as proposed by Agarwal et al. (2021) summarizing scores over all FluidGym environments. Error bars indicate pointwise 95% confidence intervals based on 2 k stratified bootstrap replications across random seeds. Right: Interquartile mean (IQM) scores over environment classes (middle) and difficulty levels (right). For all panels, scores are computed as min–max normalized relative improvements over the baseflow, with normalization performed independently for each environment–difficulty pair. 0 50  

> Episode Step t
> −0.5
> 0.0
> 0.5
> at

Control Action   

> 050
> Episode Step t
> 3.1
> 3.2
> 3.3
> CD

Drag Coefficient    

> Baseflow PPO SAC D-MPC

Figure 5. Time evolution of the control action at and drag co-efficient CD for the uncontrolled baseflow, the final PPO and SAC policies, and the differentiable model predictive control (D-MPC, see Algorithm 1 in Appendix D) controller evaluated on the 

CylinderJet2D-easy-v0 test environment. 

trolled baseflow, with SAC achieving the lowest final CD

corresponding to a drag reduction of approximately 8% , and PPO in agreement with findings by Rabault et al. (2019). In addition to the classical RL policies, we evaluate D-MPC, which selects actions exclusively by ascending the reward gradient through the differentiable simulation. Its observed drag reduction indicates that reward gradients provide effec-tive control signals for AFC and underscores the value of FluidGym as the first fully differentiable AFC benchmark. Beyond single-agent cylinder control, FluidGym also en-ables studying multi-agent AFC tasks. Figure 6 shows a test episode on RBC3D-easy-v0 using MA-PPO, where agents coordinate bottom-wall heating to form two stable convection rolls. Notably, when investigating the actuation, we observe emerging coordinated behavior between the in-dividual agents, leading to two separate convection rolls. These spatial heating patterns are consistent with the find-ings of Vasanth et al. (2024) and suggest that RL can learn a spatially invariant control policy forming globally coordi-0 50 100 150 200  

> Episode Step t
> 1.8
> 2.0
> 2.2Nu instant

Test Episode   

> 0 1 2 3 4 5 6 7 Heater xIndex 01234567Heater  z Index

Actuation at t = 175    

> 01
> Temperature
> Baseflow MA-PPO t= 175

Figure 6. Time evolution of the Nusselt number Nu instant for the baseflow and MA-PPO policy (left) and bottom-plate actuation at 

T = 175 (right) on the RBC3D-easy-v0 test environment. 

nated behavior. This highlights the potential of MARL for AFC, a key capability of FluidGym. 

4.4. Policy Transfer Across Environment Variations 

We further evaluate policy transfer in FluidGym, consid-ering (i) dimensionality transfer for the cylinder flow and (ii) domain-size transfer for the TCF. 

Transfer across Dimensionalities We investigate how policies trained in 2D transfer to their 3D counterparts using the cylinder environment. Figure 7 shows the mean test set drag reduction for three approaches: 3D SARL and MARL trained in 3D, and a transferred 2D → 3D policy applied to the eight actuators in 3D individually. On the easy task, the transferred policy outperforms the 3D-trained baselines. On medium difficulty, it is on par, slightly below PPO and MA-SAC. On the hard task, it again achieves the highest drag reduction. These findings indicate that direct transfer from 2D to 3D can be robust despite the added complexity. 7Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control easy medium hard Difficulty                  

> 0
> 10
> Drag Reduction (%)  PPO
> easy medium hard Difficulty
> SAC
> SARL 3D MARL 3D SARL 2D →MARL 3D
> Figure 7. CylinderJet3D : Drag reduction across difficulty levels for PPO and SAC comparing SARL 3D, MARL 3D, and transferred SARL 2D →MARL 3D with 95% confidence intervals. 0500 1000
> Episode Step t
> 0
> 10
> 20
> 30
> Drag Reduction (%)  Small Channel
> 0500 1000
> Episode Step t
> Large Channel
> Opp. Control MA-PPO (S) MA-SAC (S) MA-PPO (L) MA-SAC (L)
> Figure 8. TCF: Mean test-episode drag reduction with 95% confi-dence intervals for opposition control (Opp. Control) as well as policies trained on the small (S) and large (L) channel, respectively.

Transfer across Domain Sizes Finally, we study whether policies trained in smaller TCF domains transfer to larger ones. This setting is motivated by two factors: (i) lower simulation cost in smaller domains, and (ii) MARL may yield control policies that are translation-equivariant and thus insensitive to the absolute domain size. Figure 8 shows mean test-episode drag reduction in the large domain for policies trained either on the small channel (S) or directly on the large channel (L), together with an opposition control baseline. Notably, policies trained in the small domain perform comparably to opposition control and substantially outperform those trained directly in the large domain. This suggests that MARL can learn spatially transferable control strategies that generalize across domain scales. 

## 5. Limitations and Future Work 

While FluidGym provides a unified and extensible platform for studying RL for AFC, limitations remain. First, the current evaluation is based on a limited number of random seeds due to the substantial computational cost associated with CFD simulations. As a result, the statistical robustness of the results is still limited when it comes to comparisons between algorithms. Second, FluidGym currently requires a CUDA-enabled GPU for fast simulation, as the underlying solver depends on custom CUDA kernels. Although installa-tion is simplified through pre-built wheels, CPU-only execu-tion is not yet supported. Third, despite full differentiability of FluidGym, we focus on model-free RL and only demon-strate D-MPC leveraging reward gradients as a proof of concept. Systematic comparisons with other differentiable control approaches are not included. Finally, baseline algo-rithms are evaluated using standard hyperparameters from off-the-shelf libraries, which promotes comparability but may not reflect each algorithm’s optimal performance. Over-all, these limitations stem from computational and practical considerations rather than inherent constraints of FluidGym. Several directions offer potential for extending FluidGym and broadening its utility and scope. First, increasing the number of random seeds used during training and evalua-tion will improve the statistical robustness of the reported baseline results. Additionally, evaluating gradient-based methods, e.g., DPC (Drgo ˇna et al., 2022) and differentiable RL (Xu et al., 2022; Xing et al., 2025), where the latter combines gradient-based control with classical RL, is a natural next step. Expanding the set of environments to cover additional geometries and physical regimes would provide a more comprehensive assessment of control strate-gies across diverse flow configurations. Beyond incompress-ible Navier–Stokes, we also plan to extend FluidGym to magnetohydrodynamic (MHD) flows, enabling the study of control in electrically conducting fluids (e.g., in fusion-relevant settings). Finally, we intend to add progressively more challenging environments as control methods advance to keep the benchmark aligned with the state of the art. 

## 6. Conclusion 

In this work, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for reinforcement learn-ing in active flow control. By combining a GPU-accelerated CFD solver with a standardized RL interface, FluidGym removes the dependency on external CFD code and pro-vides a unified, accessible, and reproducible platform that bridges RL research and fluid dynamics. Our benchmark suite provides diverse 2D and 3D environments with consis-tent observation, actuation, and reward definitions, unified evaluation protocols, and support for single- and multi-agent RL as well as gradient-based methods. PPO and SAC baselines align with prior findings and show FluidGym’s suitability for RL and gradient-based control, with D-MPC demonstrating the effectiveness of leveraging reward gradients for AFC. By releasing all environments and trained models, we aim to lower the barrier to entry for researchers and foster reproducibility and comparability. 8Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

## Acknowledgements 

JB and SP acknowledge funding from the European Re-search Council (ERC Starting Grant “KoOpeRaDE”) under the European Union’s Horizon 2020 research and innova-tion programme (Grant agreement No. 101161457). The computations were performed on the compute cluster of the Lamarr Institute for Machine Learning and Artificial Intelligence, as well as on the high-performance computer “Noctua 2” at the NHR Center Paderborn Center for Parallel Computing (PC2), both of which are funded by the Federal Ministry of Research, Technology and Space and by the state of Northrhine-Westfalia. 

## Impact Statement 

In this work, we introduce a benchmark suite for reinforce-ment learning in active flow control with the goal of improv-ing algorithms and policies for controlling fluid systems. Potential positive societal impacts include more energy-efficient transport and industrial processes, emission reduc-tion, energy harvesting, and improved study of fluid flows. At the same time, deploying learning-based controllers in safety-critical settings without rigorous validation could pose risks. The environments in FluidGym are idealized and do not capture the full complexity, including uncertainties and constraints of real systems. Training reinforcement learning algorithms on high-fidelity simulations can also be computationally expensive and energy-consuming, which motivates future work on more sample-efficient algorithms. Overall, this benchmark is a research tool to advance control methods for fluid systems, and we do not foresee direct societal harms associated with its use. 

## References 

Abuduweili, A. and Liu, C. An optical control envi-ronment for benchmarking reinforcement learning algo-rithms. Transactions on Machine Learning Research ,2023. Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice. In Advances in Neural In-formation Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , 2021. Albrecht, S. V., Christianos, F., and Sch ¨afer, L. Multi-agent reinforcement learning: Foundations and modern approaches . MIT Press, 2024. Alnæs, M., Blechta, J., Hake, J., Johansson, A., Kehlet, B., Logg, A., Richardson, C., Ring, J., Rognes, M. E., and Wells, G. N. The FEniCS project version 1.5. Archive of Numerical Software , 2015. Publisher: University Library Heidelberg Version Number: 1.0.0. Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voz-nesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., and Chin-tala, S. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS ’24) . ACM, 2024. ANSYS Inc. ANSYS Fluent, 2026. URL 

https://www.ansys.com/products/ fluids/ansys-fluent .Bae, H. J. and Koumoutsakos, P. Scientific multi-agent reinforcement learning for wall-models of turbulent flows. 

Nature Communications , 13, 2022. Barthelmie, R. J., Hansen, K., Frandsen, S. T., Rathmann, O., Schepers, J. G., Schlez, W., Phillips, J., Rados, K., Zervos, A., Politis, E. S., and Chaviaropoulos, P. K. Mod-elling and measuring flow and wind turbine wakes in large wind farms offshore. Wind Energy , 12, 2009. Batikh, A., Baldas, L., and Colin, S. Application of active flow control in aircrafts – State of the art. In Proceed-ings of the International Workshop on Aircraft System Technologies, Hamburg, Germany , 2017. Beintema, G., Corbetta, A., Biferale, L., and Toschi, F. Con-trolling Rayleigh–B ´enard convection via reinforcement learning. Journal of Turbulence , 21, 2020. Bewley, T. R., Moin, P., and Temam, R. DNS-based pre-dictive control of turbulence: an optimal benchmark for feedback algorithms. Journal of Fluid Mechanics , 447, 2001. Bhan, L., Bian, Y., Krstic, M., and Shi, Y. PDE control gym: A benchmark for data-driven boundary control of partial differential equations. In 6th Annual Learning for Dynamics & Control Conference, 15-17 July 2024, University of Oxford, Oxford, UK , volume 242. PMLR, 2024. Bou, A., Bettini, M., Dittert, S., Kumar, V., Sodhani, S., Yang, X., Fabritiis, G. D., and Moens, V. TorchRL: A data-driven decision-making library for PyTorch. arXiv preprint arXiv:2306.00577 , 2023. 9Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL 

http://github.com/jax-ml/jax .Brunton, S. L. and Noack, B. R. Closed-loop turbulence control: Progress and challenges. Applied Mechanics Reviews , 67, 2015. B ´enard, H. Les tourbillons cellulaires dans une nappe liq-uide. Revue G ´en ´erale des Sciences Pures et Appliqu ´ees ,11, 1900. Chourdakis, G., Davis, K., Rodenberg, B., Schulte, M., Si-monis, F., Uekermann, B., Abrams, G., Bungartz, H.-J., Cheung Yau, L., Desai, I., Eder, K., Hertrich, R., Lindner, F., Rusch, A., Sashko, D., Schneider, D., Totounferoush, A., Volland, D., Vollmer, P., and Koseomur, O. Z. pre-CICE v2: A sustainable and user-friendly coupling library. 

Open Research Europe , 2, 2022. Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A., De Las Casas, D., Donner, C., Fritz, L., Galperti, C., Huber, A., Keeling, J., Tsimpoukelli, M., Kay, J., Merle, A., Moret, J.-M., Noury, S., Pesamosca, F., Pfau, D., Sauter, O., Sommariva, C., Coda, S., Duval, B., Fasoli, A., Kohli, P., Kavukcuoglu, K., Hassabis, D., and Riedmiller, M. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature , 602, 2022. Drgo ˇna, J., Ki ˇs, K., Tuor, A., Vrabie, D., and Klau ˇco, M. Differentiable predictive control: Deep learning alter-native to explicit model predictive control for unknown nonlinear systems. Journal of Process Control , 116, 2022. Duriez, T., Brunton, S. L., and Noack, B. R. Machine Learning Control – Taming Nonlinear Dynamics and Tur-bulence , volume 116 of Fluid Mechanics and Its Applica-tions . Springer International Publishing, Cham, 2017. Ferziger, J. H., Peri ´c, M., and Street, R. L. Computational methods for fluid dynamics . Springer International Pub-lishing, 2020. Franz, A., Wei, H., Guastoni, L., and Thuerey, N. PICT–A differentiable, GPU-accelerated multi-block PISO solver for simulation-coupled learning tasks in fluid dynamics. 

Journal of Computational Physics , 544, 2026. Garcia, X., Mir ´o, A., Su ´arez, P., ´Alcantara ´Avila, F., Rabault, J., Font, B., Lehmkuhl, O., and Vinuesa, R. Deep-reinforcement-learning-based separation control in a two-dimensional airfoil. arXiv preprint arXiv:2502.16993 ,2025. Gautier, N., Aider, J.-L., Duriez, T., Noack, B. R., Segond, M., and Abel, M. Closed-loop separation control using machine learning. Journal of Fluid Mechanics , 770, 2015. Guastoni, L., Rabault, J., Schlatter, P., Azizpour, H., and Vinuesa, R. Deep reinforcement learning for turbulent drag reduction in channel flows. The European Physical Journal E , 46, 2023. ISSN 1292-895X. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft Actor-Critic: Off-policy maximum entropy deep rein-forcement learning with a stochastic actor. In Proceed-ings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm ¨assan, Stockholm, Sweden, July 10-15, 2018 , volume 80 of Proceedings of Machine Learning Research . PMLR, 2018. Han, B.-Z., Huang, W.-X., and Xu, C.-X. Deep reinforce-ment learning for active control of flow over a circular cylinder with rotational oscillations. International Jour-nal of Heat and Fluid Flow , 96, 2022. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. Deep reinforcement learning that matters. In Proceedings of the Thirty-Second AAAI Con-ference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Ad-vances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018 , 2018. Institute of Aerodynamics. m-AIA, 2024. Issa, R. Solution of the implicitly discretised fluid flow equations by operator-splitting. Journal of Computational Physics , 62, 1986. Jim ´enez, J. Near-wall turbulence. Physics of Fluids , 25, 2013. Kaelbling, L. P., Littman, M. L., and Cassandra, A. R. Plan-ning and acting in partially observable stochastic domains. 

Artificial Intelligence , 101, 1998. Kajishima, T. and Taira, K. Computational Fluid Dynamics . Springer International Publishing, 2017. URL http://link.springer.com/10.1007/ 978-3-319-45304-0 .Koizumi, H., Tsutsumi, S., and Shima, E. Feedback control of K ´arm ´an vortex shedding from a cylinder using deep re-inforcement learning. In 2018 Flow Control Conference ,2018. Krais, N., Beck, A., Bolemann, T., Frank, H., Flad, D., Gassner, G., Hindenlang, F., Hoffmann, M., Kuhn, T., Sonntag, M., and Munz, C.-D. FLEXI: A high order dis-continuous Galerkin framework for hyperbolic–parabolic 10 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

conservation laws. Computers & Mathematics with Ap-plications , 81, 2021. Kurz, M., Offenh ¨auser, P., Viola, D., Resch, M., and Beck, A. Relexi — A scalable open source reinforcement learn-ing framework for high-performance computing. Soft-ware Impacts , 14, 2022. Lagemann, C., Mokbel, S., Gondrum, M., R ¨uttgers, M., Callaham, J., Paehler, L., Ahnert, S., Zolman, N., Lage-mann, K., Adams, N., Meinke, M., Schr ¨oder, W., Loiseau, J.-C., Lagemann, E., and Brunton, S. L. HydroGym: A reinforcement learning platform for fuid dynamics. arXiv preprint arXiv:2512.17534 , December 2025a. Lagemann, C., Paehler, L., Callaham, J., Mokbel, S., Ahnert, S., Lagemann, K., Lagemann, E., Adams, N., and Brun-ton, S. Hydrogym: A Reinforcement Learning Platform for Fluid Dynamics. In Proceedings of the 7th Annual Learning for Dynamics & Control Conference . PMLR, 2025b. Liu, Q., Corona, L. J. T., Shu, F., and Gross, A. Rein-forcement learning-based closed-loop airfoil flow control. 

arXiv preprint arXiv:2505.04818 , 2025. Maliska, C. R. Fundamentals of Computational Fluid Dy-namics: The Finite Volume Method , volume 135 of Fluid Mechanics and Its Applications . Springer International Publishing, 2023. Mao, Y., Zhong, S., and Yin, H. DRLFluent: A distributed co-simulation framework coupling deep reinforcement learning with Ansys-Fluent on high-performance comput-ing systems. Journal of Computational Science , 74, 2023. ISSN 1877-7503. Markmann, T., Straat, M., Peitz, S., and Hammer, B. Con-trol of Rayleigh-B ´enard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime. arXiv preprint arXiv:2504.12000 , 2025. McGreivy, N. and Hakim, A. Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations. Nature Machine Intelligence , 6, 2024. Montal `a, R., Font, B., Su ´arez, P., Rabault, J., Lehmkuhl, O., Vinuesa, R., and Rodriguez, I. Deep reinforcement learn-ing for active flow control around a three-dimensional flow separated wing at Re = 1,000. arXiv preprint arXiv:2509.10195 , 2025. Moslem, F., Jebelli, M., Masdari, M., Askari, R., and Ebrahimi, A. Deep reinforcement learning for active flow control in bluff bodies: A state-of-the-art review. 

Ocean Engineering , 327, 2025. Mouchamps, A., Malherbe, A., Bolland, A., and Ernst, D. Gym-TORAX: Open-source software for integrat-ing RL with plasma control simulators. arXiv preprint arXiv:2510.11283 , 2025. Nair, A. G., Yeh, C.-A., Kaiser, E., Noack, B. R., Brunton, S. L., and Taira, K. Cluster-based feedback control of turbulent post-stall separated flows. Journal of Fluid Mechanics , 875, 2019. Navier, C.-L. M ´emoire sur les lois du mouvement des fluides. M ´emoire de l’Acad ´emie des Sciences de l’Institut des Sciences, Paris , 1827. Pandey, A., Scheel, J. D., and Schumacher, J. Turbulent superstructures in Rayleigh-B ´enard convection. Nature Communications , 9, 2018. Pawar, S. and Maulik, R. Distributed deep reinforcement learning for simulation control. Machine Learning: Sci-ence and Technology , 2, 2021. Peitz, S., Stenner, J., Chidananda, V., Wallscheid, O., Brun-ton, S. L., and Taira, K. Distributed control of partial differential equations using convolutional reinforcement learning. Physica D: Nonlinear Phenomena , 461, 2024. Pironti, A. and Walker, M. Fusion, tokamaks, and plasma control: an introduction and tutorial. IEEE Control Sys-tems Magazine , 25, 2005. Rabault, J., Kuchta, M., Jensen, A., R ´eglade, U., and Cer-ardi, N. Artificial neural networks trained through deep reinforcement learning discover control strategies for ac-tive flow control. Journal of Fluid Mechanics , 865, 2019. Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N. Stable-Baselines3: Reliable rein-forcement learning implementations. Journal of Machine Learning Research , 22, 2021. Rayleigh, L. LIX. On convection currents in a horizontal layer of fluid, when the higher temperature is on the under side . The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science , 32, 1916. Ren, F., Rabault, J., and Tang, H. Applying deep reinforce-ment learning to active flow control in weakly turbulent conditions. Physics of Fluids , 33, 2021. Ren, F., Zhang, F., Zhu, Y., Wang, Z., and Zhao, F. En-hancing heat transfer from a circular cylinder undergoing vortex induced vibration based on reinforcement learning. 

Applied Thermal Engineering , 236, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. 

arXiv preprint arXiv:1707.06347 , 2017. 11 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

Sch ¨afer, M., Turek, S., Durst, F., Krause, E., and Rannacher, R. Benchmark computations of laminar flow around a cylinder. In Flow Simulation with High-Performance Computers II , volume 48. 1996. Shams, M. and Elsheikh, A. H. Gym-preCICE: Reinforce-ment learning environments for active flow control. Soft-wareX , 23, 2023. Sonoda, T., Liu, Z., Itoh, T., and Hasegawa, Y. Reinforce-ment learning of control strategies for reducing skin fric-tion drag in a fully developed turbulent channel flow. 

Journal of Fluid Mechanics , 960, 2023. Stokes, G. G. On the theories of the internal friction of fluids in motion, and of the equilibrium and motion of elastic solids. Transactions of the Cambridge Philosophical Society , 8:287–341, 1845. Stroh, A., Frohnapfel, B., Schlatter, P., and Hasegawa, Y. A comparison of opposition control in turbulent boundary layer and turbulent channel flow. Physics of Fluids , 27, 2015. Sutton, R. S. and Barto, A. G. Reinforcement Learning: An introduction . The MIT Press, Cambridge, MA, 1998. Su ´arez, P., Alc ´antara- ´Avila, F., Rabault, J., Mir ´o, A., Font, B., Lehmkuhl, O., and Vinuesa, R. Flow control of three-dimensional cylinders transitioning to turbulence via multi-agent reinforcement learning. Communications Engineering , 4, 2025. Tang, H., Rabault, J., Kuhnle, A., Wang, Y., and Wang, T. Robust active flow control over a range of Reynolds numbers using an artificial neural network trained through deep reinforcement learning. Physics of Fluids , 32, 2020. Terry, J., Black, B., Grammel, N., Jayakumar, M., Hari, A., Sullivan, R., Santos, L. S., Dieffendahl, C., Horsch, C., Perez-Vicente, R., et al. PettingZoo: Gym for multi-agent reinforcement learning. Advances in Neural Information Processing Systems , 34, 2021. Tokarev, M., Palkin, E., and Mullyadzhanov, R. Deep rein-forcement learning control of cylinder flow using rotary oscillations at low Reynolds number. Energies , 13, 2020. Towers, M., Kwiatkowski, A., Terry, J., Balis, J. U., De Cola, G., Deleu, T., Goul ˜ao, M., Kallinteris, A., Krimmel, M., KG, A., et al. Gymnasium: A standard interface for reinforcement learning environments. arXiv preprint arXiv:2407.17032 , 2024. Vasanth, J., Rabault, J., Alc ´antara- ´Avila, F., Mortensen, M., and Vinuesa, R. Multi-agent Reinforcement Learning for the Control of Three-Dimensional Rayleigh–B ´enard Convection. Flow, Turbulence and Combustion , 2024. Vignon, C., Rabault, J., Vasanth, J., Alc ´antara- ´Avila, F., Mortensen, M., and Vinuesa, R. Effective control of two-dimensional Rayleigh–B ´enard convection: Invari-ant multi-agent reinforcement learning is all you need. 

Physics of Fluids , 35(6), 2023. Viquerat, J., Meliga, P., Larcher, A., and Hachem, E. A re-view on deep reinforcement learning for fluid mechanics: An update. Physics of Fluids , 34, 2022. Wang, Q., Yan, L., Hu, G., Li, C., Xiao, Y., Xiong, H., Rabault, J., and Noack, B. R. DRLinFluids: An open-source Python platform of coupling deep reinforcement learning and OpenFOAM. Physics of Fluids , 34, August 2022a. Wang, Y.-Z., Mei, Y.-F., Aubry, N., Chen, Z., Wu, P., and Wu, W.-T. Deep reinforcement learning based synthetic jet control on disturbed flow over airfoil. Physics of Fluids , March 2022b. Weiner, A. and Geise, J. drlFoam: Deep reinforcement learn-ing with OpenFOAM, 2022. URL https://github. com/OFDataCommittee/drlfoam .Weller, H. G., Tabor, G., Jasak, H., and Fureby, C. A ten-sorial approach to computational continuum mechanics using object-oriented techniques. Computers in Physics ,12, 1998. Werner, S. and Peitz, S. Numerical evidence for sample efficiency of model-based over model-free reinforcement learning control of partial differential equations. In Euro-pean Control Conference (ECC) . IEEE, 2024. Williamson, C. H. K. Vortex Dynamics in the Cylinder Wake. Annual Review of Fluid Mechanics , 28, 1996. Xiao, M., Wang, Y., Rodach, F., Font, B., Kurz, M., Su ´arez, P., Zhou, D., Alc ´antara- ´Avila, F., Zhu, T., Liu, J., Mon-tal `a, R., Chen, J., Rabault, J., Lehmkuhl, O., Beck, A., Larsson, J., Vinuesa, R., and Pirozzoli, S. SmartFlow: A CFD-solver-agnostic deep reinforcement learning frame-work for computational fluid dynamics on HPC platforms. 

arXiv preprint arXiv:2508.00645 , 2025. Xing, E., Luk, V., and Oh, J. Stabilizing reinforcement learning in differentiable multiphysics simulation. In The Thirteenth International Conference on Learning Rep-resentations, ICLR 2025, Singapore, April 24-28, 2025 ,2025. Xu, H., Zhang, W., Deng, J., and Rabault, J. Active flow control with rotating cylinders by an artificial neural net-work trained by deep reinforcement learning. Journal of Hydrodynamics , 32, 2020. 12 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

Xu, J., Makoviychuk, V., Narang, Y., Ramos, F., Matusik, W., Garg, A., and Macklin, M. Accelerated policy learn-ing with parallel differentiable simulation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 , 2022. Zhang, X., Mao, W., Mowlavi, S., Benosman, M., and Basar, T. ControlGym: Large-scale control environments for benchmarking reinforcement learning algorithms. In 6th Annual Learning for Dynamics & Control Conference, 15-17 July 2024, University of Oxford, Oxford, UK , volume 242. PMLR, 2024. Zhao, F., Zhou, Y., Ren, F., Tang, H., and Wang, Z. Miti-gating the lift of a circular cylinder in wake flow using deep reinforcement learning guided self-rotation. Ocean Engineering , 306, 2024. Zhao, Z., Li, Z., Hassibi, K., Azizzadenesheli, K., Yan, J., Bae, H. J., Zhou, D., and Anandkumar, A. Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows. arXiv preprint arXiv:2510.03360 , 2025. Zolman, N., Lagemann, C., Fasel, U., Kutz, J. N., and Brunton, S. L. SINDy-RL for interpretable and efficient model-based reinforcement learning. Nature Communi-cations , 16, 2025. 13 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

## A. Reinforcement Learning for Active Flow Control 

Based on the definition by Sutton & Barto (1998), a reinforcement learning (RL) agent interacts with a Markov decision process (MDP) M = ( S, A, R, T ) with finite set of states S, finite set of actions A, reward function R : S × A 7 → R, and transition function T : S × A 7 → S . We note that we focus on deterministic MDPs here and do consider the discount factor 

γ as RL hyperparameter and not as part of the MDP. At each time step t, the agent selects an action at = π(st) based on its policy π. In practice, π is often described by a neural network with parameters θ and therefore denoted as πθ . Then, the environment returns the next state st+1 and a reward rt

computed by R(st, a t).When RL is applied to active flow control (AFC), the information based on which the agent selects its action is typically not the full state st but a set of sensor observations ot. This can be formalized as a partially observable Markov decision process (POMDP, Kaelbling et al. (1998)), which extends the MDP tuple by a finite set of observations Ω and observation function O : S × A 7 → Ω. Again, we only consider POMDPs here. This leads to the following MDP definition for single-agent RL (SARL) used in this paper: MSARL = ( S, A, R, T, Ω, O ).Based on the definition of a partially observable stochastic game (POSG, Albrecht et al. (2024)), we can extend our SARL definition to multiple agents. However, in the following, we again consider deterministic scenarios. In this setting, we consider i ∈ I individual agents. While the sets of states, actions, and observations are shared between agents, each agent i has an individual observation function Oi and reward function Ri. This leaves us with the following MDP: 

MMARL = ( I, S, A, R i, T, Ω, O i).

## B. The PICT Solver 

In the following, we describe the core numerical details of the PICT solver (Franz et al., 2026) and provide numerical evidence to validate the underlying simulation of our benchmark. 

B.1. The PISO Algorithm 

The Pressure Implicit with Splitting of Operators (PISO) algorithm introduced by Issa (1986) is a common method for the simulation of incompressible flows, which are governed by the Navier-Stokes equations (Navier, 1827; Stokes, 1845), consisting of the momentum equation 

∂u

∂t + ∇ · (uu ) − ν∇2u = −∇ p + S (1) and the continuity equation 

∇ · u = 0 , (2) with time t, velocity u, pressure p, viscosity ν, and external source term S.The PISO algorithm consists of two main procedures: (i) A predictor step, which advances the simulation and produces a predicted velocity u∗, and (ii) typically two predictor steps computing the pressure, which is then used to make the predicted velocity u∗ divergence free. In PICT, the PISO algorithm is discretized using the finite volume method (FVM, Kajishima & Taira (2017); Maliska (2023))on a collocated grid. For the time advancement, the implicit Euler scheme is used. For buoyancy-driven convection, we employ the Boussinesq approximation. 

B.2. Gradient Computation 

Simulation gradients in PICT are obtained via a combination of the Discretize-then-Optimize (DtO) and Optimize-then-Discretize (OtD) paradigms, with DtO applied to the global algorithmic structure and OtD to the inner linear system solves. 

B.3. Validation 

First and foremost, the PICT solver was numerically validated by Franz et al. (2026). Additionally, we provide numerical evidence for the correctness of the environments in FluidGym. 14 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

Flow Past a Cylinder For the cylinder, the temporal mean of the uncontrolled drag coefficient of 3.328 closely aligns with the value of approximately 3.205 reported by Rabault et al. (2019), resulting in a relative deviation of 3.84% . We partially attribute this to the difference between a non-reflecting advective outflow boundary in PICT and the free-stress boundary condition implemented by Rabault et al. (2019). Nevertheless, as described in Section 4.3, the resulting drag reductions achieved by the RL policies match both quantitatively and qualitatively.             

> Table 3. RBC grid refinement study. Reported Nusselt numbers correspond to the temporal mean of Nu instant over 10 uncontrolled episodes.
> xRESOLUTION ⟨Nu instant ⟩#C ELLS
> 96 4.896 5 856 144 4.755 13 248 192 4.786 23 242

Rayleigh-B ´enard Convection Prior work has largely relied on numerical setups that differ from the standard non-dimensional formulation (Pandey et al., 2018), partially yielding inconsistent Nusselt numbers (Vignon et al., 2023; Markmann et al., 2025). To validate our environment, we perform a grid refinement study (Table 3). The grid with resolution 

96 shows a relative deviation of 2.298% , and demonstrates learning behavior consistent with previous studies (Vignon et al., 2023). 

Flow Past an Airfoil For the airfoil, we obtain a mean drag coefficient of 0.278 and a mean lift coefficient of 0.993 . These values compare well with those reported by Wang et al. (2022b), who obtained an average drag of 0.324 and an average lift of 1.003 . We note that our computational domain is longer (6 chord lengths versus 3.5), which accounts for part of the discrepancy. Nevertheless, we observe consistent quantitative and qualitative behavior across all flow states. 

Turbulent Channel Flow For the channel configuration, we adopt the same numerical setup previously validated by Franz et al. (2026), including the wall-stress computation used in the forcing term. Therefore, additional baseline validation is not required. Our opposition-control case yields a 20% drag reduction, and the RL-controlled case reaches 30%, both of which are in close agreement with prior work (Guastoni et al., 2023). 15 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

## C. Environments 

Table 4. Difficulty levels and corresponding physical parameters for all FluidGym environments. Cylinder and airfoil tasks are parameterized by the Reynolds number Re , RBC by the Rayleigh number Ra , and turbulent channel flow (TCF) by the friction Reynolds number Re τ .

ID P REFIX DIFFICULTY PARAMETER VALUE DOMAIN SIZE (L × H[×D])

CY L I N D E R RO T 2D  

> EASY

Re 100 22 × 4.1 

> MEDIUM

Re 250 22 × 4.1 

> HARD

Re 500 22 × 4.1

CY L I N D E R JE T 2D  

> EASY

Re 100 22 × 4.1 

> MEDIUM

Re 250 22 × 4.1 

> HARD

Re 500 22 × 4.1

CY L I N D E R JE T 3D  

> EASY

Re 100 22 × 4.1 × 4 

> MEDIUM

Re 250 22 × 4.1 × 4 

> HARD

Re 500 22 × 4.1 × 4

RBC2D  

> EASY

Ra 8 × 10 4 π × 1 

> MEDIUM

Ra 4 × 10 5 π × 1 

> HARD

Ra 8 × 10 5 π × 1

RBC2D-W I D E  

> EASY

Ra 8 × 10 4 2π × 1 

> MEDIUM

Ra 4 × 10 5 2π × 1 

> HARD

Ra 8 × 10 5 2π × 1

RBC3D  

> EASY

Ra 6 × 10 3 π × 1 × π 

> MEDIUM

Ra 8 × 10 3 π × 1 × π 

> HARD

Ra 1 × 10 4 π × 1 × π

RBC3D-W I D E  

> EASY

Ra 6 × 10 3 2π × 1 × 2π 

> MEDIUM

Ra 8 × 10 3 2π × 1 × 2π 

> HARD

Ra 1 × 10 4 2π × 1 × 2π

AI R F O I L 2D  

> EASY

Re 1 × 10 3 6 × 1.4 

> MEDIUM

Re 3 × 10 3 6 × 1.4 

> HARD

Re 5 × 10 3 6 × 1.4

AI R F O I L 3D  

> EASY

Re 1 × 10 3 6 × 1.4 × 1.4 

> MEDIUM

Re 3 × 10 3 6 × 1.4 × 1.4 

> HARD

Re 5 × 10 3 6 × 1.4 × 1.4

TCFS M A L L 3D-B O T H  

> EASY

Re τ 180 π × 2 × π/ 2 

> MEDIUM

Re τ 330 π × 2 × π/ 2 

> HARD

Re τ 550 π × 2 × π/ 2

TCFS M A L L 3D-B O T T O M  

> EASY

Re τ 180 π × 2 × π/ 2 

> MEDIUM

Re τ 330 π × 2 × π/ 2 

> HARD

Re τ 550 π × 2 × π/ 2

TCFL A R G E 3D-B O T H  

> EASY

Re τ 180 2π × 2 × π 

> MEDIUM

Re τ 330 2π × 2 × π 

> HARD

Re τ 550 2π × 2 × π

TCFL A R G E 3D-B O T T O M  

> EASY

Re τ 180 2π × 2 × π 

> MEDIUM

Re τ 330 2π × 2 × π 

> HARD

Re τ 550 2π × 2 × π

Initial domains are publicly available in our HuggingFace dataset at https://huggingface.co/datasets/ safe-autonomous-systems/fluidgym-data . All environments provide a unified action space of [−1, 1] and scale the actions internally. A summary of all environments is stated in Table 4. We note that the medium and hard cases for the 3D Airfoil environment are not considered in this work due to computational limitations. 16 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

C.1. Flow Past Cylinder      

> (a) 2D cylinder configuration. (b) 3D cylinder configuration.
> Figure 9. Overview of the 2D and 3D cylinder environments used in our benchmark. Jets are shown in orange and feature a parabolic profile with a total deflection angle of 10 ◦. Sensor locations are indicated in pink (dots in 2D, planes in 3D, with sensors placed analogously within each plane). In 3D, the domain is extended along the spanwise direction, yielding eight individual jet pairs.

Reward Function The objective is to reduce the drag coefficient CD of the cylinder. Thus, the reward at step t is defined as rt = CD, ref − ⟨ CD ⟩Tact − ω⟨| CL|⟩ Tact , where the lift penalty ω is set to 1.0 as proposed by Ren et al. (2021) and the reference value corresponds to the uncontrolled baseline. ⟨·⟩ TAact corresponds to the temporal average over an actuation period, i.e., the simulation steps where the agent’s actions are kept fixed. Following Rabault et al. (2019), the respective drag and lift coefficients are computed as 

CD = FD 

> 12

ρU 2D and CL = FL 

> 12

ρU 2D (3) (4) with the density ρ = 1 and forces acting on the cylinder 

FD =

Z

> S

(σ · n) · ex dS and FL =

Z

> S

(σ · n) · ey dS. (5) Here, σ is the Cauchy stress tensor, n the unit normal vector at the cylinder surface S pointing into the fluid, and 

ex = (1 , 0, 0) and ey = (0 , 1, 0) the normal vectors along the x and y directions, respectively. In the MARL case, individual agent rewards are computed as rit = β r i, local  

> t

+ (1 − β) rglobal  

> t

. Local rewards are computed over the cylinder segment controlled by agent i, whereas the global reward is computed for the full cylinder. The local reward weight β defines the impact of the local rewards and is set to 0.8 following Su´ arez et al. (2025). 

Actuation Our 2D setups are based on jet actuators with a parabolic profile (Rabault et al., 2019) and cylinder rota-tion (Tokarev et al., 2020) with a maximum absolute value of U for the jet and rotation velocity, respectively. We further extend the jet actuation setup to 3D following a setup similar to previous work (Su ´arez et al., 2025). Additionally, as proposed by Rabault et al. (2019), the action is smoothed over time using cs = cs−1 + α(at − cs−1), where cs denotes the applied control value at simulation sub-step s given the current action at at episode step t and previous control step cs−1.

Observations Observations consist of vertical and horizontal velocity components at the sensor locations indicated in Figure 9. In 3D, the observations also include the spanwise velocity component. To enable transfer from 2D to 3D, the number of sensor planes as well as the included velocity components can be set to match the 2D observations. 

Difficulty Levels Difficulty is defined via the Reynolds number ( Re ) and we use easy at Re = 100 , medium at 

Re = 250 , and hard at Re = 500 . Higher Reynolds numbers increase turbulence intensity and flow unsteadiness, which 17 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

makes control more challenging. The medium and hard settings introduce three-dimensional flow interactions (Williamson, 1996). 

C.2. Rayleigh-B´ enard Convection       

> (a) 2D RBC configuration. Dashed lines indicate heater segments used for actuation.
> (b) 3D RBC configuration. Actuation is applied via discretized heater patches along the bottom boundary. Each agent receives temperature and velocity observations within a local window of size 3surrounding its actuator.
> Figure 10. Overview of the 2D and 3D Rayleigh–B ´enard convection (RBC) environments used in our benchmark. Control is provided through thermal actuation applied at the bottom boundary, while the top boundary is held at a fixed lower temperature. The environments support both centralized and decentralized control depending on the number and placement of actuators. For 3D, we omit centralized control in this work due to the large number of actuators.

Reward Function The objective is to reduce convective heat transfer. We use the instantaneous dimensionless Nusselt number Nu instant = √RaPr ⟨uy T ⟩V (Pandey et al., 2018) as performance measure, where ⟨·⟩ V denotes spatial averaging over the domain. The reward is defined as rt = Nu ref − Nu instant , where the reference Nusselt number corresponds to the uncontrolled case. 

Actuation The control is implemented via localized heaters at the bottom boundary. Before being applied to the domain, the heater temperatures are normalized and clipped to ensure a mean of the default bottom temperature and a maximum heater limit. Additionally, spatial smoothing is applied to avoid hard transitions in temperature between neighboring heaters. 

Observations Observations include all velocity components and the temperature at the sensor locations shown in Figure 10. 

Difficulty Levels We vary the Rayleigh number ( Ra ) to adjust the turbulence intensity. In 2D: easy at Ra = 8 ·

10 4 (Vignon et al., 2023), medium at Ra = 4 · 10 5, and hard at Ra = 8 · 10 5. In 3D: easy at Ra = 6 · 10 3 (Vasanth et al., 2024), medium at Ra = 8 · 10 3, and hard at Ra = 10 4. Higher Rayleigh numbers lead to stronger plume interactions and increasingly chaotic convection patterns. 18 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

C.3. Flow Past Airfoil 

Figure 11. Schematic visualization of the 2D airfoil control environment. A stationary NACA 0012 airfoil is immersed in a uniform inflow at an angle of attack of 20 ◦. Actuation is provided through surface-mounted blowing and suction jets distributed along the airfoil surface (highlighted in orange), and sensors are placed at the pink marker locations. The corresponding 3D configuration follows the same setup but extends the domain spanwise with a depth of D = 1 .4. In 3D, the actuation is discretized into four spanwise jet segments, yielding 12 individual actuators. In the MARL setting, each agent controls a group of three adjacent jets (one spanwise segment), enabling decentralized control. 

Reward Function The objective is to improve aerodynamic efficiency by increasing lift relative to drag. The reward at timestep is defined as 

rt = ⟨CL⟩Tact 

⟨CD ⟩Tact 

− CL, ref 

CD, ref 

, (6) where CL and CD denote lift and drag coefficients, respectively, and averaging is performed over the actuation interval Tact .The reference value corresponds to the uncontrolled baseline. 

Actuation Actuation is implemented using surface-mounted synthetic jet actuators placed on top of the airfoil (Garcia et al., 2025). A zero-net mass flux is enforced. As in previous environments, the raw RL control signal is temporally filtered using exponential smoothing to ensure physically consistent actuation. 

Observations Observations follow the definition for the cylinder flow with sensor locations as shown in Figure 11. As the 3D case is extended similarly to the cylinder, we only visualize the 2D case. 

Difficulty Levels Difficulty is determined by the Reynolds number ( Re ), where higher Reynolds numbers lead to more abrupt separation and larger turbulence, which increases the challenge of effective flow control. We define easy at 

Re = 10 3, medium at Re = 3 · 10 3, and hard at Re = 3 · 10 5.19 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

C.4. Turbulent Channel Flow 

Figure 12. Schematic visualization of the large TCF environment. The configuration consists of a rectangular channel with a constant-height cross section, where actuation is applied through spanwise-oriented blowing and suction jets (indicated by the orange plane) along the bottom wall. Sensor measurements are sampled at a distance of y+ = 15 from the wall at locations directly above the actuator (shown by the pink plane). A smaller channel variant shares the same height but has half the streamwise length and spanwise depth. In the 

bottom-actuation variant, only the sensor observations from the bottom wall are provided to the control policy. The actuator visualizations are not drawn to scale and do not represent the actual number of control units; they are shown purely for illustration. In the small channel, 

32 × 32 actuators are placed per wall, whereas in the large channel 64 × 64 actuators are used. 

Reward Function The reward is defined based on the reduction of instantaneous wall shear stress rt = 1 − τwall /τ wall ,ref ,where τwall ,ref is the reference value of the uncontrolled flow. The wall shear stress is computed as 

τwall = ν ∂u x

∂y y=0 

. (7) For environments with single-wall actuation, only the bottom wall is considered; for dual-wall actuation, the stress is averaged across both walls. 

Actuation The control is applied via wall-normal blowing and suction at the boundary using multiple spatially distributed actuators, where a zero net-mass-flux is enforced. Two configurations are provided: one with actuation at the bottom wall only, and one with actuation at both walls. As in previous environments, we apply exponential smoothing to the action signal to avoid abrupt control variations. 

Observations Observations include the velocity fluctuations, i.e., the difference from the volume mean velocity, right over the corresponding actuator at wall distance y+ = 15 .

Difficulty Levels Difficulty is defined using the friction Reynolds number ( Re τ ). We use easy at Re τ = 180 , medium 

at Re τ = 330 , and hard at Re τ = 550 .

Opposition Control Baseline For the TCF, a common baseline (Guastoni et al., 2023; Sonoda et al., 2023) is opposition control, which sets the wall normal velocity to the negative vertical velocity, i.e., observation. 

## D. Experimental Setup 

D.1. Hardware and Software Configuration General Experimental Setup Unless stated otherwise, all experiments were conducted using the following shared hardware and software configuration: • Python: 3.10 • PyTorch: 2.9.1 20 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

• CUDA: 12.8 • System Memory: 32 GB RAM • CPU: 32 cores of an AMD EPYC 7742 (64-core processor) • GPU: 1 × NVIDIA A100 (40 GB or 80 GB) 

CylinderJet3D-hard-v0 Environment Experiments for the CylinderJet3D-hard-v0 environment and SARL were conducted on compute nodes with the following differing hardware configuration: • CPU: 8 cores of an AMD EPYC 7763 (Milan architecture) • GPU: 2 × NVIDIA A100 (40 GB) 

D.2. Algorithm Hyperparameter Configurations 

The hyperparameters used in our experiments are stated in Tables 5 and 6 for PPO and SAC, respectively. We note that for all SAC experiments on TCF environments, we set the number of gradient steps per update to 1 to avoid excessive gradient updates due to the large number of pseudo multi-agent environments. 

Table 5. PPO hyperparameters used in all experiments. 

HYPERPARAMETER VALUE 

POLICY NETWORK ML P PO L I C Y 

LEARNING RATE 3 × 10 −4

STEPS PER ROLLOUT (n steps ) 2048 

BATCH SIZE 64 

UPDATE EPOCHS (n epochs ) 10 

DISCOUNT FACTOR (γ) 0.99 

GAE λ 0.95 

CLIP RANGE 0.2

ADVANTAGE NORMALIZATION TRUE 

ENTROPY COEFFICIENT (c ent ) 0.01 

VALUE FUNCTION COEFFICIENT (c vf ) 0.5

MAX GRADIENT NORM 0.5

DEVICE CPU 

Table 6. SAC hyperparameters used in all experiments except TCF environments. For TCF, we set the number of gradient steps per update to 1.

HYPERPARAMETER VALUE 

POLICY NETWORK ML P PO L I C Y 

LEARNING RATE 3 × 10 −4

DISCOUNT FACTOR (γ) 0.99 

SOFT UPDATE COEFFICIENT (τ ) 0.005 

REPLAY BUFFER SIZE 10 6

BATCH SIZE 256 

LEARNING STARTS 100 

TRAINING FREQUENCY 1

GRADIENT STEPS PER UPDATE −1 (EQUAL TO T R A I N F R E Q )ENTROPY COEFFICIENT (α) A U T O 

TARGET ENTROPY A U T O 

TARGET UPDATE INTERVAL 1

DEVICE CUDA 

21 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

D.3. Differentiable Model Predictive Control 

To isolate the value of reward gradients in our fully differentiable AFC benchmark, we evaluate a differentiable model predictive control (D-MPC) baseline that relies solely on gradient information. D-MPC is inspired by differentiable predictive control (DPC, Drgo ˇna et al. (2022)) and, at each control step, optimizes a sequence of future actions via gradient ascent through the differentiable flow simulator in order to maximize predicted rewards, without using a policy network, value function, or model-free exploration. Only the first action of the optimized sequence is executed on the environment, and the horizon is shifted forward, yielding a standard receding-horizon control loop. This gradient-only optimization procedure is summarized in Algorithm 1. In our experiments, we set H = 20 , N = 10 , α = 0 .1, and γ = 0 .999 and evaluate ten seeds with each one test set episode for all three 2D Cylinder environments. 

Algorithm 1 D-MPC: Optimize Action Sequence 

Input: differentiable env env , start state s0, horizon H, iterations N , learning rate α, discount γ, previous actions aprev 

> 0: H−1

(optional) 

Output: optimized action sequence a0: H−1

if aprev  

> 0: H−1

is not provided then 

Initialize a0: H−1 ← 0

else 

Initialize a0: H−2 from aprev 

> 1: H−1

Initialize aH−2 ← 0

end if for k = 1 to N do 

Rest env 

Set env to state s0

Detach gradients in env 

Initialize return R ← 0, discount factor g ← 1

for t = 0 to H − 1 do 

Clamp at to action bounds: ˜at ← clip (at, a min , a max )

Step env: (st+1 , r t, terminated , truncated ) ← env.step (˜ at)

R ← R + g · rt

g ← g · γ

if terminated or truncated then break end if end for 

Compute loss L ← − R

Backpropagate gradients of L w.r.t. a0: H−1

Update a0: H−1 with gradient descent/Adam using step size α

Clamp a0: H−1 to action bounds 

end for return a0: H−1

22 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

## E. Additional Results 

E.1. Runtime Benchmarks 

Table 7. Experiment details and GPU runtimes. Total GPU hours are computed as #steps × #seeds × #seconds × #algorithms, where environments not included in this study are set to zero. 

ENVIRONMENT DIFFICULTY #S TEPS #S EEDS #A LGORITHMS SECONDS PER STEP GPU H OURS 

CYLINDER ROT 2D EASY 50000 5 1.241 2 172.400 CYLINDER ROT 2D MEDIUM 50000 5 2.059 2 285.960 CYLINDER ROT 2D HARD 50000 5 2.561 2 355.700 CYLINDER JET 2D EASY 50000 5 1.259 2 174.890 CYLINDER JET 2D MEDIUM 50000 5 2.209 2 306.740 CYLINDER JET 2D HARD 50000 5 2.552 2 354.430 CYLINDER JET 3D EASY 50000 3 4.209 4 701.580 CYLINDER JET 3D MEDIUM 50000 3 7.684 4 1280.710 CYLINDER JET 3D HARD 50000 3 16.679 4 2779.910 RBC2D EASY 50000 5 1.265 4 351.260 RBC2D MEDIUM 50000 5 2.232 4 620.000 RBC2D HARD 50000 5 2.260 4 627.730 RBC2D-WIDE EASY 50000 5 1.314 4 0.000 RBC2D-WIDE MEDIUM 50000 5 2.292 4 0.000 RBC2D-WIDE HARD 50000 5 2.349 4 0.000 RBC3D EASY 50000 5 1.168 2 162.250 RBC3D MEDIUM 50000 5 1.157 2 160.730 RBC3D HARD 50000 5 1.199 2 166.570 RBC3D-WIDE EASY 50000 5 1.675 2 0.000 RBC3D-WIDE MEDIUM 50000 5 1.689 2 0.000 RBC3D-WIDE HARD 50000 5 1.754 2 0.000 AIRFOIL 2D EASY 20000 5 18.851 2 1047.290 AIRFOIL 2D MEDIUM 20000 5 30.145 2 1674.740 AIRFOIL 2D HARD 20000 5 37.278 2 2071.010 AIRFOIL 3D EASY 20000 3 34.526 4 2301.760 AIRFOIL 3D MEDIUM 20000 3 60.244 4 0.000 AIRFOIL 3D HARD 20000 3 63.913 4 0.000 TCFS MALL 3D-BOTH EASY 100000 5 0.481 2 133.680 TCFS MALL 3D-BOTH MEDIUM 100000 5 0.250 2 69.380 TCFS MALL 3D-BOTH HARD 100000 5 0.248 2 68.940 TCFS MALL 3D-BOTTOM EASY 100000 5 0.427 2 0.000 TCFS MALL 3D-BOTTOM MEDIUM 100000 5 0.218 2 0.000 TCFS MALL 3D-BOTTOM HARD 100000 5 0.220 2 0.000 TCFL ARGE 3D-BOTH EASY 100000 5 0.846 2 235.060 TCFL ARGE 3D-BOTH MEDIUM 100000 5 0.417 2 115.920 TCFL ARGE 3D-BOTH HARD 100000 5 0.417 2 115.780 TCFL ARGE 3D-BOTTOM EASY 100000 5 0.759 2 0.000 TCFL ARGE 3D-BOTTOM MEDIUM 100000 5 0.387 2 0.000 TCFL ARGE 3D-BOTTOM HARD 100000 5 0.408 2 0.000 

TOTAL 16 334.420 

Table 7 states individual environment wall-clock times per step as well as the number of steps, seeds, and total GPU hours of the experiments presented in this paper. Results were obtained by running 80 (8 for medium and hard 3D airfoil cases) RL steps with random actions and averaging the results. Experiments were conducted on a single NVIDIA A100 GPU. 23 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

E.2. Quantitative Training Results 0 5e4 Training Step 

> −1.5
> −1.0
> −0.5
> 0.0
> Mean Reward

CylinderJet2D-easy-v0  

> 05e4 Training Step
> −2
> −1
> 0

CylinderJet2D-medium-v0  

> 05e4 Training Step
> −2
> −1
> 0
> 1

CylinderJet2D-hard-v0  

> PPO SAC

Figure 13. Mean training reward for CylinderJet2D . Error bars indicate 95% confidence intervals. 0 5e4 Training Step 

> −1.0
> −0.5
> 0.0
> Mean Reward

CylinderRot2D-easy-v0  

> 05e4 Training Step
> −1
> 0

CylinderRot2D-medium-v0  

> 05e4 Training Step
> −2
> −1
> 0
> 1

CylinderRot2D-hard-v0  

> PPO SAC

Figure 14. Mean training reward for CylinderRot2D . Error bars indicate 95% confidence intervals. 0 5e4 Training Step 

> −1.00
> −0.75
> −0.50
> −0.25
> 0.00
> Mean Reward

CylinderJet3D-easy-v0  

> 05e4 Training Step
> −1.0
> −0.5
> 0.0

CylinderJet3D-medium-v0  

> 05e4 Training Step
> −1.0
> −0.5

CylinderJet3D-hard-v0    

> PPO SAC MA-PPO MA-SAC

Figure 15. Mean training reward for CylinderJet3D . Error bars indicate 95% confidence intervals. 

24 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 0 5e4 Training Step 

> 0.00
> 0.25
> 0.50
> 0.75
> Mean Reward

RBC2D-easy-v0  

> 05e4 Training Step
> −1.0
> −0.5
> 0.0
> 0.5

RBC2D-medium-v0  

> 05e4 Training Step
> −2
> −1
> 0

RBC2D-hard-v0    

> PPO SAC MA-PPO MA-SAC

Figure 16. Mean training reward for RBC2D . Error bars indicate 95% confidence intervals. 0 5e4 Training Step 

> 0.0
> 0.2
> 0.4
> Mean Reward

RBC3D-easy-v0  

> 05e4 Training Step
> 0.0
> 0.2
> 0.4

RBC3D-medium-v0  

> 05e4 Training Step
> 0.0
> 0.2
> 0.4

RBC3D-hard-v0  

> MA-PPO MA-SAC

Figure 17. Mean training reward for RBC3D . Error bars indicate 95% confidence intervals. 0 2e4 Training Step 

> −0.5
> 0.0
> 0.5
> 1.0
> 1.5
> Mean Reward

Airfoil2D-easy-v0  

> 02e4 Training Step
> 1
> 2
> 3

Airfoil2D-medium-v0  

> 02e4 Training Step
> 1
> 2

Airfoil2D-hard-v0  

> PPO SAC

Figure 18. Mean training reward for Airfoil2D . Error bars indicate 95% confidence intervals. 

25 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 0 2e4 Training Step 

> 0.0
> 0.5
> 1.0
> 1.5
> Mean Reward

Airfoil3D-easy-v0    

> PPO SAC MA-PPO MA-SAC

Figure 19. Mean training reward for Airfoil3D . Error bars indicate 95% confidence intervals. 0 1e5 Training Step 

> 0.0
> 0.1
> 0.2
> Mean Reward

TCFSmall3D-both-easy-v0  

> 01e5 Training Step
> 0.05
> 0.10
> 0.15
> 0.20

TCFSmall3D-both-medium-v0  

> 01e5 Training Step
> 0.00
> 0.05
> 0.10
> 0.15

TCFSmall3D-both-hard-v0  

> MA-PPO MA-SAC

Figure 20. Mean training reward for TCFSmall3D-both . Error bars indicate 95% confidence intervals. 0 1e5 Training Step 

> −0.1
> 0.0
> 0.1
> 0.2
> Mean Reward

TCFLarge3D-both-easy-v0  

> 01e5 Training Step
> −0.05
> 0.00
> 0.05
> 0.10
> 0.15

TCFLarge3D-both-medium-v0  

> 01e5 Training Step
> 0.00
> 0.05
> 0.10
> 0.15

TCFLarge3D-both-hard-v0  

> MA-PPO MA-SAC

Figure 21. Mean training reward for TCFLarge3D-both . Error bars indicate 95% confidence intervals. 

26 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

Table 8. Cylinder test set metrics. All values report the interquartile mean (IQM) over test episodes and random seeds. Drag reduction is measured relative to the mean drag over 10 uncontrolled training episodes of the baseflow. Best result per environment is highlighted in 

bold .ENVIRONMENT ALGORITHM REWARD CD CL DRAG REDUCTION (%) CYLINDER ROT 2D-EASY -V0 BASEFLOW - 3.328 −0.042 -CYLINDER ROT 2D-EASY -V0 PPO −0.002 3.191 0.035 4.125 

CYLINDER ROT 2D-EASY -V0 SAC 0.037 3.179 0.016 4.477 

CYLINDER ROT 2D-MEDIUM -V0 BASEFLOW - 3.152 0.037 -CYLINDER ROT 2D-MEDIUM -V0 PPO 0.309 2.489 −0.060 21 .033 

CYLINDER ROT 2D-MEDIUM -V0 SAC 0.344 2.475 −0.093 21 .496 

CYLINDER ROT 2D-HARD -V0 BASEFLOW - 3.619 0.057 -CYLINDER ROT 2D-HARD -V0 PPO 0.162 2.962 −0.028 18 .162 

CYLINDER ROT 2D-HARD -V0 SAC 0.552 2.440 −0.180 32 .587 

CYLINDER JET 2D-EASY -V0 BASEFLOW - 3.328 −0.042 -CYLINDER JET 2D-EASY -V0 PPO −0.052 3.141 0.065 5.638 

CYLINDER JET 2D-EASY -V0 SAC 0.051 3.105 0.032 6.697 

CYLINDER JET 2D-MEDIUM -V0 BASEFLOW - 3.152 0.037 -CYLINDER JET 2D-MEDIUM -V0 PPO 0.274 2.487 −0.066 21 .110 

CYLINDER JET 2D-MEDIUM -V0 SAC 0.426 2.484 −0.004 21 .216 

CYLINDER JET 2D-HARD -V0 BASEFLOW - 3.619 0.057 -CYLINDER JET 2D-HARD -V0 PPO 1.173 2.158 0.038 40 .385 

CYLINDER JET 2D-HARD -V0 SAC 1.352 2.011 0.001 44 .426 

CYLINDER JET 3D-EASY -V0 BASEFLOW - 3.305 −0.028 -CYLINDER JET 3D-EASY -V0 PPO −0.217 3.216 −0.009 2.719 

CYLINDER JET 3D-EASY -V0 SAC −0.040 3.224 0.039 2.471 

CYLINDER JET 3D-EASY -V0 MA-PPO −0.178 3.193 0.030 3.398 

CYLINDER JET 3D-EASY -V0 MA-SAC −0.041 3.103 0.095 6.118 

CYLINDER JET 3D-MEDIUM -V0 BASEFLOW - 2.984 −0.008 -CYLINDER JET 3D-MEDIUM -V0 PPO −0.187 2.764 −0.205 7.395 

CYLINDER JET 3D-MEDIUM -V0 SAC 0.027 2.791 0.024 6.486 

CYLINDER JET 3D-MEDIUM -V0 MA-PPO −0.280 2.955 0.067 0.972 

CYLINDER JET 3D-MEDIUM -V0 MA-SAC 0.034 2.718 0.045 8.934 

CYLINDER JET 3D-HARD -V0 BASEFLOW - 2.571 −0.018 -CYLINDER JET 3D-HARD -V0 PPO −0.184 2.564 −0.086 0.286 

CYLINDER JET 3D-HARD -V0 SAC −0.646 2.692 0.190 −4.696 

CYLINDER JET 3D-HARD -V0 MA-PPO −0.222 2.565 0.040 0.249 

CYLINDER JET 3D-HARD -V0 MA-SAC −0.133 2.509 −0.030 2.424 

27 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

Table 9. RBC test set metrics. All values report the interquartile mean (IQM) over test episodes and random seeds. Heat transfer improvement is measured relative to the mean instant Nusselt number Nu instant over 10 uncontrolled training episodes of the baseflow. Best result per environment is highlighted in bold .ENVIRONMENT ALGORITHM REWARD Nu instant HEAT TRANSFER IMPROVEMENT (%) RBC2D-EASY -V0 BASEFLOW - 4.841 -RBC2D-EASY -V0 PPO 0.888 4.008 17 .200 

RBC2D-EASY -V0 SAC 0.779 4.117 14 .952 

RBC2D-EASY -V0 MA-PPO 1.024 3.872 20 .015 

RBC2D-EASY -V0 MA-SAC 0.650 4.246 12 .285 

RBC2D-MEDIUM -V0 BASEFLOW - 6.856 -RBC2D-MEDIUM -V0 PPO 0.138 6.291 8.238 

RBC2D-MEDIUM -V0 SAC 0.790 5.639 17 .746 

RBC2D-MEDIUM -V0 MA-PPO 0.056 6.373 7.041 

RBC2D-MEDIUM -V0 MA-SAC −0.018 6.447 5.960 

RBC2D-HARD -V0 BASEFLOW - 7.854 -RBC2D-HARD -V0 PPO −0.304 7.547 3.911 

RBC2D-HARD -V0 SAC 0.525 6.717 14 .467 

RBC2D-HARD -V0 MA-PPO −0.484 7.726 1.622 

RBC2D-HARD -V0 MA-SAC −0.715 7.958 −1.327 

RBC3D-EASY -V0 BASEFLOW - 2.182 -RBC3D-EASY -V0 MA-PPO 0.367 1.815 16 .815 

RBC3D-EASY -V0 MA-SAC 0.400 1.782 18 .333 

RBC3D-MEDIUM -V0 BASEFLOW - 2.444 -RBC3D-MEDIUM -V0 MA-PPO 0.340 2.105 13 .893 

RBC3D-MEDIUM -V0 MA-SAC 0.384 2.061 15 .692 

RBC3D-HARD -V0 BASEFLOW - 2.684 -RBC3D-HARD -V0 MA-PPO 0.341 2.343 12 .713 

RBC3D-HARD -V0 MA-SAC 0.323 2.361 12 .050 

Table 10. Airfoil test set metrics. All values report the interquartile mean (IQM) over test episodes and random seeds. Aerodynamic efficiency improvement is measured relative to the mean aerodynamic efficiency over 10 uncontrolled training episodes of the baseflow. Best result per environment is highlighted in bold .ENVIRONMENT ALGORITHM REWARD AERODYNAMIC EFFICIENCY IMPROVEMENT (%) AIRFOIL 2D-EASY -V0 BASEFLOW - 2.887 -AIRFOIL 2D-EASY -V0 PPO 1.422 4.309 49 .265 

AIRFOIL 2D-EASY -V0 SAC 1.705 4.592 59 .072 

AIRFOIL 2D-MEDIUM -V0 BASEFLOW - 3.572 -AIRFOIL 2D-MEDIUM -V0 PPO 3.134 6.706 87 .747 

AIRFOIL 2D-MEDIUM -V0 SAC 3.666 7.238 102 .633 

AIRFOIL 2D-HARD -V0 BASEFLOW - 6.063 -AIRFOIL 2D-HARD -V0 PPO 1.338 7.401 22 .065 

AIRFOIL 2D-HARD -V0 SAC 2.636 8.699 43 .470 

AIRFOIL 3D-EASY -V0 BASEFLOW - 2.838 -AIRFOIL 3D-EASY -V0 PPO −0.105 2.733 −3.691 

AIRFOIL 3D-EASY -V0 SAC 1.462 4.300 51 .513 

AIRFOIL 3D-EASY -V0 MA-PPO 0.084 2.922 2.951 

AIRFOIL 3D-EASY -V0 MA-SAC 1.584 4.422 55 .808 

28 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

Table 11. TCF test set metrics. All values report the interquartile mean (IQM) over test episodes and random seeds. Drag reduction is measured relative to the mean wall stress τwall over 10 uncontrolled training episodes of the baseflow. Best result per environment is highlighted in bold .ENVIRONMENT ALGORITHM REWARD τwall DRAG REDUCTION (%) TCFS MALL 3D-BOTH -EASY -V0 BASEFLOW - 0.002 -TCFS MALL 3D-BOTH -EASY -V0 MA-PPO 0.207 0.001 20 .689 

TCFS MALL 3D-BOTH -EASY -V0 MA-SAC 0.171 0.001 17 .091 

TCFS MALL 3D-BOTH -MEDIUM -V0 BASEFLOW - 0.002 -TCFS MALL 3D-BOTH -MEDIUM -V0 MA-PPO 0.193 0.001 19 .281 

TCFS MALL 3D-BOTH -MEDIUM -V0 MA-SAC 0.173 0.001 17 .290 

TCFS MALL 3D-BOTH -HARD -V0 BASEFLOW - 0.001 -TCFS MALL 3D-BOTH -HARD -V0 MA-PPO 0.120 0.001 11 .999 

TCFS MALL 3D-BOTH -HARD -V0 MA-SAC 0.089 0.001 8.945 

TCFL ARGE 3D-BOTH -EASY -V0 BASEFLOW - 0.002 -TCFL ARGE 3D-BOTH -EASY -V0 MA-PPO 0.129 0.002 12 .885 

TCFL ARGE 3D-BOTH -EASY -V0 MA-SAC 0.045 0.002 4.514 

TCFL ARGE 3D-BOTH -MEDIUM -V0 BASEFLOW - 0.002 -TCFL ARGE 3D-BOTH -MEDIUM -V0 MA-PPO 0.019 0.002 1.903 

TCFL ARGE 3D-BOTH -MEDIUM -V0 MA-SAC 0.094 0.001 9.415 

TCFL ARGE 3D-BOTH -HARD -V0 BASEFLOW - 0.001 -TCFL ARGE 3D-BOTH -HARD -V0 MA-PPO 0.001 0.001 0.113 

TCFL ARGE 3D-BOTH -HARD -V0 MA-SAC 0.059 0.001 5.877 

E.3. Quantitative Test Results PPO SAC D-MPC Algorithm 

> −0.2
> −0.1
> 0.0
> Mean Reward

CylinderJet2D-easy-v0   

> PPO SAC D-MPC Algorithm
> 0.0
> 0.2
> 0.4

CylinderJet2D-medium-v0   

> PPO SAC D-MPC Algorithm
> 1.0
> 1.1
> 1.2
> 1.3
> 1.4
> 1.5

CylinderJet2D-hard-v0 

Figure 22. Mean test reward for CylinderJet2D .PPO SAC Algorithm 

> −0.04
> −0.02
> 0.00
> 0.02
> 0.04
> Mean Reward

CylinderRot2D-easy-v0  

> PPO SAC Algorithm
> 0.2
> 0.3
> 0.4

CylinderRot2D-medium-v0  

> PPO SAC Algorithm
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8

CylinderRot2D-hard-v0 

Figure 23. Mean test reward for CylinderRot2D .

29 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control PPO SAC MA-PPO MA-SAC Algorithm 

−0.4

−0.3

−0.2

−0.1

0.0

> Mean Reward

CylinderJet3D-easy-v0 

PPO SAC MA-PPO MA-SAC Algorithm 

−1.5

−1.0

−0.5

0.0

CylinderJet3D-medium-v0 

PPO SAC MA-PPO MA-SAC Algorithm 

−1.5

−1.0

−0.5

0.0

CylinderJet3D-hard-v0 

Figure 24. Mean test reward for CylinderJet3D .PPO SAC MA-PPO MA-SAC Algorithm 

0.4

0.6

0.8

1.0

1.2

> Mean Reward

RBC2D-easy-v0 

PPO SAC MA-PPO MA-SAC Algorithm 

−0.5

0.0

0.5

1.0

RBC2D-medium-v0 

PPO SAC MA-PPO MA-SAC Algorithm 

−1.0

−0.5

0.0

0.5

1.0

RBC2D-hard-v0 

Figure 25. Mean test reward for RBC2D .MA-PPO MA-SAC Algorithm 

0.1

0.2

0.3

0.4

0.5

0.6

> Mean Reward

RBC3D-easy-v0 

MA-PPO MA-SAC Algorithm 

0.25 

0.30 

0.35 

0.40 

0.45 

RBC3D-medium-v0 

MA-PPO MA-SAC Algorithm 

0.1

0.2

0.3

0.4

RBC3D-hard-v0 

Figure 26. Mean test reward for RBC3D .

30 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control PPO SAC Algorithm 

> 1.2
> 1.4
> 1.6
> Mean Reward

Airfoil2D-easy-v0  

> PPO SAC Algorithm
> 3.0
> 3.2
> 3.4
> 3.6
> 3.8

Airfoil2D-medium-v0  

> PPO SAC Algorithm
> 1.0
> 1.5
> 2.0
> 2.5
> 3.0

Airfoil2D-hard-v0 

Figure 27. Mean test reward for Airfoil2D .PPO SAC MA-PPO MA-SAC Algorithm 

> 0.0
> 0.5
> 1.0
> 1.5
> Mean Reward

Airfoil3D-easy-v0 

Figure 28. Mean test reward for Airfoil3D .MA-PPO MA-SAC Algorithm 

> 0.0
> 0.1
> 0.2
> 0.3
> Mean Reward

TCFSmall3D-both-easy-v0  

> MA-PPO MA-SAC Algorithm
> 0.10
> 0.15
> 0.20
> 0.25

TCFSmall3D-both-medium-v0  

> MA-PPO MA-SAC Algorithm
> −0.1
> 0.0
> 0.1
> 0.2

TCFSmall3D-both-hard-v0 

Figure 29. Mean test reward for TCFSmall3D-both .

31 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control MA-PPO MA-SAC Algorithm 

> 0.0
> 0.1
> 0.2
> Mean Reward

TCFLarge3D-both-easy-v0  

> MA-PPO MA-SAC Algorithm
> 0.00
> 0.05
> 0.10
> 0.15
> 0.20

TCFLarge3D-both-medium-v0  

> MA-PPO MA-SAC Algorithm
> −0.1
> 0.0
> 0.1

TCFLarge3D-both-hard-v0 

Figure 30. Mean test reward for TCFLarge3D-both .

32 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control 

E.4. Qualitative Test Results 

In the following, we present qualitative visualizations of uncontrolled and final controlled flow fields for all environments and algorithms for seed 0.Baseflow  Easy Medium Hard 

> PPO SAC D-MPC

Figure 31. Qualitative test results for CylinderJet2D .

33 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Baseflow  Easy Medium Hard 

> PPO SAC

Figure 32. Qualitative test results for CylinderRot2D .

34 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Baseflow 

Easy Medium Hard 

> PPO SAC MA-PPO MA-SAC

Figure 33. Qualitative test results for CylinderJet3D .

35 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Baseflow 

Easy Medium Hard 

> PPO SAC MA-PPO MA-SAC

Figure 34. Qualitative test results for RBC2D .

36 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Baseflow 

Easy Medium Hard 

> MA-PPO MA-SAC

Figure 35. Qualitative test results for RBC3D .

37 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Baseflow 

Easy Medium Hard 

> PPO SAC

Figure 36. Qualitative test results for Airfoil2D .

38 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Baseflow 

Easy 

> PPO SAC MA-PPO MA-SAC

Figure 37. Qualitative test results for Airfoil3D .

39 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Baseflow 

Easy Medium Hard 

> MA-PPO MA-SAC

Figure 38. Qualitative test results for TCFSmall3D-both .

40 Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Baseflow 

Easy Medium Hard 

> MA-PPO MA-SAC

Figure 39. Qualitative test results for TCFLarge3D-both .

41