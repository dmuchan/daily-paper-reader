# Improving Regret Approximation for Unsupervised Dynamic Environment Generation
# 改进无监督动态环境生成的遗憾近似

**Authors**: Harry Mead, Bruno Lacerda, Jakob Foerster, Nick Hawes
**Date**: 2026-01-21
**PDF**: https://arxiv.org/pdf/2601.14957v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span>
**Score**: 8.0
**Evidence**: Reinforcement learning environment generation for agent generalization

---

## Abstract
Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.

## 摘要
无监督环境设计 (UED) 旨在为强化学习 (RL) 智能体

---

## 论文详细总结（自动生成）

这篇论文由牛津大学的研究团队提出，旨在解决强化学习中**无监督环境设计（UED）**在大规模、复杂环境下的扩展性问题。以下是对该论文的深度结构化总结：

### 1. 核心问题与整体含义
*   **研究动机**：UED 的目标是自动生成训练课程（即环境关卡），以提升 RL 智能体的泛化能力。然而，现有方法在环境规模变大或存在“阶跃式难度”（如必须拿钥匙才能开门）时表现不佳。
*   **核心挑战**：
    1.  **信用分配（Credit Assignment）困难**：现有的生成式方法通常在学生开始训练前生成完整关卡，导致生成器的奖励信号极其稀疏。
    2.  **遗憾近似（Regret Approximation）不准**：现有的指标（如 PVL, MaxMC）难以准确识别真正具有挑战性且可解的关卡。
*   **整体含义**：论文通过改进生成方式和评价指标，显著提升了 UED 在大型复杂环境中的训练效率和智能体的零样本（Zero-shot）迁移性能。

### 2. 论文提出的方法论
论文提出了两个核心贡献：**DEGen** 生成框架和 **MNA** 遗憾近似指标。

*   **DEGen (Dynamic Environment Generation)**：
    *   **核心思想**：不再预先生成整个关卡，而是利用智能体的**局部观测**，在智能体探索过程中动态生成环境。
    *   **关键技术**：当智能体移动到未生成的区域时，生成器（Teacher）根据智能体当前的观测范围填充障碍物、目标或道具。
    *   **优势**：这为生成器提供了更密集的奖励信号（每步都有反馈），降低了信用分配难度，并增加了关卡的多样性。

*   **MNA (Maximised Negative Advantage, 最大化负优势)**：
    *   **核心思想**：遗憾（Regret）定义为“最优策略收益”与“当前策略收益”之差。MNA 通过计算当前策略在轨迹中可能达到的最大价值上限来近似最优收益。
    *   **公式逻辑**：$MNA = \text{Mean}(\hat{G}_{\lambda t}) \cdot \hat{C}$。其中 $\hat{G}_{\lambda t}$ 类似于负的 n 步优势估计，用于衡量当前策略表现与潜在最优表现的差距；$\hat{C}$ 是一个**可解性惩罚项**（如果关卡从未被解开，则得分为 0），以确保生成器不会生成不可解的死局。

### 3. 实验设计
*   **实验场景**：
    1.  **Standard Minigrid**：标准的迷宫导航任务。
    2.  **Key Minigrid**：增加了钥匙和锁门逻辑，难度更高。
    3.  **Sokoban (推箱子)**：测试逻辑推理能力。
*   **Benchmark 与对比方法**：
    *   **基准**：Domain Randomization (DR, 随机环境)。
    *   **对比 UED 方法**：PLR, ACCEL (基于回放的方法)；PAIRED, Initial Gen (基于生成的方法)；SFL (基于可学习性的方法)。
    *   **对比指标**：将 MNA 与传统的 PVL 和 MaxMC 进行消融对比。

### 4. 资源与算力
*   **硬件**：实验使用了 **Nvidia A40 GPU**。
*   **训练时长**：
    *   DEGen 的计算成本较高，在 Minigrid 任务中单次运行约需 **25 小时 50 分钟**。
    *   相比之下，PLR 和 ACCEL 仅需约 6-7 小时。
    *   DEGen 的训练耗时大约是传统回放方法的 **4 倍**。

### 5. 实验数量与充分性
*   **实验规模**：每个实验配置均运行了 **8 个不同的随机种子**，并报告了平均值和标准差。
*   **充分性**：
    *   涵盖了从 13x13 到 21x21 不同尺寸的环境，验证了扩展性。
    *   进行了详细的消融实验，证明了 MNA 指标在不同 UED 框架（PLR, ACCEL, DEGen）下均有提升。
    *   实验设计客观、公平，对比了当前最先进的（SOTA）多类 UED 算法。

### 6. 主要结论与发现
*   **DEGen 的优越性**：在小型环境中，DEGen 表现与最强基线持平；但在大型环境（17x17, 21x21）中，DEGen 显著优于所有回放类方法，证明了动态生成的必要性。
*   **MNA 的有效性**：MNA 能够比 PVL 和 MaxMC 更准确地识别出“困难但可解”的关卡。将其集成到 PLR 或 ACCEL 中也能带来性能提升。
*   **生成 vs 回放**：传统的生成式方法（如 PAIRED）通常弱于回放式方法，但 DEGen 扭转了这一局面，成为首个在大规模环境下超越回放方法的生成式 UED 框架。

### 7. 优点（亮点）
*   **创新的生成机制**：将环境生成与智能