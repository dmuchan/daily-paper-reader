Title: Why Inference in Large Models Becomes Decomposable After Training

URL Source: https://arxiv.org/pdf/2601.15871v1

Published Time: Fri, 23 Jan 2026 01:40:59 GMT

Number of Pages: 39

Markdown Content:
# Why Inference in Large Models Becomes Decomposable After Training 

Jidong Jin âˆ—

2026-01 

Abstract 

In contemporary large-scale AI models, inference is typically carried out by operating on full parameter matrices. As model size continues to increase, this paradigm leads to inference cost and system complexity that scale in an unsustainable manner. The root cause does not lie in limitations of model capacity or representational form. Rather, post-training inference systems have long been treated as monolithic operators, while internal structures formed during training are never explicitly identified. Based on an analysis of neural network learning dynamics, we show that gradient update events in large models are highly localized and selective in parameter space. After training, parameter matrices commonly contain a substantial fraction of parameter components that receive no effective sample support: the corresponding dependencies fail to accumulate sta-ble gradient updates and remain statistically indistinguishable, at the distributional level, from their initialization distribution. Consequently, the post-training inference system is structurally non-uniform and inherently decomposable. Building on this observation, we take the parameter initialization distribution as a null hypothesis and introduce a post-training statistical criterion to distinguish dependencies that are significantly confirmed by learning from parameter components that never acquire structural effect. By applying structural annealing to the latter, that is, the systematic removal of statistically unsupported dependencies, the original dense parameter system is transformed into a sparse structural representation, in which stable and mutually indepen-dent substructures are explicitly revealed. These results indicate that post-training inference in large-scale AI models should be understood as the execution of a decomposable system rather than a single monolithic operator. The proposed theory and methodology operate solely on trained parameter ma-trices, are independent of specific model implementations, and preserve model functionality and inputâ€“output interfaces. They enable post-training conversion of monolithic inference into index-routed parallel execution of independent sub-operators, providing an engineering-controllable mechanism for structured inference and systematic control of inference complex-ity. 

# Contents 

1 Introduction 22 Training Dynamics Analysis 4

2.1 Scope and Objects of Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42.2 Local Gradient Updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42.3 Model Decomposability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62.4 Implications for the Further Development of Large Models . . . . . . . . . . . . . 9

> âˆ—

School of Artificial Intelligence, Capital University of Economics and Business, Beijing 100070, China. Email: jjd@cueb.edu.cn, jidongjin@Gmail.com 

1

> arXiv:2601.15871v1 [cs.LG] 22 Jan 2026

3 Inference System Annealing 9

3.1 Motivation and the Choice of Restructuring Units . . . . . . . . . . . . . . . . . 93.2 Statistical Testing of Edge Properties in Simple Neural Networks . . . . . . . . . 10 3.2.1 Initial Population Test (Neyman Significance Test) . . . . . . . . . . . . . 10 3.2.2 Random-Walk Hypothesis Test (Equiprobability Test) . . . . . . . . . . . 11 3.3 Statistical Tests for Edge Properties in Multi-Channel Joint-Operator FNNs . . . 12 3.3.1 Initial Population Test Based on Sequence Norm . . . . . . . . . . . . . . 12 3.3.2 Multi-Channel Consistency Test Based on the Random-Walk Hypothesis 13 3.4 Summary of Annealing Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 

4 Inference System Restructuring 14 

4.1 General Structure of Large Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.2 Permutations, Projections, and Embeddings . . . . . . . . . . . . . . . . . . . . . 16 4.3 Principle of System Restructuring . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.3.1 Structural Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.3.2 Key Properties of the Restructuring Principle . . . . . . . . . . . . . . . . 20 4.4 Further Subdivision of Diagonal Blocks in the Inference System . . . . . . . . . . 21 4.5 Three-Stage Framework for Inference-System Restructuring . . . . . . . . . . . . 22 4.5.1 Validation Stage (Trial Run Stage) . . . . . . . . . . . . . . . . . . . . . . 22 4.5.2 Engineering Recommendations for the Production Stage . . . . . . . . . 22 4.5.3 Upgradable Learning after Restructuring . . . . . . . . . . . . . . . . . . 23 4.6 Construction of the Permutation . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 

5 Conclusion 24 A Appendix A: Matrix-Based Algorithm for Computing the Permutation 25 

A.1 Artificial Neural Networks and Simple Directed Graphs . . . . . . . . . . . . . . 26 A.2 Mathematical Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 A.3 Algorithm Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 A.3.1 Node Attribute Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 A.3.2 Algorithmic Principle and Steps . . . . . . . . . . . . . . . . . . . . . . . 30 A.4 Algorithm Description: Procedural Pseudocode . . . . . . . . . . . . . . . . . . . 32 A.5 Algorithm Execution Results and Structural Verification . . . . . . . . . . . . . . 34 

# 1 Introduction 

In recent years, large-scale artificial neural networksâ€”particularly deep learning models and foundation-scale systemsâ€”have demonstrated remarkable performance in perception, reasoning, and generation tasks [1, 2, 3, 4, 5]. At the algorithmic and engineering levels, these systems are typically implemented as com-putations over high-dimensional parameter matrices. After training, the parameter organization inherited from the generative phase is usually retained and directly carried into long-term in-ference and deployment [6, 7, 4, 8, 9]. While this practice remains acceptable for models of moderate scale, it becomes increasingly untenable for very large models, where inference cost and system complexity grow beyond sustainable limits [10, 11, 12, 13]. A substantial body of empirical and engineering evidence indicates that, as model scale continues to expand, trained parameter matrices exhibit pronounced effective sparsity [14, 15, 16, 17, 18, 19]. Only a relatively limited subset of parameter relations receives persistent sample support and plays a structurally effective role during learning and inference [20, 21, 15, 19]. In contrast, a large fraction of parameters remains, after training, statistically close to their initialization noise level [22, 23, 24, 25, 16]. 2However, this sparsity is typically manifested only as numerical attenuation and is rarely consolidated at the structural level [16, 17]. As a result, post-training inference systems continue to be executed, in a structural sense, as monolithic operators, remaining internally unstructured and operationally opaque [4, 7, 26]. A large body of prior work has addressed structural aspects of neural networks. One class of approaches, exemplified by neural architecture search, introduces structural design constraints before or during training to obtain architectures with improved performance [27, 28, 29]. An-other class of methods operates after training, performing pruning, annealing, or compression based on trained parameters [30, 7, 14, 18]. The former addresses which structures should be adopted during model construction, while the latter focuses primarily on numerical reduc-tion and acceleration. Neither treats the posterior structures objectively formed by parameter evolution after training as system-level objects in their own right. From a mechanistic perspective, existing studies indicate that, under overparameterization, learning dynamics do not act uniformly across parameter space, but instead exhibit systematic bias toward solutions with specific structural characteristics [31, 32, 20, 24, 25]. At the same time, interpretability research has shown that, after training, models contain stable functional circuits and organized internal structures [33, 34, 35]. Together, these findings point to a common fact: trained models are not structurally homogeneous systems, but already contain latent organizational structures that go beyond explicit architectural design, yet have not been systematically identified or consolidated. Our analysis shows that such posterior structures do not arise from arbitrary sparsification or numerical degradation, but are induced by the long-term action of the gradient update mecha-nism during learning. In large models, gradient update events are highly localized and selective in parameter space. Consequently, only a subset of parameter relations receives persistent and stable update support, while a substantial number of parameters never undergo effective gra-dient updates and remain statistically indistinguishable from their initialization distribution. As a result, the post-training inference system naturally exhibits a decomposable substructural form .Based on this structural fact, we take the trained parameter matrix itself as the sole object of post-training analysis. Dependencies that are statistically confirmed during learning are explicitly identified and consolidated, while parameter components that never acquire learning support are systematically eliminated. This process does not rely on activation traces or gradient histories, does not modify model functionality or inputâ€“output interfaces, and constitutes a purely posterior structural reorganization. Through this reorganization, an inference system originally executed as a monolithic operator is transformed into a composite system consisting of multiple mutually independent sub-operators, providing a direct and engineering-controllable foundation for structured execution, parallel scheduling, and system-level control of inference complexity. 

We refer to this process as structural consolidation of post-training inference systems in large AI models : the explicit identification, stabilization, and system-level reorganization of dependencies that are statistically confirmed during learning, without altering model seman-tics or functionality. The objective of this work is to establish a complete, verifiable, and engineering-operable theoretical framework for characterizing and realizing the posterior struc-tural independence of post-training inference systems, thereby providing a principled pathway for controlling complexity and scalability in large-scale AI systems. 32 Training Dynamics Analysis 

2.1 Scope and Objects of Study 

This work considers artificial neural networks and their variants, including feedforward neural networks (FNNs) and large-scale deep learning systems built upon them [1, 36]. Although these models differ substantially in architectural design and engineering realization, they share a common structural characteristic: their core computational mechanisms can be uniformly represented as collections of operators parameterized by matrices. Within this abstraction, artificial neurons are treated as state variables, connections between neurons are represented by matrix entries, and each parameter quantifies the strength and direction of influence from an upstream state to a downstream state [36]. This representation allows learning dynamics, structural dependency, and operator organization to be analyzed directly at the matrix level, independently of specific implementation details. Dimensional expansion mechanisms commonly used in feedforward networks do not intro-duce fundamentally new operator types at the structural level. As a representative example, consider the widely adopted feedforward block with dimensionality d â†’ 4d â†’ d in modern deep learning models [6]. Its linear component can be equivalently rewritten as four parallel d Ã— d

channel-wise sub-operators arranged in series: 

W1 =

ï£«ï£¬ï£¬ï£¬ï£­

W (1) 1

W (2) 1

W (3) 1

W (4) 1

ï£¶ï£·ï£·ï£·ï£¸ , W2 =  W (1) 2 , W (2) 2 , W (3) 2 , W (4) 2

, (2.1) where each pair  W (g)1 , W (g)2

 defines an independent d â†’ d linear channel. Within each channel, an element-wise nonlinearity Ïƒ(Â·) is applied, and the channel outputs are linearly aggregated, yielding the overall mapping 

Y =

> 4

X

> g=1

W (g)2 Ïƒ



W (g)1 X



. (2.2) From the perspective of structural dependency and modular organization, this formulation is strictly equivalent to the original d â†’ 4d â†’ d feedforward architecture [36]. Under this equivalent representation, a directed edge ( j â†’ i) no longer corresponds to a single scalar parameter. Instead, it is associated with a finite parameter sequence spanning multiple channels: 

wij = W (1) 1 (i, j ), . . . , W (4) 1 (i, j ), W (1) 2 (i, j ), . . . , W (4) 2 (i, j ) . (2.3) Consequently, in a d â†’ 4d â†’ d multi-channel feedforward operator, each structural edge naturally corresponds to a k = 8 dimensional parameter sequence. Based on this abstraction, the structural reorganization problem studied in this work is uniformly formulated on neural systems whose core modules consist of collections of n Ã— n pa-rameter matrices, or on representations that are structurally equivalent in the sense described above. This unified viewpoint allows learning dynamics, structural coupling, and post-training reorganization to be analyzed without loss of generality across a broad class of neural architec-tures. 

2.2 Local Gradient Updates 

In large-scale models, although parameter matrices may formally be dense, only local portions of them are actually involved in computation at any given time. Input vectors typically exhibit 4a pronounced zeroâ€“nonzero structure: during forward propagation, only sub-operators associ-ated with nonzero components are activated; during backpropagation, error signals propagate exclusively along these activated paths. As a consequence, gradient updates occur only on the corresponding parameters. The linear operator W therefore does not participate in training as a globally dense matrix, but is repeatedly engaged through different local substructures. We analyze, from the perspective of training dynamics, under what conditions a parameter 

wij âˆˆ W is actually reached by the forwardâ€“backward propagation induced by training samples. Let the forward propagation be given by 

Y = W X. 

Under gradient descent, the parameter update dynamics admit the matrix form [36, 37] 

W (t + 1) = W (t) âˆ’ Î· âˆ†W (t) âˆ†W (t) = âˆ‚L âˆ‚W (t) =

 âˆ‚L âˆ‚Y (t)



XT (t).

Formally, âˆ† W (t) has the same dimension as W . However, in large models, only a subset of parameters actually participates in the update. This can be expressed as 

W (t + 1) = W (t) âˆ’ Î·  M (t) âˆ§ âˆ†W (t), (2.4) where M (t) = ( mij ) âˆˆ { 0, 1}nÃ—n is a binary mask matrix, and âˆ§ denotes elementwise gating: a parameter wij is updated if and only if mij = 1. Define the locally activated state set 

S(t) = { s âˆˆ { 1, . . . , n } | xs(t)Ì¸ = 0 } (2.5) associated with the t-th training sample. The local update mask is then given by 

mij =

(

1, i âˆˆ S(t) âˆ§ j âˆˆ S(t),

0, otherwise . (2.6) Equation (2.4) thus formalizes the rule of local gradient updates [37, 36, 38]. It is important to emphasize that local gradient updates are not an engineered approxima-tion, but a consequence jointly determined by model scale and task structure. Large models encode a wide variety of semantic categories within a single parameter system, whereas each individual training sample typically activates only one category, or at most a small number of closely related ones. As a result, for any given training instance, a substantial fraction of states remains inactive, and the input vector X(t) is sparse at the component level. Under such conditions, backpropagated gradients are necessarily confined to the subset S(t) activated by the current sample. From (2.4), (2.5), and (2.6), it follows that each gradient update event is restricted to the submatrix of W indexed by S(t) in both rows and columns. This leads to the following fundamental question: over the entire training process, does a given parameter (edge) wij in W

ever experience a gradient update event? The local update rule yields the following result. 

Lemma 2.1. Let wij and wji be parameters (edges) in the matrix W . If gradient updates of the inference system Y = W X follow the local update rule (2.4) , then a necessary and sufficient condition for wij or wji to experience a gradient update event induced by training samples is 

âˆƒ t [{i, j } âŠ‚ S(t)] â‡” âˆƒ t [xi(t)Ì¸ = 0 âˆ§ xj (t)Ì¸ = 0] .

Here S(t) denotes the activated state set during the t-th inference, and xi(t), x j (t) are the cor-responding components of the input vector X(t).

5This lemma characterizes the existence of gradient update events only ; it makes no claim about the magnitude or direction of the updates. Even if the computed gradient value is zero, the event is still counted as a gradient update occurrence. We deliberately distinguish the occurrence of a gradient update event from the magnitude or direction of the update. Even a zero-valued gradient constitutes an update event in the sense of structural exposure. Define 

WG = {wij | âˆƒ t [xi(t)Ì¸ = 0 âˆ§ xj (t)Ì¸ = 0] }

as the set of parameters in W that experience at least one gradient update event during training, and let 

WG = {wij | âˆ€ t [xi(t) = 0 âˆ¨ xj (t) = 0] }

denote its complement. For any wij âˆˆ WG, the parameter neither participates in training-sample-induced forward inference nor is ever reached by backpropagated gradients. Its value therefore remains statistically close to the initialization distribution. If such parameters are left unprocessed and directly carried into post-training deployment, they effectively constitute background noise in the inference system. When the deployment input distribution is highly consistent with the training samples, these parameters behave as long-dormant inactive components. However, under distributional mis-match, they may re-enter inference computation and interfere with outputs in an uncontrolled manner. The size of WG is primarily governed by two factors: (1) the degree of non-overlap among semantic categories represented within the model; and (2) the selection strategy and granularity of training samples. As model scale increases, encompassing more diverse content types with stronger inter-category separation, the size of WG typically grows accordingly. 

2.3 Model Decomposability 

To more fully characterize the training-dynamical phenomena revealed by Lemma 2.1 and to analyze their structural consequences, we introduce the following relational framework. The purpose of this section is to show that the locality of gradient updates induces an intrinsic equivalence relation on the state space, which uniquely determines the decomposable structure of the post-training inference system. Let 

S = {1, . . . , n }

denote the set of system states. 

Definition 2.1. Let 

S(t) = { i âˆˆ S | xi(t)Ì¸ = 0 }

denote the set of states activated by the t-th training sample. 

The co-occurrence relation O on S is defined by (i, j ) âˆˆ O â‡â‡’ âˆƒ t such that {i, j } âŠ‚ S(t).

Definition 2.2. Let O be the co-occurrence relation on S. The co-occurrence coupling relation C on S is defined as the relation satisfying (1) (i, j ) âˆˆ O â‡’ (i, j ) âˆˆ C ,

(2) (i, k ) âˆˆ C âˆ§ (k, j ) âˆˆ C â‡’ (i, j ) âˆˆ C .

6Lemma 2.2. Let C be the co-occurrence coupling relation on S. Then C is an equivalence relation on S. Consequently, C induces a partition of S:

Â¯S = {S1, . . . , S k}, S =

> k

[

> p=1

Sp, pÌ¸ = q â‡’ Sp âˆ© Sq = âˆ…. (2.7) 

Proof. Consider the co-occurrence relation O on S. By Definition 2.1, (i, j ) âˆˆ O â‡â‡’ âˆƒ t such that {i, j } âŠ‚ S(t).

It follows directly that O is reflexive and symmetric. These properties are inherited by C via condition (1) in Definition 2.2, while condition (2) ensures transitivity. Therefore, C is an equivalence relation on S, and hence induces the partition (2.7). Based on the partition (2.7), we define a permutation that re-indexes the states so that the elements within each block Sp become contiguous. Let rp â‰œ |Sp| and Pkp=1 rp = n. Define the permutation P by 

P =

 1 Â· Â· Â· r1 r1 + 1 Â· Â· Â· r1 + r2 Â· Â· Â· r1 + Â· Â· Â· + rkâˆ’1 + 1 Â· Â· Â· nS1(1) Â· Â· Â· S1(r1) S2(1) Â· Â· Â· S2(r2) Â· Â· Â· Sk(1) Â· Â· Â· Sk(rk)



,

where Sp(i) denotes the i-th element of the set Sp under some fixed ordering. Under this re-indexing, the original inference system admits an equivalent representation. Applying the permutation to the input and output vectors and reordering the parameter ac-cordingly, we obtain (P Y ) = ( P W P âŠ¤)( P X ),

where readers unfamiliar with permutation notation may temporarily regard P as the corre-sponding permutation matrix. The reordered parameter matrix takes the block form 

P W P âŠ¤ =

ï£«ï£¬ï£¬ï£¬ï£­

ËœW11 ËœW12 Â· Â· Â· ËœW1k

ËœW21 ËœW22 Â· Â· Â· ËœW2k

... ... . . . ...ËœWk1 ËœWk2 Â· Â· Â· ËœWkk 

ï£¶ï£·ï£·ï£·ï£¸ .

(P Y ) = ( P W P âŠ¤)( P X ) â‰œ

ï£«ï£¬ï£¬ï£¬ï£­

ËœY1

ËœY2

...ËœYk

ï£¶ï£·ï£·ï£·ï£¸ =

ï£«ï£¬ï£¬ï£¬ï£­

ËœW11 ËœW12 Â· Â· Â· ËœW1k

ËœW21 ËœW22 Â· Â· Â· ËœW2k

... ... . . . ...ËœWk1 ËœWk2 Â· Â· Â· ËœWkk 

ï£¶ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£­

ËœX1

ËœX2

...ËœXk

ï£¶ï£·ï£·ï£·ï£¸ . (2.8) 

Theorem 2.1. Suppose that the gradient updates of the inference system Y = W X follow the local update rule given in (2.4) . Then, in the equivalent representation (2.8) , all parameters (edges) contained in the off-diagonal blocks 

ËœWpq , p, q = 1 , . . . , k, pÌ¸ = q, 

never experience any gradient update event induced by the training samples. Consequently, the permuted parameter matrix admits the form 

(P W P âŠ¤) = 

ï£«ï£¬ï£¬ï£¬ï£­

ËœW11 Ï†0

ËœW22 

. . .

Ï†0 ËœWkk 

ï£¶ï£·ï£·ï£·ï£¸ , (2.9) 

where Ï†0 denotes the initialization distribution of parameters. In particular, for any wij âˆˆ ËœWpq 

with pÌ¸ = q, the parameter wij remains distributed according to Ï†0 throughout training. 

7Proof. Let pÌ¸ = q, and consider the parameter wij . The row index i âˆˆ Sp, that is, i corresponds to a row belonging to the p-th diagonal block of the matrix P W P âŠ¤, and the column index 

j âˆˆ Sq, that is, j corresponds to a column belonging to the q-th diagonal block of P W P âŠ¤.Assume that a gradient update event has occurred on wij . By Lemma 2.1, 

âˆƒ t [ i, j âˆˆ S(t) ] .

By Definition 2.1, this implies ( i, j ) âˆˆ O . Furthermore, by Definition 2.2, we obtain ( i, j ) âˆˆ C .Hence, i and j must belong to the same subset in the partition (2.7), which implies p = q,contradicting the assumption. In other words, the block-diagonal structure in (2.9) is not a numerical sparsification result, but a structural invariant imposed by training dynamics. 

Theorem 2.2. Under the same assumptions as Theorem 2.1, each diagonal block ËœWpp in (2.9) 

cannot be further decomposed into a block-diagonal form. That is, for any rowâ€“column permu-tation Q, if 

Q ËœWpp QâŠ¤ =

 Ë†Wp1p1 Ë†Wp1p2

Ë†Wp2p1 Ë†Wp2p2



,

then the off-diagonal blocks Ë†Wp1p2 or Ë†Wp2p1 must contain a parameter wij that has experienced a gradient update event during training. Proof. Suppose otherwise. By Lemma 2.1, this is equivalent to 

Â¬âˆƒ t [ i, j âˆˆ S(t) ] â‡” âˆ€ t [ i / âˆˆ S(t) âˆ¨ j / âˆˆ S(t) ] .

Therefore, by Definition 2.1, 

âˆ€ i âˆˆ Sp1 âˆ€ j âˆˆ Sp2 , (i, j ) /âˆˆ O .

Furthermore, by Definition 2.2, states in Sp1 and states in Sp2 neither form a co-occurrence re-lation nor can they be connected through transitivity to form a co-occurrence-coupling relation. Hence, ( i, j ) /âˆˆ C , which contradicts the assumption that {i, j } âŠ‚ Sp.

Remark 2.1. Theorem 2.1 and Theorem 2.2 characterize the structural distribution of gradient update events in parameter space during training .(1) Concerning equation (2.9): if the activated state sets Sp and Sq are disjoint, then the parameter submatrix W [Sp Ã— Sq] cannot experience any gradient update events under the local gradient update rule. Equation (2.9) merely makes such parameters explicit as off-diagonal blocks through row and column permutations. Therefore, all parameters located in off-diagonal blocks correspond exactly to the set of parameters that never participate in any gradient update event throughout the entire training process .(2) Concerning Theorem 2.2: each diagonal block ËœWpp is already a minimal update-closed structure. Any attempt to further decompose it into smaller diagonal blocks would necessarily require assigning a subset of parameters that have experienced gradient update events to positions outside the block. It should be emphasized that a diagonal block may still contain parameters that never undergo gradient updates; what Theorem 2.2 asserts is that without sacrificing any parameter that has experienced an update event, a diagonal block cannot be further block-diagonalized .

Remark 2.2. The co-occurrence relation O and the co-occurrence-coupling relation C admit an equivalent graph-theoretic interpretation: O defines an undirected graph whose connected components are exactly the equivalence classes induced by C. Under this correspondence, each diagonal block in (2.9) corresponds to one connected component of the co-occurrence graph. 8From a systems perspective, this analysis shows that post-training inference in large-scale neural networks admits an explicit operator-level decomposition: a trained model should no longer be regarded as a uniformly coupled monolithic operator, but as a composite inference system consisting of multiple mutually independent sub-operators, with direct implications for structured execution, parallel scheduling, and controllable inference complexity. 

2.4 Implications for the Further Development of Large Models 

The above results indicate that the continued growth of model scale does not inherently imply an uncontrollable increase in the complexity of a single monolithic operator. Rather, it manifests primarily as an increase in the number of structurally independent sub-operators and in the complexity of their composition. From this perspective, the fundamental bottleneck faced by large models lies not in parameter count itself, but in the manner in which structure is organized and executed at the system level. In light of the current trajectory of large-model development, this analysis provides a direct theoretical basis and a concrete feasibility guarantee for post-training system restructuring and structural consolidation. Since a substantial fraction of parameters never receive structural confirmation during training, continuing to treat them as active components of the inference system not only introduces unnecessary computational overhead, but may also lead to uncon-trolled interference under distribution-shifted inputs. By contrast, extracting, freezing, and reorganizing trained models according to the block structure revealed by training dynamics does not eliminate any dependencies that have been confirmed by learning. On the contrary, it improves system stability, interpretability, and operational efficiency. More importantly, these conclusions point toward a clear direction for the next stage of large-model development: the critical bottleneck no longer lies in indiscriminately increasing parameter counts, but in identifying, consolidating, and exploiting the structures that naturally emerge during training. Whether in modular execution and parallel scheduling during infer-ence, or in blockwise and partially independent optimization during training, structural analysis should no longer be regarded as an auxiliary engineering technique. Instead, it should be treated as a central theoretical foundation for the architecture, execution, and long-term evolution of large-scale model systems. Based on this understanding, the remainder of this work presents a post-training treatment framework for existing models. Through systematic restructuring and structural consolida-tion, the effective structures implicitly formed during learning are made explicit and stabilized, thereby providing a sustainable structural foundation for the next generation of inference sys-tems. 

# 3 Inference System Annealing 

3.1 Motivation and the Choice of Restructuring Units 

The previous section has shown, from the perspective of learning dynamics, that the parameter matrices of trained neural networks are not structurally uniform. Instead, co-activation relations induced by training samples generate equivalence classes in the state space, which correspond, under appropriate row and column permutations, to a block-diagonal structure of the parameter matrix. Based on this result, the structural restructuring discussed in this section does not aim to redesign neural networks or to impose externally prescribed architectures. Rather, it adopts a strictly post-training perspective and focuses on identifying, extracting, and consolidating 

structural dependencies that have already formed during learning but have not yet been made explicit . The objective of restructuring is not to introduce new structural assumptions, but to 9elevate these dependencies from numerical manifestations at the parameter level to stable and discrete structural objects at the system level. This section addresses the problem of post-training structural annealing of inference systems, and explicitly adopts edge-wise annealing as the fundamental principle. Here, an â€œedgeâ€ refers to a directed interaction between artificial neurons, as represented by entries in the parameter matrix. For clarity of exposition, we first consider the simplest setting in which each edge is associated with a scalar parameter. This setting covers single-layer, single-channel, single-operator neural networks. We then extend the discussion to more general cases involving multi-layer, multi-channel, and jointly parameterized operators, with multi-layer multi-channel FNNs serving as a representative example. 

3.2 Statistical Testing of Edge Properties in Simple Neural Networks 

3.2.1 Initial Population Test (Neyman Significance Test) 

This subsection considers the simplest class of neural networks, namely single-layer, single-channel, single-operator models, in which each edge corresponds to a scalar parameter. All scalar parameters in the trained weight matrix are treated as a sample population 

W = {wij }.

Structural annealing can then be naturally formulated as a hypothesis testing problem in the Neyman significance framework: for each parameter wij , we test whether it significantly deviates from the null hypothesis of no structural effect .During the model generation phase, parameters are typically initialized by independent sampling from a prescribed distribution f0(w) (e.g., a zero-mean symmetric distribution). This distribution characterizes the random level that parameters would exhibit in the absence of any learning effect , and therefore serves as a natural theoretical null model for structural testing. 

Null hypothesis ( H0): The parameter wij is drawn from the initialization distribution f0(w), reflecting only random initialization or noise perturbations, and does not correspond to a struc-turally confirmed dependency induced by learning. 

Alternative hypothesis ( H1): The parameter wij deviates significantly from the initialization distribution, indicating that the learning process has established a stable structural effect along the corresponding direction. Given a significance level x% (0 < x â‰ª 100), a rejection region is determined by f0(w). For example, using a two-sided tail criterion, one may define 

|wij | â‰¥ cx, Pf0 (|w| â‰¥ cx) = x%.

Neyman tail criterion: If wij falls within the rejection region, the null hypothesis H0 is rejected, and the corresponding edge j â†’ i is regarded as a structurally significant and effective edge. Otherwise, H0 is accepted, and the edge is considered not to have been statistically confirmed by the learning process; such an edge may therefore be removed at the structural level. This test distinguishes which edges have been significantly modified by learning and which have not , but does not yet differentiate the source or nature of the modification. The initialization distribution is not treated as a model prior, but as the only accessible empirical baseline for distinguishing learning-confirmed structure from residual randomness in post-training systems. 10 3.2.2 Random-Walk Hypothesis Test (Equiprobability Test) 

After training, the weight matrix W = [ wij ]nÃ—n encodes the influence of state j on state i

through the parameter wij . Such influence may arise either from a structurally confirmed dependency or merely from directionless random perturbations. To characterize the latter case, we introduce a random-walk noise hypothesis : if a state j

does not develop a stable structural preference, its influence on downstream states i should be approximately uniformly distributed. Under this hypothesis, we perform row-wise normalization of the parameter matrix: Ëœwij = |wij |

> n

P

> j=1

|wij |

,

> n

X

> j=1

Ëœwij = 1 .

To examine whether the parameter wij originates from a random-walk perturbation of state j.In the ideal random-walk regime, one expects Ëœwij â‰ˆ 1

n ,

so that no outgoing direction is statistically distinguishable. To convert the random-walk hypothesis into an operational statistical criterion, we first deter-mine an admissible tolerance bandwidth Î´ for equiprobable random perturbations. Let ËœW = { Ëœwij }

denote the population of all normalized parameters. For a candidate bandwidth Î´ > 0, consider the subset ËœW(Î´) = 



Ëœwij âˆˆ ËœW Ëœwij âˆˆ

 1

n âˆ’ Î´, 1

n + Î´

 

.

Under the random-walk null hypothesis, ËœW(Î´) should be statistically consistent with an equiprobable distribution centered at 1 

> n

. Accordingly, within the Neymanâ€“Pearson framework, a goodness-of-fit test may be applied, such as Pearsonâ€™s Ï‡2 test or an equivalent distributional test. The test proceeds by interval expansion. Starting from a small initial bandwidth Î´0, if the null hypothesis cannot be rejected at significance level Î±, the interval is enlarged iteratively, 

Î´k+1 = Î´k + Ï„, 

and the test is repeated. When the null hypothesis is rejected for the first time, the previous accepted bandwidth Î´k is taken as the maximal admissible tolerance for equiprobable random-walk noise, denoted by Î´.Under this tolerance bandwidth Î´, the normalized parameters are classified into three categories: (1) Ëœwij > 1 

> n

+ Î´ (structural preference) ,

(2) Ëœwij âˆˆ  1 

> n

âˆ’ Î´, 1 

> n

+ Î´ (random-walk noise) ,

(3) Ëœwij < 1 

> n

âˆ’ Î´ (systematic suppression) .

The purpose of this test is not to compare parameter magnitudes, but to distinguish the ori-gin and nature of parameter variations: whether they reflect structural preference, directionless random perturbation, or directions that are systematically suppressed by the learning process. 11 3.3 Statistical Tests for Edge Properties in Multi-Channel Joint-Operator FNNs 

In multi-layer, multi-channel feedforward neural networks (FNNs) with jointly parameterized operators, a single directed edge no longer corresponds to a scalar parameter. Instead, it is represented by a finite-dimensional parameter sequence composed of multiple channels and operator components. Accordingly, structural annealing and statistical validation must take the parameter sequence , rather than an individual parameter, as the basic object of analysis. According to (2.3), for a standard d â†’ 4d â†’ d FNN, the parameters associated with an edge ( j â†’ i) form the sequence 

wij = W (1) 1 (i, j ), . . . , W (4) 1 (i, j ), W (1) 2 (i, j ), . . . , W (4) 2 (i, j ) ,

which we denote by 

wij = {x1, . . . , x 8}ij .

Therefore, in a d â†’ 4d â†’ d multi-channel joint-operator FNN, each directed edge naturally corresponds to a k = 8 dimensional parameter sequence. 

3.3.1 Initial Population Test Based on Sequence Norm 

As the first step of structural annealing, it is necessary to determine whether the parameter sequence wij as a whole significantly deviates from the random level induced by initialization. To this end, we regard wij as a vector in R8 and adopt its Euclidean norm 

âˆ¥wij âˆ¥2 =

ï£«ï£­

> 8

X

> g=1

x2

> g

ï£¶ï£¸

> 1/2

as a test statistic that measures the overall participation strength of the edge. Under the null hypothesis H0, the components of the parameter sequence are assumed to be independently drawn from the initialization distribution, which in typical settings can be approximated by 

xg âˆ¼ N (0 , Ïƒ 2), g = 1 , . . . , 8.

It then follows that 1

Ïƒ2 âˆ¥wij âˆ¥22 âˆ¼ Ï‡28.

Given a significance level Î±, a threshold cÎ± is determined from the Ï‡28 distribution, leading to the following annealing rule: 

â€¢ If âˆ¥wij âˆ¥22 â‰¥ cÎ±, the null hypothesis is rejected, and the edge ( j â†’ i) is deemed to have deviated significantly from the random initialization level in an aggregate sense, and is therefore passed to subsequent structural tests; 

â€¢ If âˆ¥wij âˆ¥22 < c Î±, the edge is regarded as having failed to form a stable overall participation, and the corresponding parameter sequence may be directly removed during structural restructuring. This test addresses the question: whether an edge has been significantly activated by the learning process at the aggregate level . It does not, however, distinguish the directionality, consistency, or structural preference of such activation, which will be examined in subsequent tests. 12 3.3.2 Multi-Channel Consistency Test Based on the Random-Walk Hypothesis 

For edges that pass the initial population test, it is necessary to further determine whether their multi-channel effects still primarily reflect directionless random perturbations, or whether a stable and consistent structural preference has already emerged across channels. To this end, each channel operator is normalized separately, and all normalized parameters are merged into a global parameter set Wall . The sequence {x1, . . . , x 8}ij is then regarded as eight parallel components induced by the joint-operator structure under the same inference activation. Under the random-walk null hypothesis, if the edge ( j â†’ i) has not formed a structural pref-erence, the normalized influence strength of each channel component should not be statistically distinguishable from the equiprobable level 1 

> n

.Using the random-walk goodness-of-fit procedure introduced in the previous subsection, an admissible random-walk tolerance bandwidth Î´ can be estimated from the population Wall .Based on this bandwidth, we define the counting statistic 

Kij =

> 8

X

> g=1

1



xg âˆˆ

 1

n âˆ’ Î´, 1

n + Î´

 

.

Here, Wall denotes the population of all normalized parameters across all channels and all edges, and we define 

p0 = #{x âˆˆ W all : x âˆˆ [ 1 

> n

âˆ’ Î´, 1 

> n

+ Î´]}

#Wall 

.

The symbol # denotes the cardinality of a set. Under the random-walk null hypothesis, p0 can be interpreted as the probability that a single channel component behaves as random-walk noise. Consequently, the statistic Kij follows 

Kij âˆ¼ Binomial(8 , p 0).

Given a significance level Î±, if the observed value of Kij is significantly small, this indicates that the majority of channel components have systematically deviated from the random-walk regime. The random-walk null hypothesis is then rejected, and the edge is deemed to have formed a stable and consistent structural dependency in the multi-channel sense. Conversely, if 

Kij falls within the acceptance region, the multi-channel effect of the edge is regarded as still dominated by random perturbations, and the edge may be treated as a candidate for annealing during structural restructuring. 

3.4 Summary of Annealing Rules 

In summary, the structural annealing framework proposed in this work follows a unified and transparent principle: edges are taken as the basic annealing units, statistical significance serves as the decision criterion, and the objective is to elevate effective dependencies implicitly formed during learning from the parameter level to stable structural objects at the system level. 

In simple neural networks with a single layer, single channel, and single operator, each edge corresponds to a scalar parameter. Structural annealing can therefore be directly formulated as a Neyman-style significance test based on the parameter initialization distribution. This test determines whether a parameter has significantly deviated from the random level associated with â€œno structural effect,â€ thereby naturally partitioning edges into those confirmed by learning and those that are structurally removable. However, parameter deviation from the initialization distribution alone is insufficient to characterize the nature of parameter changes. To address this limitation, we further introduce a random-walk noise hypothesis, under which parameter variations are normalized and tested 13 for equiprobable, directionless behavior. The purpose of this test is not to compare parame-ter magnitudes, but to distinguish three fundamentally different cases: structural preferences formed by learning, statistically indistinguishable random perturbations, and dependency di-rections that are systematically suppressed by the learning process. As a result, structural annealing is no longer equivalent to numerical pruning, but becomes a statistically grounded procedure for structural extraction. In complex networks with multiple layers, multiple channels, and jointly parameterized operators, a single edge no longer corresponds to a scalar, but to a finite-dimensional parameter sequence. Such edges are treated as joint structural objects spanning channels and operators. Accordingly, two complementary statistical criteria are introduced: one to determine whether the parameter sequence has participated in the learning process as a whole, and another to assess whether its variation constitutes a stable and directionally consistent structural dependency. It is important to emphasize that these two criteria are complementary rather than substi-tutive. The former answers the question â€œwhether the edge has been significantly altered by learning,â€ while the latter addresses â€œwhether such alteration represents a genuine structural dependency.â€ Only edges that pass both tests are regarded as valid structural objects to be retained during the restructuring stage. From a methodological perspective, the proposed annealing strategy differs fundamentally from heuristic pruning based on parameter magnitude, as well as from sparsification methods that introduce regularization during training. Instead, it constitutes a post-training structural restructuring mechanism. This mechanism does not alter the modelâ€™s inputâ€“output interface, does not impose new structural assumptions, and relies strictly on statistical distinctions pro-duced by the learning process itself, thereby providing a stable and interpretable structural foundation for subsequent system restructuring, modular execution, and structural evolution. 

# 4 Inference System Restructuring 

4.1 General Structure of Large Matrices 

Figure 4.1: Schematic structure of a large matrix 14 There exists a one-to-one correspondence between matrices and graphs: any nÃ—n square matrix can be viewed both as a linear operator and as a graph that encodes its structural properties via an associated graph. Graph-theoretic tools provide an intuitive and powerful language for analyzing large sparse matrices. For general matrices, their structural form is particularly transparent under graph representations. 

Definition 4.1. Let M = [ mij ] be an n Ã— n square matrix. A directed graph 

G = âŸ¨V = {1, . . . , n }, E âŠ‚ V Ã— V âŸ©

is called the associated graph of M if (i, j ) âˆˆ E â‡â‡’ mij Ì¸ = 0 .

From a graph-theoretic perspective, any general matrix admits a natural hierarchical struc-ture under suitable row and column permutations: an outer block-diagonal structure, and an inner triangular block structure within each diagonal block. The outer diagonal blocks corre-spond to the maximal weakly connected components of the associated graph, while the inner irreducible diagonal blocks correspond to maximal strongly connected components (SCCs). In classical linear algebra discussions, one often encounters only degenerate cases of this struc-ture, such as the presence of a single weakly connected component, or diagonal blocks that are themselves irreducible. In the context of this work, the parameter matrix W is typically close to a full matrix prior to annealing. Edge-removal operations replace parameters satisfying the annealing criteria by zeros, thereby transforming W into a general sparse matrix. This structure can be made explicit through suitable row and column permutations (equivalently, by reindexing the nodes of the associated graph). Such permutations do not alter the algebraic meaning of the matrix, nor do they change its action as a linear operator: if P denotes a permutation (which may be represented by a permutation matrix), then W and P W P âŠ¤ represent the same operator under different state orderings. 

(1) Outer structure: block-diagonal decomposition 

There exists a permutation P such that 

P W P âŠ¤ =

ï£«ï£¬ï£¬ï£¬ï£­

W 1

. . .

W m

0

ï£¶ï£·ï£·ï£·ï£¸ , (4.1) where W r (r = 1 , . . . , m ) are square matrices, and the zero block in the lower-right corner cor-responds to isolated nodes. In operator terms, the blocks W r constitute mutually independent sub-operators, with no cross-block coupling. If W is regarded as the parameter matrix of a neural network 

N = âŸ¨V, E âŠ‚ V Ã— V, W âŸ©,

then each diagonal block W r corresponds to a maximal weakly connected subnetwork 

Nr = âŸ¨Vr, E r, W râŸ©. (4.2) After annealing, the original network decomposes into several mutually disconnected weakly connected subnetworks; via row and column permutations, their parameter matrices appear as diagonal blocks in P W P âŠ¤.Comparing with (2.9), one observes that if annealing is performed thoroughly, the diagonal blocks W r in (4.1) may be viewed as submatrices obtained by further edge removal within 15 some ËœWpp . This is because (2.9) provides only the coarsest structural partition based on the existence of gradient update events, whereas the annealing process also removes suppressive edges that, although having experienced update events, do not exhibit statistically significant deviation from the initialization distribution, which may cause further fragmentation of the original diagonal blocks. 

(2) Inner structure: Frobenius normal form 

For each outer diagonal block W r, there exists a permutation Pr such that it can be trans-formed into Frobenius normal form: 

PrW rP âŠ¤ 

> r

=

ï£«ï£¬ï£­

W r

> 11

... . . .

W rpr 1 Â· Â· Â· W rpr pr

ï£¶ï£·ï£¸ , (4.3) where each diagonal block W rqq is an irreducible square matrix. If W r itself is irreducible, the decomposition degenerates to 

W r = W r

> 11

. (4.4) From a graph-theoretic viewpoint, the irreducible diagonal blocks in (4.3) correspond to the maximal strongly connected components of the subnetwork Nr, while the nonzero lower-triangular blocks encode directed dependencies among these components. The condensation graph obtained by contracting each SCC into a single node is necessarily a directed acyclic graph (DAG), which is precisely the graph-theoretic meaning of the Frobenius structure. It should be noted that, although Lemma 2.1 shows that gradient update events occur symmetrically on edges ( i, j ) and ( j, i ), symmetric update events do not necessarily result in symmetric parameter values. After training and annealing, parameters in some directions may significantly deviate from the initialization distribution and thus be retained, while parameters in other directions may be suppressed and removed during annealing. This numerical asymmetry ultimately breaks symmetry at the structural level, leading to the asymmetric form shown in (4.3). After learning, the inference system of an artificial neural network typically undergoes sub-stantial changes both numerically and structurally. A large number of connections gradually decay or vanish during training and annealing, allowing structural patterns that were previously implicit in the full parameter matrix to become explicit. This section addresses how to restruc-ture the inference system without altering its functional behavior, so as to obtain an equivalent inference structure that is both engineering-feasible and theoretically analyzable. 

4.2 Permutations, Projections, and Embeddings 

To characterize the structural mapping between the original system and the reorganized system, we introduce three classes of mapping operators: permutations , projections , and embeddings . It should be emphasized that, throughout this paper, a â€œpermutationâ€ always refers to a bijection on a finite index set, in accordance with the standard definition in permutation group theory. A permutation matrix is used only as a possible representation, and should not be identified with the permutation itself [48]. The permutation P appearing in Fig. 4.1 may be interpreted as a permutation matrix, which is the most common way to represent permutations in linear algebra. However, the primitive meaning of a permutation acting on vectors or matrices is a unary bijective function on finite index sets, that is, a mapping of row/column orderings (note that a unary function can be regarded as a unary operator): 

P âŸ¨Order target , Order source âŸ© Order target = P (Order source ).

16 Since the permutation is a finite function, it can be represented explicitly by the following value table: 

P = âŸ¨o, s âŸ© =

ï£«ï£¬ï£­

1 s1

... ...

n sn

ï£¶ï£·ï£¸ si âˆˆ { 1, . . . , n } iÌ¸ = j â‡’ siÌ¸ = sj . (4.5) Let X = ( x1, . . . , x n)âŠ¤ and Y = ( y1, . . . , y n)âŠ¤. Then Y = P X corresponds to 

Y (o) = X(s) â‡” Y (i) = X(si), i = 1 , . . . , n. 

From an implementation perspective, the operation Y = P X requires only n component-wise assignment (reindexing) operations. The permutation P specifies the correspondence between source and target indices, and thus essentially defines a reordering of vector components. Formally, (4.5) resembles an indexing mechanism in database systems. While database indices usually represent one-to-many mappings between non-primary and primary keys and have a broader semantic scope, both share a common feature: they are finite functions that can be defined at the application level and used as operators. We adopt the convention that P in (4.5) denotes a row permutation (row-order trans-formation), acting on matrices and column vectors, while P âŠ¤ denotes the corresponding col-umn permutation acting on matrices and row vectors. The inverse permutation P âˆ’1 satisfies 

P (P âˆ’1) = ( P âˆ’1)P = In:

P âŠ¤ =

 1 Â· Â· Â· ns1 Â· Â· Â· sn



P âˆ’1 =

ï£«ï£¬ï£­

s1 1... ...

sn n

ï£¶ï£·ï£¸ In =

ï£«ï£¬ï£­

1 1... ...

n n

ï£¶ï£·ï£¸ .

Accordingly, 

P W P âŠ¤ =

ï£«ï£¬ï£­

1 s1

... ...

n sn

ï£¶ï£·ï£¸ W

 1 Â· Â· Â· ns1 Â· Â· Â· sn



corresponds to simultaneously reordering the rows s1, . . . , s n of W into positions 1 , . . . , n and reordering the columns in the same manner. This operation matches the familiar permutation format in linear algebra. However, here permutations replace permutation matrices: P and 

P âŠ¤ act as permutation operators on W , rather than participating in algebraic multiplication as linear operators. Similarly, 

P X and Y P âŠ¤

correspond to reordering the rows of the column vector X and the columns of the row vector 

Y , respectively. In addition to permutations, we require other index-order operators on matrices and vectors: 

projections Ï€i (restriction from higher to lower dimension) and embeddings Î³i (insertion from lower to higher dimension). These form a pair of complementary index operators that are mutually inverse on their domains. 

Ï€i =

ï£«ï£¬ï£­

1 s1

... ...

k sk

ï£¶ï£·ï£¸ Ï€âŠ¤ 

> i

=

 1 Â· Â· Â· ks1 Â· Â· Â· sk



Ï€âˆ’1 

> i

=

ï£«ï£¬ï£­

s1 1... ...

sk k

ï£¶ï£·ï£¸ .

Here {s1, . . . , s k} âŠ‚ { 1, . . . , n } with siÌ¸ = sj for iÌ¸ = j.

B = Ï€iA B = AÏ€ âŠ¤ 

> i

B = Ï€iAÏ€ âŠ¤ 

> i

B = Ï€âˆ’1 

> i

A. 

17 Their actions are interpreted as follows: (1) if A is a column vector or matrix, B = Ï€iA extracts rows s1, . . . , s k into positions 1 , . . . , k ;(2) if A is a row vector or matrix, B = AÏ€ âŠ¤ 

> i

extracts columns s1, . . . , s k;(3) if A is a matrix, B = Ï€iAÏ€ âŠ¤ 

> i

extracts the submatrix indexed by s1, . . . , s k;(4) B = Ï€âˆ’1 

> i

A embeds rows 1 , . . . , k into positions s1, . . . , s k.In this paper, Ï€i is used for input projection. Although Ï€âˆ’1 

> i

provides a corresponding embedding, it is not always suitable for output variables. Therefore, we introduce a dedicated embedding operator Î³i:

Î³i =

ï£«ï£¬ï£­

s1 1... ...

sk k

ï£¶ï£·ï£¸ Î³âŠ¤ 

> i

=

 s1 Â· Â· Â· sk

1 Â· Â· Â· k



Î³âˆ’1 

> i

=

ï£«ï£¬ï£­

1 s1

... ...

k sk

ï£¶ï£·ï£¸

Here {s1, . . . , s k} âŠ‚ { 1, . . . , n } with siÌ¸ = sj for iÌ¸ = j.

B = Î³iA B = AÎ³ âŠ¤ 

> i

B = Î³iAÎ³ âŠ¤ 

> i

B = Î³âˆ’1 

> i

A. 

Their actions are interpreted as follows: (1) B = Î³iA embeds rows 1 , . . . , k into positions s1, . . . , s k;(2) B = AÎ³ âŠ¤ 

> i

embeds columns 1 , . . . , k into positions s1, . . . , s k;(3) B = Î³iAÎ³ âŠ¤ 

> i

embeds a submatrix into the corresponding positions; (4) B = Î³âˆ’1 

> i

A performs the inverse projection. 

4.3 Principle of System Restructuring 

4.3.1 Structural Principle 

Figure 4.2: Structural principle of inference system restructuring We begin the discussion of inference system restructuring from the outermost block structure. Figure 4.2 illustrates the fundamental principle of inference system restructuring. Panel (a) depicts the original inference system before restructuring, while panel (b) shows an equivalent reorganized representation that preserves the functional behavior of the original operator. The entire restructuring process consists of three structural stages. 18 (I) Input Decomposition Stage The original input vector XInput is first reordered by a permutation P , resulting in an intermediate representation X. Subsequently, a collection of projection operators 

Ï€1, . . . , Ï€ m, Ï€ m+1 

decomposes X into multiple sub-inputs: 

XInput P

âˆ’â†’

ï£«ï£¬ï£¬ï£¬ï£­

X1

...

Xm

Xm+1 

ï£¶ï£·ï£·ï£·ï£¸

> Ï€1

âˆ’â†’

...

> Ï€m

âˆ’âˆ’â†’

> Ï€m+1

âˆ’âˆ’âˆ’â†’

X1

...

Xm

Xm+1 

The permutation P is precisely the same permutation that transforms the parameter matrix 

W into its block-structured form as shown in Fig. 4.1. Applying P to XInput produces the reordered vector X.Let the operator part of the original inference system be 

YOutput = WÏƒXInput .

Since P is a permutation, we have 

P Y Output = P W ÏƒP âŠ¤(P X Input ).

Define 

X = P X Input , Y = P Y Output .

Then X and Y are respectively the input and output of the equivalent operator P W ÏƒP âŠ¤.Writing 

X = ( X1, . . . , X m, X m+1 )âŠ¤,

each subvector Xi (i = 1 , . . . , m + 1) corresponds directly to the input of an independent sub-operator 

W (i)

> Ïƒ

of P W ÏƒP âŠ¤. After the decomposition of P W ÏƒP âŠ¤, each subvector Xi must be routed to the corresponding parallel sub-operator, and this routing is exactly the role played by Ï€i.This stage involves only index reordering and subset mapping of input variables. No numer-ical computation is performed. The original input interface and the semantic meaning of state variables are fully preserved. 

(II) Parallel Sub-Operator Stage Each sub-input Xi is processed independently by its corresponding sub-operator W (i) 

> Ïƒ

:

Yi = W (i) 

> Ïƒ

Xi, i = 1 , . . . , m. 

These sub-operators are structurally independent, corresponding to independent sub-operators of the parameter matrix, and can therefore be executed in parallel at the computational level. 

(III) Output Embedding and Inverse Permutation Stage The outputs Y1, . . . , Y m, Y m+1 

are embedded into the reordered output space Y via a collection of pairwise disjoint embedding operators 

Î³1, . . . , Î³ m, Î³ m+1 iÌ¸ = j â‡’ Im( Î³i) âˆ© Im( Î³j ) = âˆ…. (4.6) 19 The combined output is then mapped back to the original coordinate system by the inverse permutation P âˆ’1, yielding the final output YOutput :

Y1

> Î³1

âˆ’â†’

...

YmÎ³m

âˆ’âˆ’â†’

Ym+1 

> Î³m+1

âˆ’âˆ’âˆ’â†’

ï£«ï£¬ï£¬ï£¬ï£­

Y1

...

Ym

Ym+1 

ï£¶ï£·ï£·ï£·ï£¸ 

> Pâˆ’1

âˆ’âˆ’âˆ’â†’ YOutput 

Analogous to the input reordering and decomposition, each independent sub-operator output 

Yi is first embedded by Î³i into the appropriate output coordinates of the equivalent operator 

P W ÏƒP âŠ¤, that is, into Y = P Y Output . The original system output is then recovered via 

YOutput = P âˆ’1Y. 

This stage involves only the aggregation of output components and a permutation of output indices. No numerical computation is performed. The original output interface and state semantics remain unchanged. 

Dormant Neurons After annealing and structural restructuring, the directed graph associ-ated with the parameter matrix may contain weakly connected subgraphs consisting of a single node (isolated vertices). In standard feedforward neural networks (FNNs), self-loop connections are typically absent, i.e., wii = 0. Consequently, such isolated nodes correspond, in the matrix representation, to rows and columns that are entirely zero. From an operator-theoretic perspective, neurons of this type exhibit no effective response to any input, and their output is identically the zero mapping. More importantly, even in the sense of operator superposition, their corresponding terms do not participate in the overall system mapping. We refer to such structures as dormant neurons .In the restructured system, all dormant neurons are collectively gathered into a single zero-operator channel: 

Ym+1 = F (Xm+1 ) â‰¡ 0.

At this stage, we do not recommend immediately removing this channel. Instead, it is explicitly retained as a placeholder module: on the one hand, this preserves the consistency and reversibil-ity of state variables; on the other hand, the additional redundancy it introduces is only of order 

O(n) and is therefore negligible in practical implementations. 

4.3.2 Key Properties of the Restructuring Principle (1) Functional Equivalence. Structural restructuring introduces no new inference capa-bility and does not alter the overall operator functionality. Under an appropriate permutation 

P , the systems before and after restructuring are strictly equivalent in the operator sense: for any given input, they produce exactly identical outputs. 

(2) Invariance of Inputâ€“Output Format. Both the original and the restructured sys-tems operate on the same input vector XInput and produce the same output vector YOutput .The dimensionality, component ordering, and semantic interpretation of state variables remain unchanged. As a result, the restructured system can replace the original one directly, without any modification to external calling interfaces. 20 (3) Parallelism of Permutation, Projection, and Embedding. The permutation op-erator P , projection operators Ï€i, and embedding operators Î³i are implemented in this work as bijective mappings on finite index sets: 

P : {1, . . . , n } â†’ { 1, . . . , n },Ï€i, Î³ i : {1, . . . , k } â†’ { s1, . . . , s k} âŠ‚ { 1, . . . , n }.

Under this implementation, permutation ( P ), projection ( Ï€i), and embedding ( Î³i) amount to index reordering, selective extraction of components, and position-wise insertion, respectively. They involve no multiplication or accumulation operations, and their access and rearrangement cost is O(n) in complexity. Moreover, the projection operators Ï€i (i = 1 , . . . , m + 1) and embedding operators Î³i (i =1, . . . , m + 1) act on mutually disjoint index sets and therefore incur no memory conflicts. In principle, they can be executed fully in parallel. Similarly, the permutation operations 

X = P X Input and YOutput = P âˆ’1Y can also be parallelized at an appropriate granularity. In summary, inference restructuring decomposes a single monolithic operator into inde-pendent sub-operators that can be executed in parallel; the entire process introduces no new computational semantics, does not alter the model interface, and achieves equivalent inference solely through inputâ€“output index mapping. 

4.4 Further Subdivision of Diagonal Blocks in the Inference System 

When a diagonal block remains too large after restructuring and does not match the target hardware granularity (e.g., GPU execution units), it can be further subdivided while preserving structural equivalence. Such subdivision does not need to adopt the finest granularity of â€œone strongly connected component (SCC) per subsystemâ€. Instead, multiple irreducible diagonal blocks may be grouped into a single inference subsystem, allowing a practical trade-off between computational scale and scheduling efficiency. 

Figure 4.3: Principle of diagonal-block subdivision in inference restructuring In Fig. 4.3, âˆ— denotes irreducible diagonal blocks, and # denotes inter-block dependency modules. After subdivision, the resulting inference sub-operators exhibit the following structural properties: 21 (1) The sub-operator W (r) 

> Ïƒ

is no longer required to be square; it may take the form of a general rectangular operator. (2) The input of a sub-operator may depend on state components produced by preceding blocks: 

Yr = W (r) 

> Ïƒ

Xr, Xr =

ï£«ï£¬ï£¬ï£¬ï£­

x1

...

xrâˆ’1

xr

ï£¶ï£·ï£·ï£·ï£¸ .

(3) Due to such intrinsic data dependencies, the construction of Xr via the corresponding pro-jection Ï€r is no longer fully parallelizable in principle. This limitation does not arise from implementation conflicts, but from structural read dependencies and ordering constraints im-posed by the block decomposition itself. In contrast, permutation operators P, P âˆ’1, embedding operators Î³i, and the internal dataflow within each sub-operator retain their inherent parallelism at the theoretical level. 

4.5 Three-Stage Framework for Inference-System Restructuring 

From the perspectives of engineering deployment and system evolution, the restructuring of an inference system is best organized as a staged process: (1) a validation stage; (2) a produc-tion (formal operation) stage; and (3) an upgrade stage (refinement, extension, and continued learning). 

4.5.1 Validation Stage (Trial Run Stage) 

The core objective of the validation stage is to verify the correctness and consistency between the restructured system and the original system. This necessity arises from the fact that the annealed structure depends on statistical significance thresholds: different confidence levels may induce different edge-removal sets and hence different structural decompositions. Therefore, it is essential to conduct comparative testing under representative confidence levels. The validation methodology follows a unified principle: the original system and the restruc-tured system are executed in parallel under identical inputs, and their outputs are compared. Validation data should preferably be drawn from late-stage validation sets used during training; when necessary, unseen data not involved in training may be introduced to assess generaliza-tion effects. The focus of comparison is not strict equality at the level of individual samples or components, but rather the consistency of task-level outputs, whether deviations remain within acceptable bounds, and whether the results vary continuously and interpretably as the confidence level changes. From an engineering perspective, the validation stage may directly adopt the structural principle illustrated in Fig. 4.2: permutation â†’ projection â†’ parallel sub-operators â†’ embed-ding â†’ inverse permutation. This approach maximally isolates the effects of structural change 

from those of scheduling optimization , while preserving full rollback capability. At this stage, maximal parallel efficiency is not required; temporary module idling caused by partial activa-tion is acceptable, since the primary goal is to validate the correctness and controllability of the restructuring process. 

4.5.2 Engineering Recommendations for the Production Stage 

After validation and structural consolidation, the system enters the formal operation stage. The main efficiency challenge at this stage stems from the locality of inference activation: in any given inference request, only a subset of sub-modules is activated, while the remaining modules remain idle, thereby limiting instantaneous parallel efficiency. 22 Without sacrificing the core advantages of restructuring (functional equivalence, unchanged interfaces, and low-overhead index operations), we recommend adopting a two-tier service ar-chitecture for transformation and inference . In this design, the transformation layer centrally handles permutation P, P âˆ’1, projection {Ï€i}, sub-module activation and scheduling, and em-bedding {Î³i}, all of which are purely structural operations. The inference layer deploys each sub-operator W (i) 

> Ïƒ

as an independent service, responsible solely for numerical inference Yi = W (i) 

> Ïƒ

Xi.Under this architecture, the transformation layer maintains centralized control over structure and scheduling logic, while inference-layer sub-modules can elastically scale according to actual activation frequency: high-frequency modules are provisioned with more instances, and low-frequency modules with fewer instances. This significantly reduces idle resource waste and improves overall utilization. Importantly, this implementation does not alter the mathematical structure of any operator; it improves engineering efficiency purely through runtime scheduling. 

4.5.3 Upgradable Learning after Restructuring 

A restructured inference system can still undergo continued or incremental learning without expanding the state space. Consider a single-layer linear operator as an example, with forward propagation Y = AX , corresponding to the linear mapping Y (t) = A(t)X(t). Under gradient descent, the standard backpropagation update, derived directly from the chain rule [36, 37], takes the matrix form 

A(t + 1) = A(t) âˆ’ Î· G Y (t)XâŠ¤(t), GY (t) = âˆ‚L âˆ‚Y (t) .

This update admits a clear two-step structure: âˆ†A(t) = Î· G Y (t)XâŠ¤(t), A(t + 1) = A(t) âˆ’ âˆ†A(t).

As long as the restructured system preserves the same input X and output Y , the algebraic form of the learning update remains unchanged. Restructuring modifies the organization of operators, not the update rule itself. In the restructured system, the global update âˆ† W (t) can be redistributed back to individual sub-operators via permutation and inverse embedding: 

Wi(t + 1) = Wi(t) âˆ’ Î³âˆ’1

> i



P âˆ†W (t) P âŠ¤

Ï€âŠ¤ 

> i

. (4.7) Here âˆ†W (t) = Î· G YOutput (t)XâŠ¤

> Input

(t), GYOutput (t) = âˆ‚L âˆ‚Y Output (t) .

The operator Î³âˆ’1 

> i

denotes the inverse projection associated with the embedding Î³i; together with Ï€âŠ¤ 

> i

, it extracts the sub-matrix corresponding to each sub-operator from the global update matrix P âˆ†W (t) P âŠ¤.

4.6 Construction of the Permutation 

A central operational issue in this work is how to construct an appropriate permutation so that the structure of the annealed system can be made explicit and clearly visible. This subsection provides a concise description of the key steps involved in this process. 

Node attribute table. The permutation can be obtained by maintaining a node attribute table: 

V âŸ¨VTAG , S TAG , G TAG , L TAG , I TAG , V NewTAG âŸ©n,

where each row corresponds to a node, and the fields are defined as follows: 23 â€¢ VTAG : the original index of the node; 

â€¢ STAG : the index of the strongly connected component (SCC) to which the node belongs; 

â€¢ GTAG : the index of the weakly connected subgraph to which the node belongs; 

â€¢ LTAG : the layer index of the nodeâ€™s SCC in the condensation graph; 

â€¢ ITAG : an isolated-node indicator (the corresponding weakly connected subgraph contains only this node); 

â€¢ VNewTAG : the new node index generated according to the structural ordering rules. After all node attributes have been computed, the nodes are sorted according to the following priority: first by the weakly connected subgraph index GTAG , then by the isolated-node indicator 

ITAG , next by the layer index LTAG , and finally by the SCC index STAG . This ordering uniquely determines the new node indices VNewTAG , thereby yielding the desired node permutation 

Q =  VNewTAG VTAG 

 .P -adjacency matrix. For convenience in trial computation and structural verification during the permutation process, we introduce a Boolean matrix M = [ mij ] âˆˆ { 0, 1}nÃ—n, referred to as the P -adjacency matrix of the parameter matrix W . It is defined by 

mij =

(

1, P (wij ),

0, Â¬P (wij ), i, j = 1 , . . . , n, 

where P (x) denotes a prescribed structural predicate and Â¬P (x) its negation. Without directly modifying the parameter matrix W , one can vary the predicate P and apply the permutation induced by Q to M to repeatedly test the reordering effects under different annealing criteria. This procedure strictly decouples structural determination from numerical parameter values , thereby providing a controllable and reproducible framework for structural design and verification. A complete permutation construction algorithm based on matrix operations is provided in the Appendix. The algorithm can be directly implemented in practice and also serves as the theoretical and algorithmic reference for the permutation design adopted in this work. 

# 5 Conclusion 

From a system and engineering design perspective, this work demonstrates that large-scale artificial neural systems, once training is completed, are not irreducible black-box monoliths. Instead, at the parameter level, they naturally give rise to structural units that can be stably identified and effectively exploited. These structures are not introduced by externally imposed rules, but are the inevitable outcome of the long-term action of gradient reinforcement and suppression in learning dynamics. The posterior structural perspective adopted in this paper provides a practical and feasible pathway for analyzing and handling existing models. Because activation traces and gradient histories from the training phase are typically unavailable once a model is deployed, engineering practice should not attempt to reconstruct or retrace the training process itself. Rather, by performing statistical annealing directly on the trained parameter matrices, one can extract the effective structures that have been genuinely confirmed by learning, without relying on any historical training information. In this sense, structural restructuring should be understood as a form of post-training static analysis , rather than an intrusive modification of the original training pipeline. 24 Furthermore, the structures obtained after annealing naturally induce modular partitions and dependency relations in the graph-theoretic sense. These modules exhibit a degree of functional self-containment, while the directed relations among them encode stable forward de-pendency orders. Although this work does not attempt a quantitative characterization of the numerical dynamics during training, the revealed structural decomposition already provides direct guidance for the engineering realization of inference systems, including blockwise inde-pendent execution, module-level parallel scheduling, and progressive inference under resource constraints. From the viewpoint of system evolution, the conclusions of this work point to a development path distinct from the continued expansion of monolithic model size. As model scale increases, the primary engineering bottleneck gradually shifts away from parameter count itself toward modes of structural execution and scheduling efficiency. By explicitly identifying and consol-idating, at the post-training stage, the effective structures that have already emerged during learning, it becomes possible to significantly improve system scalability, maintainability, and long-term evolutionary potential, without sacrificing model capability. Accordingly, this work does not propose a new model architecture, but rather articulates a 

principle for the structural treatment of existing models . By elevating the structures implicitly formed during learning to system-level objects, the focus of inference system design can shift from indiscriminate growth in parameter scale to a structure-centered paradigm of sustainable operation and evolution. 

# A Appendix A: Matrix-Based Algorithm for Computing the Permutation 

For an annealed parameter matrix W , we require a node permutation (reindexing) P in or-der to reorder W (or its Boolean adjacency matrix M ) according to node indices, so as to expose the block-diagonal structure and the triangular block structure within diagonal blocks. This appendix presents a solution based purely on matrix operations. Although its total work complexity under the classical RAM model is not superior to that of standard graph-search algorithms, in the context of GPU-based matrix-parallel computation, its iteration dependency chain (critical path / sequential depth) is only on the order of O(âŒˆlog 2 nâŒ‰), making it practically feasible for implementation. Traditional graph-search algorithms (such as DFS-based SCC decomposition) have a total work complexity of O(n + m) under the RAM model, where n = |V | is the number of nodes and 

m = |E| is the number of edges; for dense graphs, m may reach O(n2). Moreover, graph-search methods typically require converting W into an explicit graph representation (e.g., edge lists or sparse storage formats), which involves a large number of irregular memory-access operations. Given the typical hardware resource composition and parallel-computation characteristics of AI service infrastructures, matrix-based algorithms constitute a viable alternative. The most distinctive feature of the matrix-based approach is that it does not require chang-ing the graph representation paradigm: the Boolean adjacency matrix M is constructed directly from W , the permutation P is obtained through matrix operations on M , and P can then be applied to M (or W ) to produce an isomorphic reordering. This makes it possible to directly observe the structural form of the inference system under different annealing confidence lev-els. As a result, the method is well suited for repeated computation under varying annealing thresholds, thereby facilitating the identification of a reasonable annealing criterion. If applied during the learning stage, it can further be used to construct monitors for the formation and evolution of system structure. 25 A.1 Artificial Neural Networks and Simple Directed Graphs 

Let W = [ wij ] âˆˆ RnÃ—n be the parameter matrix of an artificial neural network (either before or after annealing). Define 

G = âŸ¨V, E âŠ† V Ã— V, W âŸ©

as the corresponding simple directed graph, where n = |V | is called the order of G. A simple directed graph is well suited for representing structural relations, since between any pair of nodes there exists at most one structural directed edge. Denoting a directed edge in G by (vi, v j ) âˆˆ E, the correspondence between G and W is given by 

 (vi, v j ) âˆˆ E â‡â‡’ wij Ì¸ = 0 ,

(vi, v j ) /âˆˆ E â‡â‡’ wij = 0 .

Throughout this paper, we adopt the convention that ( vi, v j ) âˆˆ E represents a directed edge from vj to vi. From the perspectives of operators and dynamics, this convention is more natural and convenient. We now consider the case of multi-channel FNNs. Equation (2.1) gives the operator structure of the common feedforward architecture d â†’ 4d â†’ d. Its linear part can be equivalently represented as a pair of jointly parameterized operators consisting of four parallel dÃ—d channels: 

W1 =

ï£«ï£¬ï£¬ï£¬ï£­

W (1) 1

W (2) 1

W (3) 1

W (4) 1

ï£¶ï£·ï£·ï£·ï£¸ , W2 =  W (1) 2 , W (2) 2 , W (3) 2 , W (4) 2

,

with a component-wise nonlinear mapping Ïƒ(Â·) applied within each channel. The overall map-ping is 

Y =

> 4

X

> g=1

W (g)2 Ïƒ



W (g)1 X



.

In this structure, 

W (1) 1 , Â· Â· Â· , W (4) 1 ; W (1) 2 , Â· Â· Â· , W (4) 2

are mutually isomorphic, and 

W (1) 1 (i, j ), Â· Â· Â· , W (4) 1 (i, j ); W (1) 2 (i, j ), Â· Â· Â· , W (4) 2 (i, j ) (A.1) jointly correspond to the same structural directed edge ( vi, v j ) âˆˆ E. Therefore, at the level of structural abstraction, a multi-channel FNN still corresponds to a simple directed graph. 

A.2 Mathematical Background 

This subsection collects the terminology and mathematical background required by the algo-rithm. These concepts constitute the theoretical foundation for the construction of the algo-rithm. 

Definition A.1. [45] P -adjacency matrices, Boolean products (powers) of Boolean matrices, and the transitive closure of Boolean (adjacency) matrices. 1. Let M = [ mij ] âˆˆ { 0, 1}nÃ—n be a Boolean matrix. It is called the P -adjacency matrix of 

W (abbreviated as the P -adjacency matrix) if 

mij =

(

1, P (wij ),

0, Â¬P (wij ), i, j = 1 , . . . , n, 

where P (x) denotes that x satisfies property P , and Â¬P (x) denotes that x does not satisfy property P .26 2. Let A = [ aij ], B = [ bij ], and C = [ cij ] be Boolean matrices in {0, 1}nÃ—n. If C = A âŠ™ B,then C is called the Boolean product of A and B, with entries 

cij =

> n

_

> k=1

(aik âˆ§ bkj ) .

Accordingly, the k-th Boolean power of A is defined recursively by Ak = Akâˆ’1 âŠ™ A.3. For a Boolean matrix A = [ aij ] âˆˆ { 0, 1}nÃ—n, A+ and Aâˆ— are respectively called the 

transitive +-closure and the transitive âˆ—-closure of A [43] (where I denotes the Boolean identity matrix): 

A+ =

> nâˆ’1

_

> k=1

Ak = A âˆ¨ A2 âˆ¨ Â· Â· Â· âˆ¨ Anâˆ’1,Aâˆ— = I âˆ¨ A+ =

> nâˆ’1

_

> k=0

Ak = I âˆ¨ A âˆ¨ A2 âˆ¨ Â· Â· Â· âˆ¨ Anâˆ’1.

4. Let D = A âˆ¨ AT . Then D is the matrix representation of the underlying undirected graph of G, i.e., 

dij = 1 â‡â‡’ (vi, v j ) âˆˆ E âˆ¨ (vj , v i) âˆˆ E. 

Theorem A.1. Let A = [ aij ] be the adjacency matrix of a directed graph G = âŸ¨V, E âŸ©, and let 

A+ = [ a+ 

> ij

], Aâˆ— = [ aâˆ— 

> ij

].1. Matrix representation of path existence: 

a+ 

> ij

= 1 â‡â‡’ aâˆ— 

> ij

= 1 â‡â‡’ there exists a path from vj to vi.

Compared with A+, Aâˆ— additionally incorporates reflexivity, that is, a node is assumed to have a path to itself. 2. (Kleeneâ€“Valiant) [44] Let A(0) = I âˆ¨ A and define recursively 

A(t+1) = A(t) âŠ™ A(t), t = 0 , 1, 2, . . . 

Then for t = âŒˆlog 2(n âˆ’ 1) âŒ‰, we have A(t) = Aâˆ—.3. Let 

B = Aâˆ— âˆ§ (Aâˆ—)âŠ¤.

Then B is the matrix representation of the mutual reachability relation on G: bij = 1 if and only if vi and vj are mutually reachable. This relation induces a partition of V : each equivalence class corresponds to a maximal strongly connected subgraph (strongly connected component, SCC), and every node belongs to exactly one SCC. 4. Let D = A âˆ¨ AT be the adjacency matrix of the underlying undirected graph of G. Then 

Dâˆ— also induces a partition of V : each equivalence class corresponds to a maximal weakly connected subgraph, and there exists no undirected path between nodes belonging to different equivalence classes. Proof. (1) This is a fundamental result in graph theory and transitive-closure theory, corre-sponding to the classical theorem: if there exists a path between two nodes, then there exists a path of length at most n âˆ’ 1. The idea is as follows: if there exists a path from vp to vq of length greater than n âˆ’ 1, then the path must contain repeated nodes. By removing the intermediate segments between repeated nodes, one obtains a path of length at most n âˆ’ 1. 27 (2) Boolean polynomials satisfy the easily verified property 

ï£«ï£­

> k

_

> p=0

Ap

ï£¶ï£¸

> 2

=

> 2k

_

> p=0

Ap.

That is, the Boolean self-product (of either âˆ§ or the Boolean matrix product âŠ™) of a univariate full polynomial of degree k with identity element A0 = I yields a univariate full polynomial of degree 2 k with identity element. Since Aâˆ— is a univariate full polynomial of degree n âˆ’ 1, the result follows. (3) The relation represented by B = [ bij ] = Aâˆ— âˆ§ (Aâˆ—)âŠ¤ inherits reflexivity and transitivity from Aâˆ—, and symmetry is introduced by the conjunction with ( Aâˆ—)âŠ¤. Hence, B represents an equivalence relation. Every equivalence relation induces a partition. Nodes in the same equivalence class are mutually reachable and cannot be further enlarged; thus each equivalence class corresponds to a maximal strongly connected subgraph (strongly connected component, SCC). (4) The symmetry of D = A âˆ¨ AâŠ¤ is inherited by Dâˆ—, and since Dâˆ— is also reflexive and transitive, it likewise represents an equivalence relation, which therefore induces a partition. The equivalence classes correspond to maximal weakly connected subgraphs: if there existed an edge between two equivalence classes, the âˆ—-closure would merge them into a single class. Hence, no edges exist between distinct equivalence classes, and the corresponding subgraphs are completely disconnected. 

Definition A.2. Let G = âŸ¨V, E âŸ© be a simple directed graph, A its adjacency matrix, and 

B = Aâˆ— âˆ§ (Aâˆ—)T the matrix representation of the mutual reachability relation on V . Let Â¯V =

{V1, V 2, . . . , V s} be the collection of mutual-reachability equivalence classes. Each equivalence class Vp may be viewed as an index set: i âˆˆ Vp corresponds to node i in G.1. The subgraph Gp = âŸ¨Vp, E p = ( Vp Ã— Vp) âˆ© EâŸ© is called a maximal strongly connected subgraph of G, or equivalently, a strongly connected component (SCC) of G.2. Â¯G = âŸ¨ Â¯V , Â¯EâŸ© is called the condensation graph of G. The nodes of the condensation graph are the SCCs of G; for VpÌ¸ = Vq,(Vp, V q) âˆˆ Â¯E â‡” âˆƒ i âˆˆ Vp âˆƒ j âˆˆ Vq

(vi, v j ) âˆˆ E.

3. Let s be the order of the condensation graph Â¯G (the number of SCCs), and n the order of G. A matrix R âˆˆ { 0, 1}sÃ—n is called the condensation compression matrix : its rows are taken from the mutual-reachability matrix B, with exactly one row selected from each equivalence class. 

Theorem A.2. Let G = âŸ¨V, E âŸ© be a simple directed graph, A its adjacency matrix, and Â¯G =

âŸ¨ Â¯V , Â¯EâŸ© its condensation graph. 1. The condensation graph Â¯G is acyclic (a DAG), with the following corollaries: (1-1) Â¯G has at least one node of zero in-degree; (1-2) removing nodes of zero in-degree still yields a layered acyclic graph. 2. Let R be the condensation compression matrix of G, and Is the s Ã— s Boolean identity matrix. If Â¯A denotes the adjacency matrix of the condensation graph, then 

Â¯A =



R âŠ™ A âŠ™ RâŠ¤

âˆ§ Â¬ Is.

28 Proof. (1) By definition, Â¯G does not include self-loops ( Vp, V p) âˆˆ Â¯E. If a cycle existed, it would necessarily consist of distinct equivalence classes, which would then be mutually reachable and hence merged into a single equivalence class, contradicting the assumption. (1-1) Starting from V1, trace backward a longest path with no repeated nodes: 

Vk â†’ Â· Â· Â· â†’ V1.

The node Vk must have zero in-degree. If there existed Vs â†’ Vk with Vs /âˆˆ { V1, . . . , V k}, then the above path would not be maximal; if Vs âˆˆ { V1, . . . , V k}, then a cycle would exist, contradicting (1). (1-2) Removing nodes of zero in-degree cannot introduce new cycles, so the conclusion follows immediately. (2) The p-th row of the condensation compression matrix R is taken from the mutual-reachability matrix B corresponding to the SCC Vp. Thus, 

Rpi = 1 â‡â‡’ i âˆˆ Vp.

Consider the Boolean product ( R âŠ™ A âŠ™ RT )pq . First, (R âŠ™ A)pj =

> n

_

> i=1

(Rpi âˆ§ aij ) = _

> iâˆˆVp

aij .

Hence, (R âŠ™ A)pj = 1 â‡â‡’ âˆƒ i âˆˆ Vp s.t. aij = 1 .

Furthermore, (R âŠ™ A âŠ™ RâŠ¤)pq =

> n

_

> j=1

 (R âŠ™ A)pj âˆ§ RâŠ¤

> jq

 = _

> jâˆˆVq

(R âŠ™ A)pj .

Therefore, (R âŠ™ A âŠ™ RâŠ¤)pq = 1 â‡â‡’ âˆƒ j âˆˆ Vq âˆƒ i âˆˆ Vp

aij = 1 .

By the definition of the adjacency matrix, aij = 1 â‡â‡’ (vi, v j ) âˆˆ E, which is precisely the condition for the existence of an edge ( Vp, V q) âˆˆ Â¯E in the condensation graph. Hence, R âŠ™ A âŠ™ RâŠ¤ gives the adjacency relation of the condensation graph. Since self-loops are excluded, an additional element-wise conjunction with Â¬Is yields the adjacency matrix of the condensation graph. 

A.3 Algorithm Description 

A.3.1 Node Attribute Table 

This algorithm determines several key structural attributes of each node by successively com-puting the adjacency matrix M of a directed graph and the adjacency matrix MC of its con-densation graph. These attributes include: the strongly connected component (SCC) to which a node belongs, the layer index of that SCC in the condensation graph, the weakly connected component to which the node belongs, and whether the node becomes isolated after structural annealing. The algorithm maintains a node attribute table 

V âŸ¨VT AG , S T AG , G T AG , L T AG , I T AG , V N ewT AG âŸ©n

to record the results at each stage of the computation, where the fields are defined as follows: 29 â€¢ VT AG : the original index of the node; 

â€¢ ST AG : the index of the strongly connected component (SCC) to which the node belongs; 

â€¢ GT AG : the index of the weakly connected component to which the node belongs; 

â€¢ LT AG : the layer index of the nodeâ€™s SCC in the condensation graph; 

â€¢ IT AG : an indicator of structural isolation (equal to 1 if the node has neither incoming nor outgoing edges after annealing, and 0 otherwise); 

â€¢ VN ewT AG : the new node index generated according to the structural classification rules. After all node attributes have been computed, the nodes are sorted according to the following priority order: first by the weakly connected component index GT AG , then by the isolation indicator IT AG , then by the layer index LT AG , and finally by the SCC index ST AG . This ordering yields the new node indices VN ewT AG and thereby determines the required node permutation 

P =   VN ewT AG VT AG 

 .

A.3.2 Algorithmic Principle and Steps 

We now present a matrix-based procedure for constructing the node permutation P from a given parameter matrix W . The key definitions and theoretical results on which the steps rely are given in Definition A.1â€“A.2 and Theorem A.1â€“A.2. 1. Construction of the P -adjacency matrix. From the parameter matrix W (before or after annealing), construct its P -adjacency matrix M (Definition A.1(1)): 

mij =

(

1, P (wij ),

0, Â¬P (wij ), i, j = 1 , . . . , n. 

Here P (x) denotes that x satisfies property P , and Â¬P (x) denotes that it does not. For a simple neural network, one may choose, for example, 

P (x) : |x| > Îµ, 

where Îµ is a prescribed threshold. For the common feedforward structure d â†’ 4d â†’ d,whose isomorphic parameter sequences are given in (A.1), a norm-based criterion may be adopted: 

P (W (i, j )) : 

ï£«ï£­

> 4

X

> p=1 2

X

> q=1



W (p) 

> q

(i, j )

2

ï£¶ï£¸

> 1/2

> Îµ. 

2. Computation of SCC indices ST AG . The strongly connected component (SCC) index 

ST AG for each node is obtained from M as follows: (2-1) Compute the transitive âˆ—-closure M âˆ— of M using the Kleeneâ€“Valiant iteration (The-orem A.1(2)). (2-2) Form the mutual-reachability matrix (Theorem A.1(3)): 

B = M âˆ— âˆ§ (M âˆ—)âŠ¤

(2-3) Use the equivalence-class partition induced by B to assign to each node its SCC index ST AG in the node attribute table V (Theorem A.1(3)). 30 3. Construction of the condensation graph adjacency matrix MC .

(3-1) From the correspondence between node indices and SCC indices stored in V , con-struct the condensation compression matrix RkÃ—nC (Definition A.2(3)). (3-2) Compute the condensation graph adjacency matrix (Theorem A.2(2)): 

MC = ( RC âŠ™ M âŠ™ RâŠ¤ 

> C

) âˆ§ Â¬ Ik

4. Computation of SCC layer indices LT AG . By Theorem A.2(1), the condensation graph is a DAG. Hence SCC layers can be determined by iteratively removing the set of nodes with zero in-degree. Let 1 = (1 , 1, . . . , 1) âŠ¤ be the all-ones Boolean column vector, and let Scomp denote the set of SCCs that have already been processed (initially Scomp = âˆ…). Define 

Y = ( MC âˆ§ Mask( Scomp )) âŠ™ 1, (A.2) where âŠ™ denotes Boolean matrixâ€“vector multiplication (Definition A.1(2)). Note that 

Y (i) = 

> k

_

> j=1

MC (i, j ).

Thus, when Y (i) = 0, the i-th SCC has zero in-degree in the currently unprocessed condensation graph, and is assigned to the current layer (Theorem A.2(1-1)). These SCCs are added to Scomp . In the next iteration, the mask matrix Mask( Scomp ) is used to zero out the corresponding rows and columns, thereby allowing the layer indices LT AG of all SCCs to be obtained iteratively (Theorem A.2(1-2)). In practice, no physical deletion of rows or columns is required; masking suffices. 5. Computation of weakly connected component indices GT AG . Weakly connected components are computed on the condensation graph, rather than on the original graph, because each SCC belongs to exactly one weakly connected component, and the conden-sation graph order k is typically much smaller than n.(5-1) Construct the underlying undirected graph of the condensation graph (Definition A.1(4)): 

BC = MC âˆ¨ M âŠ¤ 

> C

.

(5-2) Compute the transitive âˆ—-closure Bâˆ— 

> C

using the Kleeneâ€“Valiant iteration (Theorem A.1(2)). (5-3) Use the equivalence-class partition induced by Bâˆ— 

> C

to determine the weakly connected component index GT AG for each SCC (Theorem A.1(4)). 6. Structural isolation indicator IT AG . Count the number of nodes contained in each weakly connected component. If a weakly connected component contains exactly one node, then the corresponding node is marked as structurally isolated by setting IT AG = 1; otherwise, set IT AG = 0. 7. Sorting and generation of new indices VN ewT AG . Sort the node attribute table 

V âŸ¨VT AG , S T AG , G T AG , L T AG , I T AG , V N ewT AG âŸ©n

according to the priority order: GT AG , IT AG , LT AG , ST AG . Assign consecutive new indices from top to bottom, thereby generating VN ewT AG .8. Output of the permutation. The algorithm outputs the node attribute table. The subtable 

P = âŸ¨VN ewT AG , V T AG âŸ©

is exactly the desired node permutation. 31 A.4 Algorithm Description: Procedural Pseudocode 

The overall structure of the algorithm is relatively simple. Except for a single loop required to generate indices from matrix computation results and to write them into the node attribute table, the procedure is essentially sequential. For accessibility to readers from different back-grounds, we adopt a procedural description rather than a formal pseudocode diagram. 

Main Procedure. 

1. Input the matrix order n.2. Construct global matrices/tables: (2-1) W nÃ—n: input parameter matrix (numeric); (2-2) M nÃ—n = 0: P -adjacency matrix (Boolean); (2-3) M âˆ— âˆˆ { 0, 1}nÃ—n: transitive âˆ—-closure of M ;(2-4) BnÃ—n = 0: mutual-reachability matrix B = M âˆ— âˆ§ (M âˆ—)âŠ¤ (Boolean); (2-5) Node attribute table V âŸ¨VT AG , S T AG , G T AG , L T AG , I T AG , V N ewT AG âŸ©n, initialized as 

VT AG (i) = i, i = 1 , . . . , n. 

(2-6) Condensation graph order k = 0 (to be determined by a subprocedure); (2-7) Condensation adjacency matrix cache CnÃ—nM = 0 (Boolean; the final output takes the top-left k Ã— k block). 3. Input W .4. Construct the P -adjacency matrix M from W (Definition A.1(1)): 

mij =

(

1, P (wij ),

0, Â¬P (wij ), i, j = 1 , . . . , n. 

5. Compute M âˆ— (Kleeneâ€“Valiant recursion, Theorem A.1(2)): 

( M âˆ— = M âˆ¨ I, M âˆ— â‡ M âˆ— âŠ™ M âˆ—, t = 1 , . . . , âŒˆlog 2(n âˆ’ 1) âŒ‰ .

6. Compute the mutual-reachability matrix (Theorem A.1(3)): 

B = M âˆ— âˆ§ (M âˆ—)âŠ¤.

7. Determine SCC indices ST AG :(7-1) Initialize: TSCC = 1, VU ndo = {1, . . . , n }.(7-2) While VU ndo Ì¸ = âˆ…, repeat: 

{

i = min( VU ndo ), VSCC = { j : B(i, j ) = 1 }.

Assign 

ST AG (v) â‡ TSCC , âˆ€v âˆˆ VSCC .

Update 

VU ndo â‡ VU ndo \ VSCC , TSCC â‡ TSCC + 1 .

}

32 8. Construct the condensation graph (steps 8â€“10 may be implemented as a subprocedure): (8-1) Set the condensation order 

k = max  

> i

ST AG (i).

(8-2) If k = 1, directly set 

GT AG (i) = 1 , LT AG (i) = 1 , IT AG (i) = 0 , i = 1 , . . . , n, 

and jump to Step 12; otherwise continue. (8-3) Initialize condensation-related matrices: M kÃ—kC = 0, RkÃ—nC = 0. (8-4) Construct the condensation compression matrix RC (Definition A.2(3)): 

RC

 ST AG (i), V T AG (i) â‡ 1, i = 1 , . . . , n. 

(8-5) Generate the condensation adjacency matrix (Theorem A.2(2)): 

MC = ( RC âŠ™ M âŠ™ RâŠ¤ 

> C

) âˆ§ Â¬ Ik.

(8-6) Write to output cache: 

CM (i, j ) â‡ MC (i, j ), i, j = 1 , . . . , k. 

9. Process SCC layer indices LT AG (Theorem A.2(1)): (9-1) Initialize: LSCC = 1, SComp = âˆ….(9-2) Define the masking function Mask( S), which masks rows and columns corresponding to set S:Mask( S)( i, j ) = 

(

0, i âˆˆ S âˆ¨ j âˆˆ S, 

1, otherwise .

(9-3) While {1, . . . , k } \ Scomp Ì¸ = âˆ…, repeat: 

{

Y = ( MC âˆ§ Mask( Scomp )) âŠ™ 1, 1 = (1 , . . . , 1) âŠ¤,SL = { i âˆˆ { 1, . . . , k } \ Scomp : Y (i) = 0 }.

Assign 

LT AG (j) â‡ LSCC , âˆ€j âˆˆ { 1, . . . , n } s.t. ST AG (j) âˆˆ SL.

Update 

Scomp â‡ Scomp âˆª SL, LSCC â‡ LSCC + 1 .

}

10. Process weakly connected component indices GT AG (Theorem A.1(4)): (10-1) Construct the underlying undirected graph of the condensation graph (Definition A.1(4)): 

BC = MC âˆ¨ M âŠ¤ 

> C

.

(10-2) Compute Bâˆ— 

> C

(Kleeneâ€“Valiant): 

( Bâˆ— 

> C

= BC âˆ¨ I, Bâˆ— 

> C

â‡ Bâˆ— 

> C

âŠ™ Bâˆ— 

> C

, t = 1 , . . . , âŒˆlog 2(k âˆ’ 1) âŒ‰ .

33 (10-3) For undirected graphs, Bâˆ— 

> C

is the mutual-reachability matrix; use Bâˆ— 

> C

(i, j ) = 1 to determine membership in the same weakly connected component. (10-4) Partition SCCs by Bâˆ— 

> C

and backfill into the node table: Initialize GSCC = 1, SU ndo = {1, . . . , k }.While SU ndo Ì¸ = âˆ…, repeat: 

{

i = min( SU ndo ), SG = { j : Bâˆ— 

> C

(i, j ) = 1 }.

Assign 

GT AG (v) â‡ GSCC , âˆ€v âˆˆ { 1, . . . , n } s.t. ST AG (v) âˆˆ SG.

Update 

SU ndo â‡ SU ndo \ SG, GSCC â‡ GSCC + 1 .

}

11. Structural isolation indicator IT AG : for each weakly connected component index g, count the number of contained nodes Ng. If Ng = 1, set IT AG = 1 for that node; otherwise set 

IT AG = 0. 12. Data consolidation and generation of new indices: sort 

V âŸ¨VT AG , S T AG , G T AG , L T AG , I T AG , V N ewT AG âŸ©n

in ascending order by GT AG , IT AG , LT AG , ST AG ; after sorting, assign 

VN ewT AG (i) = i, i = 1 , . . . , n. 

13. Output: (13-1) The node attribute table V âŸ¨VT AG , S T AG , G T AG , L T AG , I T AG , V N ewT AG âŸ©n;(13-2) The condensation adjacency matrix: the top-left k Ã— k block of CM ;(13-3) The condensation graph order k.

A.5 Algorithm Execution Results and Structural Verification 

Figure A.1: Original graph Figure A.1 shows the original graph used in the verification example. The numbers inside the circles indicate the original node indices. Table A.1 lists the corresponding adjacency matrix. 34 Under the original indexing, the nonzero entries of the adjacency matrix M exhibit a globally scattered and seemingly irregular distribution. Although the complete topological information is already encoded, no clear block structure corresponding to strongly connected components, hierarchical layers, or weakly connected subgraphs can be directly observed.                                         

> 12345678910 11 12 13 14 15 16 17 18 112131415116171181910 11111 112 113 1114 115 116 117 1118 1

Table A.1: Adjacency matrix M (original node indices) 

i VT AG ST AG GT AG LT AG IT AG VN ewT AG 

1 1 1 1 1 0 12 2 1 1 1 0 23 3 1 1 1 0 34 4 2 1 2 0 45 5 2 1 2 0 56 10 6 1 3 0 67 18 6 1 3 0 78 11 7 1 3 0 89 17 7 1 3 0 910 14 9 2 1 0 10 11 15 9 2 1 0 11 12 12 8 2 2 0 12 13 13 8 2 2 0 13 14 6 3 2 3 0 14 15 7 3 2 3 0 15 16 8 4 3 1 0 16 17 16 4 3 1 0 17 18 9 5 4 1 1 18 

Table A.2: Final computation results Table A.2 shows the node attribute table produced after the algorithm terminates. Here, 

ST AG denotes the strongly connected component (SCC) index, LT AG denotes the layer index of that SCC in the condensation graph, GT AG denotes the weakly connected component in-dex, IT AG is the structural isolation indicator, and VN ewT AG is the new node index generated according to the sorting rule ( GT AG , I T AG , L T AG , S T AG ). Figure A.2 visualizes the results in Table A.2. The numbers outside the circles are the newly assigned node indices. Inside each circle, the subscript i of Si indicates the computed SCC index. The label Dii denotes the position index of the corresponding diagonal block after reordering by the new indices. The leftmost column shows the SCC layer indices, and the topmost row shows the weakly connected component indices. Some edges labeled Uij represent inter-SCC coupling blocks from SCC Si to SCC Sj .35 From the figure, we observe the following: 1. The new node indices form contiguous intervals for each weakly connected component: 

G1 : 1â€“9, G2 : 10â€“15, G3 : 16â€“17, G4 : 18. This property guarantees that the permuted matrix exhibits an outer block-diagonal structure. 2. Within each SCC, the new node indices are consecutive (nodes in the same SCC are placed adjacently). Moreover, nodes belonging to SCCs at higher layers receive smaller new indices. This guarantees that each diagonal block can be further permuted into a lower triangular block structure. 3. The unique isolated node is assigned the last new index 18. Its corresponding weakly connected component is also the last one, G4, and it therefore corresponds to the final diagonal block D99 .

Figure A.2: Schematic illustration of the permuted structural decomposition 

Figure A.3: Adjacency matrix after permutation by the new node indices Figure A.3 shows the result of applying the permutation 

P = âŸ¨VN ewT AG , V T AG âŸ©

36 to the original adjacency matrix (Table A.1). The resulting matrix exhibits an outer block-diagonal structure, with a triangular block structure inside each diagonal block. Two degenerate cases appear in the figure: the bottom-right diagonal block corresponds to a weakly connected component that degenerates into a single isolated node, and the second diagonal block from the bottom-right corresponds to a weakly connected component that degenerates into a single strongly connected component. 

# References 

[1] Y. LeCun, Y. Bengio, and G. Hinton, â€œDeep learning,â€ Nature , vol. 521, no. 7553, pp. 436â€“ 444, 2015. [2] T. B. Brown et al. , â€œLanguage Models are Few-Shot Learners,â€ Advances in Neural Infor-mation Processing Systems , vol. 33, pp. 1877â€“1901, 2020. [3] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. Le, and D. Zhou, â€œChain-of-Thought Prompting Elicits Reasoning in Large Language Models,â€ in Advances in Neural Information Processing Systems (NeurIPS) , vol. 35, 2022. [4] R. Bommasani et al., â€œOn the Opportunities and Risks of Foundation Models,â€ arXiv preprint arXiv:2108.07258 , 2021. [5] A. Srivastava et al., â€œBeyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models,â€ Transactions on Machine Learning Research (TMLR) ,2023. [6] A. Vaswani et al. , â€œAttention Is All You Need,â€ Advances in Neural Information Processing Systems , vol. 30, 2017. [7] Y. Cheng, D. Wang, P. Zhou, and T. Zhang, â€œModel compression and acceleration for deep neural networks: The principles, progress, and challenges,â€ IEEE Signal Processing Magazine , vol. 35, no. 1, pp. 126â€“136, 2018. [8] D. Narayanan, S. H. Hashemi, P. Patel, et al., â€œEfficient Large-Scale Language Model Training on GPU Clusters,â€ in Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI) , 2021. [9] T. Dao, D. Fu, S. Ermon, A. RÂ´ e, and C. RÂ´ e, â€œFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,â€ in Advances in Neural Information Processing Sys-tems (NeurIPS) , 2022. [10] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-ford, J. Wu, and D. Amodei, â€œScaling Laws for Neural Language Models,â€ arXiv preprint arXiv:2001.08361 , 2020. [11] E. Strubell, A. Ganesh, and A. McCallum, â€œEnergy and Policy Considerations for Deep Learning in NLP,â€ in Proceedings of ACL , 2019. [12] J. Rae et al., â€œScaling Language Models: Methods, Analysis & Insights from Training Gopher,â€ arXiv:2112.11446 , 2022. [13] A. Chowdhery et al., â€œPaLM: Scaling Language Modeling with Pathways,â€ in Proceedings of the 39th International Conference on Machine Learning (ICML) , 2022. [14] S. Han, J. Pool, J. Tran, and W. Dally, â€œLearning both weights and connections for efficient neural networks,â€ Advances in Neural Information Processing Systems , vol. 28, 2015. 37 [15] J. Frankle and M. Carbin, â€œThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks,â€ in International Conference on Learning Representations (ICLR) , 2019. [16] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, â€œRethinking the Value of Network Pruning,â€ in International Conference on Learning Representations (ICLR) , 2019. [17] T. Hoefler, D. Alistarh, T. Ben-Nun, A. Dryden, and P. Peste, â€œSparsity in Deep Learning: Pruning and Growth for Efficient Inference and Training,â€ Journal of Machine Learning Research , vol. 22, no. 241, 2021. [18] Z. Liu, J. Xu, Z. Chen, Y. Wang, and Y. Jia, â€œSparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,â€ in Proceedings of the 40th International Conference on Machine Learning (ICML) , 2023. [19] A. Kusupati, M. Jain, A. Gholami, et al., â€œSoftmax Collapse and the Role of Sparse Features in Language Models,â€ arXiv:2301.04589 , 2023. [20] S. Gunasekar, J. D. Lee, D. Soudry, and N. Srebro, â€œImplicit regularization in matrix factorization,â€ Advances in Neural Information Processing Systems , vol. 31, 2018. [21] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, â€œOutra-geously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,â€ in Inter-national Conference on Learning Representations (ICLR) , 2017. [22] A. Jacot, F. Gabriel, and C. Hongler, â€œNeural Tangent Kernel: Convergence and Gen-eralization in Neural Networks,â€ in Advances in Neural Information Processing Systems (NeurIPS) , 2018. [23] J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, â€œWide Neural Net-works of Any Depth Evolve as Linear Models Under Gradient Descent,â€ in Advances in Neural Information Processing Systems (NeurIPS) , 2019. [24] L. Chizat, E. Oyallon, and F. Bach, â€œOn Lazy Training in Differentiable Programming,â€ in Advances in Neural Information Processing Systems (NeurIPS) , 2020. [25] G. Yang and E. Hu, â€œFeature Learning in Infinite-Width Neural Networks,â€ in International Conference on Learning Representations (ICLR) , 2023. [26] J. Lin, X. Zhang, Y. Zhu, and S. Han, â€œA Survey of Large Language Model Inference Systems,â€ arXiv:2302.09237 , 2023. [27] B. Zoph and Q. V. Le, â€œNeural architecture search with reinforcement learning,â€ arXiv:1611.01578, 2016. [28] T. Elsken, J. H. Metzen, and F. Hutter, â€œNeural architecture search: A survey,â€ Journal of Machine Learning Research , vol. 20, no. 55, pp. 1â€“21, 2019. [29] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, â€œLearning Structured Sparsity in Deep Neural Networks,â€ in Advances in Neural Information Processing Systems (NeurIPS) , 2016. [30] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, â€œLearning structured sparsity in deep neural networks,â€ Advances in Neural Information Processing Systems , vol. 29, 2016. [31] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, â€œUnderstanding deep learning requires rethinking generalization,â€ ICLR , 2017. [32] B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro, â€œExploring generalization in deep learning,â€ Advances in Neural Information Processing Systems , vol. 30, 2017. 38 [33] C. Olah, A. Mordvintsev, and L. Schubert, â€œThe Building Blocks of Interpretability,â€ 

Distill , 2018. [34] C. Olah et al. , â€œZoom in: An introduction to circuits,â€ Distill , 2020. [35] N. Elhage, C. Olsson, N. Nanda, et al., â€œA Mathematical Framework for Transformer Circuits,â€ arXiv:2104.08691 , 2021. [36] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning , MIT Press, 2016. [37] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, â€œLearning representations by back-propagating errors,â€ Nature , vol. 323, pp. 533â€“536, 1986. [38] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng, Large Scale Distributed Deep Networks, in 

Advances in Neural Information Processing Systems (NeurIPS) , vol. 25, 2012. [39] T. Jacobs, A. Gadhikar, C. Rubio-Madrigal, and R. Burkholz, â€œHAM: A Hyperbolic Step to Regulate Implicit Bias,â€ arXiv preprint arXiv:2506.02630 , 2025. [40] On the Implicit Bias in Deep-Learning Algorithms, Communications of the ACM , 2022. [41] H. Yang, Z. Jiang, R. Zhang, Y. Liang, and Z. Wang, â€œNeural Networks with Sparse Activation Induced by Large Bias,â€ Journal of Machine Learning Research , 2024. [42] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, â€œSparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints, arXiv preprint, 2023-2024. [43] S. C. Kleene, â€œRepresentation of events in nerve nets and finite automata,â€ in Automata Studies , C. E. Shannon and J. McCarthy, Eds., Princeton University Press, pp. 3â€“41, 1956. [44] L. G. Valiant, â€œGeneral context-free recognition in less than cubic time,â€ Journal of Com-puter and System Sciences , vol. 10, no. 2, pp. 308â€“315, 1975. [45] S. Warshall, â€œA theorem on Boolean matrices,â€ Journal of the ACM , vol. 9, no. 1, pp. 11â€“ 12, 1962. [46] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, â€œGradient-based learning applied to document recognition,â€ Proceedings of the IEEE , vol. 86, no. 11, pp. 2278â€“2324, 1998. [47] S. Haykin, Neural Networks and Learning Machines , 3rd ed., Prentice Hall, 2009. [48] M. A. Armstrong, Groups and Symmetry , Springer, 1988. 39