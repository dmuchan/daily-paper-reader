# Why Inference in Large Models Becomes Decomposable After Training
# 为什么大模型推理在训练后变得可分解

**Authors**: Jidong Jin
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15871v1
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 6.0
**Evidence**: Discusses internal structures and training methodologies of large-scale AI models

---

## Abstract
Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

## 摘要
大规模人工智能模型的推理通常在稠密参数矩阵上进行，导致推理成本

---

## 速览摘要（自动生成）

**问题：** 大模型推理通常将参数视为稠密矩阵，导致计算成本随规模不可持续增长，忽略了训练中形成的内部结构。

**方法：** 发现梯度更新具有高度局部性，据此提出统计准则与结构退火算法，剔除冗余依赖并提取独立的子结构。

**结论：** 证明大模型推理具有天然可分解性，可在不改变功能的前提下实现高效的结构化并行推理。