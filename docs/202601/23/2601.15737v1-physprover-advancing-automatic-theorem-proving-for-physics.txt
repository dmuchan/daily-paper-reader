Title: PhysProver: Advancing Automatic Theorem Proving for Physics

URL Source: https://arxiv.org/pdf/2601.15737v1

Published Time: Fri, 23 Jan 2026 01:30:33 GMT

Number of Pages: 18

Markdown Content:
# PhysProver: Advancing Automatic Theorem Proving for Physics 

Hanning Zhang *1 , Ruida Wang *1 , Rui Pan *1 , Wenyuan Wang *2 ,Bingxu Meng 1, Tong Zhang 1

> 1

University of Illinois Urbana-Champaign 

> 2

Rutgers University 

{hanning5, ruidaw, ruip4, bingxum2, tozhang}@illinois.edu ww462@scarletmail.rutgers.edu 

Abstract 

The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science commu-nities because it provides a rigorous founda-tion for theorem proving. Recent advance-ments in the field provide foundation mod-els and sophisticated agentic systems pushing the boundaries of formal mathematical rea-soning to approach the natural language ca-pability of LLMs (Chen et al., 2025b). How-ever, little attention has been given to the for-mal physics reasoning, which also heavily re-lies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowl-edge, the first approach to enhance formal the-orem proving in the physics domain. We com-pose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean (Tooby-Smith, 2025) and data gener-ated by a conjecture-based formal data genera-tion pipeline. In the training pipeline, we lever-age DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and ap-ply Reinforcement Learning with Verifiable Re-wards (RLVR) to train our model PhysProver .Comprehensive experiments demonstrate that, using only ∼5K training samples, PhysProver 

achieves an overall 2.4% improvement in mul-tiple sub-domains. Furthermore, after formal physics training, we observe 1.3% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers out-side mathematical domains. To foster further research, we will release both our dataset and model to the community. 

1 Introduction 

Formal reasoning has long been recognized as a cornerstone of human intelligence and a critical domain in machine learning research (Newell and Simon, 1956). With the recent advancements in Large Language Models (LLMs), much research has investigated their applications in formal theo-rem proving. They explored domains from training foundation models (Lin et al., 2025b; Ren et al., 2025; Wang et al., 2025c) and specialized agent framework (Wang et al., 2025d; Chen et al., 2025b; Varambally et al., 2025). Among these, math theo-rem proving in Lean4 (Moura and Ullrich, 2021a) has emerged as one of the most extensively studied areas (Wang et al., 2024; Lin et al., 2025a; Xin et al., 2024). Researchers typically start from a general-purpose LLM, employing Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to enhance the formal reasoning capability. This ap-proach has achieved strong results on formal math benchmarks, such as MiniF2F (Zheng et al., 2022) and PutnamBench (Tsoukalas et al., 2024) 1.Previous works have demonstrated that develop-ing expert models for Lean4 theorem proving de-mands substantial training data and a large amount of GPU hours. For instance, DeepSeek-Prover (Xin et al., 2024) applies a 120B math-related tokens continue pretraining and 8M formal statements with proofs to train an expert prover. Similarly, Goedel-Prover (Lin et al., 2025a) applies expert iteration on more than 1 million formal statements. Despite these advancements, formal theorem prov-ing faces significant challenges due to a scarcity of high-quality data that is able to give the model a general formal reasoning capability, rather than focusing on a narrow field (Li et al., 2025). While significant progress has been made in mathematical theorem proving, the formal physics domain remains largely overlooked. Physics, with its reliance on rigorous mathematical foundations and formal derivations, offers a natural yet under-explored extension to formal reasoning. Li et al. 

> 1https://github.com/hanningzhang/PhysProver

1 

> arXiv:2601.15737v1 [cs.AI] 22 Jan 2026 Figure 1: Physics Prover Framework : (a) Data Generation Stage: the training set comprises 5,541 physics statements from both PhysLean (Tooby-Smith, 2025) and synthetic lemmas from Claude-4.5-Sonnet, where the latter are further filtered by Lean syntax and proof existence checks. (b) Self-Evolving Stage: after obtaining the training set, GRPO (Shao et al., 2024) is adopted to train the base prover models, with reward signals of proof correctness provided by Lean.

(2025) highlights that SOTA theorem-proving mod-els perform poorly in physics-related tasks but fail to propose methods for improvement. To settle this gap, we, as far as we are concerned, take the first step toward enhancing theorem prov-ing in the physics domain by constructing a spe-cialized data pipeline and employing Reinforce-ment Learning with Verifiable Rewards (RLVR). The overview of our framework can be found in Figure 1. Specifically, we collect foundational the-orems and lemmas from the open-source repository of PhysLean (Tooby-Smith, 2025), which contains Lean4-based results across advanced physics do-mains such as Quantum Field Theory and String Theory. The extracted data, along with their head-ers, are divided into the training and testing sets. To augment the training dataset, we apply Claude-4.5 to generate additional conjectures based on the dataset. Subsequently, we apply formal LLMs to annotate these conjectures, thereby formulating the Basic Physics Lean training dataset, which contains approximately 5K training samples and 250 testing samples. With the dataset, we leverage RLVR (Lambert et al., 2025) to enhance physical theorem-proving capability using the GRPO algorithm. Our evalua-tion demonstrates consistent improvement across multiple physics domains and achieves a 2.4% of overall improvements compared to SOTA math provers on the testing dataset. Furthermore, when tested on the Out-of-Distribution (OOD) MiniF2F benchmark (Zheng et al., 2022), PhysProver 

achieved over 1% of improvement compared to the base model under pass@16. It demonstrates the effectiveness of our approach and shows that physics dataset training can enhance the formal math capability of the model. We summarize our contribution as follows: 1. Introducing the first methods specifically de-signed to train the formal theorem provers for physics. 2. Developing and releasing a compact and com-prehensive small dataset, along with a conjec-ture synthesis pipeline for physical theorems to benefit the research community. 3. Training a formal physics prover that outper-forms the SOTA model and achieves superior performance in both physics and mathemati-cal theorem proving. 

2 Related Works 

2.1 Formal Math Reasoning 

Formal math reasoning involves representing math-ematical components in a computer-verifiable for-mat. It reduces the ambiguity and establishes a rigorous foundation for logical reasoning. Over 2the past decades, researchers have developed nu-merous Formal Languages (FLs) based on two pri-mary theoretical frameworks. The first relies on dependent type languages, such as Lean (De Moura et al., 2015; Moura and Ullrich, 2021b) and Coq (Coq, 1996), where formal verification is achieved through a small kernel to perform type checking. The second line utilizes higher-order logic to quantify functions and predicates. This line of work is represented by languages such as Isabelle (Paulson, 1994), HOL, and HOL Light (Harrison, 2009). Among the above lan-guages, Lean4 (Moura and Ullrich, 2021b) has gained significant attention due to its expressive-ness and extensive Mathlib4 repository, which en-compasses almost all major mathematical domains. The rise of LLMs has accelerated the advance-ments in formal proving tasks. Researchers have compiled extensive datasets of mathematical the-orems and proofs(Wang et al., 2025c; Lin et al., 2025a; Dong and Ma, 2025), which provide a ro-bust foundation for model training. Building on these resources, increasingly sophisticated mod-els have emerged. Early efforts, such as Ex-pert Iteration (Polu et al., 2022), employed it-erative annotation using LLMs to enhance the training data. Open-source frameworks like DeepSeek-Prover (Xin et al., 2024) and Theorem-Llama (Wang et al., 2024) further advanced the formal provers. More recently, RLVR has enabled Long CoT training for formal theorem proving, which works like MA-LoT (Wang et al., 2025c), Kimina-Prover (Wang et al., 2025a), DeepSeek-Prover-V2 (Ren et al., 2025), and Goedel-Prover-V2 (Lin et al., 2025b), achieving notable progress. The emergence of agentic frameworks, such as Hilbert (Varambally et al., 2025) and Seed-Prover-V1 (Chen et al., 2025c), achieves notable progress by enabling multi-agent theorem decomposition and sub-goal proofs. The latest works apply agen-tic RL to push LLMs’ formal reasoning capabil-ity closer to natural language proficiency (Chen et al., 2025b). Despite these advancements, for-mal reasoning in physics remains an underexplored domain, representing a significant opportunity for future research. 

2.2 LLM for Physics Reasoning 

With the rapid development of general reasoning ca-pabilities in LLMs, researchers are actively explor-ing the application of these models in more diverse fields (Wang et al., 2025b). Among them, physics reasoning is one key field that receives significant attention. In the context of benchmarks, early com-prehensive benchmarks, such as SciBench (Wang et al., 2023) and GPQA (Rein et al., 2024), evalu-ate college-level scientific problem-solving across multiple scientific fields, including physics. More recently, physics benchmarks have emerged at mul-tiple difficulty levels: UGPhysics (Xu et al., 2025) presents 5,520 undergraduate-level bilingual prob-lems that advanced thinking models are hard to solve; OlympiadBench (He et al., 2024) introduces 8,476 Olympiad-level problems with multi-module inputs; and recent HiPhO (Yu et al., 2025) compiles the latest 13 Physics Olympiad exams in 2024-2025 with human-aligned evaluation. On the model training side, researchers began exploring the potential for LLMs as physics rea-soning tools from an early stage. Early works have demonstrated that LLMs can solve complex word problems that require calculation and infer-ence (Ding et al., 2023). Such capability can be further enhanced by Reinforcement Learning from Human Feedback (RLHF) (Anand et al., 2024) or simple multi-agent collaboration (Pang et al., 2025). Recent works apply RLVR on natural language physics problems, with P1 (Chen et al., 2025a) achieving gold-level IPhO performance. However, with a lack of datasets and training methods, de-veloping LLMs for formal physics reasoning is relatively understudied currently (Li et al., 2025). 

3 Methodology 

3.1 Seed Dataset Collection 

We construct a lemma–proof dataset from the PhysLean GitHub repository (Tooby-Smith, 2025) by extracting all provable lemmas from .lean files along with their preceding formal headers. The lemma statements with context serve as inputs, while the corresponding proof scripts serve as out-puts. We filter the samples to retain only those with a total length under 4,096 tokens. The resulting corpus contains over 3,000 examples, which are randomly split into training and test sets at approx-imately a 9:1 ratio, yielding 2,933 training and 250 test instances. The dataset spans a broad range of domains in physics and mathematics, encompass-ing classical and modern physics (e.g., classical mechanics, electromagnetism, quantum mechan-ics, and relativity) as well as advanced theoretical areas such as quantum field theory, string theory, and mathematical foundations. An example of the 3collected data is shown in Figure 3. 

3.2 Synthetic Data Generation 

To augment our dataset, we construct a conjec-ture generation and verification pipeline inspired by STP (Dong and Ma, 2025). Specifically, we treat our initial data as seed data, denoted as 

Dseed = {(hi, l i, p i)}Ni=1 , where hi, li, and pi are the header, lemma, and corresponding proof of the 

ith sample, and N is the total number of seed exam-ples. For each sample, we use Claude-4.5-Sonnet (Anthropic, 2025) to generate 10 conjectures by providing the header–lemma pairs (hi, l i), yield-ing 29,330 candidate statements. The prompting template is provided in Figure 7 in the Appendix. After collecting the conjectures, we apply a two-stage pipeline to select well-formed and correct statements. We first examine the syntactic cor-rectness of each conjecture. Specifically, for each conjecture cij , we append it to the correspond-ing header hi to form Dc = {(hi, c ij ) | i =1, . . . , N, j = 1 , . . . , 10 } and use the Lean ver-ifier to check whether the statement is well-formed. This includes verifying that all variables are prop-erly defined and that all referenced definitions and theorems exist. After this step, 6,971 conjectures remain, corresponding to a retention rate of 23.8%. The second stage examines the provability of conjectures. Given a conjecture with its corre-sponding header, we leverage DeepSeek-Prover-V2-7B (Ren et al., 2025), Kimina-Prover-Distill-8B (Wang et al., 2025a), and Goedel-Prover-V2-8B (Lin et al., 2025b), to generate 16 proofs, pro-ducing response samples {(hi, c ij , r p)}16 

> p=1

. A con-jecture is deemed provable if 

∃ p, 1 ≤ p ≤ 16 : Verify (hi, c ij , r p) = True 

where Verify denotes the Lean verification result. This process yields 2,608 verified conjectures, rep-resenting an overall pipeline yield rate of 8.9%, which is comparable to STP (Dong and Ma, 2025). Combining these with the 2,933 seed training ex-amples in Section 3.1 produces a total of 5,541 training instances for our experiments. Notably, we also compared different propri-etary models, including GPT-5 (OpenAI, 2025) and Gemini-2.5-Pro (Google, 2025). However, the syntactically correct rates of their generated conjec-tures were substantially lower than those produced by Claude. We additionally explored the approach of generating conjectures in natural language and converting them to Lean4 statements using an auto-formalizer. However, auto-formalizers fail at this task due to the difficulty of identifying a uniform header, which is caused by the complex depen-dencies in physical statements. Consequently, this approach also yielded low success rates. 

3.3 Self-Evolving Pipeline 

We conduct Reinforcement Learning (RL) to lift the performance on physics domain. Specifically, our experiments are mainly based on Group Rel-ative Policy Optimization (GRPO) (Shao et al., 2024). For each prompt x in the training set, they sample G (Group size) responses during the rollout stage, and optimize the following objective: 

JGRPO (θ) = Ex∼D ,{yi}Gi=1 ∼πθold (·| x)

 1

G

> G

X

> i=1

1

|yi|

> |yi|

X

> t=1

min 



wi,t (θ) ˆAi,t ,

clip (wi,t (θ), 1 − ε, 1 + ε) ˆAi,t 

i 

− β DKL (πθ∥πθref ) ,

where yi is the ith generated sequence of tokens, 

ε is the clip ratio. The importance ratio wi,t (θ) and the advantage ˆAi,t are calculated as follows: 

wi,t (θ) = πθ(yi,t |x, y i,<t )

πθold (yi,t |x, y i,<t ) ,

ˆAi,t = ˆAi = r(x, y i) − mean  {r(x, y i)}Gi=1 



std  {r(x, y i)}Gi=1 

 ,

respectively. And all the tokens in yi share the same advantage as ˆAi,t .The reward signal r(x, y i) is provided by the Lean verifier to guide the reinforcement learning process. Specifically, a score of 1 or 0 is presented to indicate whether the proof is correct or not. Be-cause of the symbolic nature of Lean, all verified proofs with reward 1 are correct, with no halluci-nation at all, which allows the model to learn the foundation of physics in a concrete and rigorous way. To further reduce the difficulty of the learning process, curriculum learning is employed, where the input statements (conjectures) are sorted based on their ground-truth proof lengths. This enables easy-to-hard learning, encouraging prover models to proceed in a bottom-up manner. 4Method Classical Particle & String Relativity Quantum Field Theory Overall                               

> Proprietary Models
> GPT-5 (OpenAI, 2025) 37.3% 13.4% 21.3% 35.2% 26.4%
> Claude-4.5-Sonnet (Anthropic, 2025) 52.9% 19.4% 29.5% 39.4% 34.4%
> Formal Math Provers
> Kimina-Prover-Distill-8B (Wang et al., 2025a) 35.3% 14.9% 29.5% 22.5% 24.8%
> Goedel-Prover-V2-8B (Lin et al., 2025b) 49.0% 19.4% 34.4% 28.2% 31.6%
> Deepseek-Prover-V2-7B (Ren et al., 2025) 54.9% 23.9% 37.7% 25.4% 34.0%
> Formal Physics Provers
> PhysProver 58.8% (+3.9%) 26.9% (+3.0%) 39.3% (+1.6%) 26.8% (+1.4%) 36.4% (+2.4%)

Table 1: Main experimental results . We evaluate all the models on PhysLeanData test set, which includes Classical, Particle & String, Relativity, and Quantum Field Theory domains. The pass@16 accuracy is reported. 

4 Experiments 

To evaluate our methodology, we use PhysLean-Data to train popular Lean-based formal mathe-matics provers. Our experiments reveal that strong mathematical reasoning models exhibit notable lim-itations when handling formal physics problems, underscoring the importance of domain-specific formal datasets and self-evolving strategies. 

4.1 Experimental Setup 4.1.1 Dataset and Tasks 

Model performance is evaluated on the test set of 

PhysLeanData , which shares the same source as the training set with a 9:1 train-test split. To ensure fair comparison across models with different con-text lengths, we retain only samples with prompt lengths under 4,096 tokens, resulting in 250 lem-mas in the final evaluation set. For finer-grained analysis, we organize the test samples into four physics categories: Classical & Foundational Physics, Particle & String Physics, Relativity & Spacetime, and Quantum Field The-ory. This classification reflects distinct theoreti-cal frameworks and varying levels of required do-main expertise. Further details are provided in Appendix B. 

4.1.2 Models and Baselines 

We compare several popular open-source prover models, including DeepSeek-Prover-V2-7B (Ren et al., 2025), Kimina-Prover-Distill-8B (Wang et al., 2025a), and Goedel-Prover-V2-8B (Lin et al., 2025b), all of which are strong formal theorem provers tailored for mathematical domains. Since DeepSeek-Prover-V2-7B performs the best among them, our experiments will focus on training the DeepSeek prover to push the boundaries of open-source models. For baselines, we first report the performance of DeepSeek-Prover-V2-7B, Kimina-Prover-Distill-8B, and Goedel-Prover-V2-8B without any ad-ditional training. We also include comparisons with strong proprietary systems, namely GPT-5 (OpenAI, 2025) and Claude-4.5-Sonnet (Anthropic, 2025). For all baselines, we use a fixed sampling budget and report pass@16 accuracy, ensuring fair comparison under a consistent inference budget. For open-source provers, we use the prompt tem-plate provided in Appendix D.1. For proprietary models, we employ a tailored Chain-of-Thought (CoT) (Wei et al., 2023) prompt to encourage step-by-step reasoning before generating the final proof. 

4.2 Implementation Details 

We directly apply Reinforcement Learning start-ing from the DeepSeek-Prover-V2-7B using verl (Sheng et al., 2025). Specifically, we apply GRPO with rule-based rewards (Lambert et al., 2025; DeepSeek-AI et al., 2025) to guide the self-evolving training. In particular, the Lean verifier with version 4.20.0 is integrated into the verl framework for verifying the proofs. The reward score for each trajectory is calculated as follows: 

r(x, y i) = 

(

1 if Verify (x, y i) = True 

0 otherwise Additionally, if the proof contains sorry, admit, or apply? keywords, we directly assign a 0 for the reward score to avoid reward hacking. Furthermore, to allow a smooth transition of difficulties during the learning process, curriculum learning (Parashar et al., 2025) is employed by sorting the lemma based on their ground-truth proof lengths. We train all models on 8 ×H200 GPUs with a constant learning rate of 1e−6 and a batch size of 256 for 2 epochs, where the training takes ap-proximately 8 hours. Notably, we do not have a warm-up stage with Supervised Fine-Tuning (SFT) 5because it degrades performance. This behavior is investigated and further analyzed in Section 6. We also investigate the Rejection-Sampling Fine-tuning method (Yuan et al., 2023; Dong et al., 2023) in Section 6. 

4.3 Experiment Results 

Our experimental results are presented in Table 1. We first observe that all existing models achieve rel-atively low scores despite their proficiency in math-ematical theorem proving, with none exceeding 40% accuracy. Notably, even small open-source theorem prover models exhibit competitive accu-racy compared to the latest proprietary systems, such as Claude-4.5-Sonnet and GPT-5. However, proprietary models demonstrate different strengths across physical domains compared to their open-source counterparts. For instance, all open-source provers achieve below 30% accuracy on Quantum Field Theory, whereas proprietary models exceed 35%. This suggests that proprietary and open-source models may be trained on different mixtures of physics data. We also investigated context length in the Quantum Field Theory category and found that the average length is one-third longer than in other domains. These findings align with those of Li et al. (2025), suggesting that larger models such as Claude demonstrate superior in-context learning ability, thereby achieving better performance than open-source models. Our trained model, PhysProver, substantially surpasses its formal mathematics prover counter-parts, consistently achieving gains across all cate-gories. Specifically, on the most challenging do-mains—Particle & String Physics—where all base-lines exhibit low accuracy, our model still yields a notable improvement of 3.0%. These results demonstrate the effectiveness of extending a math-ematics prover to physics domains with only small high-quality datasets. Moreover, the continued per-formance gains suggest that current provers are far from saturated, indicating that constructing high-quality physics-specific datasets remains a promis-ing direction On top of that, the small 7B-sized PhysProver model outperforms both GPT-5 and Claude-4.5-Sonnet in terms of overall performance, which shows the huge potential of small expert models in specific domains of formal physics theorem prov-ing. This provides a promising path toward efficient training of physics prover models. 

Header and Context Lemmas                                 

> import PhysLean.QFT.PerturbationTheory.WickAlgebra. NormalOrder.Lemmas import PhysLean.QFT.PerturbationTheory.WickAlgebra. TimeOrder namespace FieldSpecification variable { F: FieldSpecification} def timeContract ( φ ψ :F.FieldOp) : F.WickAlgebra :=
> T(ofFieldOp φ* ofFieldOp ψ) -T(ofFieldOp φ*ofFieldOp ψ)lemma timeContract_eq_superCommute ( φ ψ :F.FieldOp) :timeContract φψ = if timeOrderRel φψ then [anPart
> φ, ofFieldOp ψ]s
> else S(F|> sφ,F|> sψ)·[anPart ψ, ofFieldOp
> φ]s:= by split_ifs
> ·rename_i h rw [timeContract_of_timeOrderRel _ _ h]
> ·rename_i h rw [timeContract_of_not_timeOrderRel_expand _ _ h ]lemma timeContract_zero_of_diff_grade ( φ ψ :F.FieldOp) (h : ( F|> sφ)̸=(F|> sψ)) : timeContract φψ = 0 := by by_cases h1 : timeOrderRel φψ
> ·rw [timeContract_of_timeOrderRel _ _ h1] rw [ superCommute_anPart_ofFieldOpF_diff_grade_zero ]exact h
> ·rw [timeContract_of_not_timeOrderRel _ _ h1] rw [timeContract_of_timeOrderRel _ _ _] rw [ superCommute_anPart_ofFieldOpF_diff_grade_zero ]simp only [instCommGroup.eq_1, smul_zero] exact h.symm have ht := IsTotal.total (r := F.timeOrderRel) φψ
> simp_all
> ········

DeepseekProver + PhysProver      

> /-- This lemma claims that the normal ordering of the time-contracted product of any two field operators φand ψis identically zero.-/ lemma normalOrder_timeContract ( φ ψ :F.FieldOp) :
> N(timeContract φψ ) = 0 := by rw [timeContract_eq_superCommute] split_ifs <;> simp_all [timeContract, superCommute_anPart_ofFieldOpF_diff_grade_zero ,smul_eq_zero, map_zero]

DeepseekProver    

> lemma normalOrder_timeContract ( φ ψ :F.FieldOp) :
> N(timeContract φψ ) = 0 := by rw [timeContract_eq_superCommute] split_ifs
> ·rw [normalOrder_ofFieldOp_pair_eq_zero] <;> simp_all [timeOrderRel_of_isContraction]
> ·rw [normalOrder_anPart_ofFieldOp_smul] <;> simp_all [timeOrderRel_of_isContraction]

Figure 2: Successful examples from the PhysProver 

and failed proofs from the base model for the same statements. PhysProver demonstrates better in-context learning ability to make good usage of lemmas. 

6Problem Category Deepseek-Prover-V2 Deepseek-Prover-V2 + PhysLeanData 

Pass@16 Pass@16 IMO 4/20 = 20.0% 4/20 = 20.0% Olympiad AIME 8/15 = 53.3% 7/15 = 46.7% AMC 25/45 = 55.6% 25/45 = 55.6% MATH Algebra 63/70 = 90.0% 65/70 = 92.9% Number Theory 51/60 = 85.0% 53/60 = 88.3% Algebra 8/18 = 44.4% 8/18 = 44.4% Custom Number Theory 4/8 = 50.0% 4/8 = 50.0% Induction 4/8 = 50.0% 4/8 = 50.0% Overall Pass Rate 167/244 = 68.4% 170/244 = 69.7%   

> Table 2: Out-of-Distribution Generalization in Formal Math Proving on MiniF2F-Test (Zheng et al., 2022).

5 Analysis 

5.1 Improved In-Context Learning Through Reinforcement Learning 

In this subsection, we provide a detailed analysis of the performance gains achieved by PhysProver 

through a comparative examination of proofs gen-erated by the baselines and our model. Figure 2 presents an illustrative example from our test set along with the corresponding generations. The header and lemmas constitute the context for phys-ical theorem proving, where the lemmas serve as auxiliary tools during the proof process. We observe that PhysProver consistently makes correct use of functions and lemmas, with successful applications highlighted in blue. For instance, to prove the given conjecture, it first applies timeContract_eq_superCommute ,followed by the function timeContract . Sub-sequently, the model correctly invokes super-Commute_anPart_ofFieldOpF_diff_grade_zero ,demonstrating effective utilization of contextual information. By synthesizing the knowledge provided in the context, PhysProver successfully completes the proof. In contrast, while the base model initially ap-plies timeContract_eq_superCommute correctly, it subsequently generates hallucinated content, including non-existent lemmas such as nor-malOrder_ofFieldOp_pair_eq_zero and timeOrder-Rel_of_isContraction (marked in red). These ob-servations suggest that the reinforcement learning process on PhysLeanData enhances performance by enabling the model to better leverage contextual information and comprehend domain-specific ter-minology. This finding also accounts for the low ac-curacy observed across all base models: their unfa-miliarity with physics-specific lemmas and contex-tual structures impedes their ability to effectively utilize these resources for proof completion. 

5.2 Out-of-Distribution Generalization 

Surprisingly, we also observe that training on physics-centered problems yields notable gener-alization improvements in formal mathematical theorem proving. In this subsection, we evaluate our trained model on MiniF2F-Test (Zheng et al., 2022), which comprises 244 Lean4 statements in the mathematics domain, ranging from high school competition problems to elementary undergraduate-level proofs. We partition the dataset into several categories following Ren et al. (2025). For each statement in MiniF2F-Test, we prompt both the baseline and our trained model to generate 16 tra-jectories and compute pass@16 accuracy. We use the same prompt template from the DeepSeek web-site 2.As shown in Table 2, PhysProver overall achieve comparable performance and even surpass their base versions. It is worth noticing that the improvement is not consistent across all categories. For example, our model demonstrates meaning-ful gains on medium-level problems from MATH (Hendrycks et al., 2021). Conversely, more chal-lenging Olympiad-level problems may not benefit from GRPO training, as performance drops in the AIME category. These results reveal both the in-trinsic connections and distinctions between mathe-matical and physical theorem proving in Lean4. In 

> 2https://huggingface.co/deepseek-ai/ DeepSeek-Prover-V2-7B

7general, training on physics problems can enhance mathematical reasoning capabilities. However, dif-ficult mathematics problems may demand substan-tially different problem-solving skills that cannot be directly acquired from physics-based training. 

6 Revisiting the Role of Supervised Fine-tuning 

We additionally investigated whether conducting Supervised Fine-tuning (SFT) on PhysLeanData 

could enhance model performance on Physics, fol-lowing standard practice in training specialized LLMs. However, we did not observe any improve-ment on our test set after SFT. Instead, we observed consistent performance degradation. Specifically, we first fine-tuned Deepseek-Prover-V2-7B on PhysLeanData , where ground-truth answers were either extracted from the PhysLean library written by humans , or generated by open-source provers with subsequent verifica-tions. The training sample template follows the RL prompt template in D.1, with loss computation restricted to the completion portion. We then con-sider Rejection Sampling Fine-tuning, or Reward-Ranked Fine-tuning (RAFT) (Dong et al., 2023; Yuan et al., 2023), where we sample Deepseek-Prover-V2-7B on the training set and retain only the correct proofs as our new training set. We fine-tuned Deepseek-Prover-V2-7B on both train-ing sets for one epoch, using a learning rate of 5e−7

and a batch size of 32. They are denoted as DS-Prover-SFT and DS-Prover-RAFT, respectively. As shown in Table 3, for the DS-Prover-SFT, we observe consistent performance degradation across all categories, with an average accuracy decline of 6.4%. On the contrary, DS-Prover-RAFT demon-strates an overall 1.6% improvement, with all other 3 categories increasing except Classical Physics. We attribute this performance difference to the dis-tributional properties of the training data. The orig-inal PhysLeanData consists of human-written ex-amples, which are Out-of-Distribution (OOD) with respect to the model’s generation capabilities. In contrast, Rejection Sampling yields In-Distribution (ID) data that more closely aligns with the model’s output distribution. Consequently, ID data may be easier for the model to learn from, leading to improved performance. To gain a closer look into this phenomenon, prob-ing experiments are conducted to compare the un-certainty of the SFT model, RAFT model (Table 3), and the GRPO model from our main experiments. To assess model uncertainty on both training and test data, we measured the average perplexity of sampled responses conditioned on input prompts. Given a prompt x from either the training or test set, we sampled K = 16 responses yk from the model and computed the mean perplexity across these samples. We randomly selected 50 samples from each of the training and test sets. The computation is defined as: 

P P L (x) = 1

K

> K

X

> k=1

P P L (y(k)), y(k) ∼ pθ(· | x)

where 

P P L (y) = exp 

− 1

|y|

> |y|

X

> t=1

log pθ(yt | y<t , x )

 .

This metric captures the model’s self-uncertainty: lower values indicate that the model generates re-sponses it considers likely and more relevant to the input, while higher values suggest greater variabil-ity or unfamiliarity with the prompt. As shown in Table 4, the findings demonstrate that the average perplexity for DS-Prover-GRPO and DS-Prover-RAFT is substantially lower than that of DS-Prover-SFT on both the training and test sets, which explains why GRPO and RAFT im-prove the performance while SFT does not. These results suggest that although supervised fine-tuning directly maximizes the probability of target tokens, it does not necessarily reduce model uncertainty, particularly for models such as DeepSeek-Prover that have already undergone extensive domain-specific (MATH) training. This observation offers an important insight for further improving expert models: supervised fine-tuning may not always be necessary or optimal. On the contrary, using the Re-jection Sampling Fine-tuning method to collect and fine-tune on In-Distribution (ID) data could be a pratical solution. Additionally, direct application of reinforcement learning can serve as a viable alterna-tive, particularly in low-resource settings. We also explore Reinforcement Learning after Rejection-Sampling Fine-tuning in Appendix C, but do not observe improvements. 

7 Conclusion 

In this paper, we present the first systematic effort to advance formal theorem proving in the physi-cal domain. We first introduce PhysLeanData , a 8Method Budget Classical Particle & String Relativity Quantum Field Theory Overall Deepseek-Prover-V2-7B pass@16 54.9% 23.9% 37.7% 25.4% 34.0%             

> Deepseek-Prover-V2-7B + Phys SFT pass@16 45.1% (-9.8%) 19.4% (-4.5%) 26.2% (-11.5%) 23.9 (-1.5%) 27.6% (-6.4%)
> Deepseek-Prover-V2-7B + Phys RAFT pass@16 52.9% (-2.0%) 25.4% (+1.5%) 41.0% (+3.3%) 28.2 (+2.8%) 35.6% (+1.6%)

Table 3: Supervised Fine-tuning (SFT) and Rejection-Sampling Fine-tuning (RAFT) on PhysLeanData of Deepseek-Prover-V2-7B. The pass@16 accuracy drops significantly after SFT, but increases after RAFT. 

Training Set Test Set DS-Prover-SFT 1.817 1.711 DS-Prover-RAFT 1.321 1.186 

DS-Prover-GRPO 1.141 1.209 

Table 4: Perplexity on the training set and the test set for DS-Prover-SFT, DS-Prover-RAFT, and DS-Prover-GRPO. 

dataset of physical theorems formalized in Lean4, along with a conjecture formulation pipeline for generating valid and correct conjectures. By ap-plying Reinforcement Learning with Verifiable Re-wards (RLVR) to an open-source state-of-the-art theorem prover, our PhysProver achieves con-sistent 2.4% improvements across physical sub-domains such as Quantum Field Theory using only 5K samples. The model also demonstrates over 1% 

improvement on the out-of-distribution MiniF2F-test benchmark, highlighting strong generalization capability. Our work bridges a critical gap between formal theorem proving in mathematics and its ap-plication to the physical sciences. We will publicly release our dataset and models to facilitate future research in this direction. 

8 Limitations 

Our work has several limitations that we acknowl-edge and hope to address in future research. First, due to computational resource constraints, we were unable to collect more data or scale the conjecture generation process to a larger extent. As noted in Section 3.2, our synthetic data pipeline has a yield rate of only 8.9%, meaning that a substantial portion of generated conjectures are filtered out during validity and correctness verification. Scal-ing up the generation process would require sig-nificantly more compute for both the LLM-based conjecture generation and the multi-prover verifi-cation stage, which was beyond our current budget. Additionally, our dataset is derived solely from the PhysLean repository, which, while comprehensive, may not cover all areas of physics uniformly. Cer-tain specialized domains may be underrepresented, potentially limiting the model’s applicability to the full breadth of physical theorem proving. 

References 

Avinash Anand, Kritarth Prasad, Chhavi Kirtani, Ash-win R Nair, Mohit Gupta, Saloni Garg, Anurag Gau-tam, Snehal Buldeo, and Rajiv Ratn Shah. 2024. En-hancing llms for physics problem-solving using rein-forcement learning with human-ai feedback. arXiv preprint arXiv:2412.06827 .Anthropic. 2025. Claude sonnet 4.5 system card. Tech-nical report, Anthropic. Jiacheng Chen, Qianjia Cheng, Fangchen Yu, Haiyuan Wan, Yuchen Zhang, Shenghe Zheng, Junchi Yao, Qingyang Zhang, Haonan He, Yun Luo, and 1 others. 2025a. P1: Mastering physics olympiads with reinforcement learning. arXiv preprint arXiv:2511.13612 .Jiangjie Chen, Wenxiang Chen, Jiacheng Du, Jinyi Hu, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Wenlei Shi, and 1 others. 2025b. Seed-prover 1.5: Mastering undergraduate-level the-orem proving via learning from experience. arXiv preprint arXiv:2512.17260 .Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, and 1 oth-ers. 2025c. Seed-prover: Deep and broad reason-ing for automated theorem proving. arXiv preprint arXiv:2507.23726 .Projet Coq. 1996. The coq proof assistant-reference manual. INRIA Rocquencourt and ENS Lyon, ver-sion , 5. Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. 2015. The lean theorem prover (system description). In Auto-mated Deduction-CADE-25: 25th International Con-ference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25 , pages 378–388. Springer. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-hong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025. Deepseek-r1: Incentivizing reasoning capa-bility in llms via reinforcement learning. Preprint ,arXiv:2501.12948. 

9Jingzhe Ding, Yan Cen, and Xinyuan Wei. 2023. Using large language model to solve and explain physics word problems approaching human level. arXiv preprint arXiv:2309.08182 .Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundation model alignment. Preprint , arXiv:2304.06767. Kefan Dong and Tengyu Ma. 2025. Stp: Self-play llm theorem provers with iterative conjecturing and proving. Preprint , arXiv:2502.00212. Google. 2025. Gemini 2.5: Our newest gemini model with thinking. https: //blog.google/technology/google-deepmind/ gemini-model-thinking-updates-march-2025/ .Accessed: 2025. John Harrison. 2009. Hol light: An overview. In Inter-national Conference on Theorem Proving in Higher Order Logics , pages 60–66. Springer. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: A challenging benchmark for pro-moting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd An-nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3828– 3850. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. Preprint ,arXiv:2103.03874. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Sol-daini, and 4 others. 2025. Tulu 3: Pushing fron-tiers in open language model post-training. Preprint ,arXiv:2411.15124. Yuxin Li, Minghao Liu, Ruida Wang, Wenzhao Ji, Zhi-tao He, Rui Pan, Junming Huang, Tong Zhang, and Yi R Fung. 2025. Lean4physics: Comprehensive rea-soning framework for college-level physics in lean4. 

arXiv preprint arXiv:2510.26094 .Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, and Chi Jin. 2025a. Goedel-prover: A frontier model for open-source automated theorem proving. Preprint ,arXiv:2502.07640. Yong Lin, Shange Tang, Bohan Lyu, Ziran Yang, Jui-Hui Chung, Haoyu Zhao, Lai Jiang, Yihan Geng, Jiawei Ge, Jingruo Sun, and 1 others. 2025b. Goedel-prover-v2: Scaling formal theorem proving with scaffolded data synthesis and self-correction. arXiv preprint arXiv:2508.03613 .Leonardo de Moura and Sebastian Ullrich. 2021a. The lean 4 theorem prover and programming language. In Automated Deduction – CADE 28 , pages 625–635, Cham. Springer International Publishing. Leonardo de Moura and Sebastian Ullrich. 2021b. The lean 4 theorem prover and programming language. In 

Automated Deduction–CADE 28: 28th International Conference on Automated Deduction, Virtual Event, July 12–15, 2021, Proceedings 28 , pages 625–635. Springer. Allen Newell and Herbert Alexander Simon. 1956. The Logic Theory Machine: A Complex Information Pro-cessing System . RAND Corporation, Santa Monica, CA. OpenAI. 2025. GPT-5 system card. Technical report, OpenAI. Xinyu Pang, Ruixin Hong, Zhanke Zhou, Fangrui Lv, Xinwei Yang, Zhilong Liang, Bo Han, and Chang-shui Zhang. 2025. Physics reasoner: Knowledge-augmented reasoning for solving physics problems with large language models. In Proceedings of the 31st International Conference on Computational Lin-guistics , pages 11274–11289. Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, and Shuiwang Ji. 2025. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning. Preprint ,arXiv:2506.06632. Lawrence C Paulson. 1994. Isabelle: A generic theorem prover . Springer. Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Man-tas Baksys, Igor Babuschkin, and Ilya Sutskever. 2022. Formal mathematics statement curriculum learning. arXiv preprint arXiv:2202.01344 .David Rein, Betty Li Hou, Asa Cooper Stickland, Jack-son Petty, Richard Yuanzhe Pang, Julien Dirani, Ju-lian Michael, and Samuel R Bowman. 2024. Gpqa: A graduate-level google-proof q&a benchmark. In 

First Conference on Language Modeling .ZZ Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, and 1 others. 2025. Deepseek-prover-v2: Advancing formal mathemati-cal reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801 .Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 .

10 Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. Hybridflow: A flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Sys-tems , EuroSys ’25, page 1279–1297. ACM. Joseph Tooby-Smith. 2025. Heplean: Digitalising high energy physics. Computer Physics Communications ,308:109457. George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, and Swarat Chaudhuri. 2024. Putnam-bench: Evaluating neural theorem-provers on the putnam mathematical competition. Preprint ,arXiv:2407.11214. Sumanth Varambally, Thomas Voice, Yanchao Sun, Zhifeng Chen, Rose Yu, and Ke Ye. 2025. Hilbert: Recursively building formal proofs with informal rea-soning. Preprint , arXiv:2509.22819. Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina Vinyes, Zhenzhe Ying, Zekai Zhu, and 1 oth-ers. 2025a. Kimina-prover preview: Towards large formal reasoning models with reinforcement learning. 

arXiv preprint arXiv:2504.11354 .Ruida Wang, Yuxin Li, Tong Zhang, and 1 others. 2025b. Let’s reason formally: Natural-formal hybrid reason-ing enhances llm’s math capability. arXiv preprint arXiv:2505.23703 .Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, and Tong Zhang. 2025c. Ma-lot: Model-collaboration lean-based long chain-of-thought reasoning en-hances formal theorem proving. arXiv preprint arXiv:2503.03205 .Ruida Wang, Jiarui Yao, Rui Pan, Shizhe Diao, and Tong Zhang. 2025d. Gar: Generative adversarial reinforcement learning for formal theorem proving. 

arXiv preprint arXiv:2510.11769 .Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, and Tong Zhang. 2024. Theoreml-lama: Transforming general-purpose llms into lean4 experts. Preprint , arXiv:2407.03203. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2023. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. 

arXiv preprint arXiv:2307.10635 .Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elic-its reasoning in large language models. Preprint ,arXiv:2201.11903. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. 2024. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. Preprint , arXiv:2405.14333. Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, and Yang Wang. 2025. Ugphysics: A comprehensive bench-mark for undergraduate physics reasoning with large language models. arXiv preprint arXiv:2502.00334 .Fangchen Yu, Haiyuan Wan, Qianjia Cheng, Yuchen Zhang, Jiacheng Chen, Fujun Han, Yulun Wu, Junchi Yao, Ruilizhen Hu, Ning Ding, and 1 others. 2025. Hipho: How far are (m) llms from humans in the lat-est high school physics olympiad benchmark? arXiv preprint arXiv:2509.07894 .Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. 

Preprint , arXiv:2308.01825. Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. 2022. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. Preprint ,arXiv:2109.00110. 

A Author Contributions 

This work stems from all authors’ valuable contri-butions and close collaborations. 

HZ , together with RW, develops the Lean verifi-cation pipeline. HZ constructs the conjecture gen-eration pipeline to generate conjectures, develops the RL training pipeline integrated with Lean eval-uation, conducts all experiments presented in this paper, and, based on the skeleton provided by RP, writes most of the paper except for the related work section and performs polishing. 

RW , provides the initial main idea for the paper. RW builds most of the Lean verification pipeline and collaborates closely with HZ on both data gen-eration and model training. Additionally, RW also contributed to analysis of case study. In terms of paper writing, RW contributed to the Introduction and Related Work sections and modified the other sections in terms of logical and conceptual coher-ence. 

RP , together with RW, initiates the project, draft-ing proposals to ensure computational resources, debugging the Lean evaluation pipeline, and driv-ing collaboration efforts between different mem-bers. RP also provides the skeleton of the paper, the first draft of the experimental section, draws Figure 1, and polishes the paper by supplementing 11 Section 3.3 and improving coherence/grammatical correctness. 

WW , together with RW, provides the initial Lean dataset, consisting of provable theorems and their proof scripts extracted from the P HYS LEAN li-brary. WW also conducts preliminary explorations on SFT experimental design and the verification pipeline. In terms of paper writing, the collected dataset is described in Section 3.1 Seed Dataset Collection. 

BM , together with HZ and RW, reviewed case stud-ies for both open-source and proprietary model re-sponses. BM contributed physics expertise, ensur-ing the mathematics accuracy and physics sound-ness of all model outputs. 

TZ supports the work and provides computational resources, guidance, and suggestions for experi-ment design and paper writing. 

B PhysProver Categories 

The Classical & Foundational Physics category groups core undergraduate-level subjects, includ-ing mathematical methods, classical mechanics, quantum mechanics, statistical mechanics, and electromagnetism. These areas represent foun-dational discoveries in physics and are primarily textbook-driven, with standardized problem formu-lations and solution methods. Particle and String Physics is grouped separately to capture topics centered on high-energy physics and fundamental interactions, often motivated by experimental programs such as those at the Large Hadron Collider. String theory topics are included in this category due to their close conceptual align-ment with high-energy theoretical frameworks. Quantum Field Theory and Relativity are treated as distinct categories due to their advanced mathe-matical structure and conceptual complexity. Both subjects are typically introduced at the graduate level, with quantum field theory extending quantum mechanics and relativity providing a foundational framework for spacetime and gravitation. 

C Reinforcement Learning after Rejection-Sampling Fine-tuning 

In this section, we investigate the effectiveness of Reinforcement Learning (RL) following Rejection-Sampling Fine-tuning (RAFT). Starting from the RAFT checkpoint described in Section 6, we apply RL using the same hyperparameters and settings as in our main experiments (Section 4.2). We save checkpoints every 5 steps and report RL perfor-mance in Table 5. We observe that overall performance does not im-prove beyond 30 steps of GRPO training, with over-all accuracy remaining unchanged (both 35.6%). While the Particle & String category shows im-provement relative to both the base model and the RAFT model, we observe a corresponding perfor-mance drop in the Relativity category, indicating inconsistent gains across categories. Throughout the RL process, both overall and category-level accuracies fluctuate without demonstrating a clear trend. Although we observe the best overall results at Step 20, we attribute this to random noise given the training process. These results suggest that RL after RAFT may not be consistently effective when both stages use the same prompt set. This observa-tion aligns with common practice in LLM training, where different prompt sets are typically employed for fine-tuning and reinforcement learning. 

D Experimental Details 

D.1 Prompt Template for Main Experiments 

We list the prompt template for DeepSeek-Prover-V2-7B in Figure 4 with a concrete example from 

PhysLeanData . Specifically, in the user round, we append the statement with sorry to act as a task for the model. And in the assistant round, we give the model the entire context of the header and the state-ment. We chose this template because we found it stable for the models to generate proof completions, as it allows the model to directly generate the proof part. The prompt template for Kimina-Prover and Goedel-Prover is exactly the same except for the special tokens which are shown in Figure 5. The prompt template for the proprietary models is different due to the inaccessibility of the actual models. Instead, we apply the Chain-of-Thought prompting (Wei et al., 2023), and the template is shown in Figure 6 We also list the prompt template for Claude-4.5-Sonnet to generate conjectures in Figure 7. 

D.2 Conjecture Generation 

We also list an example of the conjecture, given a header and a lemma, which is shown in Fig-ure 8. In this example, the conjecture is a variant of the original lemma - Both lemmas are basic al-gebraic properties of matrix-vector multiplication. In other examples we examined, the conjectures 12 Method Budget Classical Particle & String Relativity Quantum Field Theory Overall DeepSeek-Prover-V2-7B pass@16 54.9% 23.9% 37.7% 25.4% 34.0%                                            

> +RAFT pass@16 52.9% (-2.0%) 25.4% (+1.5%) 41.0% (+3.3%) 28.2 (+2.8%) 35.6% (+1.6%)
> RAFT+GRPO Step 5 pass@16 52.9% (-2.0%) 23.9% (+0.0%) 41.0% (+3.3%) 28.2 (+2.8%) 35.2% (+1.2%)
> RAFT+GRPO Step 10 pass@16 52.9% (-2.0%) 25.4% (+1.5%) 39.3% (+1.6%) 26.8% (+1.4%) 34.8% (+0.8%)
> RAFT+GRPO Step 15 pass@16 56.9% (+2.0%) 25.4% (+1.5%) 39.3% (+1.6%) 26.8% (+1.4%) 35.6% (+1.6%)
> RAFT+GRPO Step 20 pass@16 54.9% (+0.0%) 25.4% (+1.5%) 41.0% (+3.3%) 28.2 (+2.8%) 36.0% (+2.0%) RAFT+GRPO Step 25 pass@16 54.9% (+0.0%) 25.4% (+1.5%) 39.3% (+1.6%) 28.2 (+2.8%) 35.6% (+1.6%)
> RAFT+GRPO Step 30 pass@16 54.9% (+0.0%) 26.9% (+3.0%) 37.7% (+0.0%) 28.2 (+2.8%) 35.6% (+1.6%)

Table 5: Reinforcement Learning after Rejection-Sampling Fine-tuning (RAFT) on PhysLeanData of Deepseek-Prover-V2-7B. The pass@16 accuracy does not improve after RAFT. 

consistently fall into predictable categories relative to their preceding lemmas. These variations typ-ically manifest as: (1) special cases obtained by substituting specific values such as zero, (2) defi-nitional unwrapping where a concept is restated in more explicit form, or (3) algebraic variants that apply the same underlying principle to a differ-ent argument. Since PhysLean formalizes abstract concepts in mathematical physics, many lemmas establish foundational API properties—such as lin-earity, group action behavior, or interaction with zero elements—and the conjectures naturally com-plete this API by covering symmetric or boundary cases. Consequently, these conjectures primarily serve to help models become more familiar with the dataset’s conventions, naming patterns, and proof structures to improve the accuracy, rather than to evaluate deep mathematical reasoning capabilities. 13 Figure 3: An example of the PhysLeanData . The black lines denote the header, the blue lines denote the lemma statement, and the red lines denote the proof. 

Example of Header, Statement, and the Proof from PhysLean 

import PhysLean.Mathematics.Fin.Involutions import PhysLean.QFT.PerturbationTheory.WickContraction.ExtractEquiv import PhysLean.QFT.PerturbationTheory.WickContraction.Involutions open FieldSpecification variable { F : FieldSpecification} namespace WickContraction variable {n : N } (c : WickContraction n) open PhysLean.List open FieldStatistic open Nat def IsFull : Prop := c.uncontracted = ∅

instance : Decidable (IsFull c) := decEq c.uncontracted ∅

/-- This theorem states that the configuration c is full if and only if, for every index i in the finite set of size n, the involution function associated with c does not map i to itself.-/ lemma isFull_iff_equivInvolution_no_fixed_point : IsFull c ↔∀ (i : Fin n), (equivInvolution c).1 i ̸ =i := by simp only [IsFull, ne_eq] rw [Finset.eq_empty_iff_forall_notMem] simp [equivInvolution, toInvolution, uncontracted] 

14 Figure 4: Prompt template for DeepSeek Prover with a concrete example 

DeepSeek Prover Prompt Template 

<|begin_of_sentence|> You are an expert in mathematics, physics and Lean 4. <|User|>Think about and solve the following problem step by step in Lean 4. →Directly give the formal statement in Lean 4. # Problem: This theorem states that the matrix product of the Minkowski metric with itself yields the identity matrix. # Formal statement: 

``` lean4 import Mathlib.Algebra.Lie.Classical import Mathlib.Analysis.Normed.Ring.Lemmas open Matrix def minkowskiMatrix {d : N} : Matrix (Fin 1 ⊕Fin d) (Fin 1 ⊕Fin d) R:= LieAlgebra.Orthogonal.indefiniteDiagonal (Fin 1) (Fin d) R

namespace minkowskiMatrix variable {d : N}scoped[minkowskiMatrix] notation " η" => minkowskiMatrix /-- This theorem states that the matrix product of the Minkowski metric with itself yields the identity matrix.-/ @[simp] lemma sq : @minkowskiMatrix d * minkowskiMatrix = 1 := sorry 

``` 

<|Assistant|> 

``` lean4 import Mathlib.Algebra.Lie.Classical import Mathlib.Analysis.Normed.Ring.Lemmas open Matrix def minkowskiMatrix {d : N} : Matrix (Fin 1 ⊕Fin d) (Fin 1 ⊕Fin d) R:= LieAlgebra.Orthogonal.indefiniteDiagonal (Fin 1) (Fin d) R

namespace minkowskiMatrix variable {d : N}scoped[minkowskiMatrix] notation " η" => minkowskiMatrix /-- This theorem states that the matrix product of the Minkowski metric with itself yields the identity matrix.-/ @[simp] lemma sq : @minkowskiMatrix d * minkowskiMatrix = 1 := 

15 Figure 5: Prompt template for Kimina Prover and Goedel Prover with a concrete example. They are the same as the Deepseek Prover except for the special tokens, such as the BOS token. 

Kimina and Goedel Provers Prompt Template 

<|im_start|>system You are an expert in mathematics, physics and Lean 4.<|im_end|> <|im_start|>user Think about and solve the following problem step by step in Lean 4. →Directly give the formal statement in Lean 4. # Problem: This theorem states that the matrix product of the Minkowski metric with itself yields the identity matrix. # Formal statement: 

``` lean4 import Mathlib.Algebra.Lie.Classical import Mathlib.Analysis.Normed.Ring.Lemmas open Matrix def minkowskiMatrix {d : N} : Matrix (Fin 1 ⊕Fin d) (Fin 1 ⊕Fin d) R:= LieAlgebra.Orthogonal.indefiniteDiagonal (Fin 1) (Fin d) R

namespace minkowskiMatrix variable {d : N}scoped[minkowskiMatrix] notation " η" => minkowskiMatrix /-- This theorem states that the matrix product of the Minkowski metric with itself yields the identity matrix.-/ @[simp] lemma sq : @minkowskiMatrix d * minkowskiMatrix = 1 := sorry 

``` <|im_end|> <|im_start|>assistant 

``` lean4 import Mathlib.Algebra.Lie.Classical import Mathlib.Analysis.Normed.Ring.Lemmas open Matrix def minkowskiMatrix {d : N} : Matrix (Fin 1 ⊕Fin d) (Fin 1 ⊕Fin d) R:= LieAlgebra.Orthogonal.indefiniteDiagonal (Fin 1) (Fin d) R

namespace minkowskiMatrix variable {d : N}scoped[minkowskiMatrix] notation " η" => minkowskiMatrix /-- This theorem states that the matrix product of the Minkowski metric with itself yields the identity matrix.-/ @[simp] lemma sq : @minkowskiMatrix d * minkowskiMatrix = 1 := 

16 Figure 6: Prompt template for proprietary models for the main experiments. 

Proprietary Models Prompt Template 

Complete the following Lean 4 code: 

``` lean4 {THE LEAN4 HEADERS AND STATEMENT} 

``` 

Before producing the Lean 4 code to formally prove the given theorem, provide a proof plan outlining the main proof steps and strategies. The plan should highlight key ideas, intermediate lemmas, and proof structures that will guide the construction of the final formal proof. 

Figure 7: Prompt template for Claude-4.5-Sonnet to generate formal conjectures 

Claude-4.5-Sonnet Prompt Template for Conjecture Generation 

You are an expert in mathematics, physics and Lean 4. You are provided a context, a lemma, and a proof. Your task is to generate a list of 10 related physics conjecture in formal language based on the context and the seed language statements. The conjectures should be: 1. A meaningful variant of the original theorem: modify hypotheses, generalize structures, or extend scope while keeping the core mathematical insight. 2. Must differ significantly in mathematical content (changed assumptions, stronger/weaker conclusions, or different algebraic structures) but remain recognizably related. 3. The new conjecture should be in formal language. 4. Do not include the proof. When generating the conjectures, preserve all specific Lean identifiers exactly as they appear in the formal statement. You can also refer to the original formal statement. Context: {context} Natural Language Statement: {nq} Original Formal Statement: {theorem} Return the final conjectures in JSON format as a dictionary where: - The key is "conjectures" - The value is a list of dictionaries - Each dictionary in the list has a key "statement" whose value is a string containing one conjecture Please read, understand, and then generate a list of conjectures. 

17 Figure 8: An example of the conjecture based on the header and the lemma 

The Header of the Original Lemma 

import PhysLean.Relativity.PauliMatrices.SelfAdjoint import Mathlib.RepresentationTheory.Basic import PhysLean.Relativity.Lorentz.Group.Basic import Mathlib.Analysis.InnerProductSpace.PiL2 namespace Lorentz noncomputable section open Matrix open MatrixGroups open Complex structure ContrMod (d : N) where val : Fin 1 ⊕Fin d →R

namespace ContrMod variable {d : N}def toFin1d RFun : ContrMod d ≃(Fin 1 ⊕Fin d →R) where toFun v := v.val invFun f := ⟨f⟩

left_inv _ := rfl right_inv _ := rfl instance : AddCommMonoid (ContrMod d) := Equiv.addCommMonoid toFin1d RFun instance : AddCommGroup (ContrMod d) := Equiv.addCommGroup toFin1d RFun instance : Module R(ContrMod d) := Equiv.module RtoFin1d RFun def toFin1d REquiv : ContrMod d ≃l[R] (Fin 1 ⊕Fin d →R) := Equiv.linearEquiv RtoFin1d RFun abbrev toFin1d R (ψ : ContrMod d) := toFin1d REquiv ψ

@[simps!] def stdBasis : Basis (Fin 1 ⊕Fin d) R(ContrMod d) := Basis.ofEquivFun toFin1d REquiv abbrev mulVec (M : Matrix (Fin 1 ⊕Fin d) (Fin 1 ⊕Fin d) R) (v : ContrMod d) : ContrMod d := Matrix.toLinAlgEquiv stdBasis M v scoped[Lorentz] infixr:73 " ∗v " => ContrMod.mulVec 

The Original Lemma 

/-- This theorem states that for any two real matrices M and N of dimension (1+d) ×(1+d) and any vector v in ContrMod d, the matrix-vector product of their difference with v equals the difference of their individual matrix-vector products with v.-/ lemma sub_mulVec (M N : Matrix (Fin 1 ⊕Fin d) (Fin 1 ⊕Fin d) R) (v : ContrMod d) : (M - N) ∗v v = M ∗v v - N ∗v v := sorry 

The New Conjecture 

lemma mulVec_zero (M : Matrix (Fin 1 ⊕Fin d) (Fin 1 ⊕Fin d) R) : M ∗v (0 : ContrMod d) = 0 := sorry 

18