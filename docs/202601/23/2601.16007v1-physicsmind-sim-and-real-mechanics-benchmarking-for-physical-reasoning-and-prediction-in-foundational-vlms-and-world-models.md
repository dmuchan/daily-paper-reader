# PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models
# PhysicsMind：用于评估基础视觉语言模型与世界模型物理推理与预测能力的仿真与真实力学基准测试

**Authors**: Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi, Kevin Zhang, Yichen Wu, Yangfan He, Chun-Kai Fan, Wentao Lu, Kuangzhi Ge, Xinyu Fang, Hongyang He, Kuan Lu, Tianxiang Xu, Li Zhang, Yongxin Ni, Youhua Li, Shanghang Zhang
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.16007v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 8.0
**Evidence**: Benchmark for physical reasoning in foundational multimodal large language models

---

## Abstract
Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.

## 摘要
现代

---

## 论文详细总结（自动生成）

这篇论文介绍了 **PhysicsMind**，这是一个旨在评估多模态大语言模型（MLLM）和视频世界模型在物理推理与预测方面能力的综合性基准测试。

以下是对该论文的结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **核心问题**：尽管当前的基础多模态模型在数学和常识推理上表现出色，但它们对底层物理规律（力学）的理解仍然薄弱。
*   **研究动机**：现有的物理基准测试存在碎片化问题：要么依赖于合成的视觉问答（VQA）模板，容易被模型通过语言捷径破解；要么仅关注视频生成的视觉质量（如平滑度），而忽略了运动是否符合物理定律。
*   **整体含义**：论文提出了一个统一的框架，结合真实世界和仿真环境，直接测试模型是否遵循**质心（Center of Mass）**、**杠杆平衡（Lever Equilibrium）**和**牛顿第一定律（Newton’s First Law）**这三大经典力学原则。

### 2. 核心方法论
PhysicsMind 包含两个互补的任务，旨在从“理解”和“预测”两个维度评估模型：
*   **三大力学场景**：
    1.  **质心**：评估模型对物体悬挂平衡点和旋转方向的判断。
    2.  **杠杆平衡**：通过改变质量和力臂，测试模型对力矩平衡的推理。
    3.  **惯性（牛顿第一定律）**：测试模型在支撑物快速抽离时，对物体保持静止或倾覆状态的预测。
*   **两大评估任务**：
    1.  **VQA 任务**：给定图像或短视频，要求模型回答关于物理量、旋转方向或最终状态的选择题。
    2.  **视频生成（VG）任务**：给定初始帧和描述，要求模型生成后续运动轨迹。
*   **物理感知评估指标**：
    *   **质心**：使用分割掩码的 IoU 和质心距离（L2）。
    *   **杠杆**：评估最终状态（平衡/左倾/右倾）的准确率。
    *   **惯性**：计算轨迹均方根误差（RMSE）、速度/加速度相似度以及方向一致性。

### 3. 实验设计
*   **数据集/场景**：
    *   **真实数据**：在受控室内环境拍摄的 4K/60FPS 高清视频。
    *   **仿真数据**：使用确定性 2D 物理引擎生成，确保物理参数的可控性。
*   **对比方法**：
    *   **VLM 模型（24个）**：包括闭源领先模型（GPT-5, o4-mini, Claude 3.7/4.5, Gemini 2.5 Pro）和开源模型（Qwen2.5-VL, Llama-3.2, DeepSeek-VL2 等）。
    *   **视频生成模型（7个）**：包括 Sora-2, Veo 3.1, Wan 2.2, CogVideoX, LTX-Video 等。
*   **Benchmark 构成**：包含 81 个视频、122 个 VQA 问答对和 81 个视频生成样本，覆盖了多种物体形状、质量分布和运动速度。

### 4. 资源与算力
*   **算力说明**：论文**未明确说明**评估这些模型所消耗的具体算力（如 GPU 型号或训练时长）。
*   **原因**：由于该论文主要是一个基准测试（Benchmark）研究，侧重于对现有 API 或预训练模型的推理评估，而非从头训练大型模型。文中提到了使用 4K 摄像机和 2D 物理引擎进行数据采集。

### 5. 实验数量与充分性
*   **实验规模**：
    *   对 **24 个视觉语言模型**进行了详尽的 VQA 测试，并按任务类型（位置、旋转、平衡等）进行了细分统计。
    *   对 **7 个主流视频生成模型**进行了物理一致性定量评估。
    *   进行了 **Sim-to-Real（仿真到真实）** 的迁移能力分析。
    *   针对 Prompt 设计（直接回答 vs. 物理链式思考）做了**消融实验**。
*   **充分性与客观性**：实验覆盖了当前最顶尖的闭源和开源模型，评估指标从像素级到物理定律级，设计较为全面且客观。

### 6. 主要结论与发现
*   **视觉真实 ≠ 物理真实**：许多视频生成模型（如 Sora-2）虽然生成的画面非常逼真，但在质心对齐、力矩平衡和惯性轨迹上经常违反基本力学规律。
*   **依赖外观启发式**：VLM 模型在推理时往往依赖视觉表象（如“物体看起来很重”）而非真正的物理计算，在处理反事实或逻辑关联问题时表现较差。
*   **Sim-to-Real 鸿沟**：闭源 VLM 在真实视频上表现更好（得益于大规模网络数据预训练），而