Title: PAL*M: Property Attestation for Large Generative Models

URL Source: https://arxiv.org/pdf/2601.16199v1

Published Time: Fri, 23 Jan 2026 02:09:31 GMT

Number of Pages: 16

Markdown Content:
# PAL ∗M: Property Attestation for Large Generative Models 

## Prach Chantasantitam 1, Adam Ilyas Caulfield 1, Vasisht Duddu 1, Lachlan J. Gunn 2, N. Asokan 1,31University of Waterloo, 2Aalto University, 3KTH Royal Institute of Technology {pchantas, acaulfield, vasisht.duddu}@uwaterloo.ca, lachlan@gunn.ee, asokan@acm.org 

## Abstract 

Machine learning property attestations allow provers (e.g., model providers or owners) to attest properties of their mod-els/datasets to verifiers (e.g., regulators, customers), enabling accountability towards regulations and policies. But, current approaches do not support generative models or large datasets. We present PAL ∗M, a property attestation framework for large generative models, illustrated using large language mod-els. PAL ∗M defines properties across training and inference, leverages confidential virtual machines with security-aware 

GPUs for coverage of CPU-GPU operations, and proposes using incremental multiset hashing over memory-mapped datasets to efficiently track their integrity. We implement PAL ∗M on Intel TDX and NVIDIA H100, showing it is effi-cient, scalable, versatile, and secure. 

## 1 Introduction 

Machine learning (ML) models are widely deployed across domains such as healthcare, finance, and autonomous sys-tems [30,69,71]. Emerging regulations (e.g., EU’s AI Act [3]) require models meet certain properties related to accuracy, training procedures, data provenance, and inference. Provid-ing proof of such properties may be challenging for confiden-tial datasets or models. 

ML property attestation enables a model provider (Prv )to demonstrate properties about their model or datasets to a 

verifier (Vrf ) [16]. Approaches based on trusted execution environments (TEEs) [5, 7, 36, 37], first proposed in Lamina-tor [17], has been shown to be efficient, scalable, versatile, and secure for classifiers, whereas alternative approaches based on zero-knowledge proofs [1, 23, 31, 41, 56, 63] or re-purposing of inference attacks [18, 72] do not meet all requirements. But, Laminator does not scale to generative models, such as large language models (LLMs). For example, large datasets cannot reside fully in TEE memory and are accessed via ran-dom sampling from untrusted storage, undermining standard integrity measurement strategies. Enabling ML property attestations for generative models requires addressing three challenges: (1) handling of large datasets outside of TEE memory sampled in a random fash-ion, (2) supporting CPU-GPU computing environments, and (3) defining property attestation for common operations re-lated to generative models. We address these challenges with PAL ∗M, the first framework for ML property attestations of large generative models , using LLMs for illustration. PAL ∗M employs incremental multiset hashing [14] to en-able compact secure integrity tracking of large, randomly-accessed, datasets, and TEE-aware GPUs to efficiently and securely ensure integrity of heterogeneous operations. Finally, we define property attestations for a comprehensive set of op-erations (e.g., fine-tuning, quantization, LLM chat sessions) that do not require revealing confidential datasets or models to Vrf . We prototype and evaluate PAL ∗M using Intel TDX and NVIDIA H100. In summary, we claim the following contributions: we 1. use incremental multiset hash functions to construct repre-sentative measurements of datasets in external storage that are randomly sampled at run-time (Section 4.2). 2. define how to measure properties of generative model op-erations, incorporating GPU attestation evidence without exposing confidential details (Section 4.3); 3. specify a property attestation protocol that shows how such measurements and outputs can be combined to prove data/models were produced by a PAL ∗M-equipped CPU-GPU configuration (Section 4.4); and 4. implement and evaluate PAL ∗M on Intel TDX and NVIDIA H100 with real-world datasets/models, showing it meets all requirements (Sections 5 and 6). 

## 2 Background 2.1 Large Language Models 

Training. LLMs use transformer architectures with billions of parameters [11, 70], and are trained to predict the next to-kens given previous ones. During training, the model learns a 1

> arXiv:2601.16199v1 [cs.CR] 22 Jan 2026

probability distribution of the next token from a vocabulary to follow a sequence of tokens. This is often called pretraining ,as it is typically followed by another phase of training for task-specific optimization/tuning. 

Tokenizers. Modern tokenizers operate at word, sub-word (e.g., byte pair encoding, WordPiece), or character level to convert natural-language text into numerical representations for use during LLM training and inference. 

Fine-tuning. Since training LLMs is resource- and time-intensive, publicly available trained LLMs are generally fine-tuned on a task-specific dataset. Parameter-Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA) fine-tune a small subset of model parameters instead of retraining the en-tire model [29, 44]. Alternatively, adapter layers are trainable modules that can be attached to a pretrained LLM, enabling task-specific adaptation without altering the core model pa-rameters. These approaches are more efficient than fine-tuning the entire LLM. 

Evaluating LLMs. Benchmarks for LLMs cover a vari-ety of tasks and compute relevant metrics. These tasks span language understanding (e.g., MMLU [27]), math problems, multiple-choice, classification, and open-ended question an-swering. Each category is designed to probe specific capabili-ties of the model across common benchmarks. 

Chat sessions. LLMs are commonly used for chatbots, which allow a user to continuously query about a particu-lar task and get the responses from the model that consider prior conversational context (e.g., OpenAI’s ChatGPT). Chat sessions contain multiple user queries and their responses. 

Memory Management. Large generative models require significantly more data than traditional ML models, which often do not fit into main memory (e.g., Pile dataset is over 800 GB [21]). Instead of loading the entire data into memory, current LLM libraries (e.g., Hugging Face) support memory-mapped datasets [32], which map the contents of a file into a process’s virtual memory space, allowing it to be accessed like regular memory, rather than through explicit file I/O op-erations, despite residing on disk. When a dataset is memory-mapped, dataset records are only loaded into memory as needed, significantly reducing memory usage. 

Data Transformations and Processing: Before a dataset is used, it typically undergoes a series of transformations, including preprocessing steps like normalization, concate-nation, truncation, tokenization, and splitting into training, validation, and test sets, as well as additional processing for removing corrupted samples and formatting the data to match the expected input structure of the training framework [33]. During training, random sampling is commonly used to reduce bias introduced by data ordering and to improve model generalization [55]. PyTorch’s DataLoader and Sampler ab-stractions provide randomized sampling [51]. Popular training libraries, such as Hugging Face Transformers, support random sampling and enable it for default training pipelines [34].  

> (1) Challenge
> (3) Report State
> Vrf Prv
> (2) Authenticated State Measurement
> (4) Verify

Figure 1: Traditional remote attestation protocol: a prover (Prv ) demonstrates its software integrity to a verifier ( Vrf ). 

## 2.2 Trusted Execution Environments 

TEEs are a set of architectural components that provide a secure execution environment that is isolated from the “rich” host environment. They guarantee that software executing outside of the TEE cannot directly read or write to data inside the TEE’s trusted boundary. The exact details of the trusted boundary vary based on the particular approach. For example, Intel SGX [36] creates TEE in the user-space enclaves and ARM TrustZone [7] creates a TEE containing all system resources (e.g., separate trusted OS and applications with dedicated peripherals). Recent technologies have also introduced confidential vir-tual machines (CVMs), which enable entire VMs to execute within the trust boundary of a TEE (e.g., AMD SEV-SNP [5], Intel TDX [37], and ARM Realms [40]). An advantage pro-vided by CVMs is that they enable executing unmodified and full-feature operating systems without requiring TEE-specific rewrites. Additionally, TEE-aware GPUs have re-cently emerged (e.g., with NVIDIA H100 GPU). As such, CVM providers have released drivers to support integration with TEE-aware GPUs. This has enabled the potential for heavy computations (e.g., ML training) within a TEE. Both CPUs and GPUs with TEEs typically support remote attesta-tion to prove trustworthiness to verifiers. Intel Trust Domain eXtensions (TDX) [37] enables isolated CVMs in the form of trust domains (TDs). Upon creation of a TD, invoked by the untrusted virtual machine manager (VMM), the TDX module computes a measurement of TD components during TD creation, which can be used later for attestation. To attest TD’s state, the TD invokes the TDX mod-ule to generate a TDREPORT , which is generated by the CPU computing a MAC over measurements of the TD image, the TDX module image, and any REPORTDATA extended by the TD at runtime. The resulting TDREPORT and MAC are passed by the TDX Module to the quoting enclave (QE), a special-purpose Intel SGX enclave [38]. The QE uses contents of 

TDREPORT to generate a SGX-style QUOTE using a provision-ing certification key [35]. Before generating this QUOTE , the QE first validates the MAC, ensuring integrity is maintained when passed by the untrusted VMM. 22.3 Attestation Protocols 

Remote attestation is a protocol in which a verifier ( Vrf ) chal-lenges a prover ( Prv ) to demonstrate that they are installed with the correct configurations [46]. The steps for a traditional remote attestation protocol are visualized in Figure 1. The au-thenticated integrity measurement in step (2) is computed by 

Prv ’s root of trust (RoT) and is typically a message authenti-cated code (MAC) or digital signature. Thus, one requirement of remote attestation is that Prv ’s RoT can securely store and use a private key, implying some level of hardware assis-tance. Hence, TEEs can naturally enable remote attestation with their properties for isolated data and execution. For Intel TDX, step (2) involves computing TDREPORT and QUOTE .Property attestation [57] is where Vrf is interested in prop-erties of Prv ’s configuration rather than a specific config-uration itself. Here, Vrf obtains the authenticated integrity measurement from Prv , while also obtaining reference val-ues that correspond to a particular property from a trusted authority (Tru ). If the state measurement from Prv is among the reference values obtained from Tru , and was signed using the expected public key from Prv , Vrf trusts that the prop-erty holds on Prv . Notably, unlike standard remote attestation, property attestation requires Vrf to obtain reference values from Tru rather than to recompute them. Every property of interest is associated with a measurer on Prv used to measure the state of the property. The notion of property attestation has been extended re-cently to cover ML data and processes [16, 17, 57]. In par-ticular, Laminator [17] leverages Intel SGX [36] to attest properties of training data, model, and training process. This works by executing ML operations inside an SGX enclave, measuring the inputs and outputs of the target operation (e.g., training, inference, or dataset statistics), and using SGX’s quote mechanism to return these measurements to the veri-fier that issued the attestation challenge. Such attestations can then be used for verifiable ML property cards for transparency (e.g., model card, datasheet, and inference card). 

## 3 Problem Statement 

Our goal is to design a property attestation framework for generative models to prove to Vrf that a claimed property is (a) correctly measured and (b) not false or manipulated. Unlike typical remote attestation, which is an interactive protocol between Vrf and Prv , ML property attestation must be non-interactive with Vrf . We propose the protocol depicted in Figure 2. Prv interacts with an initiator (Inr ), who sends a request to Prv for a particular operation, along with other request-specific data (e.g., inputs or a challenge when fresh-ness is required). Upon request, Inr makes them available for 

Vrf , who may consult Tru for property reference values to verify the evidence.    

> Tru Vrf Inr Prv
> (6) Send Reference Values
> (7) Verify
> (4) Request State Measurement
> (5) Report State
> (1) Request
> (3) Report State
> (2) Authenticated State Measurement

Figure 2: Desired property attestation for ML, in which Prv 

interacts with an Initiator ( Inr ). Vrf requests evidence from Inr ,and may consult a Trusted Authority ( Tru ) to obtain reference values of a desired property. 

## 3.1 System Model 

We consider Prv that trains, evaluates, or deploys a model M .We consider software that is already executing within CVMs, as ML companies today are exploring use of CVMs [6,10,50], and major cloud service providers (e.g., Amazon AWS, Google Cloud, and Microsoft Azure) support TEE-aware GPUs. We consider Intel TDX and NVIDIA H100 in this work, but our framework can be extended to any CVM or TEE-aware GPU (see Section 8). Property attestations have the following components: • Operation ( op ): a computational task over inputs ( I ) to produce outputs ( O); • Property : an expression that describes semantics of some meaningful relationship between op , I , and O;• Attestation evidence : a verifiable claim that aims to prove whether a property holds true; We assume an attestation mechanism on Prv executes op and produces attestation evidence. Obtaining attestation evidence requires (1) measuring op , I , O and (2) reporting those mea-surements by producing a cryptographic token (e.g., a digital signature) binding them. For the latter, a signing key is used that must be only accessible to the attestation mechanism. 

## 3.2 Adversary Model 

We assume an adversary ( Adv ), in line with the common In-tel TDX adversary model [4], who has control over the host machine and the VMM. As a consequence, Adv can access and manipulate any memory or device that is not within the trusted boundaries (e.g., other VMs, disk, I/O peripherals). The TDX module and TDX-aware CPU are trusted to be im-plemented correctly and provide their described functionality for inter-TD memory isolation, isolation from host VMM, and hardware-protected measurement and reporting (e.g., via a QUOTE ) of TD images. Since TD creation is managed by 

Adv -controlled VMM, TDs are not inherently trusted. Hence, TDX supports attestation via QUOTE (see Section 2). We assume NVIDIA H100 GPU hardware is trusted, which measures and attests to its configuration (e.g., boot ROM, 3firmware, in confidential computing mode). By default, the interface between a GPU and the TD is not trusted; the GPU and all its memory are brought under the TD’s protection only when the GPU is directly assigned by the VMM [38]. This includes mapping its communication buffers into TD shared memory. TD must obtain an attestation token from the GPU to determine that the assigned device has the anticipated configuration. At run-time, Adv may attempt to obtain confidential data or interfere with the execution of op in a TD. Adv also may attempt to forge QUOTE s, either by learning the signing key or by tampering with relevant code/data without detection. We assume a Dolev-Yao Adv as it relates to the network (i.e., Adv 

carries the message). Therefore, Adv may attempt to inject or modify messages sent/received by Prv . However, we consider denial-of-service attacks out of scope. Side channels can be exploited to leak keys. To mitigate these attacks, additional mechanisms (e.g., constant-time software) should be added. However, this is an orthogonal research direction. We consider physical attacks on hardware as out of scope. 

## 3.3 Challenges and Requirements 

Prior works [17, 24, 53, 59, 62] demonstrate methods to attest ML models using TEEs, but require operations to stay within the one TEE. For example, Laminator [17] requires that all computations stay inside an SGX enclave to ensure integrity. However, this is not possible with reasonable performance of LLM operations. For example, large datasets may require datasets to reside in external storage during operation and re-quire accelerators (e.g., GPUs or NPUs) for reasonable perfor-mance (recall Section 2). Although it is possible to outsource computations from a TEE to a standard GPU [39, 68], this adds significant overhead and would not preserve integrity of property measurements without additional measures. Further-more, no prior work defines properties of generative models (e.g., from operations like fine-tuning, quantization, and chat sessions). Therefore, it is unclear how to perform property attestation in a way that is relevant for Vrf while protecting confidential assets of Prv .Due to these challenges, prior work cannot apply to prop-erties of generative models, requiring another approach to address the following concerns: (i) accounting for random sampling of large datasets from (untrusted) external storage devices; (ii) handling operations that depend on both CPU and GPU with reasonable computational overhead; (iii) defining how to measure properties of generative models in a way that is relevant for Vrf and does not reveal Prv ’s confidential assets. Based on these challenges, we consider the following re-quirements for designing PAL ∗M: 

[R1] Efficient (low-overhead compared to confidential com-puting in the same environment), 

[R2] Scalable (supports large numbers of provers/verifiers), 

[R3] Versatile (supports a variety of properties; requires min-imal effort for custom property measurement; can be ported to any CVM/TEE-based GPU configuration), and 

[R4] Secure (a malicious Prv cannot generate attestations of false information). 

## 4 PAL ∗M Design 4.1 Overview of PAL ∗M

PAL ∗M enables Prv to attest properties of generative mod-els through (1) design of a TD that effectively measures the operation corresponding to a particular property and (2) use of attestation support from Intel TDX and NVIDIA H100 to construct authenticated evidence these measurements. Fig-ure 3 shows a high-level overview of a Prv ’s workflow with PAL ∗M and the interaction between Prv and Inr .In 1 , Inr requests a property attestation by specifying: (a) the requested operation ( op ), (b) an attestation challenge (Chal ), and any inputs that should be used (e.g., datasets, mod-els, configuration parameters, inference query). Upon receipt, 

Prv ’s VMM saves all data to disk. In 2 , inputs are fetched by PAL ∗M into TD memory according to the dataset access strategy (discussed further in Section 4.2). Based on op , the corresponding function will start to execute in 3 . While ex-ecuting, outputs will be obtained by a combination of CPU execution 4a and GPU execution 4b . Operation outputs and attestation measurements are obtained 5 . When a GPU is used, the attestation measurements include a GPU-attestation token ( GPU att ). After outputs are obtained, PAL ∗M TD leverages TDX mechanisms to convert property attestation measurements into authenticated evidence. As a first step, it invokes the TDX module in 6 to generate an authenticated TDREPORT contain-ing PAL ∗M attestation measurements. To complete the quote process, TDREPORT is passed to the VMM in 7 , which in-vokes the QE in 8 to convert TDREPORT into a QUOTE . Finally, 

QUOTE is returned to Inr in 9 . Inr will provide QUOTE and outputs to requesting Vrf , who consults Tru (e.g., Intel and NVIDIA) to perform evidence verification. Protocol details are described further in Section 4.4. By design, PAL ∗M generates QUOTE s that attest to prop-erties of generative model operations, while addressing key challenges: 1. PAL ∗M leverages incremental multiset hash functions [14] to ensure large datasets are accurately reflected in attesta-tion evidence despite residing in untrusted external storage devices and being randomly sampled; 2. PAL ∗M ensures measurements of both CPU and GPU con-figurations are captured in attestation evidence for proper-ties that depend on CPU and GPU; Building upon these capabilities, we show how to define, measure, and attest to properties of the following generative 4Figure 3: High-level overview of PAL ∗M-enabled property attestation. Inr interacts with untrusted components of Prv , which saves inputs to disk. PAL ∗M reads and measures inputs, runs the operations (including GPU), and measures all CPU and GPU outputs. All measurements are extended to REPORTDATA , which is used to as input to obtain TDREPORT from TDX Module. Finally, a QUOTE is generated by invoking the Quoting Enclave (QE) and returned to Inr .model operations (from Section 2.1): 1. dataset transformations and distributional statistics; 2. model pretraining, optimization (e.g., fine-tuning, quanti-zation), and evaluation; 3. model interactions (e.g., inference or chat sessions). 

## 4.2 Dealing with Large Datasets 

Given the large size of LLM training datasets, ML software frameworks use two common approaches for efficient dataset handling: (1) loading the entire dataset in-memory or (2) ac-cessing the dataset outside of memory with memory mapping .Although in-memory has an advantage for integrity measure-ments, frameworks such as Hugging Face adopt and default to using memory-mapped datasets to minimize the memory footprint of large datasets that cannot fit entirely into main memory [32]. Memory mapping also has an advantage over in-memory when running with multiple processes. Since each process can access the same memory-mapped file without duplicating the dataset in memory, the operating system effi-ciently shares pages among processes. This avoids redundant memory usage and eliminates the need for each worker to load a separate copy of the dataset, which would otherwise increase both memory consumption and startup latency. As a result, memory-mapping provides better scalability and I/O efficiency in multi-process data loading scenarios. To accurately attest to properties that depend on a dataset, PAL ∗M must measure the dataset (or its items) upon use, since Adv could tamper with data in external storage at any time (recall threat model in Section 3.2). Figure 4 shows how PAL ∗M handles this for in-memory and memory-mapped 

datasets. 

Case 1: In-memory Dataset . For integrity measurement, this is the ideal approach when sufficient TD memory is avail-able. Since the TD memory is protected, data within cannot be tampered. As a result, the dataset can be measured once during loading, as long as it stays in memory for the entire operation. As shown in Figure 4 (left), our framework loads the entire dataset into memory, measures it, and has it avail-able for the program thereafter. Since one hash operation is required, the overhead is minimal (discussed further in Sec-tion 6). 

Case 2: Memory-mapped Dataset . In contrast to the in-memory case, the dataset resides on the disk, and a mem-ory mapping data structure resides in memory (see Figure 4: right). As records are sampled from the dataset, the data struc-ture is used to read just the requested data items into memory. Since the dataset stays on disk, it is vulnerable to tamper-ing at run-time, resulting in inconsistencies between what is measured and what is used (i.e., TOCTOU attacks [9]). To account for this, we measure each record at the time it is sampled, accumulating a measurement of the dataset in use. Since training algorithms often randomly sample records, a straightforward approach (e.g., a hash chain over accessed records) would be unsuitable, as different sampling orders would result in different measurements. Based on this insight, we employ an incremental multiset hash (MSH ) [14]. 

MSH is a cryptographic primitive that produces a fixed-size hash over an unordered collection. It is suitable for settings in which the order of elements is irrelevant, but integrity and compactness are required. By accumulating an MSH over the sampled records, PAL ∗M accurately measures the dataset that is used at run-time, independent of the sampling order. Additionally, this approach has low memory overhead since one multiset hash value is maintained. 5Figure 4: Depiction of PAL ∗M ’s handling of in-memory (left) and memory-mapped (right) dataset. In the in-memory approach, the entire dataset is loaded and measured once at load time. In the memory-mapped approach, individual records are measured when accessed using a multiset hash to ensure consistent measurements regardless of sampling order. We use the Mset-Mu-Hash multiplicative multiset hash construction (from Theorem 2 in [14]) defined below. Let 

B denote a set of bit vectors of length m, M a multiset of elements of B, and Mb be the number of times b ∈ B is in the multiset M. Let h be a poly-random function performing 

h : B → Zln, given n = 2√m and l = √m. The multiplicative multiset hash MSH (M) is defined as: 

MSH (M) = ∑

> b∈B

Mbh(b) mod n

Compared to straightforward dataset hashing, this approach incurs additional overhead due to iterative hash and modular operations. However, this overhead is negligible compared to computation-heavy ML tasks (shown in Section 6). It is possible that expected dataset measurement from Tru 

is not MSH . Consequently, when memory-mapped datasets are in use, PAL ∗M must also enable methods for Vrf to trace back to original dataset measurements. 

## 4.3 Property Measurements for Large Genera-tive Models 

Recall from Section 3.1 that obtaining property attestation evidence requires (1) obtaining measurements of {op , I , O}

and (2) reporting those measurements by producing an au-thentication token. In this section, we discuss the former. We categorize properties measured by PAL ∗M as one of the following based on op : dataset property, model property, model interaction property. Based on this, we focus on the following components of property measurement (subset of those from Section 3.1): • Input ( I ): inputs (e.g., datasets, models, queries, or other parameters) used by op .• Output ( O): outputs (e.g., prompt response, model, and dataset) produced by op ;• Property : an expression that describes semantics (either natural language or mathematical expression) of the rela-tionship between {op , I , O}.• Attestation Measurement ( Att ): Measurements that reflects the property under the system and adversary model (from Table 1: Notation Summary  

> Symbol Definition

I , O, Att Set of inputs, outputs, and attestation measurements          

> Pdist Distributional property
> MSH Multiset hash function
> hStandard cryptographic hash function (e.g., SHA3)
> hDDataset hash (either MSH or hbased on memory manage-ment strategy from Sec. 4.2)
> GPU att Attestation token from TEE GPU

D Dataset 

Dtr Training dataset 

Dpre Preprocessed dataset 

Mar Model architecture 

T Training configuration 

Mtr Pretrained model 

Mtok Tokenizer  

> id opt Optimization type

Mad p Adapter model 

Dopt Optimization dataset  

> [·]∗Optional components

Dte Test dataset     

> metric Evaluation metric
> qi,riThe i-th inference query and response

H Session history (i.e., set of all query-response pairs [( q0, r0), ... , (qi−1, ri−1)]) that are used as additional con-text to produce qi for ri

Section 3.1 and 3.2). 

Note: For all operations that require GPU, Att must include 

GPU att to reflect the complete environment in which the com-putations were performed. Therefore, GPU att is included as an optional input (denoted with [·]∗) for all operations de-scribed in this section. A summary of frequent notation in this section is in Table 1. 

4.3.1 Dataset Properties 

We focus on dataset properties based on the following opera-tions: dataset preprocessing, calculation of dataset attribute distribution calculation, and dataset measurement binding. Dataset preprocessing (from Section 2) may occur prior to training or inference. We define attestation measurement for preprocessing in Definition 1. For preprocessing, I contains an input dataset ( D) and O contains an output pre-processed 6dataset ( Dpre ). A property measurement of preprocessing as-serts that Dpre was the result of computing a preprocessing al-gorithm ( PreProc ) on D. Finally, Att contains measurements of D and Dpre using the dataset hash function ( hD ): either a standard cryptographic hash function ( h) or a MSH , based on whether D and Dpre are in-memory or memory mapped, respectively. 

Definition 1: Preprocessing 

Input ( I ): D

Output ( O): Dpre 

Property: Dpre = PreProc (D)

Att. Measurement ( Att ): hD (D), hD (Dpre ), [GPU att ]∗

Dataset attribute distribution calculations [16] reflect a dis-tribution of an attribute across a dataset (e.g., word lengths, word frequency for LLMs). We define attestation measure-ment for calculation of attribute distribution in Definition 2. For this operation, I contains D, and O contains an attribute distribution ( Adist ). Att , containing h(Adist ) and hD (D), as-serts the property that Adist was obtained by an algorithm (Dist) that computes an attribute’s distribution on D.

Definition 2: Attribute Distribution 

Input ( I ): D

Output ( O): Adist 

Property: Adist = Dist (D)

Att. Measurement ( Att ): h(Adist ), hD (D), [GPU att ]∗

It is possible that reference values offered by Tru are pro-duced by h while an operation requires memory-mapping, resulting in a measurement based on MSH . To account for this scenario, PAL ∗M supports measurement binding defined in Definition 3. 

Definition 3: Measurement Binding 

Input ( I ): D,

Output ( O): h(D), MSH (D)

Property: h(D) and MSH (D) were produced from D

Att. Measurement ( Att ): h(h(D)|| MSH (D)) ,

[GPU att ]∗

For measurement binding, I contains D, and O contains the measurements of D that should be bound: h(D) and 

MSH (D). A property that h(D) and MSH (D) were produced from the same dataset is asserted through Att containing 

h(h(D)|| MSH (D)) .

4.3.2 Model Properties Training Properties. First, we consider training, building on prior work [17]. We define attestation measurements for training in Definition 4. For training, I includes the speci-fied model architecture ( Mar ), training dataset ( Dtr ), training configuration ( T ), which specifies hyperparameters, and a to-kenizer ( Mtok ). Then, O contains the trained model ( Mtr ). Att 

contains measurements of ( T , Mar , Mtr ) using h and D using 

hD , asserting a property describing that Mtr was obtained by a training algorithm (Train) taking Dtr , T , Mar .

Definition 4: Training 

Input ( I ): Mar , Dtr , T , Mtok 

Output ( O): Mtr 

Property: Mtr = Train (Dtr , T , Mar )

Att. Measurement ( Att ): h(Mtr ), h(Mar ), hD (Dtr ),

h(T ), [GPU att ]∗

Definition 5 defines attestation measurements for post-training model optimizations (e.g., fine-tuning, quantization, pruning). For model optimization, I contains the following re-quired inputs: M , T , Mtok , and the optimization type ( id opt ). Optional components of I include a base adapter layer ( Mad p )and additional datasets ( Dopt ). For this operation, O contains the optimized model (Mopt ). Att contains all components of 

I and O using h and hD , reflecting its assertion that: • Opt is selected from a set of known optimization functions (Fopt ) based on id opt ;• Mopt is the result of Opt taking all components of I .

Definition 5: Weight Optimizations 

Input ( I ): M , Mtok , T , id opt , [Mad p , Dopt ]∗,

Outputs ( O): Mopt 

Property: Opt = Fopts [id opt ]

Mopt = Opt (M , Mtok , T , [Mad p , Dopt ]∗)

Att. Measurement ( Att ): h(M ), h(Mtok ), h(T ), 

h(id opt ), [GPU att , h(Mad p ), hD (Dopt )] ∗

Evaluation Properties. As described in Section 2, gen-erative models have various benchmarks to produce metrics reflecting a model’s effectiveness according to a particular criteria. Attestation measurement for model evaluation is de-fined in Definition 6. For model evaluation, I includes M ,

Mtok , and a test dataset ( Dte ), while O contains the evaluation 

metric . Measurements of M and Mtok using h and Dte using 

hD in Att assert a property about the evaluation: that metric 

was produced by an evaluation operation ( Eval ) taking M ,

Mtok , and Dte .

Definition 6: Evaluation 

Input ( I ): M , Mtok , Dte 

Output ( O): metric 

Att. Measurement ( Att ): h(M ), h(Mtok ), hD (Dte ),

[GPU att ]∗

Property: metric ← Eval (M , Mtok , Dte )

4.3.3 Inference-time Properties 

We consider two cases of inference-time properties which includes interactions between client and model: one prompt-response inference and a chat session of multiple prompt-response inference pairs. Attestation for a single inference are defined in Definition 7. A inference query ( q) is one component of I along with M

7and Mtok , while the O contains the inference response ( r). Measurements of q, M , Mtok , and r using h assert the property that q was obtained by passing r first through Mtok and then through M .

Definition 7: Single Inference 

Input ( I ): q, M , Mtok 

Output ( O): r

Property: q = M (Mtok (r)) 

Att. Measurement ( Att ): h(M ), h(I ), h(Mtok ), h(q), 

[GPU att ]∗

Attestation reflecting the influence of a chat session on an inference query-response (i.e., a session inference ) are defined in Definition 8. For a session inference operation, I

contains the same data as the single inference operation, while 

O contains r and the chat session history ( H ): a sequential list of all (qi, ri) pairs of the current chat session, including 

(q, r). Measurements of r, H , q, Mtok , and M using h assert the property that r was obtained from passing q prepended with H without (q, r) through Mtok , followed by passing that result through M .

Definition 8: Session Inference 

Input ( I ): q, Mtok , M

Output ( O): r, H

Property: r = M (Mtok (H \ [q, r] || q)) 

Att. Measurement ( Att ): h(r), h(H ), h(q), h(Mtok ),

h(M ), [GPU att ]∗

## 4.4 From Measurement to Attestation 

For each op , the corresponding components of Att , as de-fined in Section 4.3, are used to construct property attestation evidence. We refer to each property attestation as proof of <op >. A table of operations and resulting property attesta-tions is shown in Appendix A. In this section, we describe how PAL ∗M interacts with other system components and leverages Intel TDX features to respond to Inr requests and convert its property measurements into attestation evidence. Figure 5 shows the property attestation protocol enabled by PAL ∗M. Here, we describe the interactions between Inr and 

Prv containing a VMM (including QE), TDX module, and PAL ∗M-based TD. We note there are several possible inter-actions between Inr and Vrf (e.g., Inr publishes attestations to a transparency log [24, 62]). We discuss implications of alternative strategies in Section 8. In an offline phase, Prv initializes and measures the PAL ∗M TD. In step (1), VMM specifies the TD image that should be used. In step (2), the Intel TDX architecture and TDX module measure the TD image into hT D and launch it. After initialization, the online protocol can begin. In step (1), Inr sends a property attestation request to Prv contain-ing op , an attestation challenge ( Chal ), and a set of input assets ( I ). For large input assets (e.g., datasets or models), 

Inr ’s request may specify repositories from which Prv should download them. Or, Inr ’s request may specify that Prv use a dataset/model owned by Prv . Regardless of these specificities, 

Prv saves all data from the request to disk in step (2) upon receipt. In step (3), the PAL ∗M TD imports request data and all data required for op into its memory. Datasets are imported depending on whether Prv is using a memory-mapped or an in-memory dataset (from Section 4.2). PAL ∗M TD reads op to execute the corresponding func-tion in step (4). During execution, it obtains a set of input measurements ( HI ). After completing the operation, PAL ∗Mwill obtain a set of outputs ( O) and corresponding measure-ments ( HO ) in step (5). HI and HO correspond to components of Att for each op described in Section 4.3. Next, PAL ∗M prepares the request for TDREPORT genera-tion in step (6). To do so, it assigns REPORTDATA input used in TDREPORT generation (recall Section 2.2) as the concate-nation of {Chal , HI , HO }. Upon receiving REPORTDATA , the TDX module will create a TDREPORT in step (7) by invoking the CPU. During this step, the CPU will compute a MAC over hT D , measurements of the TDX module, and all data ex-tended into REPORTDATA by PAL ∗M. In step (8), the MAC-ed 

TDREPORT is passed to VMM, who passes it to QE in step (9). Upon receiving the MAC-ed TDREPORT , QE performs the following actions: • checks that the MAC on the TDREPORT is valid, to ensure VMM did not tamper with the TDREPORT it received from the TDX Module; • uses TDREPORT to produce the QUOTE containing all mea-surements from TDREPORT plus a signature ( sig ). In step (10), Prv sends QUOTE and optionally O (if it is not confidential) to Inr , who sends them to Vrf in step (11). Upon receiving QUOTE and (optionally O) from Inr , Vrf 

parses the response to perform verification. First, it checks 

sig to determine whether the QUOTE came from Prv . Next, it checks Chal to determine the freshness of the response. The type of Chal that is effective depends on the operation. We discuss methodology to select Chal in Section 8. Assuming both checks pass, Vrf continues to inspect the measurements of the TD image, TDX module image, and GPU att (by obtaining reference values from Intel and NVIDIA). Assuming this passes, Vrf continues to inspecting 

HI , HO , and O. If O and was sent to Vrf , it checks whether 

HO corresponds to O. Finally, Vrf inspects HI and HO , con-sulting reference values from Tru as needed, to determine if the desired property related to op is upheld. 

## 5 Experiment Setup 5.1 Datasets and Models 

We use the following datasets in our experiments: • BookCorpus [74]: a collection of 74 million unpublished novels. We use the full BookCorpus dataset for proof of 8Inr                                  

> Prv
> VMM (Quoting Enclave) TDX Module PAL ∗M TD
> (1) Specify TD image Invoke TD creation (2) Measure TD image into
> hT D and launch TD
> (1) Send: {I,op ,Chal }(2) Saves to disk
> (3) Import I,op ,Chal (4) Execute function for
> op , obtain HI
> (5) Produce Oand HO
> (6) Request TDREPORT
> generation with
> REPORTDATA ={Chal ,HI,HO}(7) Calls CPU to create MAC-ed TDREPORT (8) Pass MAC-ed
> TDREPORT to QE
> (9) QE validates MAC and signs QUOTE
> (10) Send QUOTE ,[O]∗
> (11) Make signature, measurements, and PAL ∗M data available to Vrf

Figure 5: PAL ∗M Property Attestation Protocol between Inr and Prv . Inr sends request to Prv containing untrusted VMM, Quoting Enclave, trusted TDX Module, and attested PAL ∗M TD. Prv responds after performing the requested operation, measuring relevant inputs and ouputs, and generating both a TDREPORT and QUOTE .training and attribute distribution, while for proof of pre-processing and proof of binding, we use a 1/100 subset to reduce experiment running time. For training, we pre-process the full BookCorpus dataset by tokenizing and concatenating text into sequences of 1024 tokens. • yahma/alpaca-cleaned [65]: a instruction-following dataset containing over 52,000 instruction-response pairs aimed at improving model alignment. We use this dataset for proof of fine-tuning (one type of proof of optimization) to provide a dataset variety other than BookCorpus .• MMLU [27]: a multiple-choice question-and-answer (Q&A) dataset used to test a model’s knowledge and reasoning across 57 fields. It is widely used to assess general-purpose intelligence in language models. We use all 57 fields for our proof of evaluation experiments. • WMT14(DE - EN) [8]: a test set of WMT14 German-English to evaluate LLM translation capability. This dataset is used to calculate the BLEU score for our proof of evalua-tion experiment. • CoQA [54]: a conversational Q&A dataset containing sto-ries paired with context-dependent questions/answers. For proof of inference, we select the first question from the first ten records. For the proof of session inference, we use all ten records and select the first five questions from each, thus simulating a session with 50 prompts. For models, we use GPT-2 [52] for proof of training. For fine-tuning, inference, and evaluation, we use three popular instruction-tuned LLMs: Llama-3.1-8B [67], 

Gemma-3-4B [66], and Phi-4-Mini [2]. We choose 

Llama-3.1-8B for its 8 billion parameters, while both 

Gemma-3-4B and Phi-4-Mini have 4 billion parame-ters, allowing us to show how the overhead of PAL ∗M varies across different model sizes and architectures. To demonstrate a realistic fine-tuning workflow, we fine-tune all three models with LoRA. For quantization, we focus on Llama-3.1-8B .

## 5.2 Metrics 

We use the following metrics to capture both the overall over-head from PAL ∗M and the main sources of overhead: • Total Time: The total execution time spent on the task. • Baseline: The baseline represents running the task on a TDX-enabled machine without any measurement or attes-tation from PAL ∗M. We omit the baseline for running our framework on non-TDX machines because the overhead of TDX compared to non-TDX systems has already been studied [13]. Our focus is solely on the overhead introduced by our framework in the environment where it is intended to be used. • Memory Usage (GB): The memory usage of the system during the operation. • Input Measurement Time: The time spent hashing the inputs. For memory-mapped datasets, this is the time spent performing MSH when records are accessed. • Output Measurement Time: The time spent hashing the outputs (e.g., model, dataset, or result from the task). • Attestation: The time spent by Intel TDX and NVIDIA H100 to produce QUOTE and GPU att , respectively. 9Table 2: Performance for proofs of dataset attribute distribution, preprocessing, and binding using BookCorpus (Full) and BookCorups (1/100).                                                     

> Proof of Attribute Distribution Preprocessing Binding Dataset BookCorpus (Full) BookCorpus (1/100) Memory Variant In-memory Memory-mapped In-memory Memory-mapped N/A Total Time 339.16 min 577.45 min 237.70 s 380.16 s 339.62 s
> Baseline 339.09 min 185.06 min 236.66 s 141.51 s 102.41 s
> Mem Usage (GB) 5.47 3.96 0.12 0.04 0.14
> Input Meas. 3.06 s 392.37 min 0.04 s 237.70 s 236.22 s
> Output Meas. 0.53 ms 0.52 ms 0.10 s 0.10 s 0.04 ms
> Attestation 1.06 s 1.02 s 0.90 s 0.85 s 0.99 s
> Meas. Overhead (%) 0.015 67.95 0.06 62.55 69.55
> Att. Overhead (%) 0.0029 0.0052 0.38 0.22 0.29

Table 3: Performance for proof of training with GPT-2 on tokenized and preprocessed 

BookCorpus .                    

> Dataset BookCorpus (Full) Memory Variant In-memory Memory-mapped Total Time 387.86 min 410.85 min
> Baseline 387.79 min 387.58 min
> Mem Usage (GB) 2.74 0.04
> Input Meas. 1.88 s 23.24 min
> Output Meas. 1.14 s 1.14 s
> Attestation 1.00 s 0.96 s
> Meas. Overhead (%) 0.01 5.66
> Att. Overhead (%) 0.004 0.004

• Measurement Overhead (%): The total measurement overhead from PAL ∗M as a percentage of the total time. • Attestation Overhead (%): The total attestation overhead from PAL ∗M as a percentage of the total time. 

## 5.3 Prototype Configuration 

All experiments were conducted on a TD configured with 32 vCPUs and 128 GB of memory, running Ubuntu 24.04.1 LTS with kernel version 6.8.0-86-generic [12]. The host machine is equipped with an Intel Xeon Silver 4514Y processor, 512 GB of RAM, and an NVIDIA H100 NVL 94 GB GPU with confidential compute mode enabled [48]. We assign the GPU directly to the TD using Direct Device Assignment (DDA) to give the TD full access to the GPU. We run each experiment five times and report the average. Standard deviation was negligible in all cases, thus we exclude from the tables. PAL ∗M is implemented in Python and provides two primary functionalities: (1) input/output measurement, and (2) attestation using Intel Data Center Attestation Primi-tives (DCAP) [58] and NVIDIA Remote Attestation Service (NRAS) [49]. PAL ∗M is built on PyTorch and the Hugging Face Datasets framework, allowing access to both dataset and model internals for measurement. For attestation, we use Intel DCAP to generate QUOTE and NRAS to verify GPU att .We also use the following operation-specific frameworks for the experiments: • Unsloth [15] : an open-source framework that provides op-timized, memory-efficient fine-tuning for large language models. We use it for proof of fine-tuning experiment. • AutoAWQ framework [26] : implements the Activation-aware Weight Quantization (AWQ) algorithm for quantiz-ing LLMs. We use it for proof of quantization experiment. • lm-evaluation-harness framework [22] : standardized toolkit that runs language models on multiple benchmark tasks through a unified API to produce comparable scores. We use it for proof of evaluation, specifically evaluating the model with MMLU .

## 6 Evaluation 6.1 R1: Efficient 

Tables 2-6 show the timing results for the performance evalu-ations. The cells in gray represent the baseline, PAL ∗M’s overhead is highlighted in blue , and the total times are in orange . We also parallelize the dataset lookup iterations across 8 cores for all experiments that require manual dataset iteration for dataset-related attestations. 

Dataset Attestations. Table 2 presents the results for all dataset attestations. While the measurement overhead of the memory-mapped approach from our framework appears large relative to the total compute time, dataset attestations corre-spond to operations that will likely occur once: • proof of attribute distribution and preprocessing are relevant for any future use of a particular dataset; • proof of binding is relevant for any MSH produced in the future. The actual computation for these dataset-related tasks is not expensive compared to training and evaluation tasks. Thus, the observed 62% to 70% reflects the raw overhead from hashing, since the baseline workloads are minimal for cal-culating an attribute distribution and preprocessing. As for binding, the main overhead comes from iterating through the dataset to compute the MSH for all records. The input measure-ment time for iterating through the dataset is similar to the proof of preprocessing experiment, demonstrating consistent measurement time when the dataset remains the same. Since the dataset is processed in parallel, the baseline time for the memory-mapped approach is much lower than that of the in-memory approach. Due to the nature of memory map-ping, parallel I/O scales better since all processes can directly access the same underlying memory pages without dupli-cation. In contrast, the in-memory approach requires each process to load or copy portions of the dataset into its own address space, which introduces additional memory copying and increases overhead. 

Model Attestations. In contrast to dataset attestations, the 10 Table 4: Performance for proof of model optimization (fine-tuning and quantization).                                                                      

> Proof of Fine-tuning Quantization Model Llama-3.1-8B Gemma-3-4B Phi-4-Mini Llama-3.1-8B
> Memory Variant In-memory Memory-mapped In-memory Memory-mapped In-memory Memory-mapped N/A Total Time 268.81 min 269.15 min 325.61 min 325.31 min 169.93 min 170.48 min 812.92 s
> Baseline 268.30 min 266.62 min 325.29 min 322.94 min 169.65 min 168.16 min 773.84 s
> Mem Usage (GB) 0.085 0.004 0.087 0.004 0.088 0.004 N/A
> Input Meas. 28.76 s 151.01 s 17.93 s 140.90 s 15.83 s 138.11 s 28.16 s
> Output Meas. 0.30 s 0.30 s 0.23 s 0.23 s 0.06 s 0.06 s 10.00 s
> Attestation 1.11 s 1.01 s 1.00 s 1.27 s 1.00 s 1.08 s 0.92 s
> Meas. Overhead (%) 0.18 0.94 0.09 0.72 0.16 1.35 4.70
> Att. Overhead (%) 0.007 0.006 0.005 0.006 0.010 0.011 0.117

Table 5: Performance for proof of evaluation based on MMLU and BLEU score with WMT14(DE-EN) dataset.                                                                                                                      

> Model Llama-3.1-8B Gemma-3-4B Phi-4-Mini
> Memory Variant In-memory Memory-mapped In-memory Memory-mapped In-memory Memory-mapped Metric MMLU BLEU MMLU BLEU MMLU BLEU MMLU BLEU MMLU BLEU MMLU BLEU Total Time 18.50 min 121.71 min 19.59 min 121.55 min 15.10 min 242.23 min 16.21 min 241.53 min 12.11 min 119.03 min 13.21 min 118.65 min
> Baseline 18 min 121.23 min 18.03 min 120.94 min 14.79 min 241.92 min 14.83 min 241.10 min 11.83 min 118.75 min 11.86 min 118.26 min
> Mem Usage (GB) 0.028 0.001 0.013 0.001 0.022 0.001 0.008 0.001 0.025 0.001 0.009 0.001
> Input Meas. 28.76 s 28.16 s 92.99 s 35.57 s 17.53 s 17.32 s 81.82 s 24.55 s 15.65 s 15.65 s 79.96 s 22.50 s
> Output Meas. 0.11 s 0.038 ms 0.11 s 0.035 ms 0.11 s 0.033 ms 0.11 s 0.035 ms 0.11 s 0.036 ms 0.11 s 0.035 ms
> Attestation 0.85 s 1.10 s 0.97 s 1.07 s 0.84 s 1.12 s 0.83 s 1.23 s 0.79 s 1.08 s 0.87 s 1.13 s
> Meas. Overhead (%) 2.60 0.39 7.92 0.49 1.95 0.12 8.42 0.17 2.17 0.22 10.11 0.31
> Att. Overhead (%) 0.08 0.015 0.08 0.015 0.09 0.008 0.09 0.009 0.11 0.015 0.11 0.016

measurement overhead of our PAL ∗M for proof of training in the memory-mapped case is only 5.66% of the total time (Table 3). LLM training is computationally expensive; hence, the measurement time results in a smaller overhead compared to the total computation. As for model optimization, similar to training, proof of fine-tuning (Table 4) also shows minimal overhead of ≤1.35% across all models, plus a substantial reduction in memory usage from 85-87 MB to only 4 MB. For proof of quantization, since no dataset is involved, the 4.7% overhead comes from measuring models. As shown in Table 5, overheads for proof of evaluation with the MMLU benchmark are 3.81-5.06% for the in-memory case and 10.03-11.84% for the memory-mapped case (Ta-ble 5). For BLEU score, since dataset access is limited (since the WMT14(DE-EN) contains only 3003 records), the over-head of performing MSH in the memory-mapped case is com-parable to hashing the dataset in the in-memory case, with the total overhead remaining ≤ 1.0% across all models. 

Inference Attestations. Table 6 presents the results for both proof of inference and proof of session inference. For proof of inference, measurement overhead is large due to time required to measure the model, while the attestation overhead remains similar to previous cases. This follows the same pat-tern observed in dataset-related attestations where the over-head appears large (64.34% for Llama-3.1-8B , around 43.28% for Gemma-3-4B , and 52.16% for Phi-4-Mini )because the underlying computation is relatively small. This represents an extreme case in which both measurement and attestation are performed for only one prompt. On the other hand, for proof of session inference, we per-form attestation only once after all interactions with the model have completed. This simulates a more realistic user-model interaction, with previous prompts and responses forming the context with a single attestation for the entire session. In this setting, the measurement overhead is significantly smaller, 11.03% for Llama-3.1-8B , 3.57% for Gemma-3-4B , and 6.28% for Phi-4-Mini .

## 6.2 R2-3: Scalable & Versatile 

Scalability. Attestations can be generated by Prv running on any hardware that supports and is configured for confidential computing. PAL ∗M can apply to any CVM-GPU configura-tion given both have TEEs. While widespread use of Intel TDX and NVIDIA confidential computing remains limited at the time of writing, more cloud service providers, such as Mi-crosoft Azure, have begun supporting confidential VMs with NVIDIA H100 GPUs [25]. Although GPU-based confidential computing is currently available only on NVIDIA Hopper and 11 Table 6: Performance of proof of inference for one query-response and proof of session inference for 50 query-responses.                                                          

> Model Llama-3.1-8B Gemma-3-4B Phi-4-Mini
> Proof of Inference Type Single Session Single Session Single Session Total Time 43.31 s 256.19 s 40.55 s 489.69 s 29.94 s 249.37 s
> Baseline 14.54 s 227.02 s 22.09 s 471.22 s 13.40 s 232.70 s
> Mem Usage (GB) 0.001 0.001 0.001 0.001 0.001 0.001
> Input Meas. 27.17 s 28.25 s 16.27 s 17.47 s 14.59 s 15.66 s
> Output Meas. 0.042 ms 0.092 ms 0.038 ms 0.103 ms 0.040 ms 0.117 ms
> Attestation 0.75 s 0.92 s 0.79 s 0.99 s 0.71 s 1.01 s
> Meas. Overhead (%) 64.34 11.03 43.28 3.57 52.16 6.28
> Att. Overhead (%) 2.09 0.36 2.24 0.20 3.10 0.41

Blackwell GPUs [47], which are high-end devices, we expect this technology to be adopted more broadly in future GPU generations. Verification of PAL ∗M attestation evidence can be performed by any Vrf that knows corresponding public keys for Prv ’s attesting devices. Thus, PAL ∗M satisfies R2 .

Versatility. PAL ∗M satisfies R3 as PAL ∗M ’s measure-ment is a software-based approach that relies only on Intel TDX and the NVIDIA H100 for isolation. Therefore, PAL ∗Mcan be adapted to any type models or dataset, beyond those used in our experiments. Additionally, property attestations can also be easily added by extending PAL ∗M with additional measurer scripts corresponding to the new property, with no changes required by Inr or Vrf .

## 6.3 R4: Secure 

Based on the threat model outlined in Section 3, Adv has con-trol over the host machine and VMM. As a result, Adv can attempt to (1) tamper with any operation inputs; (2) tamper, discard, or replay TDREPORT or QUOTE at any stage of their construction or transmission; or (3) tamper with the configu-ration of PAL ∗M TD. 

Adv may manipulate op to change the requested function or misconfigure assets in I used for the requested function. Both cases are detectable by Vrf , since PAL ∗M does not mea-sure either until they are loaded into TD-protected memory. Therefore, Vrf is ensured that the inputs reflected in the QUOTE 

were used in the process. In the case of a memory-mapped data structure, Adv may manipulate data before it is read into the TD memory. For example, they may aim to repeat or re-place data records during fine-tuning to make the resulting model unfair/biased. However, these attempts can be detected by Vrf , since PAL ∗M in this case measures records as they are sampled. Even in the case of random sampling, use of MSH 

ensures Vrf can always determine when the entire dataset was used without replacing or repeating records. 

Adv may attempt to generate false outputs and correspond-ing measurements in an attempt to create a faulty TDREPORT 

before it is passed to the QE. However, this is infeasible be-cause a MAC is computed on the TDREPORT with a key that is inaccessible to software [4], and QE verifies this MAC before generating a QUOTE . Therefore, it is computationally infeasi-ble for Adv to forge the MAC-ed TDREPORT . Similarly, Adv 

may attempt to forge the QUOTE , but this is also infeasible since QUOTE is signed using a provisioned key whose private key material never leaves QE. Finally, Adv may attempt to re-play responses from the same op to Vrf , but this is detectable by Vrf due to the inclusion of Chal in the construction QUOTE .Finally, since Adv has control over VMM, they can assign which image is used for loading. Therefore, they could load an image containing a backdoor to assist with lying about dataset-s/models that are used for a particular operation. However, the TD image is measured by the TDX module before TD cre-ation, and included in the QUOTE obtained by Vrf . Therefore, these attempts are detectable by Vrf . Furthermore, Adv may attempt to assign a GPU without a TEE to the TD. In this case, Vrf detects it due to PAL ∗M including GPU attestation evidence when the GPU is used in obtaining outputs. In this case, Vrf would observe GPU att ’s absence from the response. Under the assumed thread model, Adv cannot evade or gen-erate false outputs with corresponding measurements without detection. Therefore, PAL ∗M satisfies R4 .

## 7 Related Work 

Cryptographic Attestations. Secure multi-party computa-tion (SMPC) and zero-knowledge proofs (ZKPs) have been used to verify ML operations. SMPC has been explored for distributional property attestation but requires per-attestation interaction with a verifier, which is not scalable [16]. Arc uses SMPC with publicly verifiable commitments to bind training and inference for auditing [43]. ZKPs can provide verifiability for training simple ML models (e.g., logistic regression) [23] and neural networks [1], and LLM fine-tuning [56]. ZKPs have also been used for proof of inference across different types of ML models (including LLMs) [31, 41, 63]. Further-more, ZKPs can help verify security properties of ML models such as differential privacy [20, 61], and fairness [19, 60]. 12 However, these cryptographic primitives have a high compu-tation cost. For instance, it takes around 15 minutes to verify one iteration of VGG11 training [1], making it challenging for practical deployment for applications. 

TEE-based attestations. Prior works have extensively used TEEs for the confidentiality of data and models [42, 45, 64, 73]. Unlike these approaches, we focus on integrity, leveraging TEEs to attest properties of large generative model operations executed within them. Laminator [17] is the clos-est related work, using TEEs to attest properties of CPU-only classifier model operations. Schnabl et al. [59] use TEEs to attest the execution of the code, ML model for various in-ferences, and its outputs. Atlas uses TEEs to verify the ML lifecycle by having TEE-equipped contributors attest to the inputs and outputs of artifact creation or transformation and publish these attestations to a transparency log [62]. Verifi-ableFL proposes an integrity-only enclave (called exclave ), which generates authenticated evidence of input and output data for federated learning [24]. Similar to our approach, it uses CVM technologies (e.g., AMD SEV-SNP), but focuses on federated learning rather than generative models, and it does not focus on addressing the challenges due large datasets outside CVM memory. Rattanavipanon and Nunes [53] focus on federated learning by providing stateful proofs of execu-tion that let an external Vrf confirm correct data use, model updates, and local differential privacy on TrustZone-enabled ARM Cortex-M devices. None of these works focus on at-testing properties of generative models while accounting for challenges of their computing environments (see Section 3). 

## 8 Discussions 

Other Generative Models. Since PAL ∗M is built on Py-Torch and Hugging Face, it will work for any model hosted on Hugging Face, including LLMs not used in our experi-ments and other state-of-the-art generative models, such as diffusion models [28]. As long as the model weights are accessible for measurement, PAL ∗M applies. For image gen-eration diffusion models, the attested properties may differ from those of LLMs; for example, the output (and the input, in the case of image-to-image models) is an image during inference. Thus, the required changes are only to the types of inputs and outputs being measured. 

Alternative Designs. We implemented PAL ∗M using Intel TDX in order to take advantage of the confidential computing support of the NVIDIA H100 GPU, which requires the use of a CVM. Another CVM, such as AMD’s SEV-SNP, can be used instead with appropriate driver support, but other TEEs, such as Intel’s SGX, generally lack this support and therefore will not achieve the full performance of PAL ∗M. Nevertheless, they can be used for models that have reasonable performance with CPU-only computation, such as in previous work [17]. 

Other Data Integrity Techniques. PAL ∗M uses MSH to reduce datasets to a single value that can be computed incre-mentally as each item is read from storage, yielding the same result irrespective of the order in which the records are used. Nevertheless, the MSH is not without limitations. While the ordering of accesses does not impact the final hash, it is neces-sary to access each item exactly once , leading to unnecessary data accesses whenever a metric does not access the entire dataset. An alternative approach is to use a Merkle tree that allows arbitrary random access within a dataset, at the cost of 

O(log n) additional reads and hash evaluations per lookup. An implementation might use a proof of binding to bind a dataset hash to both an MSH and a Merkle tree root hash, allowing it to select a hash type according to the access patterns of the metric being computed. 

Selecting Chal in Property Attestation. Section 4.4 out-lines a property attestation protocol enabled by PAL ∗M ini-tiated by request from Inr containing Chal . In conventional 

RA settings, Chal is a nonce or counter used to prevent replay attacks and ensure response freshness. Given ML property attestation in this work is non-interactive with Vrf , selection of Chal should be derived from a timestamp rather than a traditional cryptographic nonce or counter. However, for proof of inference, including a nonce remains valuable to prevent Prv from cherry-picking favorable exe-cutions. In this setting, Vrf could publish nonce values with associated expiration times for use as PAL ∗M attestation challenges. To further mitigate cherry-picking, Vrf may re-quest only proof of session inference, allowing observability of response history (or lack thereof). 

## 9 Summary 

This work advances ML property attestations by introducing PAL ∗M: a practical and comprehensive framework capable of spanning diverse hardware environments (e.g., CPU-GPU), large-scale data, and emerging generative models. We define how to measure properties of generative models and use in-cremental multiset hash functions to capture properties across dataset operations and training. As a result, PAL ∗M supports deployable accountability for modern AI systems, laying the groundwork for regulators, developers, and verifiers to rea-son more concretely about the trustworthiness of ML systems. 

Acknowledgements. This work is supported in part by the Government of Ontario, the Natural Sciences and Engineering Research Council of Canada (grant number RGPIN-2020-04744), and the Wallenberg Guest Professor Program. The views expressed in the paper are those of the authors and do not reflect the position of the funding agencies. 

## References 

[1] Kasra Abbaszadeh et al. Zero-knowledge proofs of training for deep neural networks. In ACM SIGSAC 

13 Conference on Computer and Communications Security ,CCS ’24, page 4316–4330, New York, NY, USA, 2024. Association for Computing Machinery. [2] Abdelrahman Abouelenin et al. Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs. CoRR , abs/2503.01743, 2025. [3] EU Artificial Intelligence Act. The EU Artificial Intelli-gence Act. European Union , 2024. [4] Erdem Aktas et al. Intel Trust Domain Extensions (TDX) security review. Google security review , 2023. [5] AMD. AMD secure encrypted virtualization (sev). http s://www.amd.com/en/developer/sev.html , 2020. [6] Anthropic and Pattern Labs. Confidential inference via trusted virtual machines. https://www.anthropic. com/research/confidential-inference-trust ed-vms , June 2025. Research report on confidential inference systems using trusted virtual machines. [7] ARM. ARM TrustZone for Cortex-A. https://ww w.arm.com/technologies/trustzone-for-corte x-a , 2004. [8] Ondrej Bojar et al. Findings of the 2014 Workshop on Statistical Machine Translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation ,pages 12–58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. [9] Sergey Bratus et al. TOCTOU, traps, and trusted comput-ing. In International Conference on Trusted Computing ,pages 14–32. Springer, 2008. [10] Brave Software. Verifiable privacy and transparency: A new frontier for brave ai privacy. https://brave. com/blog/browser-ai-tee/ , 11 2025. Accessed: 2025-11-26. [11] Tom Brown et al. Language Models are Few-Shot Learn-ers. In Advances in Neural Information Processing Sys-tems , volume 33, pages 1877–1901. Curran Associates, Inc., 2020. [12] Canonical. Canonical TDX Github Repository. https: //github.com/canonical/tdx , September 2025. [13] Marcin Chrapek et al. Confidential LLM inference: Performance and cost across CPU and GPU TEEs. arXiv preprint arXiv:2509.18886 , 2025. [14] Dwaine Clarke et al. Incremental multiset hash functions and their application to memory integrity checking. In 

International conference on the theory and application of cryptology and information security , pages 188–207. Springer, 2003. [15] Michael Han Daniel Han and Unsloth team. Unsloth. 

https://github.com/unslothai/unsloth , 2023. [16] Vasisht Duddu et al. Attesting distributional properties of training data for machine learning. In European Symposium on Research in Computer Security , pages 3–23. Springer, 2024. [17] Vasisht Duddu, Lachlan J Gunn, and N Asokan. Lamina-tor: Verifiable ml property cards using hardware-assisted attestations. In Proceedings of the Fifteenth ACM Con-ference on Data and Application Security and Privacy ,pages 317–328, 2024. [18] Congyu Fang et al. Proof-of-learning is currently more broken than you think. In 2023 IEEE 8th European Symposium on Security and Privacy (EuroS&P) , pages 797–816. IEEE, 2023. [19] Olive Franzese et al. Secure and confidential certifi-cates of online fairness. In The Thirty-ninth Annual Conference on Neural Information Processing Systems ,2025. [20] Olive Franzese et al. Secure noise sampling for differ-entially private collaborative learning. In Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security , pages 4649–4663, 2025. [21] Leo Gao et al. The Pile: An 800GB Dataset of Di-verse Text for Language Modeling. arXiv preprint arXiv:2101.00027 , 2020. [22] Leo Gao et al. The Language Model Evaluation Harness. 

https://zenodo.org/records/12608602 , July 2024. [23] Sanjam Garg et al. Experimenting with zero-knowledge proofs of training. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security , CCS ’23, page 1880–1894, New York, NY, USA, 2023. Association for Computing Machinery. [24] Jinnan Guo et al. ExclaveFL: Providing transparency to federated learning using exclaves. arXiv preprint arXiv:2412.10537 , 2024. [25] Krishnaprasad Hande and Microsoft team. General avail-ability: Azure confidential VMs with NVIDIA H100 tensor core GPUs, 2024. [26] Casper Hansen. AutoAWQ. https://github.com/c asper-hansen/AutoAWQ , 2023. [27] Dan Hendrycks et al. Measuring Massive Multitask Language Understanding. In ICLR . OpenReview.net, 2021. 14 [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In NeurIPS , 2020. [29] Edward J. Hu et al. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR . OpenReview.net, 2022. [30] Yaqi Hu et al. LLM-based misbehavior detection ar-chitecture for enhanced traffic safety in connected au-tonomous vehicles. IEEE Transactions on Vehicular Technology , 2025. [31] Chenyu Huang et al. zkMLaaS: a verifiable scheme for machine learning as a service. In GLOBECOM 2022-2022 IEEE Global Communications Conference , pages 5475–5480. IEEE, 2022. [32] Hugging Face. Datasets Arrow. https://huggingfac e.co/docs/datasets/v4.1.1/about_arrow , 2025. Accessed: 2025-10-01. [33] Hugging Face. Preprocess. https://huggingface.co /docs/datasets/en/use_dataset , 2025. Accessed: 2025-01-19. [34] Hugging Face. Trainer. https://huggingface.co/d ocs/transformers/main_classes/trainer#tran sformers.Trainer.get_train_dataloader , 2025. Accessed: 2025-01-15. [35] Intel. Intel Provisining Certification Service for ECDSA Attestation. https://api.portal.trustedservic es.intel.com/provisioning-certification .[36] Intel. Intel Software Guard Extensions (Intel SGX). 

https://www.intel.com/content/www/us/en/de veloper/tools/software-guard-extensions/ov erview.html , 2015. [37] Intel. Intel Trust Domain Extensions (Intel TDX). ht tps://www.intel.com/content/www/us/en/deve loper/tools/trust-domain-extensions/overvi ew.html , 2023. [38] Intel. Intel Trust Domain Extensions (Intel TDX) mod-ule base architecture specification. https://cdrdv2 -public.intel.com/853286/intel-tdx-module-b ase-spec-348549006.pdf , Apr 2025. [39] Insu Jang et al. Heterogeneous isolated execution for commodity GPUs. In Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 455–468, 2019. [40] ARM Limited. Overview: Realms. https://learn. arm.com/learning-paths/servers-and-cloud-c omputing/cca-container/overview/ .[41] Tianyi Liu, Xiang Xie, and Yupeng Zhang. zkCNN: Zero knowledge proofs for convolutional neural net-work predictions and accuracy. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Com-munications Security , pages 2968–2985, 2021. [42] Ziyu Liu et al. MirrorNet: A TEE-friendly frame-work for secure on-device DNN inference. In 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD) , pages 1–9. IEEE, 2023. [43] Hidde Lycklama et al. Holding secrets accountable: Au-diting privacy-preserving machine learning. In USENIX Security Symposium , 2024. [44] Sourab Mangrulkar et al. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. https: //github.com/huggingface/peft , 2022. [45] Fan Mo et al. DarkneTZ: towards model privacy at the edge using trusted execution environments. In Interna-tional Conference on Mobile Systems, Applications, and Services , MobiSys ’20, page 161–174, New York, NY, USA, 2020. Association for Computing Machinery. [46] Ivan De Oliveira Nunes et al. Toward remotely verifiable software integrity in resource-constrained iot devices. 

IEEE Communications Magazine , 62(7):58–64, 2024. [47] NVIDIA. AI Security With Confidential Computing. 

https://www.nvidia.com/en-us/data-center/ solutions/confidential-computing/ .[48] NVIDIA. NVIDIA Deployment Guide for SecureAI. 

https://docs.nvidia.com/cc-deployment-gui de-tdx.pdf , February 2025. [49] NVIDIA. NVIDIA attestation. https://docs.nvidi a.com/attestation/index.html , [Accessed] 2025. [50] OpenAI. Reimagining secure infrastructure for ad-vanced ai. https://openai.com/index/reimagini ng-secure-infrastructure-for-advanced-ai/ ,5 2024. Accessed: 2025-11-26. [51] PyTorch. torch.utils.data — PyTorch 2.9 documentation. 

https://docs.pytorch.org/docs/stable/data. html , 2025. Accessed: 2026-01-15. [52] Alec Radford et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. [53] Norrathep Rattanavipanon and Ivan De Oliveira Nunes Nunes. SLAPP: Poisoning prevention in federated learn-ing and DP via stateful proofs of execution. IEEE Trans-actions on Information Forensics and Security (TIFS) ,2025. 15 [54] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering challenge. 

Transactions of the Association for Computational Lin-guistics , 7:249–266, 2019. [55] Herbert Robbins and Sutton Monro. A stochastic ap-proximation method. The annals of mathematical statis-tics , pages 400–407, 1951. [56] Bidhan Roy, Peter Potash, and Marcos Villagra. ZK-LoRA: Efficient zero-knowledge proofs for LoRA veri-fication. In Championing Open-source DEvelopment in ML Workshop @ ICML25 , 2025. [57] Ahmad-Reza Sadeghi and Christian Stüble. Property-based attestation for computing platforms: caring about properties, not mechanisms. In Proceedings of the 2004 workshop on New security paradigms , pages 67–77, 2004. [58] Vinnie Scarlata et al. Supporting third party attestation for Intel SGX with Intel data center attestation primi-tives. White paper , 12, 2018. [59] Christoph Schnabl et al. Attestable audits: Verifiable AI safety benchmarks using trusted execution environ-ments. In ICML Workshop on Technical AI Governance (TAIG) , 2025. [60] Ali Shahin Shamsabadi et al. Confidential-PROFITT: Confidential PROof of fair training of trees. In The Eleventh International Conference on Learning Repre-sentations , 2023. [61] Ali Shahin Shamsabadi et al. Confidential-DPproof: Confidential proof of differentially private training. In 

The Twelfth International Conference on Learning Rep-resentations , 2024. [62] Marcin Spoczynski, Marcela S. Melara, and Sebastian Szyller. Atlas: A framework for ML lifecycle prove-nance & transparency. In 2025 IEEE European Sympo-sium on Security and Privacy Workshops (EuroS&PW) ,pages 448–461. IEEE, 2025. [63] Haochen Sun, Jason Li, and Hongyang Zhang. zkLLM: Zero knowledge proofs for large language models. In 

Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security , pages 4405– 4419, 2024. [64] Tong Sun et al. TensorShield: Safeguarding on-device inference by shielding critical DNN tensors with TEE. In ACM SIGSAC Conference on Computer and Commu-nications Security (CCS) , 2025. [65] Rohan Taori et al. Stanford Alpaca: An instruction-following llama model. https://github.com/tatsu -lab/stanford_alpaca , 2023. [66] Gemma Team. Gemma 3 Technical Report. CoRR ,abs/2503.19786, 2025. [67] Llama Team. The Llama 3 Herd of Models. CoRR ,abs/2407.21783, 2024. [68] Florian Tramer and Dan Boneh. Slalom: Fast, verifi-able and private execution of neural networks in trusted hardware. In International Conference on Learning Representations , 2019. [69] Ehsan Ullah et al. Challenges and barriers of using large language models (LLM) such as ChatGPT for diagnostic medicine with a focus on digital pathology–a recent scoping review. Diagnostic pathology , 19(1):43, 2024. [70] Ashish Vaswani et al. Attention is all you need. Ad-vances in neural information processing systems , 30, 2017. [71] Yangyang Yu et al. Fincon: A synthesized LLM multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. Advances in Neural Information Processing Systems , 37:137010–137045, 2024. [72] Rui Zhang et al. “Adversarial examples” for proof-of-learning. In 2022 IEEE Symposium on Security and Privacy (SP) , pages 1408–1422. IEEE, 2022. [73] Ziqi Zhang et al. No privacy left outside: On the (in-) security of TEE-shielded DNN partition for on-device ML. In 2024 IEEE Symposium on Security and Privacy (SP) , pages 3327–3345. IEEE, 2024. [74] Yukun Zhu et al. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision , pages 19–27, 2015. 

## A Operations and Properties in PAL ∗M

Table 7 describes supported operations in PAL ∗M and result-ing property attestations. Table 7: Operations and property attestations in PAL ∗M           

> Operation ( op )Property Attestation
> Preprocessing Proof of preprocessing Attribute distribution Proof of attribute distribution Measurement binding Proof of binding Training Proof of training Weight optimization Proof of optimization Fine-tuning Proof of fine-tuning Quantization Proof of quantization Evaluation Proof of evaluation Inference Proof of inference Chat session inference Proof of session inference

16