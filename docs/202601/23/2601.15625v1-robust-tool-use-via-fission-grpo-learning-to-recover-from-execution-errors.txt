Title: Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors

URL Source: https://arxiv.org/pdf/2601.15625v1

Published Time: Fri, 23 Jan 2026 01:25:52 GMT

Number of Pages: 13

Markdown Content:
## Robust Tool Use via F ISSION -GRPO: Learning to Recover from Execution Errors 

Zhiwei Zhang 1, Fei Zhao 2, Rui Wang 1, Zezhong Wang 1,Bin Liang 1, Jiakang Wang 2, Yao Hu 2, Shaosheng Cao 2, Kam-Fai Wong 1

1The Chinese University of Hong Kong, 

2Xiaohongshu Inc. 

zhangzhiwei1019@link.cuhk.edu.hk, caoshaosheng@xiaohongshu.com 

Abstract 

Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feed-back and self-correct. This brittleness hinders reliable real-world deployment, where the ex-ecution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard rein-forcement learning (RL) treats errors as sparse negative rewards, providing no guidance on 

how to recover, while pre-collected synthetic error-correction datasets suffer from distribu-tion mismatch with the model’s on-policy er-ror modes. To bridge this gap, we propose FISSION -GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mecha-nism fissions each failed trajectory into a new training instance by augmenting it with diag-nostic feedback from a finetuned Error Simula-tor, then resampling recovery rollouts on-policy. This enables the model to learn from the pre-cise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, F ISSION -GRPO im-proves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% → 46.75%) over GRPO and outperforming specialized tool-use agents. 

1 Introduction 

Agentic AI is moving from prototypes to produc-tion, driving demand for tool-using agents that are not only capable but also efficient enough for low-latency and on-device deployment (Belcak et al., 2025). Smaller language models (SLMs) are in-creasingly recognized as the practical foundation for such systems (Sharma and Mehta, 2025). How-ever, for SLMs to fulfill this role, they must ex-hibit robustness , the ability to handle the inevitable USER: Check status of flight JL -123.               

> ASSISTANT: get_flight_status (flight _id='JL -123').
> TOOL_OUTPUT: {'status': 'CHECKED_IN'}
> The flight JL -123 is checked in and ready to go.
> TURN 1: SUCCESS
> TURN 2: ERRO RTRIGGER
> USER: Cancel this flight immediately.
> ASSISTANT: cancel _flight( flight _id ='JL -123').
> TOOL OUTPUT: Error 409: StateConflict .
> Status is CHECKED _IN. Must be OPEN.
> TURN 3: HALLUCINATED FIX
> ASSISTANT: cancel_flight (flight_id ='JL -123', force=True)
> TOOL OUTPUT: Error: cancel_flight got
> unexpected argument 'force'.
> TURN 4: REPETITIVE HALLUCINATION (COLLAPSE)
> ASSISTANT: cancel_flight (flight_id ='JL -123', force=True)
> TOOL OUTPUT: Error: cancel _flight got
> unexpected argument 'force'.
> …
> FAILURE

(a) Typical failure: an API error triggers a hallucinated retry loop. 

(b) Error recovery rates on BFCL v4 Multi-Turn across model scales and evaluation subsets. 

Figure 1: Error recovery is a key bottleneck for smaller tool-using models in multi-turn execution. (a) shows a representa-tive hallucinated retry loop after an API error, while (b) reports recovery rates on BFCL v4 across model scales. 

execution errors that arise in dynamic, multi-turn tool-use environments (Patil et al., 2025). This robustness requirement exposes a critical gap. In practice, APIs return errors, parameters become invalid, and system states change unex-pectedly; a robust agent must interpret such feed-1

> arXiv:2601.15625v1 [cs.LG] 22 Jan 2026

back, diagnose the fault, and self-correct (Yao et al., 2022; Shinn et al., 2023). Yet as shown in Fig-ure 1b, smaller models exhibit a pronounced defi-ciency in this error recovery capability (defined as success rate conditioned on at least one prior exe-cution error). On the BFCL v4 Multi-Turn bench-mark (Patil et al., 2025), Claude Sonnet 4 achieves recovery rates exceeding 50%, while Qwen3-8B averages only around 20% across the four evalu-ation subsets. Figure 1a illustrates a representa-tive failure mode: upon receiving an error (e.g., 

StateConflict ), the model fails to diagnose the root cause and instead hallucinates invalid parame-ters (e.g., a non-existent force argument), entering a repetitive loop until the conversation collapses. Bridging this robustness gap is essential for en-abling smaller models to serve as reliable founda-tions for agentic systems. Current approaches fall short of addressing this challenge. Methods based on static synthetic datasets (Liu et al., 2024; Zhang et al., 2025a,b) construct error-correction pairs offline, but the error distribution shifts as the policy improves, making offline error corpora quickly stale and leading to distribution mismatch. Meanwhile, reinforcement learning (RL) approaches such as GRPO (Shao et al., 2024) treat errors merely as sparse negative rewards. This signals that something went wrong, but offers no guidance on how to recover: the gradi-ent discourages the failed action without teaching a corrective alternative. When all sampled roll-outs fail, the advantage variance collapses, yielding vanishing gradients that stall learning entirely (Yu et al., 2025; Nan et al., 2025). In essence, exist-ing methods treat errors as outcomes to be avoided 

rather than opportunities to be learned from .To bridge this gap, we propose F ISSION -GRPO, a framework that transforms execution errors into dense, on-policy-aligned corrective supervision (Figure 2). The framework operates in three stages. In Stage 1 , we perform standard GRPO exploration, sampling multiple rollouts per query and updating the policy with group-relative advantages under the GRPO objective. In Stage 2 , failed rollouts are in-tercepted and augmented with diagnostic feedback from a learned Error Simulator , constructing cor-rective contexts of the form [ dialogue ; failed call ;

feedback ]. In Stage 3 , these contexts trigger a fis-sion update: each error is expanded into G′ parallel recovery attempts by resampling new rollouts con-ditioned on the augmented context, analogous to nuclear fission where one event induces a multi-plicative chain of subsequent reactions and thus generates many new training signals. The Error Simulator is trained via supervised fine-tuning to produce realistic, context-aware di-agnostics that resemble runtime error traces. To avoid trivial target leakage, its outputs are restricted to non-revealing error descriptions (e.g., “param-eter status expects value OPEN ”) rather than the full target call. This closed-loop process continu-ously focuses learning on the model’s current er-ror modes, mitigating the distribution mismatch of static error-correction datasets. We evaluate F ISSION -GRPO on the BFCL v4 Multi-Turn benchmark and demonstrate substantial improvements. Our main contributions are: • Fission-GRPO Framework. We propose a RL framework that dynamically converts execution errors into corrective training instances. By resampling from augmented error contexts on-policy, our approach maintains alignment with the model’s evolving error distribution. • Learned Error Simulator. We develop a su-pervised fine-tuned error simulator to generate realistic diagnostic feedback resembling runtime error traces, enabling effective recovery training without live API interactions or target leakage. • Empirical Validation. On the BFCL v4 Multi-Turn benchmark, F ISSION -GRPO achieves state-of-the-art performance across the Qwen3 model family (1.7B, 4B, and 8B), consistently outper-forming GRPO, DAPO, and Dr.GRPO baselines. For Qwen3-8B, our method improves the error recovery rate by 5.7% absolute, yielding a 4% overall accuracy gain (42.75% → 46.75%) and surpassing specialized 8B-scale tool agents. 

2 Related Work 

2.1 RL for Tool Use 

RL has become the standard for aligning LLMs (Schulman et al., 2017; Ouyang et al., 2022). Among recent algorithms, GRPO (Shao et al., 2024) reduces memory overhead by estimating baselines from group averages, making it partic-ularly suitable for tool-calling tasks characterized by binary or scalar rewards (Guo et al., 2025). Despite its efficiency, GRPO relies on intra-group variance, creating vulnerabilities when a sampled group is homogeneously incorrect. In such cases, the reward variance drops to zero, yield-ing null gradients and wasting training signals—a limitation targeted by DAPO (Yu et al., 2025) and 2Stage1:Standard GRPO   

> Sampling & Update
> Stage2:Error Identification &
> Corrective Sample Construction
> Stage3:Corrective Batch Training
> (Fission Occurs !)
> Policy
> Model Error
> Simulator
> Corrective
> Sample Pool
> Corrective GRPO Update

Core Fission Mechanism: 2 Errors (Stage 2) →2 Corrective Samples→ 2 x 4= 8 New Rollouts (Stage 3)          

> Query
> Rollout
> Rollout
> Standard
> GRPO
> Update
> Identify
> Errors
> Identify
> Errors Input :
> Input :Corrective
> Sample
> Corrective
> Sample
> Accumulated: 2
> Batch Trigger
> (Size=2 Reached!)
> Sample G
> rollouts m
> Resample
> G rollouts
> using m
> Resample
> G rollouts
> using m
> Rollout
> Rollout
> Recursive Fission Loop
> (Optional, if new errors R ʹ< )
> Recursive Fission Loop
> (Optional, if new errors R ʹ< )

Figure 2: Overview of the F ISSION -GRPO Framework. The framework operates in three stages: (1) Standard Exploration ,utilizing GRPO to optimize policy πθ on the query distribution D; (2) Error Identification & Synthesis , where a simulator Sϕ

generates diagnostic feedback for filtered error trajectories; and (3) Fission-based Update , where corrective samples trigger a multiplicative resampling process (factor G′) to align the policy with recovery paths. 

NGRPO (Nan et al., 2025). Furthermore, even when gradients exist, blindly applying negative feedback can trigger Lazy Likelihood Displacement (LLD) (Deng et al., 2025), where valid reasoning steps are suppressed simply because they appear in failed trajectories. While strategies exist to mitigate these is-sues, such as filtering homogeneous batches (DAPO), calibrating advantages (NGRPO), or down-weighting negative gradients (NTHR (Deng et al., 2025)), they primarily optimize the loss land-scape of negative signals. They do not fundamen-tally address the scarcity of positive guidance dur-ing exploration. Our approach bridges this gap by actively constructing recovery trajectories via fis-sion, transforming zero-reward errors into dense, supervised learning signals. 

2.2 Robust Tool Use and Error-Driven Synthesis 

Research in tool utilization has evolved from en-suring syntactic correctness in single-turn interac-tions (Schick et al., 2023; Patil et al., 2024) to maintaining reliability across complex, multi-turn workflows (Qin et al., 2023; Yao et al., 2022). As tasks grow in complexity, the capacity to recover from inevitable environment errors (e.g., timeouts, invalid parameters) becomes a defining metric of robustness. This requirement is codified in bench-marks like BFCL (Patil et al., 2025) and Stable-ToolBench (Guo et al., 2024), which specifically evaluate persistence under error conditions. To address these challenges, recent approaches have formalized “diagnosis-and-repair” mecha-nisms (Su et al., 2025; Huang et al., 2025) or trained models on diverse error scenarios (Vuddanti et al., 2025). In parallel, synthetic correction meth-ods, originally proven in reasoning domains (Pan et al., 2025; Xu et al., 2025), have been adapted to tool-use by frameworks like ToolACE (Liu et al., 2024) and LoopTool (Zhang et al., 2025b) to ex-pand training coverage through model-based syn-thesis. However, a critical limitation persists: these methods predominantly rely on offline data con-struction. This creates a temporal mismatch where static training data fails to reflect the model’s evolv-ing on-policy error distribution (Kumar et al., 2024; Zhang et al., 2025b). Unlike prior offline synthe-sis approaches (Pan et al., 2025; Su et al., 2025), our work integrates error simulation directly into the training loop, ensuring alignment with current policy limitations. 

3 Method 

We propose F ISSION -GRPO, a framework de-signed to imbue small language models with robust error recovery capabilities. As illustrated in Fig-ure 2, our approach operates in a dual-stream man-ner: standard exploration to maintain general tool-3use competence, and a conditional fission stream that intercepts errors to enable active remedial learning. 

3.1 Preliminaries 

We formulate tool use as a language generation task. Given a query x and a tool library, a policy 

πθ generates a trajectory τ consisting of reasoning thoughts and tool calls. We adopt GRPO (Shao et al., 2024) as our optimization backbone. Un-like PPO, GRPO eliminates the need for a value network by estimating the baseline from the group average. For each query x, we sample a group of outputs {τi}Gi=1 and optimize: 

J (θ) = Ex∼D 

"

1

G

> G

X

> i=1

ˆR(τi) · πratio (τi) − βDKL 

#

(1) where ˆR(τi) = R(τi)−μR 

> σR+ϵ

is the normalized reward, with μR and σR being the mean and standard devi-ation of rewards within the group, and πratio is the clipped probability ratio. 

3.2 Reward Design 

To guide the policy from syntactic compliance to semantic precision, we design a time-dependent composite reward function R(τ, t ), where t denotes the training step. The total reward is a weighted sum of three components: 

Format Compliance ( Rfmt ). This binary term 

Rfmt (τ ) ∈ { 0, 1} enforces structural constraints, ensuring outputs adhere to the required XML/J-SON schema. We apply a decaying weight wfmt (t)

that reduces its maximum contribution from 2 to 1, shifting focus from syntax to semantics as training progresses. 

Functional Correctness ( Rcorr ). This term eval-uates alignment between invoked tools and user intent. To accommodate partial matching in com-plex parameters, we define Rcorr ∈ [0 , 2] as: 

Rcorr (τ, y ∗) = α · I(N = N ∗)+ (1 − α) · 1

|M| 

X

> (a,a ∗)∈M

F1 (a, a ∗)

(2) where I(N = N ∗) indicates correct function selec-tion, M denotes matched argument pairs between prediction τ and ground truth y∗, and F1 measures token-level overlap. The weight wcorr (t) increases monotonically, scaling its maximum contribution from 2 to 3 to prioritize parameter precision in later stages. 

Efficiency Regularization ( Rlen ). To prevent verbose or degenerate reasoning, we impose a length penalty Rlen ∈ [0 , 1] via a piecewise Gaus-sian function with time-annealing tolerance. 

3.3 The F ISSION -GRPO Framework 

As illustrated in Figure 2, F ISSION -GRPO oper-ates in a three-stage closed loop. Stage 1 focuses on optimizing fundamental tool-use capabilities ,while Stages 2 and 3 are dedicated to developing 

error recovery skills through targeted error correc-tion. 

3.3.1 Stage 1: Standard Exploration and Update 

This stage aims to establish and maintain the model’s base performance on tool-calling tasks. 

Sampling and Evaluation. Given a query x, we sample a group of trajectories {τi}Gi=1 from the cur-rent policy πθ. We evaluate these rollouts using the composite reward function defined in §3.2, comput-ing format compliance Rfmt , functional correctness 

Rcorr , and efficiency regularization Rlen , which are then aggregated into the total reward. 

Optimization. We apply the standard GRPO up-date (Eq. 1) using these trajectories to improve the model’s fundamental tool-use capabilities. This step ensures continuous optimization on the core skill of accurate tool invocation and parameter grounding. Subsequently, all sampled trajectories from this stage are forwarded to Stage 2 for diag-nostic error analysis and corrective training. 

3.3.2 Stage 2: Error Identification and Corrective Sample Construction 

Stage 2 converts error traces produced in Stage 1 into actionable corrective instances. Concretely, we apply a two-level filter to isolate erroneous tra-jectories and then synthesize feedback that can be appended to the original context for subsequent corrective updates. 

Error Identification. We decompose error detec-tion into format validity and functional correctness .Let Rfmt denote whether the tool-call format is valid. If Rfmt (τ ) = 0 , the trajectory is immediately treated as an error without consulting correctness. Otherwise, we further evaluate correctness with a 4scalar score Rcorr and flag the trajectory when it falls below a tunable threshold δcorr :

E = {τi | Rcorr (τi) < δ corr ∨ Rfmt (τi) = 0 } (3) In Fig. 2, we use a simplified illustration (e.g., R < δ) to emphasize the gating effect; this does not contradict Eq. (3). 

Hybrid Feedback Synthesis. For effective cor-rection, a scalar penalty is insufficient; we require an explicit diagnostic message f that resembles the runtime system feedback. We adopt a hybrid strat-egy: (i) for format errors (Rfmt = 0 ), we use deter-ministic error messages (e.g., parser/compiler-style feedback) to explicitly state the violated schema/se-rialization constraints; (ii) for semantic errors 

(Rcorr < δ corr ), we query a learned Error Sim-ulator Sϕ to produce a concise, actionable runtime error string. The simulator is implemented as a Qwen3-32B model fine-tuned via SFT to emulate run-time environment responses. We construct a training set of approximately 2K instances from error logs, where each instance comprises: (i) the original system prompt and tool specification along with the dialogue state, (ii) the model’s failed tool call ( τerr ), (iii) the ground-truth tool call ( τgt ), and (iv) a teacher-written diagnos-tic error message (e.g., generated via Claude-Sonnet-4), followed by quality filtering. During both training and inference, the simulator con-sumes (system + tools , dialogue history , τ gt , τ err )

and produces a concise feedback string f ←

Sϕ(x, τ err , τ gt ), where f is constrained to be a realistic runtime response. We provide the ex-act prompting template used to query Sϕ in Ap-pendix A. 

Corrective Sample Construction and LIFO Buffering. Given a flagged trajectory τerr with feedback f , we construct a corrective context by appending the failed attempt and the diagnostic message to the original multi-turn input: 

xcorr =  x; τerr ; f . (4) We optionally deduplicate corrective instances by hashing (x, τ err ) to avoid repeatedly training on near-identical errors. All corrective samples are stored in a LIFO buffer Bcorr , so that the most re-cent errors are consumed first during corrective updates. This design keeps the corrective batch dis-tribution closer to the current policy πθ, improving the on-policy approximation in multi-turn tool-use training. 

3.3.3 Stage 3: Corrective Batch Training 

Once the LIFO buffer accumulates sufficient re-cent errors (Batch Trigger), we activate Fission to perform targeted remedial updates for recovery. 

Multiplicative Resampling. We pop the freshest corrective contexts xcorr and, for each of them, sam-ple a “fission group” of G′ trajectories conditioned on the same context: 

{τ ′ 

> j

}G′ 

> j=1

∼ πθ(· | xcorr ). (5) This turns a single error case into multiple paral-lel recovery attempts, densifying training signals around the observed error. 

More Informative Advantages. Hard queries can yield near-homogeneous outcomes in standard exploration, weakening within-group relative ad-vantages. Conditioning on explicit feedback f typi-cally increases outcome diversity within the fission group, improving the usefulness of advantage esti-mates for recovery updates. We optimize the same GRPO-style objective as Eq. (1) , but over the cor-rective distribution: 

Jcorr (θ) = Excorr 

 1

G′

> G′

X

> j=1

ˆA(τ ′ 

> j

) ∇ log πθ(τ ′ 

> j

| xcorr )

 .

(6) 

Summary. These three stages form a continuous loop; detailed pseudocode and hyperparameters are provided in Algorithm 1 (Appendix B). 

4 Experiments 

4.1 Experimental Setup Data Construction Diverging from prevalent tool-learning paradigms that emphasize extensive scaling of synthetic corpora (e.g., ToolACE (Liu et al., 2024), XLAM (Zhang et al., 2025a)), we prioritize data quality and trajectory correctness .We implement a three-stage pipeline to construct a compact yet rigorous training set: (1) Domain Schema Curation : We curated a diverse schema library spanning 11 domains (e.g., 

Healthcare, Smart Home, Vehicle Control ), prompt-ing Claude-4-Sonnet to generate realistic API defi-nitions grounded in BFCL characteristics. (2) Trajectory Synthesis : Utilizing Claude-4-Sonnet, we first synthesized multi-turn user queries 5Model Method Overall Acc Base Miss Func Miss Param Long Context Qwen3-1.7B Base 7.80% 10.00% 11.00% 8.00% 2.50% GRPO 17.12% 22.00% 18.50% 15.50% 12.50% DAPO 16.00% 22.00% 17.00% 14.00% 11.00% Dr.GRPO 16.12% 19.50% 17.50% 14.50% 13.00%                                                                     

> FISSION -GRPO (Ours) 20.38% 29.00% 18.50% 16.00% 18.00%
> Qwen3-4B Base 19.37% 24.50% 19.00% 14.50% 19.50% GRPO 36.38% 46.50% 34.50% 27.50% 37.00% DAPO 38.25% 48.50% 36.50% 28.00% 40.00%
> Dr.GRPO 34.75% 43.00% 34.50% 27.50% 34.00%
> FISSION -GRPO (Ours) 40.87% 51.50% 42.50% 30.50% 39.00%
> External 8B Agent Models
> ToolACE-2-8B –37.00% 47.00% 31.00% 28.00% 42.00% BitAgent-8B –37.75% 46.50% 37.50% 24.00% 43.00% Qwen3-8B Base 28.75% 35.50% 37.00% 22.50% 20.00% GRPO 42.75% 50.50% 41.00% 36.00% 43.50% DAPO 43.12% 54.50% 44.50% 29.00% 44.50% Dr.GRPO 44.88% 55.00% 45.00% 32.50% 47.00%
> FISSION -GRPO (Ours) 46.75% 57.50% 43.50% 38.00% 48.00% Table 1: Performance comparison on BFCL V4 Multi-Turn benchmark across model scales and training methods.

based on these schemas, followed by generating full interaction trajectories that fulfill the requests. (3) Hierarchical Filtering and Factorization :To ensure rigorous quality control, we applied a hierarchical protocol. First, raw trajectories un-derwent a global coherence check via Claude-4-Sonnet. Validated trajectories of length K were then factorized into discrete decision instances 

{(ht, a t)}Kt=1 , where ht denotes the cumulative context history. Finally, these decomposed in-stances underwent a double-blind verification via Qwen3-235B-A22B-Instruct-2507 (Team, 2025) and Kimi K2 (Team et al., 2025). Only samples achieving unanimous consensus were retained, dis-tilling an initial pool of ∼2,000 trajectories down to 630 high-quality training instances. 

Training Details All models are trained using the Verl framework (Sheng et al., 2024) on a sin-gle node with 8 ×H800 80GB GPUs. For GRPO training, we use a learning rate of 1e-6 with cosine warmup, a batch size of 8, and sample 8 rollouts per query ( G = 8 ). The maximum prompt length is set to 12,800 tokens and the maximum response length to 4,096 tokens. We use temperature 0.95 and top-k 50 for sampling. For F ISSION -GRPO, we set the correctness threshold δcorr = 1 for error identification (Eq. 3), determined empirically as a stable threshold across multiple runs. 

Benchmarks. We evaluate on the BFCL V4 Multi-Turn benchmark (Patil et al., 2025), specifi-cally chosen for its stress-testing of state tracking and robustness. A distinctive feature of this bench-mark, crucial to our study, is its interactive error feedback mechanism: upon execution error, the environment provides explicit error traces and per-mits the agent up to 20 retries to correct its action. This setup directly aligns with our research objec-tive, allowing us to measure how improved error recovery dynamics translate to overall success rates in tool use. 

Baselines. We compare F ISSION -GRPO against advanced RL baselines implemented on the Qwen3 series (1.7B/4B/8B), including: (1) GRPO (Shao et al., 2024), utilizing group-normalized advan-tages; (2) DAPO (Yu et al., 2025), incorpo-rating dynamic sampling constraints; and (3) 

Dr.GRPO (Liu et al., 2025), employing mean-centered estimators to mitigate length bias. For broader context, we also report performance of spe-cialized 8B-scale tool agents such as ToolACE (Liu et al., 2024) and BitAgent. 

4.2 Main Results 

Table 1 presents the performance on the BFCL v4 Multi-Turn benchmark. As shown, F ISSION -GRPO achieves consistent state-of-the-art (SOTA) performance across all Qwen3 model scales (1.7B, 4B, and 8B) when compared to other GRPO-based post-training methods. 

Scalability and Subset Performance. Our method demonstrates superior scalability and ro-6bustness. On Qwen3-1.7B, F ISSION -GRPO yields a substantial improvement, elevating overall ac-curacy to 20.38% , a relative gain of over 160% compared to the Base model (7.80%) and surpass-ing standard GRPO (17.12%). As the model scale increases, the performance gap remains distinct: on Qwen3-4B and Qwen3-8B, our method achieves 

40.87% and 46.75% accuracy respectively, outper-forming strong baselines like DAPO and Dr.GRPO. Notably, F ISSION -GRPO excels in the Base and 

Miss Param categories, achieving the highest scores across most settings (e.g., 57.50% on 8B Base and 30.50% on 4B Miss Param), indicating a precise understanding of function calls and parameters. 

Comparison with Specialized Agents. Fur-thermore, we compare our generalist approach with specialized 8B-scale tool agents. F ISSION -GRPO (Qwen3-8B) significantly outperforms both ToolACE-2-8B (37.00%) and BitAgent-8B (37.75%) by margins of 9.75 and 9.00 percentage points, respectively. This underscores the efficacy of our method in enhancing tool-use capabilities beyond varying baselines. 

4.3 Error Recovery Analysis 

To identify the source of performance gains, we decouple the overall success rate into two compo-nents: One-Shot Success (success achieved with-out triggering any errors) and Error Recovery Rate 

(conditional probability of success after an error occurs). Figure 3 illustrates this breakdown for the Qwen3-8B model. The results clearly indicate that the performance improvement is primarily driven by enhanced error recovery capabilities. FISSION -GRPO yields an average improvement of 

5.7% in Error Recovery Rate across all categories, with particularly substantial gains in Long Context 

(+11.8%) and Base (+5.5%) scenarios. This en-hanced recovery capability serves as the primary contributor to the overall accuracy improvement from 42.75% to 46.75%. Crucially, this gain does not come at the expense of fundamental capabilities. The One-Shot Success Rate is preserved and even modestly improved by an average of 1.75%, with notable gains in Long Context (+3.5%) and Base (+2.0%). This confirms that our framework effectively equips the model with robust recovery skills while simultaneously en-hancing its standard tool-use competence, demon-strating that the fission mechanism provides com-                                                            

> Figure 3: Performance decomposition on BFCL v4 Multi-Turn (Qwen3-8B).
> Method Avg. Base M.Func M.Param Long
> Qwen3-1.7B
> GRPO 17.12 22.00 18.50 15.50 12.50 Static 17.75 23.50 21.00 15.50 11.00
> Dynamic 20.38 29.00 18.50 16.00 18.00
> Qwen3-4B
> GRPO 36.38 46.50 34.50 27.50 37.00 Static 37.25 50.50 34.00 28.50 36.00
> Dynamic 40.87 51.50 42.50 30.50 39.00
> Qwen3-8B
> GRPO 42.75 50.50 41.00 36.00 43.50 Static 44.00 53.50 43.00 35.50 44.00
> Dynamic 46.75 57.50 43.50 38.00 48.00 Table 2: Ablation on Feedback Quality. Static denotes Fission training with generic error prompts; Dynamic uses our simulated feedback. Avg. denotes Overall Accuracy; M.Func
> and M.Param denote Missing Function and Missing Parameter errors, respectively.

plementary benefits to both error prevention and error correction. 

4.4 Impact of Feedback Quality 

To disentangle the contribution of the Fission mech-anism from the informational gain of the Error Simulator, we conduct an ablation study across three settings: (1) GRPO : The standard baseline without explicit recovery training. (2) Fission-Static : Applies the fission update but uses a fixed, generic error message for all errors. 1 (3) Fission-Dynamic : Our full method using the Error Simula-tor for context-aware feedback. Results in Table 2 reveal a hierarchical improve-ment. First, Fission-Static consistently outperforms GRPO. Even with uninformative feedback, the ex-plicit process of re-sampling and penalizing failed trajectories forces the model to refine its internal state tracking (e.g., +1.25% on Qwen3-8B Overall Acc). This validates that the structural intervention  

> 1The static prompt is: “ERROR: Function call failed. Please verify your output format, function name, required parameters, and parameter values are correct.”

71 2 3 4 5 6 7 8 9 10    

> Correction Trigger Interval N (steps)
> 20
> 25
> 30
> 35
> 40
> 45
> 50
> 55
> 60
> Accuracy (%)
> Base
> Miss Func
> Miss Param
> Long Context
> Overall
> N= 4 Figure 4: Multi-turn performance across different correction trigger intervals ( N) on BFCL v4 Multi-Turn.

of the fission mechanism is inherently valuable. Second, Fission-Dynamic yields significant marginal gains. The gap between Static and Dy-namic (e.g., +3.62% on Qwen3-4B) underscores the necessity of precise supervision. Generic sig-nals fail to guide the model through complex errors, whereas simulated feedback effectively directs the gradient towards correcting specific semantic er-rors, particularly in the Miss Param and Long Con-text subsets. 

4.5 Sensitivity to Correction Trigger Frequency 

To investigate how frequently correction updates should be triggered in training, and to balance timely suppression of recurring error patterns with scheduling overhead and potential training instabil-ity, we conduct a sensitivity study on the minimum trigger interval, denoted by N (in global steps). Here N controls how often correction can be in-serted by limiting correction updates to occur no more than once every N global steps. We fix the total training budget to 234 global steps and vary only N , enforcing the constraint that at most one correction update occurs every N global steps. We further adopt a LIFO sampling strategy to prior-itize the most recent correction samples, which keeps updates better aligned with the current policy distribution and therefore closer to the on-policy assumption. 

Results. Figure 4 shows Multi-turn Overall Ac-curacy and category-wise metrics as a function of 

N . Performance remains relatively stable when correction is triggered frequently, corresponding to small N , but declines noticeably as updates be-come sparse, corresponding to large N , with con-sistent degradation across subcategories. The drop is most pronounced on Miss Param and is also ev-ident on Long Context, indicating that parameter-level errors and long-context interactions particu-larly benefit from timely correction signals. These results suggest that standard policy optimization alone does not reliably prevent error patterns from accumulating and reappearing over long horizons, whereas correction updates serve as a stabiliz-ing mechanism for multi-turn reliability. At the same time, the stability observed over a small-to-moderate range of N indicates that correction does not need to be inserted extremely often to remain effective, highlighting a practical trade-off between correction timeliness and scheduling cost. 

4.6 Case Study: Error Recovery Behaviors 

To qualitatively illustrate the robustness improve-ments, we compare three Qwen3-8B variants (Base, GRPO, F ISSION -GRPO) on a representative multi-turn file manipulation task (from BFCL V4 Multi-Turn Base) requiring state tracking across directory changes and file moves (full logs in Appendix C). We observe three distinct error-recovery patterns. The Base model exhibits collapse : it fails to up-date internal state after partial command success, entering repetitive invalid retries until conversation breakdown. GRPO shows hallucination : it rec-ognizes errors but lacks grounding—when a file path becomes invalid, it invents non-existent pa-rameters (e.g., a path argument for ls ) rather than verifying the actual state. In contrast, FISSION -GRPO demonstrates active diagnosis : it employs a diagnose-then-correct strategy, deploying veri-fication tools (e.g., find ) to resolve state uncer-tainty before reattempting the task. This compari-son shows that F ISSION -GRPO transforms error signals into active diagnostic capabilities rather than brittle heuristics. 

5 Conclusion 

We presented F ISSION -GRPO, a framework that transforms execution errors into on-policy correc-tive supervision for multi-turn tool use. By inter-cepting errors, augmenting them with simulated feedback, and resampling recovery attempts, our approach enables smaller models to learn robust self-correction rather than collapsing into repeti-tive loops. On BFCL v4 Multi-Turn, Qwen3-8B achieves a 5.7% gain in error recovery and 4% overall accuracy improvement, outperforming spe-cialized tool agents. The fission paradigm may gen-eralize to other iterative refinement domains such as code debugging and mathematical reasoning. 8Limitations 

Our work has several limitations that suggest direc-tions for future research. 

Evaluation Scope. We evaluate F ISSION -GRPO exclusively on the BFCL v4 Multi-Turn bench-mark. We chose this benchmark because it is one of the few that features an interactive error feedback mechanism permitting retry attempts, which di-rectly aligns with our focus on error recovery. Most existing tool-use benchmarks emphasize single-turn correctness without explicit retry mechanisms. Extending evaluation to other domains with error-retry dynamics (e.g., interactive code debugging or web navigation with fallback) is a promising direction for future work. 

Computational Overhead. The fission mecha-nism introduces additional computational cost by resampling G′ rollouts for each intercepted error. We partially mitigate this through a configurable trigger interval N (Section 4.5), which allows trad-ing off correction frequency against training effi-ciency. Our analysis shows that moderate inter-vals maintain effectiveness while reducing over-head, though further optimization of this trade-off remains an open direction. 

References 

Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. 2025. Small language models are the future of agentic ai. arXiv preprint arXiv:2506.02153 .Wenlong Deng, Yi Ren, Muchen Li, Danica J Suther-land, Xiaoxiao Li, and Christos Thrampoulidis. 2025. On the effect of negative gradient in group relative deep reinforcement optimization. arXiv preprint arXiv:2505.18830 .Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-rong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 .Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. In ACL (Findings) .Shiting Huang, Zhen Fang, Zehui Chen, Siyu Yuan, Jun-jie Ye, Yu Zeng, Lin Chen, Qi Mao, and Feng Zhao. 2025. Critictool: Evaluating self-critique capabil-ities of large language models in tool-calling error scenarios. arXiv preprint arXiv:2506.13977 .Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, and 1 others. 2024. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917 .Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, and 1 others. 2024. Toolace: Winning the points of llm function calling. 

arXiv preprint arXiv:2409.00920 .Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Understanding r1-zero-like training: A critical perspective. arXiv preprint arXiv:2503.20783 .Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, and 1 others. 2025. Ngrpo: Negative-enhanced group relative policy op-timization. arXiv preprint arXiv:2509.18851 .Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow in-structions with human feedback. Advances in neural information processing systems , 35:27730–27744. Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, H Vicky Zhao, Conghui He, and Lijun Wu. 2025. Lemma: Learning from errors for mathematical advancement in llms. arXiv preprint arXiv:2503.17439 .Shishir G Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph E Gonzalez. 2025. The berkeley function calling leader-board (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning .Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2024. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems , 37:126544–126565. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, and 1 others. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789 .Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Pro-cessing Systems , 36:68539–68551. 

9John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proxi-mal policy optimization algorithms. arXiv preprint arXiv:1707.06347 .Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 .Raghav Sharma and Manan Mehta. 2025. Small lan-guage models for agentic systems: A survey of ar-chitectures, capabilities, and deployment trade offs. 

arXiv preprint arXiv:2510.03847 .Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 .Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Re-flexion: Language agents with verbal reinforcement learning. Advances in Neural Information Process-ing Systems , 36:8634–8652. Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi, Tianyang Han, Junfeng Luo, and Yurui Qiu. 2025. Failure makes the agent stronger: Enhancing accu-racy through structured reflection for reliable tool interactions. arXiv preprint arXiv:2509.18847 .Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, and 1 others. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534 .Qwen Team. 2025. Qwen3 technical report. Preprint ,arXiv:2505.09388. Sri Vatsa Vuddanti, Aarav Shah, Satwik Kumar Chit-tiprolu, Tony Song, Sunishchal Dev, Kevin Zhu, and Maheep Chaudhary. 2025. Paladin: Self-correcting language model agents to cure tool-failure cases. 

arXiv preprint arXiv:2509.25238 .Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, and Wenjie Li. 2025. Subtle errors in reasoning: Preference learning via error-injected self-editing. In Proceedings of the 63rd Annual Meet-ing of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 31184–31203. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations .Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, and 1 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 .Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Quoc Hoang, Shirley Kokane, Weiran Yao, Jun-tao Tan, Akshara Prabhakar, Haolin Chen, and 1 oth-ers. 2025a. xlam: A family of large action models to empower ai agent systems. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Lin-guistics: Human Language Technologies (Volume 1: Long Papers) , pages 11583–11597. Kangning Zhang, Wenxiang Jiao, Kounianhua Du, Yuan Lu, Weiwen Liu, Weinan Zhang, and Yong Yu. 2025b. Looptool: Closing the data-training loop for robust llm tool calls. arXiv preprint arXiv:2511.09148 .

A Prompt Template for the Error Simulator 

To improve reproducibility, we provide the prompt-ing template used to query the error simulator 

Sϕ. We use a two-message chat format: a sys-tem prompt that specifies the simulator role and output constraints, followed by a user prompt that injects the original context, ground-truth tool calls, and the model’s failed attempt. 

B Training Algorithm Details 

Algorithm 1 outlines the detailed execution flow of the F ISSION -GRPO framework. The process alternates between standard exploration (to main-tain general capability and mine errors) and fission-based updates (to learn specific recovery strate-gies). 

C Extended Case Study Analysis 

In this section, we provide a detailed breakdown of the case study referenced in Section 4.6. Figure 6 visualizes the trajectories of Qwen3-8B under three training conditions on a multi-turn file manipula-tion task (Sample ID: multi_turn_base_1 ). 

Scenario Overview. The user requests to verify the current directory, move a log.txt file into a new archive folder, and then search for a keyword within that file. The key challenge arises in Turn 2: mkdir archive fails (directory already exists), but cd workspace and mv log.txt succeed. This partial-success state requires careful tracking—in Turn 3, since the file was moved to archive , a direct grep will fail, requiring the agent to locate the file first. 10 Detailed Behavioral Comparison. 1. Qwen3-8B (Base): State Awareness Col-lapse. The Base model correctly issues the initial batch command [cd, mkdir, mv] . However, it fails to update its internal state to reflect that it is already inside workspace after the successful 

cd . When attempting to handle the mkdir error, it redundantly retries cd workspace , which fails (“No such directory” within the current directory). Confused by this feedback, it spirals into a loop of invalid operations, ultimately failing to realize the file was already moved. 

2. Qwen3-8B + GRPO: Latent State Mis-match & Hallucination. The GRPO model succeeds in Turn 2 (the file is moved), but fails to track the consequence—specifically, that 

log.txt is no longer in the current directory but in the archive subdirectory. This latent state mismatch surfaces in Turn 3: it first tries 

grep("log.txt") (fails), then attempts a heuris-tic guess grep("archive/log.txt") (also fails). Lacking a grounded fallback strategy, it resorts to hallucination, inventing a non-existent path param-eter for ls .

3. Qwen3-8B + F ISSION -GRPO: Active Diagnosis. Our model handles the Turn 2state transition correctly. More importantly, in Turn 3, when faced with the same “No such file” error, it demonstrates a superior recov-ery mechanism: instead of guessing, it deploys 

find(name="log.txt", path="workspace") to empirically verify the file’s location. Using the confirmed path, it performs a precise state update via cd(folder="archive") , then executes grep 

successfully. This confirms that F ISSION -GRPO learns to bridge state gaps through active diagnosis rather than relying on fragile internal memory or hallucinated corrections. 

Algorithm 1 Detailed Training Procedure of FISSION -GRPO 

Require: Policy πθ , Reference Policy πref , Error Simulator 

Sϕ

Require: Training dataset D

Require: Hyperparameters: Learning rate η, KL coefficient 

β, Clip ratio ϵ

Require: Group sizes: G (Exploration), G′ (Fission/Correc-tion) 

Require: Thresholds: Buffer trigger Btrig , Success score 

Rthresh = 1 .0

1: Initialize Corrective Sample Pool B ← ∅ ▷ Implemented as LIFO Stack 2: for iteration k = 1 , . . . , K do 

3: // S TAGE 1: S TANDARD EXPLORATION & M INING 

4: Sample batch of user queries x ∼ D 

5: Generate exploration group {τi}Gi=1 ∼ πθ (·| x)

6: Compute rewards for each trajectory: ri ←

Rcorr (τi) + Rfmt (τi)

7: Compute GRPO Advantages: 

8: μR ← 1

> G

P ri, σR ← Std (ri)

9: ˆAi ← ri−μR

> σR+ϵ

10: Update Policy (Standard): 

11: LGRPO ←

> 1
> G

PGi=1 

h

min( ρi ˆAi, clip (ρi, 1 ± ϵ) ˆAi) − βDKL (πθ ∥πref )

i

12: θ ← θ + η∇θ LGRPO 

13: // S TAGE 2: S YNTHESIS & A CCUMULATION 

14: Identify error set E = {τi | Rcorr (τi) < R thresh ∨

Rfmt (τi) = 0 }

15: for each error trajectory τerr ∈ E do 

16: if Rfmt (τerr ) == 0 then 

17: f ← GetFormatError (τerr )

18: else 

19: f ← S ϕ(x, τ err ) ▷ Generate diagnostic feedback 20: end if 

21: Construct corrective context xcorr ← [x; τerr ; f ]

22: Compute Deduplication Key k ←

Hash (x, τ err , f )

23: if k / ∈ Keys (B) then 

24: Push (xcorr ) → B ▷ LIFO Push 25: end if 

26: end for 

27: // S TAGE 3: F ISSION -B ASED REMEDIAL UPDATE 

28: if |B| ≥ Btrig then 

29: Xbatch ← Pop (Btrig ) items from top of B ▷ LIFO: Fetch freshest errors 30: Initialize batch loss Ltotal ← 0

31: for each corrective context xcorr ∈ Xbatch do 

32: Fission Resampling: 

33: Generate recovery group {τ ′ 

> j

}G′ 

> j=1

∼

πθ (·| xcorr )

34: Compute rewards {r′ 

> j

} for recovery at-tempts 35: Compute Corrective Advantages: 

36: μ′ 

> R

← 1

> G′

P r′ 

> j

, σ′ 

> R

← Std (r′ 

> j

)

37: ˆA′ 

> j

← r′  

> j−μ′
> R
> σ′
> R+ϵ

▷ Variance restored via Fission 38: Accumulate Gradients: 

39: Lcorr ←

> 1
> G′

PG′

> j=1

h

min( ρ′ 

> j

ˆA′ 

> j

, . . . ) − βDKL 

i

40: Ltotal ← L total + Lcorr 

41: end for 

42: θ ← θ + η∇θ Ltotal ▷ Apply corrective update 43: end if 

44: end for 

11 Prompt Templates 

Prompt 1: System prompt for querying the error simulator. 

You are a Runtime Environment Simulator for an AI Agent. Your role is to act as the API Server or Operating System that executes tool calls. IMPORTANT CONTEXT: You are receiving a tool call from an Agent that has ALREADY FAILED validation or logic checks against the Ground Truth. Your task is NOT to judge correctness. Your task is to generate the specific ERROR MESSAGE that the system would return to the Agent. GOAL: Generate a short, realistic, and actionable error message (starting with "ERROR: ") that will help the Agent understand why its call failed compared to the expected Ground Truth. PRIORITY ERROR CATEGORIES: - Dependency & Sequence Violations - Parameter Hallucination - Schema & Parameter Errors - Business Logic Errors EVALUATION LOGIC: - Reference the Ground Truth: ground_truth is usually correct and serves as the primary standard. - Verify Context: cross-check the Agent output against the User Request in the Original Context. - Ambiguity Rule: if the Agent output differs from ground_truth but is still plausible, note missing validation. OUTPUT RULES: - Start with "ERROR: " (case-sensitive) - Be specific: mention actual parameter names/values from the failed attempt - Sound like a system/API response - Keep it concise (1--2 sentences) - Return ONLY the error string (no JSON, no markdown, no extra explanation) ERROR MESSAGE EXAMPLES (Real error style): <<ERROR_EXAMPLES_SNIPPET>> CRITICAL REMINDERS: - Do NOT output JSON like {"error": "..."}; output plain text only - Do NOT add any preamble; only the "ERROR: ..." line - Do NOT fabricate placeholders unless they appear in the failed attempt - The error should be what the runtime system returns, not an analysis 

Prompt 2: User prompt template (simulation input). 

## Simulation Task The Agent attempted to execute a tool call, but it was INCORRECT compared to the Ground Truth. Generate the system error message triggered by the Agent 's specific mistake. 1) Original Context (what the Agent saw) [System instructions & tools] <<SYSTEM_AND_TOOLS>> [Dialogue history before this attempt (non-system turns)] <<DIALOGUE_HISTORY>> 2) Execution Comparison [Ground-truth tool call(s)] <<GROUND_TRUTH_TOOL_CALLS>> [Failed tool call(s) extracted from the model output] <<FAILED_TOOL_CALLS>> 3) Instruction Compare the failed attempt against the ground truth under the given context. Identify the first critical failure and generate the runtime error. Output: Return ONLY one error string starting with "ERROR:". 

Figure 5: Two-message prompting format used to query the error simulator Sϕ.

12 ASSISTANT : [ cd(folder="workspace"), mkdir 

(dir_name ="archive"), mv(source="log .txt", 

destination="archive") ]

ASSISTANT :[cd(folder="workspace"),mv(source= 

"log .txt", destination="archive")] 

ASSISTANT :[ls(),mv(source=" log .txt",destination 

="archive")] 

USER: I am alex . Check if the current directory 

is under my name and list all the visible and 

hidden contents in the current directory now, 

please. 

ASSISTANT: [pwd (), ls(a=True)] 

TOOL :{" current_working_directory ":"/ ale 

x"} 

ASSISTANT: The current working directory is 

"/ alex ", which is under your name " alex ". The 

visible and hidden contents in the current 

directory are listed as ["workspace"] 

TURN 1 

TURN 2 ：FAILURE TRIGGER 

USER: Go to workspace directory and move one 

of the 'log.txt' files into a new directory 'archive'. 

TOOL :{" current_directory_content ":

["workspace"]} 

TOOL: {" current_working_directory ": 

"workspace"} 

TOOL: {"error": " mkdir : cannot create 

directory 'archive': File exists"} 

TOOL: {"result": "'log.txt' moved to 

'archive / log.txt '"} 

TOOL: {"error": "cd: workspace: No such 

directory. You cannot use path to change 

directory."} 

TOOL: {"error": "mv: cannot move 

'log.txt': No such file or directory"} 

TOOL: {" current_directory_content ": 

["archive"]} 

TOOL: {"error": "mv: cannot move 

'log.txt': No such file or directory"} 

ASSISTANT : {" error" :"log .txt not found in the 

current directory ."} 

ASSISTANT : [grep( file_name ="log .txt", 

pattern="Error")] 

TURN 3 : GIVE UP 

USER: Investigate within 'log.txt' for the 

occurrence of the keyword 'Error'. 

TOOL: {"error": "grep: log.txt: No such 

file or directory "} 

ASSISTANT :{"error" : "log .txt not found in the 

current directory . Please verify the file name and 

ensure it exists ."} 

ASSISTANT :[cd(folder="workspace"), mkdir (dir_ 

name ="archive"), mv(destination="archive", 

source="log .txt")] 

USER: I am alex . Check if the current directory is 

under my name and list all the visible and 

hidden contents in the current directory now, 

please. 

ASSISTANT: [ls(a=true)] 

TOOL :{" current_directory_content ":["wor 

kspace"]} 

ASSISTANT: [pwd ()] 

TURN 1 

TURN 2 

USER: Go to workspace directory and move one 

of the 'log.txt' files into a new directory 'archive'. 

TOOL: {" current_working_directory":"wor 

kspace "} 

TOOL: {"error": " mkdir : cannot create 

directory 'archive': File exists"} 

TOOL: {"result": "'log.txt' moved to 

'archive /log. txt'"} 

TOOL :{" current_working_directory ":

"/ alex "} 

ASSISTANT :[grep( file_name =“ log .txt ”, 

pattern="Error")] 

TURN 3 : HALLUCINATION 

USER: Investigate within 'log.txt' for the 

occurrence of the keyword 'Error'. 

TOOL: {"error": "grep: log.txt: No such file 

or directory"} 

ASSISTANT :[grep( file_name ="archive/ log .txt",pa 

ttern ="Error")] 

TOOL: {"error": "grep: archive/log.txt: No 

such file or directory"} 

ASSISTANT : [ls(a=false, path="archive")] 

TOOL: Error during execution: 

GorillaFileSystem.ls() got an unexpected 

keyword argument ‘path’ 

ASSISTANT :[cd(folder="workspace")] 

USER: I am alex . Check if the current directory is 

under my name and list all the visible and 

hidden contents in the current directory now, 

please. 

ASSISTANT: [ls(a= true)] 

TOOL :{" current_directory_content ":["wor 

kspace"]} 

TURN 1 

TURN 2 

USER: Go to workspace directory and move one 

of the 'log.txt' files into a new directory 'archive'. 

TOOL: {" current_working_directory":"wor 

kspace "} 

ASSISTANT :[grep( file_name =“ log .txt ”, 

pattern="Error")] 

TURN 3 : ERROR RECOVERY 

USER: Investigate within 'log.txt' for the 

occurrence of the keyword 'Error'. 

TOOL: {"error": "grep: log.txt: No such file 

or directory"} 

ASSISTANT :[find(name="log .txt", 

path="workspace")] 

TOOL: {"matches":["workspace/archive/ lo 

g.txt "]} 

ASSISTANT : [cd(folder="archive")] 

TOOL: {" current_working_directory ": 

"archive"} 

ASSISTANT : [mkdir (dir_name ="archive")] 

TOOL: {"error": " mkdir : cannot create 

directory 'archive': File exists"} 

ASSISTANT : [mv(destination="archive", 

source="log .txt")] 

TOOL: {"result": "'log.txt' moved to 

'archive/log. txt'"} 

ASSISTANT : [grep( file_name ="log .txt", 

pattern="Error")] 

TOOL: {" matching_lines ": ["This is a log 

file. No errors found. Another line. Yet 

another line. Error: Something went 

wrong. Final line."] 

Qwen3 -8B 

Collapse: Loop & Context Loss 

Qwen3 -8B + GRPO 

Partial: Hallucination 

Qwen3 -8B + Fission -GRPO 

Success: Robust Recovery Figure 6: Detailed visualization of Multi-turn Error Recovery. Comparisons of trajectories generated by Qwen3-8B under different training regimes. The Base model collapses due to immediate state loss; the GRPO model suffers from latent state mismatch leading to hallucination in later turns; F ISSION -GRPO overcomes this by employing diagnostic tools ( find ) to actively resolve state uncertainties. 

13