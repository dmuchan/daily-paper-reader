# Decoupling Return-to-Go for Efficient Decision Transformer
<<<<<<< HEAD
# 解耦待获得回报以实现高效的决策 Transformer
=======
# 解耦待得回报以实现高效的决策 Transformer
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002

**Authors**: Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li, Qirui Zheng, Xionghui Yang, Wenxin Li
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15953v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span>
**Score**: 8.0
<<<<<<< HEAD
**Evidence**: Proposes an efficient architecture for offline reinforcement learning by decoupling return-to-go
=======
**Evidence**: Improves Decision Transformer efficiency for offline reinforcement learning
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002

---

## Abstract
The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

## 摘要
<<<<<<< HEAD
决策 Transformer (DT) 为离线强化学习建立了一种强大的序列建模方法。它将动作预测以待获得回报 (RTG) 为条件，在训练期间利用其区分轨迹质量，并在推理时指导动作生成。在这项工作中，我们发现了该设计中的一个关键冗余：从理论上讲，将整个 RTG 序列输入 Transformer 是不必要的，因为只有最近的 RTG 会影响动作预测
=======
决策 Transformer (DT) 为离线强化学习建立了一种强大的序列
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002

---

## 论文详细总结（自动生成）

<<<<<<< HEAD
这篇论文由北京大学团队提出，旨在解决决策 Transformer（Decision Transformer, DT）在离线强化学习中的冗余设计问题。以下是对该论文的深度结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **核心问题**：传统的 DT 将“待获得回报”（Return-to-Go, RTG）、观测（Observation）和动作（Action）作为等地位的 Token 全部输入 Transformer。作者指出，**输入完整的 RTG 历史序列在理论上是冗余的**。
*   **研究动机**：
    *   **理论层面**：在部分可观测马尔可夫决策过程（POMDP）中，决策仅依赖于当前信念状态（由历史观测和动作构建）和未来的目标（当前的 RTG）。历史 RTG 之间的差值仅代表已获得的奖励，对预测未来动作没有额外帮助。
    *   **性能层面**：这种冗余不仅增加了计算开销（Transformer 的计算量随序列长度平方增长），还可能干扰注意力机制的分配，导致性能下降。

### 2. 论文提出的方法论：DDT (Decoupled DT)
*   **核心思想**：将 RTG 从 Transformer 的输入序列中**解耦**出来。Transformer 仅处理观测和动作序列，而当前的 RTG 则作为一种“调制信号”直接作用于输出层。
*   **关键技术细节**：
    *   **输入简化**：输入序列从 $(R_1, o_1, a_1, \dots, R_t, o_t)$ 缩减为 $(o_1, a_1, \dots, o_t)$，序列长度从 $3k$ 减少到 $2k$。
    *   **自适应层归一化 (adaLN)**：借鉴了 Diffusion Transformer (DiT) 的设计。使用一个简单的 MLP 将当前的 RTG $\hat{R}_t$ 映射为缩放参数 $\gamma$ 和偏移参数 $\beta$。
    *   **动作预测流程**：Transformer 输出观测 $o_t$ 对应的隐藏状态后，通过 adaLN 注入 RTG 信息，再经过线性解码器预测动作 $a_t$。
*   **算法流程**：
    1. 将观测和动作序列进行线性嵌入并加入位置编码。
    2. 通过 GPT 骨干网络提取特征。
    3. 提取最后一个观测的隐藏状态。
    4. **注入条件**：使用当前 RTG 通过 adaLN 对该隐藏状态进行调制。
    5. 输出预测动作。

### 3. 实验设计
*   **数据集/场景**：
    *   **D4RL 标准基准**：包含 Hopper, Walker2d, HalfCheetah 三个环境，涵盖 Medium, Medium-Replay, Medium-Expert 三种数据质量。
    *   **离散/随机环境**：2048 游戏（具有高随机性和稀疏奖励特点）。
*   **对比方法 (Baselines)**：
    *   **经典离线 RL**：CQL, IQL, TD3+BC, BRAC-v。
    *   **DT 及其变体**：原始 DT, VDT (Value-guided DT), LSDT (Long-short DT)。

### 4. 资源与算力
*   **算力说明**：论文**未明确列出**具体的 GPU 型号、数量或总训练时长。
*   **效率提升**：文中强调了由于输入序列长度减少了 1/3，基于 Transformer 的二次方复杂度特性，DDT 在推理和训练时的计算开销和内存占用均显著低于原始 DT。

### 5. 实验数量与充分性
*   **实验规模**：
    *   在 9 个 D4RL 任务上进行了对比实验，每个任务运行了 4 个不同随机种子，每个种子进行 100 次评估。
    *   进行了 **2048 游戏** 的扩展实验，验证了在离散和稀疏奖励场景下的泛化性。
*   **消融实验**：
    *   **Blocked-DT**：仅通过注意力掩码屏蔽历史 RTG（不使用 adaLN），证明了 adaLN 注入信息的有效性。
    *   **
=======
这篇论文由北京大学的研究团队提出，旨在解决决策 Transformer（Decision Transformer, DT）在处理序列信息时的冗余问题。以下是对该论文的结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **研究背景**：离线强化学习（Offline RL）旨在从静态数据集中学习策略。决策 Transformer（DT）将 RL 建模为序列预测任务，通过输入“待得回报（Return-to-Go, RTG）、状态、动作”序列来预测下一步动作。
*   **核心问题**：标准的 DT 将**整个历史 RTG 序列**输入 Transformer。作者指出这在理论上是冗余的：在马尔可夫决策过程（MDP）或部分观测马尔可夫决策过程（POMDP）中，历史 RTG 的变化仅反映了已获得的奖励，而这些奖励对于预测未来动作并无额外帮助，只有**当前的 RTG** 才是引导未来行为的关键。
*   **研究动机**：这种冗余不仅增加了计算开销（序列变长），还可能干扰模型的注意力分配，降低学习效率和性能。

### 2. 论文提出的方法论：DDT (Decoupled DT)
*   **核心思想**：将 RTG 从 Transformer 的输入序列中解耦。Transformer 仅处理状态（Observation）和动作（Action）序列，而将当前的 RTG 作为一种“调制信号”直接作用于输出层。
*   **关键技术细节**：
    *   **输入简化**：输入序列从 $(R_t, s_t, a_t)$ 简化为 $(s_t, a_t)$，序列长度从 $3k$ 缩减为 $2k$（$k$ 为上下文窗口长度）。
    *   **自适应层归一化 (adaLN)**：借鉴了 Diffusion Transformer (DiT) 的设计。Transformer 输出的隐藏状态在进入动作预测头（MLP）之前，通过 adaLN 模块进行处理。
    *   **调制公式**：使用当前步的 RTG ($\hat{R}_t$) 通过一个线性层生成缩放参数 $\gamma$ 和偏移参数 $\beta$，对隐藏状态进行动态调整：$adaLN(x, \hat{R}_t) = \gamma(\hat{R}_t) \odot \text{LayerNorm}(x) + \beta(\hat{R}_t)$。
*   **算法流程**：1. 嵌入状态和动作；2. 经过 Causal Transformer 提取特征；3. 提取最后一步的隐藏状态；4. 使用当前 RTG 通过 adaLN 进行调制；5. 预测动作。

### 3. 实验设计
*   **数据集/场景**：
    *   **D4RL 标准基准**：包括 Gym-MuJoCo 任务（Hopper, Walker2d, HalfCheetah），涵盖 Medium, Medium-Replay, Medium-Expert 三种数据质量。
    *   **2048 游戏**：用于验证在离散动作空间、高随机性和稀疏奖励场景下的泛化能力。
*   **Benchmark 与对比方法**：
    *   **传统离线 RL**：BRAC-v, TD3+BC, IQL, CQL。
    *   **DT 变体**：原始 DT, VDT (Value-guided DT), LSDT (Long-short DT)。
    *   **消融变体**：Blocked-DT（仅通过注意力掩码屏蔽历史 RTG，但不改变架构）。

### 4. 资源与算力
*   **算力说明**：论文**未明确列出**具体的 GPU 型号、数量或总训练时长。
*   **效率提升**：文中提到由于输入序列长度减少了 1/3，且 Transformer 的计算复杂度随序列长度呈平方级增长，因此 DDT 在推理和训练时的计算开销显著低于原始 DT。

### 5. 实验数量与充分性
*   **实验规模**：在 9 个 D4RL 任务上进行了对比实验，每个任务运行了 4 个不同随机种子，每个种子进行 100 次评估。
*   **消融实验**：
    *   对比了 adaLN 与简单掩码（Masking）的效果。
    *   测试了 adaLN 模块的深度（单层 vs 多层 MLP）对性能的影响。
    *   通过注意力图（Attention Map）可视化分析了模型内部的依赖关系。
*   **充分性评价**：实验设计较为全面，不仅覆盖了连续控制任务，还扩展到了离散随机游戏，且通过可视化手段解释了性能提升的原因，论证比较客观。

### 6. 主要结论与发现
*   **性能超越**：DDT 在大多数 D4RL 任务上显著优于原始 DT，并达到了与 VDT、LSDT 等复杂变体持平甚至更优的 SOTA 水平。
*   **注意力优化**：可视化显示，DDT 的注意力分配更集中在对角线附近（即更关注近期状态），这符合马尔可夫特性，说明去除冗余 RTG 有助于模型聚焦关键信息。
*   **架构简洁性**：简单的单层 adaLN 效果最好，增加复杂度反而可能导致性能下降。
*   **计算高效**：在提升性能的同时，降低了内存占用和推理延迟。

### 7. 优点（亮点）
*   **理论深刻**：通过公式推导揭示了 DT 长期以来被忽视的 RTG 序列冗余问题，具有很强的启发性。
*   **大道至简**：相比于其他通过引入复杂 Q 函数或辅助任务改进 DT 的方法，DDT 仅通过解耦和 adaLN 就实现了性能飞跃，易于实现和部署。
*   **解释性强
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
