Title: PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models

URL Source: https://arxiv.org/pdf/2601.16007v1

Published Time: Fri, 23 Jan 2026 01:59:43 GMT

Number of Pages: 40

Markdown Content:
# PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models 

## Chak-Wing Mak *,1 , Guanyu Zhu *,1 , Boyi Zhang *,1 , Hongji Li 2, Xiaowei Chi 1, Kevin Zhang 1,

## Yichen Wu 3, Yangfan He 4, Chun-Kai Fan 1, Wentao Lu 5, Kuangzhi Ge 1, Xinyu Fang 1, Hongyang He 6, Kuan Lu 7, Tianxiang Xu 1, Li Zhang 5,8 , Yongxin Ni 9, Youhua Li 10 , Shanghang Zhang †,1 

> 1

Peking University 2Mohamed bin Zayed University of Artificial Intelligence 3National University of Singapore 4University of North Carolina at Chapel Hill 5University of Science and Technology of China 6Manifold.AI 7Cornell University 8Hong Kong Polytechnic University 9National University of Singapore 10 City University of Hong Kong 

> *

Equal contribution. †Corresponding author. 

## Abstract 

Modern foundational Multimodal Large Language Mod-els (MLLMs) and video world models have advanced sig-nificantly in mathematical, common-sense, and visual rea-soning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer tem-plates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind ,a unified benchmark with both real and simulation environ-ments that evaluates law-consistent reasoning and genera-tion over three canonical principles: Center of Mass, Lever Equilibrium, and Newton’s First Law. PhysicsMind com-prises two main tasks: i) VQA tasks, testing whether mod-els can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating ba-sic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical under-standing, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be re-leased upon acceptance. 

## 1. Introduction 

Recent multimodal foundation models and video genera-tion models (often used as world models in recent works) Center     

> of Mass
> Varying Object Shapes and Suspension Points
> Lever
> Equilibrium
> Varying Initial Setups and Object Types
> Inertia
> Varying Pulling Speeds and Object Masses
> String hung at
> arrow ’s front
> String hung at
> arrow ’s back
> String hung on
> random shape
> Paper pulled quickly Paper pulled slowly Paper pulled quickly
> Objects with weight Humans with weight unknown weight

Figure 1. Three canonical mechanics scenarios in PhysicsMind: Center of Mass, lever equilibrium, and Newton’s first law, each realized with various real tabletop and simulated configurations. 

have made rapid progress in mathematical reasoning, com-monsense inference, and generic multimodal understand-ing [1, 33, 40], but their physical understanding and pre-diction of real-world dynamics are often limited [11, 30]. Compared to symbolic or linguistic tasks, such capabilities require models to not only reason about abstract concepts such as force, mass, momentum, and inertia, but also to predict how these quantities manifest in visually observable and temporally coherent motion. These laws of mechanics govern how objects move, interact, or remain stable, and are therefore central to perception, prediction, and embod-ied decision making in both artificial and biological agents, thus making it an important quality to measure across cur-rent Visual-Language Models (VLMs) and Video Genera-tion Models (VGMs). Prior works on the physical understanding of gen-erative models fall primarily into two large categories. The first line studies VLMs on physical question an-

> arXiv:2601.16007v1 [cs.CV] 22 Jan 2026

swering and visual reasoning, such as IntPhys [37], CRAFT [5], CausalVQA [15], MVP [26], PhysBench [11], and PhysUniBench [42]. These benchmarks provide short videos or images paired with template questions about col-lisions, stability, and causal relations. While they are valu-able for probing whether models can answer physics ques-tions, the visual scenarios are often synthetic, with regular question formats highly regular, and plain evaluation meth-ods, allowing methods to exploit linguistic or visual short-cuts without interpreting and reasoning about mechanical interactions. The second category evaluates VGMs and World Mod-els on their abilities to produce physically plausible dynam-ics. Recent datasets and benchmarks such as VBench [23], PhyGenBench [29], Physics-IQ [30], MORPHEUS [47], WorldModelBench [27], and WorldScore [14] are widely used to test large diffusion models such as Sora, Veo, etc. However, these methods suffer from simple evaluation pro-tocols focusing on pixel-level reconstruction quality, mo-tion smoothness, or human-judged visual plausibility. In practice, visually impressive sequences can still violate ba-sic mechanics, exhibit unstable or energy non-conserving motion, or hallucinate physically impossible trajectories. To move beyond this fragmented landscape, we focus on three canonical mechanics problems, Center of Mass, Lever Equilibrium, and Newton’s First Law, which capture com-plementary aspects of physical understanding while remain-ing simple enough for both controlled tabletop experiments and 2D simulations. Building on this structure, we design PhysicsMind, a unified benchmark that bridges physical reasoning and physical prediction with two complementary tasks. The first is video question answering (VQA), where VLMs receive images or short clips together with multiple-choice questions about the end state, direction of rotation, or stability of the system. This task probes whether mod-els can extract relevant physical quantities from visual in-put and apply the appropriate law to reach a discrete con-clusion. The second is video generation, where state-of-the-art video generators are conditioned on an initial state and produce future trajectories. In this work, we evaluate whether generated motions follow the same center-of-mass, lever, and inertia laws that govern the ground-truth exper-iments. This design allows us to explore two main topics. First, how well do real-looking generated videos abide by physical laws, and second, if models that appear compe-tent in simulation environments generalize well to the same principles under real-world noise, dynamic differences, and visual variability. We perform extensive evaluations of state-of-the-art VLMs and video generators. Despite strong perceptual ca-pabilities and high visual fidelity, current baselines show consistent gaps with basic physics: in VQA tasks, cur-rent VLMs rely on appearance heuristics and struggle with counterfactual and logically related questions, while in video generation VGMs often violate center-of-mass con-straints, mispredict lever outcomes, or ignore inertia, indi-cating that current scaling and training strategies have not yielded robust physical understanding or faithful prediction. Our main contributions are as follows. 1. We introduce PhysicsMind, a unified physics benchmark that evaluates both reasoning and prediction under the same laws in simulated and real-world settings. We fo-cus on 3 mechanics scenarios, including Center of Mass, Lever Equilibrium, and Newton’s First Law. 2. We design physics-aware evaluation protocols that go beyond overall accuracy or visual quality: for VQA, law-specific subtasks (e.g., position vs. rotation, equilibrium vs. adjustment), and for video generation, metrics for center-of-mass alignment, lever final-state correctness, and trajectory, speed, and acceleration consistency. 3. We conduct a systematic study of modern VLMs and video generators on PhysicsMind, revealing consistent failure modes such as reliance on appearance cues, weak causal reasoning, and frequent violations of basic me-chanics, thereby outlining concrete targets for future physics-aware multimodal modeling. 

## 2. Related Work 

Physical reasoning benchmarks. Physical reasoning benchmarks aim to test whether a model can infer and apply basic physical principles from visual input. Early work relied on synthetic, highly controlled environments. CLEVR [25] targets compositional reasoning in rendered scenes, while IntPhys [37], CRAFT [5], and PHYRE [6] probe intuitive physics and causal interactions in simulators. More recent datasets, such as CausalVQA [15], MVP [26], PhysBench [11], PhysUniBench [42], and Physics-IQ [30] move toward richer video question answering with more re-alistic footage. However, most of these benchmarks use templated question formats, focus on narrow sets of sce-narios, and report aggregate accuracy without explicitly ty-ing evaluation to concrete mechanics laws or to predictive behavior. PAC Bench [18] systematically evaluates VLMs on their understanding of physical properties, affordances, and constraints for robot manipulation. Recent work has also explored why spatial reasoning remains challenging for VLMs from an attention mechanism perspective [8]. 

Video generation and world models. World models [1, 9, 10, 33, 48] and video generators learn predictive dynamics from streams of observations.Classical world models such as PlaNet [20] and the Dreamer family [21] are typically evaluated through downstream control returns rather than explicit tests of physical consistency. More recent bench-marks for generative dynamics, including VBench [23], PhyGenBench [29], Physics-IQ [30], MORPHEUS [47], WorldModelBench [27], WorldScore [14], WorldPredic-Table 1. Comparison of physics-oriented VQA benchmarks. Columns summarize dataset realism, task modality, annotation format, and the scope of physical evaluation; PhysicsMind adds law-targeted cross-checks on shared real and simulated setups.                                                         

> Benchmark Models Realism Task Modal Answer Type Video Length Max Objects Eval Aspects CrossCheck IntPhys [37] 2Sim Dynamic Score 7s 33No CRAFT [5] 9Sim Dynamic Choice 10s 6+ 3No CausalVQA [15] 6Real Dynamic Choice 24s 35No MVP [26] 13 Real Dynamic Choice 9s 6+ 5No PhysBench [11] 39 Real+Sim Dynamic+Static Choice 8 frames 54Yes PhysicsMind (Ours) 22 Real+Sim Dynamic+Static Choice 4–11s 6+ 6Yes

Table 2. Comparison of video generation and world-model benchmarks. The table contrasts realism, supervision, and evaluation scope; PhysicsMind provides paired real–sim trajectories and law-aware cross-checks for mechanics-focused video prediction.                                                                 

> Benchmark Realism Ground-Truth Video Video Length (s) Input Modalities Max Objects Video Dim Eval Aspects CrossCheck WorldModelBench [27] Sim No 2–5 Text/Image 137No MORPHEUS [47] Real No 2–4 Image/Video 233No WorldScore [14] Real yes 2–3 Image 6+ 310 No PhyGenBench [29] Sim No 3–11 Text 134No VBench [23] Sim No 2Text 3316 No Physics-IQ [30] Real+Sim yes 8Image/Video 334No PhysicsMind (Ours) Real+Sim Yes 5–10 Image/Video 6+ 2/3 8Yes

tion [7], and AutumnBench [43], assess visual fidelity, motion smoothness, and human preference for sequences produced by text- or image-conditioned generators.These metrics provide useful signals for perceptual quality but only indirectly reflect whether trajectories respect center-of-mass balance, lever behavior, or inertial motion, and they are rarely aligned with law-targeted reasoning tasks on the same physical scenarios. He et al. [22] reviewed the inter-section of LLMs with multimodal generation and editing. Motamed et al. [30] investigated whether generative video models truly understand physical principles, revealing that visual realism does not imply physical understanding. Lin et al. [28] systematically reviewed the evolution of physical cognition in video generation, highlighting the gap between visual realism and physical consistency. 

Position. Large multimodal foundation models[4, 12, 31, 40] and modern video generators[16, 33, 41] have greatly advanced visual recognition, language-conditioned reason-ing, and open-domain synthesis, yet they still often misin-terpret basic mechanics and produce physically implausible motion, as is shown in Table 1 and 2. PhysicsMind makes this gap explicit by evaluating law-targeted VQA and video prediction on shared center-of-mass, lever, and inertia sce-narios with paired real–simulation data and metrics that di-rectly test adherence to physical laws. 

## 3. The PhysicsMind Benchmark 

3.1. Problem Formulation 

PhysicsMind formulates the evaluation of physical com-monsense reasoning in foundation models and world mod-els as a unified understanding task that spans both VQA and VG. The objective is to assess whether a model can both vi-sualize and reason about fundamental physical phenomena with scientific validity and adherence to physical laws. The evaluation of physical reasoning in both foundation and world models can be formalized within a unified frame-work that captures their shared structure and distinct modal-ities. For foundation models, the problem is defined as fol-lows: given an initial frame f and a question q, the model produces a response ˆa = fθ (f, q ), which is then evalu-ated by comparing ˆa with the ground truth a∗. For world models, the formulation extends to temporal data: given an input video sequence x = ( x1, x 2, . . . , x T ) and a corre-sponding physical query q, the model generates a response 

ˆa = fθ (x, q ). Evaluation in this case considers both the fi-nal output and the transitional states through an evaluation function E(ˆ a, a ∗), which measures the presence and cor-rectness of physical commonsense, where a∗ denotes the ground-truth or physically consistent outcome. 

3.2. Overview of PhysicsMind 

PhysicsMind evaluates the physical reasoning capabilities of both foundation models and world models. The bench-mark introduces two complementary tasks, Visual Ques-tion Answering (VQA) and Video Generation Assessment, shown in the left region of Figure 2. As illustrated in Fig-ures 1, PhysicsMind covers three physics scenarios: Cen-ter of Mass, Lever Equilibrium, and Newton’s First Law. These three physical domains are intentionally selected to represent orthogonal dimensions of physical reasoning: ge-ometry (Center of Mass), causality (Lever Equilibrium), and dynamics (Newton’s First Law). This triad collectively PhysicsMind            

> Dynamic & Controlled
> Conditions
> Varying Object Shapes
> Varying Masses
> Varying Positions
> Varying Setup
> Quantitative
> Reasoning
> Unified Model
> Evaluation
> Fine‑grained
> Temporal Metrics
> Physics‑Aware
> Metrics
> Final State Accuracy
> Seg. Mask IoU +
> Center Diff
> Velocity Accel.
> Similarity

PhysicsMind Diverse Scenarios 

> Center of Mass
> Lever Equilibrium
> Newton ’s First Law
> Real + Sim

Prob lem & Goal 

Input 

Video Query +

Foundational 

Model 

> Video generation
> prediction
> VQA
> understanding

Dataset Construction 

> Physics
> TextBook
> Concept
> Formulation
> Variable
> Selection
> Real-World
> Rec or ding
> Simulator
> Raw Data

Dataset Annotation & Quality Control  

> Raw Data Questions
> Designed
> by Experts
> Answers
> generated
> by LLM
> Expert
> Review
> Alignment
> Clarity
> Physics
> Validity
> Accuracy

World 

Model Spatial Alignment 

> Directional
> Consistency

Figure 2. Overview of the PhysicsMind framework. It combines a foundational model with physics-guided dataset construction, expert-verified annotations, and diverse controlled scenarios to enable robust video understanding and physics-aware evaluation. Center of Mass 

Segmentation Mask 

IoU 

Lever Equilibrium 

Final State Accuracy 

Final Frame GT 

Final Frame Pred 

VLM 

Same 

Not Same 

Inertia 

Trajectory 

RMSE 

Directional 

Consistency 

Final Position 

Error 

Speed 

Similarity 

Acceleration 

Similarity 

> Sgt
> Spred

Center Difference of 

Segmentation Mask 

L2            

> Cgt Cpred
> ��������=�
> ��
> || ��
> ����−��
> ��|| �

��� = || ��� ���   

> ����

− ��� ��� 

> ��

|| �                                 

> ����=���−�
> ���
> ����
> =�
> �−��=�
> �
> ���
> �∙�����
> �
> || ���
> �|||| �����
> �||
> ����
> =�
> �−��=�
> �
> ���
> �∙�����
> �
> || ���
> �|||| �����
> �||

� �� = |�풑풓   �품  |

|�풑풓   �품  | �� �� = || �풑풓   − �품  || �

Figure 3. Physics-Aware evaluation metrics for Video Generation Models (VGM). Inertia metrics assess motion and trajectory consistency, Center-of-Mass metrics measure segmentation alignment, and Lever-Equilibrium evaluates final-state agreement. 

bridges the spectrum from static perception to mechanical interaction to temporal prediction, allowing PhysicsMind to probe both intuitive and formal physical understand-ing in foundation models. Each scenario is implemented through both real-world recordings and two-dimensional (2D) physics simulations, enabling systematic comparisons between natural and controlled physical conditions. The dataset comprises short, high-quality video clips with a maximum resolution of 3840 × 2160, recorded at 60 FPS and lasting up to 10 seconds. Real-world samples are captured in controlled indoor environments with fixed camera positions to ensure consistent framing and illumi-nation. Simulated samples are generated using a determin-istic 2D physics engine, allowing fine-grained control over object attributes and motion reproducibility. Across all sce-narios, the dataset incorporates diverse configurations that vary in initial setups, object shapes, masses, and conditions. 

Dataset Construction. As shown in the top-middle region of Figure 2, the PhysicsMind dataset is developed through a structured pipeline consisting of conceptualization, exper-imental design, and data generation. The process begins with identifying visually interpretable and experimentally verifiable laws from a high school physics textbook. These principles are translated into controlled physical setups, and a systematic variable selection process ensures diversity and control by varying conditions. To ensure data diversity, both simulated and real-world recordings are used. Simulated data is generated with a deterministic 2D physics engine with precise control, while real-world recordings are pro-duced under fixed camera viewpoints and consistent light-ing for stable visual conditions. This dual-source design en-sures reproducibility in simulated settings while preserving natural variability in real experiments. 

Data Annotation and Quality Control. PhysicsMind em-ploys a multi-stage annotation and verification workflow (shown in the bottom-middle region of Figure 2). Expert curators first design physics-based questions and answer templates, which are automatically populated using large language models and then manually refined. Each video is paired with its physical descriptors, annotated variables, and validated VQA entries. A series of manual and au-tomated consistency checks follows to confirm alignment, clarity, and physical correctness. Videos that exhibit am-biguity, instability, or deviations from expected dynamics are systematically excluded. This rigorous curation process yields a dataset that integrates controlled precision with re-alistic diversity, forming a reliable foundation for evaluating physical reasoning in both foundation and world models. 

3.3. Evaluation Metrics 

Video Question Answering. The core metric is binary exact-match accuracy with respect to the ground-truth op-tion. To obtain finer diagnostics, we further report: • Category-wise accuracy , split by law-specific subtypes (e.g., equilibrium vs. adjustment for levers, position vs. rotation for Center of Mass, position vs. stability for iner-tia), to distinguish different reasoning skills. • Experiment-wise accuracy , aggregated over physical se-tups and configurations, to reveal which conditions are easier or harder for models. • Variant-wise accuracy , grouped by object type, mass pat-tern, or motion profile, to test robustness to appearance and parameter changes. 

Video Generation. To quantify the physical plausibil-ity of generated videos, we introduce a law-aware evalua-tion framework that decomposes performance by mechan-ics scenario (see Figure 3): • Center of Mass. We measure geometric fidelity via Intersection-over-Union (IoU) between predicted and ref-erence object masks, together with the distance between their mask centroids. • Lever Equilibrium. We compute final-state prediction ac-curacy, checking whether generated sequence converges to the correct lever outcome implied by torque balance. • Newton’s first law. We use a set of kinematic met-rics—trajectory deviation, velocity consistency, and ac-celeration fluctuation—to test whether objects maintain constant motion in the absence of external forces. Detailed formulations are provided in the Appendix. 

## 4. Experiments 

Our study is guided by the following research questions: • RQ1. Physical Reasoning in Perception: Can large multimodal foundation models reason about physical laws from static visual inputs? • RQ2. Physical Consistency in Generation: Do world or video generation models produce motion that remains consistent with core physical principles? • RQ3. Sim-to-Real Generalization: Does this physical common sense learned in simulation generalize to real-world visual data? 

4.1. Experimental Setup and Evaluation Protocol 

All models are evaluated on the three PhysicsMind do-mains, Center of Mass, Lever Equilibrium, and Newton’s First Law, under both perception (VQA) and generation set-tings. In VQA, a model receives an image and a multiple-choice physics question, and we report accuracy per do-main. In video generation, a model is conditioned on an initial frame and a textual description of the setup to pro-duce a short video, and we compute law-aware metrics that assess segmentation overlap and centroid alignment (Center of Mass), final lever state (equilibrium), and trajectory/ve-locity/acceleration consistency (Newton’s first law). Fur-ther implementation details and metric definitions are pro-vided in the Appendix. 

4.2. Evaluation of Physical Common Sense in VQA (RQ1 )

Table 3 summarizes the performance of multimodal founda-tion models on the PhysicsMind VQA benchmark, address-ing RQ1 : to what extent contemporary vision-language models exhibit physical common sense when reasoning about controlled visual scenarios governed by basic me-chanical principles. 

Overall Performance Trends. Across all models, accu-racy spans from 25% to 85%, revealing wide disparities in physical reasoning competence. Across all models, over-all accuracy ranges from roughly random guessing (25%) up to 85%, revealing large disparities in physical reasoning competence. No single model dominates across all three do-mains. GPT-5 achieves the best overall scores on Center of Mass and Lever Equilibrium (70% overall in both), whereas o4-min attains the highest Newton’s First Law accuracy (85%). This fragmentation suggests specialized rather than generalized physical understanding and is consistent with models relying partly on pattern familiarity instead of ro-bust, law-grounded inference. 

Domain-Specific Insights. Performance differs markedly by physical domain. Newton’s First Law questions obtain the highest mean accuracy (68.5% on average), likely be-Table 3. VQA Physics Evaluation Results across Models. The Acc columns report accuracy in percentage (%). Values after ± represent standard deviation across multiple runs ( n = 5 ). 

Model Center of Mass (VQA) Lever Equilibrium (VQA) Newton’s First Law (VQA) Position Rotation Overall Equilibrium Balance Adj. Overall Obj. Pos. Obj. Stability Overall Acc (%) ↑ Acc (%) ↑ Acc (%) ↑ Acc (%) ↑ Acc (%) ↑ Acc (%) ↑ Acc (%) ↑ Acc (%) ↑ Acc (%) ↑

Closed-source Reasoning Models 

GPT-5 [31] 60.00 ±2.1 80.00 ±1.8 70.00 ±1.5 66.67 ±2.4 85.71 ±1.5 76.19 ±1.3 60.00 ±2.8 95.00 ±1.0 77.50 ±1.8 

o4-min [32] 50.00 ±2.4 55.00 ±2.2 52.50 ±1.9 61.90 ±2.5 52.38 ±2.4 57.14 ±2.1 75.00 ±2.2 95.00 ±1.0 85.00 ±1.5 

GPT-4o [34] 40.00 ±2.4 35.00 ±2.1 37.50 ±1.9 42.86 ±2.5 66.67 ±2.1 54.76 ±1.9 45.00 ±2.8 85.00 ±1.6 65.00 ±2.1 

GPT-4-turbo [35] 25.00 ±2.2 80.00 ±1.8 47.50 ±2.0 28.57 ±2.3 57.14 ±2.2 42.86 ±2.0 40.00 ±2.5 80.00 ±1.8 60.00 ±2.0 

Claude-4.5-sonnet [4] 45.00 ±2.5 20.00 ±1.8 32.50 ±2.1 61.90 ±2.4 61.90 ±2.0 61.90 ±1.8 10.00 ±1.4 90.00 ±1.3 50.00 ±2.2 

Claude-3.7-sonnet [3] 30.00 ±2.3 25.00 ±2.0 27.50 ±1.9 52.38 ±2.4 66.67 ±2.0 59.52 ±1.9 40.00 ±2.5 65.00 ±2.1 52.50 ±2.0 

Gemini-2.5-pro [12] 30.00 ±2.2 70.00 ±1.9 50.00 ±1.8 57.14 ±2.3 76.19 ±1.7 66.67 ±1.6 20.00 ±1.8 45.00 ±2.2 32.50 ±1.9 

Closed-source Chat Models 

GPT-4o-mini [34] 10.00 ±1.5 20.00 ±1.8 15.00 ±1.6 23.81 ±2.1 38.10 ±2.2 30.95 ±1.8 10.00 ±1.4 80.00 ±1.8 45.00 ±2.1 

GPT-4.1 [35] 21.30 ±2.2 46.75 ±2.3 43.00 ±2.0 50.50 ±2.5 55.25 ±2.2 37.75 ±2.1 47.25 ±2.6 41.03 ±2.3 47.60 ±2.0 

Claude-3.5-sonnet [2] 25.00 ±2.2 20.00 ±1.8 22.50 ±1.9 19.05 ±2.0 52.38 ±2.2 35.71 ±1.9 40.00 ±2.5 90.00 ±1.3 65.00 ±2.0 

Gemini-2.5-flash [12] 15.00 ±1.8 55.00 ±2.2 35.00 ±1.9 52.38 ±2.4 61.90 ±2.0 57.14 ±1.8 40.00 ±2.5 85.00 ±1.6 62.50 ±1.9 

Gemini-2.5-flash-image [12] 25.00 ±2.0 40.00 ±2.1 32.50 ±1.8 42.86 ±2.4 61.90 ±2.0 52.38 ±1.9 45.00 ±2.6 90.00 ±1.3 67.50 ±1.8 

Open-source Reasoning Models 

Qwen3-vl-8b-instruct [40] 25.00 ±2.0 40.00 ±2.1 32.50 ±1.9 38.10 ±2.3 28.57 ±2.0 33.33 ±1.8 40.00 ±2.5 95.00 ±1.0 67.50 ±1.9 

Qwen-vl-max [39] 25.00 ±2.0 30.00 ±1.9 27.50 ±1.8 52.38 ±2.4 57.14 ±2.1 54.76 ±1.9 40.00 ±2.5 85.00 ±1.6 62.50 ±1.9 

Qwen2.5-vl-72b-instruct [36] 30.00 ±2.1 30.00 ±1.9 30.00 ±1.7 47.62 ±2.4 57.14 ±2.0 52.38 ±1.8 40.00 ±2.5 85.00 ±1.6 62.50 ±1.9 

Deepseek-vl2 [44] 40.00 ±2.4 70.00 ±2.0 55.00 ±1.8 42.86 ±2.3 23.81 ±1.9 33.33 ±1.7 10.00 ±1.3 95.00 ±1.0 52.50 ±2.0 

Deepseek-r1 [13] 27.55 ±2.2 55.30 ±2.2 48.00 ±1.9 61.75 ±2.4 61.00 ±2.0 46.50 ±2.1 59.25 ±2.5 95.00 ±1.0 67.50 ±1.8 

Open-source Chat Models 

Qwen2.5-vl-32b-instruct [40] 40.00 ±2.4 60.00 ±2.1 50.00 ±1.9 23.81 ±2.0 47.62 ±2.1 35.71 ±1.8 40.00 ±2.5 75.00 ±1.9 57.50 ±2.0 

Llama-3.2-90b-vision-instruct [17] 55.00 ±2.5 55.00 ±2.2 55.00 ±1.9 42.86 ±2.3 61.90 ±2.0 52.38 ±1.8 35.00 ±2.4 95.00 ±1.0 65.00 ±1.9 

Llama-3.2-11b-vision-instruct [17] 20.00 ±1.8 50.00 ±2.2 35.00 ±1.9 33.33 ±2.2 28.57 ±2.0 30.95 ±1.7 15.00 ±1.6 80.00 ±1.8 47.50 ±2.0 

Grok-4 [45] 25.00 ±2.0 60.00 ±2.2 42.50 ±1.9 47.62 ±2.3 28.57 ±2.0 38.10 ±1.8 75.00 ±2.2 80.00 ±1.6 77.50 ±1.7 

Glm-4.5V [38] 35.00 ±2.2 35.00 ±2.0 35.00 ±1.8 61.90 ±2.3 28.57 ±2.0 45.24 ±2.0 60.00 ±2.5 75.00 ±1.9 67.50 ±1.8 

Table 4. Video generation physics evaluation. Columns report physics-aware metrics (mean ± std over n = 5 runs); the best value in each column is bold. Model Center of Mass (Video) Lever Equilibrium (Video) Newton’s First Law (Video) Seg. Mask IoU ↑

Seg. Mask Center ↓

Final State Acc. (%) ↑

Trajectory RMSE ↓

Final. Position Error ↓

Speed Similarity ↑

Acceleration Similarity ↑

Directional Consistency ↑

Veo3.1 [16] 0.019 ±0.003 108.39 ±12.5 35 ±6.2 0.384 ±0.012 0.198 ±0.009 -0.011 ±0.008 -0.021 ±0.007 0.5419 ±0.014 Sora-2 [33] 0.167 ±0.008 121.42 ±11.8 40 ±6.5 0.380 ±0.011 0.199 ±0.008 -0.042 ±0.009 0.017 ±0.006 0.5494 ±0.013 LTX-Video [19] 0.005 ±0.001 76.37 ±9.2 4.76 ±2.1 0.406 ±0.013 0.213 ±0.010 -0.011 ±0.008 -0.056 ±0.009 0.5594 ±0.015 CogVideoX1.5-5B-I2V [46] 0.014 ±0.002 223.70 ±18.5 38.10 ±6.8 0.414 ±0.014 0.323 ±0.012 0.090 ±0.010 -0.043 ±0.008 0.4884 ±0.016 Pyramid Flow [24] 0.012 ±0.002 322.97 ±22.3 47.62 ±7.2 0.381 ±0.011 0.276 ±0.011 -0.047 ±0.009 -0.019 ±0.007 0.6437 ±0.012 Wan2.2 14B [41] 0.136 ±0.007 181.39 ±15.2 33.3 ±6.0 0.395 ±0.012 0.134 ±0.007 0.014 ±0.006 -0.098 ±0.010 0.4775 ±0.015 Cosmos-predict2 2B [1] 0.009 ±0.001 217.33 ±17.8 42.85 ±6.7 0.350 ±0.010 0.243 ±0.010 -0.013 ±0.008 -0.019 ±0.007 0.4884 ±0.016 

cause they rely on visually salient motion cues and rela-tively simple binary decisions (move vs. remain still), which align well with everyday intuitions and common physical understanding. Center of Mass questions average 47.3%; these items require implicit localization of equilibrium axes and reasoning about hidden mass distributions, which re-main particularly challenging for most models. Lever Equi-librium questions lie in between (mean 52.9%), indicating partial mastery of torque balance and lever dynamics when textual mass annotations are explicitly provided. 

Error Characterization. Qualitative inspection (Figure 4) reveals two dominant error modes that appear across models and domains. First, models frequently misread fine-grained visual details such as small mass labels or lever markers, leading to incorrect torque comparisons ( visual parsing er-rors ). Second, they often fail to complete the full reason-ing chain from perception to physics computation and tex-tual inference, producing answers that are locally plausible but globally inconsistent with mechanics ( incomplete rea-soning ). For example, some models compare left and right object masses but ignore whether the objects are actually placed on the lever, yielding logically invalid torque calcu-lations. These patterns expose a gap between visual percep-tion and multi-step, physically grounded reasoning. 

Interpretation. Overall, our results indicate that large mul-timodal models can capture coarse spatial regularities (e.g., heavier objects tend to lower the lever) but still struggle with precise quantitative relationships and counterfactual reason-Figure 4. Error analysis of visual reasoning and video generation. Left: Gemini 2.5 Pro correctly predicts lever balance, while Claude 4.5 gives an incorrect prediction. Right: Sora 2 generates physically consistent motion per Newton’s First Law, unlike LTX-Video’s generation 

ing. Although top-performing systems substantially outper-form random baselines, the persistent domain gaps and sys-tematic error modes suggest that genuine, law-level physi-cal common sense in VQA has yet to emerge. PhysicsMind thus serves as a diagnostic probe of current limitations in static and short-horizon visual physics reasoning. 

4.3. Evaluation of Physical Consistency in Video Generation ( RQ2 )

Table 4 reports quantitative results for seven recent video generation models across the three PhysicsMind domains. This section addresses RQ2 , asking whether current world models produce motion that remains consistent with core mechanical principles. 

Overall Performance Trends. Across all experiments, physical consistency is limited and highly fragmented. No model achieves uniformly strong performance across spa-tial (Center of Mass), mechanical (torque balance), and dy-namic (motion prediction) metrics. For the Center of Mass, models such as Sora-2 and Wan2.2 14B obtain the highest segmentation IoU, whereas Pyramid Flow yields the best lever final-state accuracy (47.62%). Under Newton’s First Law, Cosmos-predict2 2B achieves the lowest trajectory er-ror (normalized RMSE = 0.350), yet its directional consis-tency remains far from perfect. These mixed patterns indi-cate that existing generators can produce visually plausible videos, but struggle to encode a unified, law-consistent no-tion of physical dynamics. 

Domain-Specific Insights. Center of Mass. All models find it difficult to reproduce accurate object geometry and centroids. Even the strongest systems reach only modest IoU scores (on the order of 10 −2–10 −1), and centroid errors span from roughly 75 to over 320 pixels, suggesting that generators capture coarse placement but lack robust repre-sentations of gravitational alignment or equilibrium. 

Lever Equilibrium. Mechanical reasoning around torque balance remains weak. Most models operate near the 

50% random baseline (e.g., Sora-2: 40% , Pyramid Flow: 

47 .62% ), and some fall well below chance (LTX-Video: 

4.76% ). This indicates that lever outcomes are often driven by learned appearance patterns rather than an internalized notion of torque. 

Newton’s First Law. Kinematic stability is also lim-ited. While Cosmos-predict2 2B attains the smallest tra-jectory error, directional consistency values cluster around 

0.5–0.65 (where 0.5 corresponds to random direction align-ment), indicating only weak preservation of velocity direc-tion. Combined with sizable speed and acceleration similar-ity, these results suggest that generated motions frequently deviate from inertia-consistent trajectories. 

Error Characterization. Qualitative inspection (Figure 4, right) highlights systematic temporal reasoning failures. In some cases (e.g., Sora-2), objects remain approximately at rest when the supporting paper is pulled quickly, aligning with Newton’s First Law; in others (e.g., LTX-Video), ob-jects lift or accelerate in implausible ways. Similar to the Table 5. Sim2Real Center-of-Mass results (VQA). Comparison of model accuracies on real and simulated videos (all values in %). Gap = Sim Overall − Real Overall. Values after ± represent standard deviation across multiple runs ( n = 5 ).                                                                              

> Model Real Videos Simulated Videos Gap Position Acc. ↑Rotation Acc. ↑Overall ↑Position Acc. ↑Rotation Acc. ↑Overall ↑
> GPT-5 80.00 ±1.2 60.00 ±1.5 70.00 ±1.3 25.00 ±1.5 45.00 ±1.8 35.00 ±1.5 −35.00
> o4-min 55.00 ±1.5 50.00 ±1.6 52.50 ±1.4 5.00 ±0.8 50.00 ±1.6 27.50 ±1.3 −25.00 Deepseek-r1 15.00 ±1.1 35.00 ±1.5 25.00 ±1.2 35.00 ±1.8 40.00 ±1.8 37.50 ±1.5 12.50
> Claude-sonnet-4-5 20.00 ±1.2 45.00 ±1.6 32.50 ±1.3 5.00 ±0.8 30.00 ±1.4 17.50 ±1.1 −15.00 Gemini-2.5-pro 70.00 ±1.4 30.00 ±1.4 50.00 ±1.3 25.00 ±1.5 40.00 ±1.7 32.50 ±1.5 −17.50 Grok-4 60.00 ±1.4 25.00 ±1.2 42.50 ±1.3 25.00 ±1.5 30.00 ±1.4 27.50 ±1.4 −15.00 Qwen3-vl-8b-instruct 40.00 ±1.5 25.00 ±1.2 32.50 ±1.3 40.00 ±1.8 40.00 ±1.8 40.00 ±1.5 7.50 Deepseek-vl2 70.00 ±1.4 40.00 ±1.6 55.00 ±1.4 65.00 ±1.8 50.00 ±1.8 57.50 ±1.6 2.50 Glm-4.5v 35.00 ±1.4 35.00 ±1.4 35.00 ±1.3 30.00 ±1.3 45.00 ±1.7 37.50 ±1.5 2.50

Table 6. Sim2Real center-of-mass leaderboard for video prediction. The table compares real and simulated video performance using segmentation-mask IoU and center-distance metrics; Gap (IoU) = Sim IoU − Real IoU and Gap (Center) = Sim Center − Real Center. Values after ± represent standard deviation across multiple runs ( n = 5 ).                                                       

> Model Real Videos Simulated Videos Gap Segm. Mask IoU ↑Segm. Mask Center ↓Segm. Mask IoU ↑Segm. Mask Center ↓IoU Center
> Veo3.1 0.0185 ±0.0012 108.39 ±5.5 0.1517 ±0.0068 104.28 ±5.2 0.1332 −4.11 Sora-2 0.1636 ±0.0065 121.42 ±6.2 0.0770 ±0.0042 99.35 ±5.0 −0.0866 −22.07 LTX-Video 0.0050 ±0.0008 76.37 ±4.8 0.0276 ±0.0022 101.98 ±5.1 0.0226 25.61
> CogVideoX1.5-5B-I2V 0.0140 ±0.0011 222.70 ±9.5 0.0714 ±0.0045 206.17 ±8.8 0.0574 −16.53 Pyramid Flow 0.0120 ±0.0010 322.97 ±12.0 0.0609 ±0.0040 131.45 ±6.5 0.0489 −191.52
> Wan2.2 14B 0.1360 ±0.0060 181.39 ±7.8 0.1839 ±0.0072 90.48 ±4.8 0.0479 −90.91 Cosmos-predict2 2B 0.0090 ±0.0009 217.33 ±9.2 0.1678 ±0.0070 206.17 ±8.5 0.1588 −11.16

VQA setting, these errors arise from inadequate object-state tracking and missing causal links between external forces and motion response, with models often treating frames as nearly independent images instead of propagating coherent velocity over time. 

Interpretation. Current video generation systems exhibit perceptual realism without reliable physical realism. They rely heavily on visual priors rather than embedded phys-ical simulation, yielding sequences that look convincing but frequently violate balance, torque, and inertia. Bridg-ing this gap will likely require world models that explic-itly encode and enforce conservation principles across both space and time, rather than relying solely on large-scale text–image–video pretraining. 

4.4. Sim-to-Real in Physical Reasoning ( RQ3 )

To test whether learned physics transfers beyond clean sim-ulators, we use paired center-of-mass scenes with both ren-dered and visually matched real videos and evaluate models on VQA (Table 5) and video prediction (Table 6). 

VQA. Closed-source VLMs perform better on real videos than on simulated ones. GPT-5 and o4-min, for example, drop by 25 –35 points when moving from real to simulated inputs, and Claude 4.5, Gemini 2.5 Pro, and Grok-4 show similar negative Sim–Real gaps. This is consistent with pretraining on web imagery that is closer to real tabletop scenes than to stylized renders. Open-source models show much smaller or even positive gaps: Qwen3-VL-8B, Deepseek-VL2, and Glm-4.5V have Sim– Real gaps between +2 .50 and +7 .50 , and Deepseek-R1 performs noticeably better on simulation ( +12 .50 ), sug-gesting a stronger reliance on the clean edges and simple backgrounds of synthetic data. 

Video generation. For video prediction, synthetic worlds are systematically easier. Most generators obtain higher segmentation IoU and lower centroid error on simulated videos than on real ones (positive IoU gaps for Veo3.1, LTX-Video, CogVideoX, Pyramid Flow, Wan2.2, Cosmos-predict2; large negative centroid gaps for Pyramid Flow and Wan2.2). Sora-2 is the only model whose IoU slightly fa-vors real videos, but its centroid accuracy still degrades when moving from renders to real footage. These trends indicate that current world models overfit to simplified syn-thetic geometry and struggle with clutter, lighting variation, and ambiguous boundaries in real scenes. Across perception and generation, PhysicsMind points to a sim-to-real gap in physical understanding. Closed-source VLMs tend to be stronger on real videos than on simulations, while video generators often show the opposite trend. This asymmetry suggests that models may rely more on visual priors than on domain-invariant notions of mass, balance, and motion, highlighting physics-aware training on both simulated and real videos as a promising direction. 5. Conclusion and Limitations 

PhysicsMind offers a controlled testbed for examining how modern Foundation and world models connect percep-tion, reasoning, and prediction to concrete mechanics laws, rather than to visual plausibility alone. Focusing on three textbook scenarios with paired real–simulated videos, we find that both vision–language and video generation models still rely heavily on superficial cues and frequently violate basic constraints on balance, torque, and inertia, indicating that robust physical understanding has yet to emerge. The benchmark is necessarily limited: PhysicsMind cur-rently focuses on rigid-body setups, short time horizons, and off-the-shelf models. Extending it to richer phenom-ena (e.g., friction, collisions, fluids), longer and interactive scenarios, and to guide training or design of physics-aware architectures is left for future work. References 

[1] Niket Agarwal et al. Cosmos world foundation model plat-form for physical ai, 2025. 1, 2, 6 [2] Anthropic. Claude sonnet 3.5, 2024. 6 [3] Anthropic. Claude sonnet 3.7, 2025. 6 [4] Anthropic. Claude sonnet 4.5, 2025. 3, 6 [5] Tayfun Ates, M. Samil Atesoglu, Cagatay Yigit, Ilker Kesen, Mert Kobas, et al. Craft: A benchmark for causal reasoning about forces and interactions. In Findings of ACL , 2022. 2, 3[6] Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. Phyre: A new bench-mark for physical reasoning. In NeurIPS , 2019. 2 [7] Delong Chen, Willy Chung, Yejin Bang, Ziwei Ji, and Pascale Fung. Worldprediction: A benchmark for high-level world modeling and long-horizon procedural planning. 

arXiv preprint arXiv:2506.04363 , 2025. 3 [8] Shiqi Chen, Tongyao Zhu, Ruochen Zhou, Jinghan Zhang, Siyang Gao, et al. Why is spatial reasoning hard for vlms? an attention mechanism perspective on focus areas. arXiv preprint arXiv:2503.01773 , 2025. 2 [9] Xiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, et al. Eva: An embodied world model for future video anticipation. arXiv preprint arXiv:2410.15461 ,2024. 2 [10] Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, et al. Wow: Towards a world om-niscient world model through embodied interaction. arXiv preprint arXiv:2509.22642 , 2025. 2 [11] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world under-standing. arXiv preprint arXiv:2501.16411 , 2025. 1, 2, 3 [12] Gheorghe Comanici et al. Gemini 2.5: Pushing the fron-tier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 , 2025. 3, 6 [13] DeepSeek-AI et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025. 6 [14] Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Ji-ajun Wu. Worldscore: A unified evaluation benchmark for world generation. In arXiv preprint arXiv:2504.00983 , 2025. 2, 3 [15] Aaron Foss, Chloe Evans, Sasha Mitts, Koustuv Sinha, Am-mar Rizvi, and Justine T. Kao. Causalvqa: A physically grounded causal reasoning benchmark for video models, 2025. 2, 3 [16] Google DeepMind. Veo: Google deepmind video generation model card. Model card, 2025. Accessed: 2025-11-10. 3, 6 [17] Aaron Grattafiori et al. The llama 3 herd of models, 2024. 6 [18] Atharva Gundawar, Som Sagar, and Ransalu Senanayake. Pac bench: Do foundation models understand prerequi-sites for executing manipulation policies? arXiv preprint arXiv:2506.23725 , 2025. 2 [19] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103 , 2024. 6 [20] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Learning latent dynamics for planning from pix-els. In ICML , 2019. 2 [21] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Dream to control: Learning behaviors by latent imagination. In ICLR , 2020. 2 [22] Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, et al. Llms meet multimodal generation and editing: A survey. arXiv preprint arXiv:2405.19334 , 2024. 3[23] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, et al. Vbench: Comprehensive benchmark suite for video generative models. In arXiv preprint arXiv:2401.12915 ,2024. 2, 3 [24] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, et al. Pyramidal flow match-ing for efficient video generative modeling. arXiv preprint arXiv:2410.05954 , 2024. 6 [25] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional lan-guage and elementary visual reasoning. In CVPR , pages 2901–2910, 2017. 2 [26] Benno Krojer, Mojtaba Komeili, Candace Ross, Quentin Garrido, Koustuv Sinha, et al. A shortcut-aware video-qa benchmark for physical understanding via minimal video pairs, 2025. 2, 3 [27] Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, et al. Worldmodelbench: Judging video generation models as world models. In arXiv preprint arXiv:2502.20694 , 2025. 2, 3 [28] Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, et al. Exploring the evolution of physics cognition in video generation: A survey. arXiv preprint arXiv:2503.21765 , 2025. 3 [29] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quan-feng Lu, et al. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363 , 2024. 2, 3 [30] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models understand physical principles? arXiv preprint arXiv:2501.09038 , 2025. 1, 2, 3 [31] OpenAI. GPT-5 System Card, 2025. Accessed: 2026-01-09. 3, 6 [32] OpenAI. OpenAI o3 and o4-mini System Card, 2025. Ac-cessed: 2026-01-09. 6 [33] OpenAI. Sora 2 system card. System card, 2025. Accessed: 2025-11-10. 1, 2, 3, 6 [34] OpenAI et al. Gpt-4o system card, 2024. 6 [35] OpenAI et al. Gpt-4 technical report, 2024. 6 [36] Qwen et al. Qwen2.5 technical report, 2025. 6 [37] Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, V´ eronique Izard, and Emmanuel Dupoux. Intphys: A benchmark for visual intuitive physics reasoning. In NeurIPS , 2018. 2, 3 [38] GLM-V Team et al. Glm-4.5v and glm-4.1v-thinking: To-wards versatile multimodal reasoning with scalable rein-forcement learning, 2025. 6 [39] Qwen Team. Qwen-vl-max, 2024. 6 [40] Qwen Team. Qwen3 technical report, 2025. 1, 3, 6 [41] Team Wan. Wan: Open and advanced large-scale video gen-erative models. arXiv preprint arXiv:2503.20314 , 2025. 3, 6[42] Lintao Wang, Encheng Su, Jiaqi Liu, Pengze Li, Peng Xia, et al. Physunibench: An undergraduate-level physics rea-soning benchmark for multimodal models. arXiv preprint arXiv:2506.17667 , 2025. 2 [43] Archana Warrier, Dat Nguyen, Michelangelo Naim, Moksh Jain, Yichao Liang, et al. Benchmarking world-model learn-ing. arXiv preprint arXiv:2510.19788 , 2024. 3 [44] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302 , 2024. 6 [45] xAI. Grok-4: The most intelligent model in the world, 2025. 6[46] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, et al. Cogvideox: Text-to-video dif-fusion models with an expert transformer. arXiv preprint arXiv:2408.06072 , 2024. 6 [47] Chenyu Zhang, Daniil Cherniavskii, Antonios Tragoudaras, Antonios Vozikis, Thijmen Nijdam, et al. Morpheus: Bench-marking physical reasoning of video generative models with real experiments. In arXiv preprint arXiv:2504.02918 , 2025. 2, 3 [48] Kevin Zhang, Kuangzhi Ge, Xiaowei Chi, Renrui Zhang, Shaojun Shi, Zhen Dong, Sirui Han, and Shanghang Zhang. Can world models benefit vlms for world dynamics? arXiv preprint arXiv:2510.00855 , 2025. 2 Supplemental Material of PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models 

A. Data Curation 13 

A.1. Dataset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 A.2. Dataset Annotation and Quality Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 A.3. Overall Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 A.4. Categories and Subtypes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.5. Dataset Breadth and Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.6. VQA Question Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.7. Video Generation Prompt Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 

B. Experiments 19 

B.1. Experimental Setup and Evaluation Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.2. Specification of VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.3. Specifications of Video Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 

C. Evaluate Metrics 20 

C.1. Center-of-Mass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.2. Lever Equilibrium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.3. Newton’s First Law (Inertia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 

D. Additional Statistical analyses for PhysicsMind 23 

D.1. VQA performance across tasks and model families (RQ1, RQ2) . . . . . . . . . . . . . . . . . . . . . . . . . 23 D.2. Video-based physical evaluation and cross-modality consistency (RQ1, RQ3) . . . . . . . . . . . . . . . . . . 27 

E. Prompt Ablations for Physics-Aware VQA and Video Generation 30 

E.1. Prompt Design and Ablation for VQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 E.2. VQA: Effect of Prompt Design on Physical Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . 30 E.3. Detailed Prompt Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 E.4. Video Generation: Effect of Prompt Design on Physical Rollout . . . . . . . . . . . . . . . . . . . . . . . . . 33 

F. Case Studies 34 

F.1. VQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 F.2. Video Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 

G. Future work 40 A. Data Curation 

This section outlines the construction of the PhysicsMind benchmark and summarizes its data composition, categories, and statistics. We highlight the dataset’s breadth and controlled design, which support reliable evaluation of physical reasoning in multimodal models. 

A.1. Dataset Construction 

PhysicsMind’s dataset is built through a structured pipeline that ensures clear physical interpretability, controlled variation, and consistency across real and simulated settings. • Physics Textbook Identification. Core mechanics concepts were selected from high-school physics, focusing on laws that are visually interpretable in short clips and experimentally reproducible (Center of Mass, Lever Equilibrium, Newton’s First Law). • Concept Formulation. Each concept was translated into a standardized scene design with stable camera viewpoints, clear object layouts, and well-defined physical outcomes. This ensured that all scenarios remained unambiguous and visually grounded. • Variable Selection. Primary variables were chosen based on direct influence on the physical law. Their ranges were restricted to maintain visual clarity. Secondary variables (colors, minor background changes) were varied within controlled bounds. • Real-World Recording and Simulation. Real videos were captured using a fixed 4K camera on a rigid overhead tripod with controlled lighting. Simulated videos were produced using a deterministic 2D physics engine with fixed parameters. All real setups were first prototyped in simulation to ensure matching dimensions and feasible dynamics. • Raw Data Generation. Both simulated and real-world experiments produced short ( ≤ 10 s), high-resolution clips. We intentionally over-generated raw data to allow filtering during the quality-control stage. 

A.2. Dataset Annotation and Quality Control 

Annotation and QC follow a multi-stage process combining expert-written templates, LLM assistance, and strict manual verification. • Raw Data Intake. Only videos with stable framing, clear object visibility, and physically plausible motion were admitted. Clips with camera shake, artifacts, or unexpected behavior were removed. • Expert Question Design. 3 domain experts created structured templates with slots for physical variables and visual references. This ensured consistent phrasing and parallel coverage across similar configurations. • LLM-Generated Draft Answers. LLMs produced first-draft answers for each template. These drafts served as placehold-ers and were not directly used without revision. • Expert Review and Validation. Experts reviewed all entries for alignment with the video, clarity, and physics correctness. Automated checks verified object positions and motion consistency. Any example exhibiting ambiguity or annotation mismatch was excluded. 

A.3. Overall Statistics 

PhysicsMind comprises a diverse collection of videos and question-answer pairs across three canonical mechanics domains: Center of Mass (CoM), Lever Equilibrium (LE), and Newton’s First Law (NI). We leverage both simulated and real-world data to provide varied testing environments. Table 7 summarizes the overall data statistics for each domain, distinguishing between simulated and real-world videos, as well as the number of VQA pairs and Video Generation (VG) samples. The dataset features a single core variant for each physical configuration type across all domains. This design ensures consistent testing scenarios while focusing on the model’s ability to generalize across different data modalities (simulated vs. real) and tasks (VQA vs. VG). Table 7. PhysicsMind Statistics. Breakdown of real and simulated videos, VQA pairs, and video generation (VG) samples across all domains. 

Domain # Real Videos # Sim Videos # Total Videos # VQA Pairs # VG Samples 

Center of Mass (CoM) 20 20 40 40 40 Lever Equilibrium (LE) 0 21 21 42 21 Newton’s First Law (NI) 20 0 20 40 20 

Total 40 41 81 122 81 

A.4. Categories and Subtypes 

PhysicsMind categorizes VQA questions and Video Generation (VG) prompts into specific subtypes within each physical domain. This fine-grained categorization allows for a detailed analysis of model performance across different aspects of physical reasoning. We define distinct subtypes that probe diverse reasoning skills, as summarized in Table 8. 

Table 8. Question subtypes within PhysicsMind domains. Each domain contains distinct reasoning subtypes that capture different dimensions of physical understanding for Video Question Answering (VQA) and Video Generation (VG) tasks. 

Domain Subtype Example Reasoning Task 

Center of Mass (CoM) Position Determine the location of an object’s Center of Mass relative to a suspension point. Rotation Predict the rotational direction of an object around its suspension point when released. Lever Equilibrium (LE) Equilibrium Predict the final balanced state of a lever based on masses and dis-tances from the fulcrum. Balance Adjustment Determine the lever’s state after adjusting the position of an object along the lever arm. Newton’s First Law (NI) Object Position Predict the object’s final position after its supporting surface is re-moved. Object Stability Determine whether an object tips or remains stable when the support is removed. 

A.5. Dataset Breadth and Control 

To demonstrate the range and systematic control present in our dataset, Figure 5 provides a visual summary. The illustration showcases how PhysicsMind incorporates variations in object attributes and environmental dynamics across both real-world experiments and physics-based simulations. This structured diversity enables a comprehensive and controllable benchmark for evaluating models of physical reasoning. A.6. VQA Question Examples. 

To further clarify the nature of reasoning tasks, we provide specific examples of VQA questions for each subtype. These illustrate the direct queries posed to models. 

VQA Prompt Example of Center of Mass (CoM) 

Example image illustrating the CoM scenario. • Position: “In the image, where is the hand-held (suspension) point located relative to the object’s center?” 

A) To the left of the center 

B) To the right of the center (Correct) 

C) Above the center 

D) At the center / No clear offset • Rotation: “The small irregular block is suspended by a vertical red string passing through a hole near its upper edge (the suspension point). The hand pinches the block away from that point. When released, around which direction will the block rotate?” 

A) Clockwise rotation 

B) Anticlockwise (counter-clockwise) rotation (Correct) 

C) Remain stationary (no rotation) 

D) Oscillate without net rotation 

VQA Prompt Example of Lever Equilibrium (LE) 

Example image illustrating the Lever Equilibrium scenario. • Equilibrium: “In the image, a lever rests on a central fulcrum with objects hung on both sides. Using the displayed mass labels and distances from the fulcrum, determine the lever’s final state after release.” 

A) Remain horizontally balanced 

B) Left end sinks, right end rises (Correct) 

C) Right end sinks, left end rises 

D) Cannot be determined • Balance Adjustment: “If the objects on the left side are moved one position closer to the fulcrum, what new state will the lever reach?” 

A) Left end sinks 

B) Right end sinks (Correct) 

C) Remain horizontally balanced 

D) Cannot be determined VQA Prompt Example of Newton’s First Law (NI) 

Example image illustrating the Newton’s First Law experiment. • Object Position: “In the image, a sheet of white paper lies flat on a table with a vertical cylindrical object placed on it. The experimenter quickly pulls the paper out horizontally. After the paper is pulled, what is the most likely state of the cylinder?” 

A) Remains at original position (Correct) 

B) Moves with the paper and falls off the table 

C) Moves slightly in the paper’s direction 

D) Moves in the opposite direction of the paper pull • Object Stability: “A lightweight cylindrical object is placed vertically on a sheet of paper. When the paper is rapidly pulled out, what happens to the cylinder?” 

A) Will tip over (Correct) 

B) Will not tip over (remains vertical) 

C) Tilts slightly but stays upright 

D) Cannot be determined 

A.7. Video Generation Prompt Examples 

For the video generation tasks, models receive an initial frame (observed from the scene setup) along with a descriptive text prompt . Each example illustrates an Image-to-Video (I2V) generation instruction, where the model must produce physically consistent motion continuing from the given frame and description. Below are representative example prompts for each physical reasoning domain: 

Video Generation Prompt Example of Center of Mass (CoM) 

Initial frame provided to the I2V model for CoM video generation. “In the suspension method experiment for determining the center of gravity, against a black background, the string is attached to an irregular cardboard whose bottom has been folded, causing the center of gravity to shift downward. The object is held steady and then released. It naturally hangs down and eventually comes to rest in its equilibrium position.” Video Generation Prompt Example of Newton’s First Law (NI) 

Initial frame provided to the I2V model for Newton’s First Law video generation. “In a lever balance condition experiment, against a blue background, a lever is horizontally mounted on a fulcrum with support pillars underneath keeping it balanced. One object is suspended on each side of the lever at equal distances from the fulcrum, with the left side object being lighter.” 

Video Generation Prompt Example of Newton’s First Law (NI) 

Initial frame provided to the I2V model for Newton’s First Law video generation. “In an experiment to verify Newton’s First Law, against a black background, a white paper is laid flat on the table with a lightweight chopstick placed on it. The experimenter quickly pulls the paper out horizontally.” Figure 5. Visual overview of dataset breadth and variation across real and simulated settings. B. Experiments 

B.1. Experimental Setup and Evaluation Protocol 

This section describes how we evaluate models on the three canonical physics domains of P HYSICS MIND : Center of Mass, Lever Equilibrium, and Newton’s First Law. Vision–language foundation models are assessed on VQA, while video genera-tion models are assessed on physical rollouts, using standardized inputs, controlled settings, and shared metrics to ensure fair and reproducible comparison. 

VQA Evaluation Protocol. In the VQA setting, each vision–language model takes an image and a physics question as input and outputs one of four multiple-choice answers (A, B, C, or D). Evaluation is performed by comparing the predicted answer directly against expert-verified ground truth. To reflect the different types of physical reasoning in P HYSICS MIND ,we evaluate models on three task domains: • Center-of-Mass Task: models reason about geometric and spatial relationships in suspension and balance scenes. • Lever Equilibrium Task: models determine the correct lever outcome by applying torque-based reasoning. • Newton’s First Law Task: models infer inertial behavior when external support is removed, predicting displacement or stability. 

Video Generation Evaluation Protocol. For video generation, each model receives an initial frame and a text prompt describing the physical setup, and is required to generate a short video that depicts the resulting motion. We then compare the generated video against the ground-truth rollout to measure physical consistency in object geometry, causal interaction, and kinematic behavior. We use the same three task domains as in VQA: • Center-of-Mass Task: evaluates whether the generated motion preserves the object shape and the location of the effective Center of Mass. • Lever Equilibrium Task: checks whether the lever reaches the correct terminal state (tilt left, tilt right, or balanced) under the applied torques. • Newton’s First Law Task: assesses trajectory, speed, and acceleration stability under zero external force. 

B.2. Specification of VLMs 

We evaluate 24 vision language models. Table 9 summarizes their main characteristics, including availability, functional category (reasoning oriented vs. chat style), and the architecture used for visual encoding. Most models in our study are accessed through proprietary APIs, such as those from OpenAI, Google Gemini, Anthropic Claude, and xAI Grok. Six models provide publicly released weights: the Qwen VL series (8B, 32B, 72B), DeepSeek-VL2, GLM-4.5V, and the Llama-3.2 vision models (11B and 90B). Models designed with explicit multistep inference capabili-ties are grouped as reasoning models (e.g., GPT-5, DeepSeek-R1, Claude Sonnet), whereas assistants designed for general purpose multimodal tasks are categorized as chat models. All evaluated systems adopt a ViT or CLIP style backbone for visual encoding, although the integration mechanisms vary across model families. Common designs include fusion via cross attention (OpenAI, Llama-3.2), Q-Former mod-ules (DeepSeek), projection layers based on resamplers (Qwen VL), and proprietary multimodal encoders (Gemini). These components project visual features into the language model’s embedding space, enabling unified multimodal processing. 

B.3. Specifications of Video Models 

We evaluate a diverse set of contemporary video generation models with different frame rates and output resolutions (Ta-ble 10). For all models, we standardize the prompt format and generated clip length to make the physics evaluation as comparable as possible. Table 9. Specifications of the 24 evaluated vision–language models. The table reports availability (open vs. closed), functional category (reasoning vs. chat), and the architecture of each model’s multimodal vision encoder. 

Model Open/Closed Type Vision Backbone 

Claude-Sonnet-4.5 Closed Reasoning ViT-based encoder Claude-3.5-Sonnet Closed Reasoning ViT-based encoder Claude-3.7-Sonnet Closed Reasoning ViT-based encoder DeepSeek-R1 Closed Reasoning Q-Former + ViT DeepSeek-VL2 Open Chat Hybrid ViT + Q-Former Gemini-2.5-Pro Closed Reasoning Gemini multimodal encoder Gemini-2.5-Flash Closed Chat Gemini encoder Gemini-2.5-Flash-Image Closed Chat Gemini encoder Gemini-Pro-Vision Closed Chat Gemini encoder GLM-4.5V Open Chat ViT-based encoder Grok-4 Closed Reasoning ViT + cross-attn GPT-4-Turbo Closed Reasoning ViT + cross-attn GPT-4o Closed Chat ViT + cross-attn GPT-4o-mini Closed Chat ViT + cross-attn GPT-5 Closed Reasoning ViT + cross-attn Llama-3.2-11B-Vision Open Chat ViT + cross-attn Llama-3.2-90B-Vision Open Chat ViT + cross-attn o4-mini Closed Reasoning ViT + cross-attn Qwen-VL-Max Closed Chat ViT + resampler Qwen2.5-VL-32B-Instruct Open Chat ViT + resampler Qwen2.5-VL-72B-Instruct Open Chat ViT + resampler Qwen3-VL-8B-Instruct Open Chat ViT + resampler Table 10. Video generation models used in the PhysicsMind evaluation. 

Model FPS Resolution 

Veo-3.1 24 1280 ×720 Sora-2 24 1280 ×704 LTX-Video 30 768 ×512 CogVideoX1.5-5B-I2V 16 768 ×1360 Pyramid Flow 24 1280 ×768 Wan-2.2 14B 16 1280 ×720 Cosmos-predict2 2B 16 960 ×704 

## C. Evaluate Metrics 

In this section, we introduce the physics-aware metrics used to evaluate video generation on P HYSICS MIND . Rather than relying on generic perceptual scores, we compare generated videos against ground truth along three canonical mechanics scenarios: (i) Center-of-Mass , where we measure whether the model preserves object shape and the location of the effective mass distribution; (ii) Lever Equilibrium , where we check if the final lever state matches the torque implied by masses and lever arms; and (iii) Newton’s First Law (Inertia) , where we examine full-rollout trajectories, speeds, accelerations, and overall motion direction. Together, these metrics provide a complementary view of how well a video model respects basic physical laws, beyond producing visually plausible footage. 

C.1. Center-of-Mass 

The Center of Mass is the latent variable that governs how a rigid body hangs, rotates, and re-balances under gravity. If a video model does not place objects with a physically consistent Center of Mass, subsequent lever and inertia behaviors will also be unreliable. Our center-of-mass metrics therefore focus on whether the generator can recover the shape and location of the mass distribution, rather than only producing visually plausible silhouettes. 

C.1.1. Segmentation Mask IoU Definition: This metric measures the geometric overlap fidelity between the generated object shape and the ground truth. We utilize the Segment Anything Model (SAM) to derive binary masks from tracked points to assess shape preservation. 

Formulation: Let Mgt and Mpred correspond to the binary masks of the ground-truth and predicted objects, respectively. The Intersection over Union (IoU) is defined as: IoU =

P 

> i,j

Mgt (i, j ) · Mpred (i, j )

P 

> i,j

max( Mgt (i, j ), M pred (i, j )) (1) The IoU ranges from [0 , 1] , where higher values indicate better segmentation accuracy. 

C.1.2. Segmentation Mask Center Distance Definition: This metric quantifies the spatial implementation error of the mass distribution. It calculates the pixel-level Euclidean distance between the centroid of the predicted object mask and the ground-truth mask. 

Formulation: The centroids of the ground-truth mask ( cgt ) and predicted mask ( cpred ) are calculated as the average coordi-nates of their non-zero pixels. The Center Distance is: Center Distance = ∥cgt − cpred ∥2 (2) This metric is measured in pixels, with smaller values indicating higher spatial accuracy. 

C.2. Lever Equilibrium 

Lever Equilibrium exposes whether a model can respect torque balance, instead of just copying common “left-down/right-down” visual patterns. In our tabletop setups the lever dynamics are essentially one-dimensional and the outcome is categor-ical (left, right, balanced), so the most physically meaningful signal is whether the final state matches the torque implied by the masses and lever arms, rather than pixel-wise similarity of intermediate frames. 

C.2.1. Final State Accuracy Definition: The proportion of samples for which the model correctly predicts the final state of the lever (e.g., left-down, right-down, or balanced), consistent with the VLM-based comparison in Fig. X. 

Formulation: 

Accuracy = 1

S

> S

X

> i=1

I y(i) 

> pred

= y(i)

> true

 (3) where S is the total number of samples and I(·) is the indicator function. The accuracy ranges from [0 , 1] .

C.3. Newton’s First Law (Inertia) 

Newton’s first law controls whether objects keep moving at constant velocity when no external forces act on them. For this scenario, it is not enough to check a single end frame: a model might coincidentally land at the right location while producing highly non-inertial motion in between. We therefore evaluate the entire rollout using trajectory-, speed-, and acceleration-level metrics, together with a global direction check. Let T be the number of frames and N the number of tracked points (objects). The position of point n at frame t is denoted as p(t,n ) 

> gt

for ground truth and p(t,n ) 

> pred

for prediction. C.3.1. Trajectory RMSE Definition: The Root Mean Squared Error (RMSE) measures the global deviation between the predicted motion path and the ground-truth trajectory. It is computed in normalized coordinates to ensure scale invariance across different video resolutions. 

Formulation: We first normalize pixel coordinates (x, y ) to [0 , 1] by dividing by the frame width W and height H. The normalized trajectory RMSE is then: RMSE traj =

vuut 1

T N 

> T

X

> t=1
> N

X

> n=1

p(t,n ) 

> gt

− p(t,n ) 

> pred
> 22

(4) Smaller values indicate higher trajectory accuracy. 

C.3.2. Final Position Error (FPE) Definition: This metric assesses the model’s long-term forecasting ability by measuring the Euclidean distance between the predicted and ground-truth final positions, normalized by the total length of the actual motion path. 

Formulation: 

FPE =

p(T ) 

> gt

− p(T ) 

> pred 2

Lgt 

(5) where 

Lgt = 

> T−1

X

> t=1

p(t+1)  

> gt

− p(t) 

> gt 2

(6) is the total length of the ground-truth trajectory path. Smaller values are better. 

C.3.3. Speed Similarity Definition: The cosine similarity between predicted and ground-truth velocity vectors, averaged over the trajectory, as in the “Speed Similarity” block in Fig. X. This metric captures whether the model keeps the magnitude and local direction of motion consistent with an inertia-respecting rollout. 

Formulation: For each point n and frame t ≥ 2, we compute velocity vectors 

v(t,n ) 

> gt

= p(t,n ) 

> gt

− p(t−1,n ) 

> gt

, v(t,n ) 

> pred

= p(t,n ) 

> pred

− p(t−1,n ) 

> pred

. (7) The speed similarity is defined as the average cosine similarity: 

Svel = 1(T − 1) N

> N

X

> n=1
> T

X

> t=2

v(t,n ) 

> gt

· v(t,n )

> pred

v(t,n ) 

> gt 2

v(t,n ) 

> pred 2

. (8) This metric lies in [−1, 1] ; in our setting it typically falls in [0 , 1] . Larger values indicate that the instantaneous motion directions and magnitudes are more consistent with the ground truth. 

C.3.4. Acceleration Similarity Definition: The cosine similarity between predicted and ground-truth acceleration vectors, averaged over the trajectory, corresponding to the “Acceleration Similarity” block in Fig. X. This term is particularly sensitive to non-physical jitter or unrealistic speed-up/slow-down patterns that would violate constant-velocity motion. 

Formulation: For each point n and frame t ≥ 3, accelerations are obtained by differencing velocities: 

a(t,n ) 

> gt

= v(t,n ) 

> gt

− v(t−1,n ) 

> gt

, a(t,n ) 

> pred

= v(t,n ) 

> pred

− v(t−1,n ) 

> pred

. (9) The acceleration similarity is then 

Sacc = 1(T − 2) N

> N

X

> n=1
> T

X

> t=3

a(t,n ) 

> gt

· a(t,n )

> pred

a(t,n ) 

> gt 2

a(t,n ) 

> pred 2

. (10) Higher values indicate that the acceleration (i.e., speed-up and slow-down patterns) is better matched. 

C.3.5. Directional Consistency Definition: A global measure of whether the predicted trajectory moves in the same overall direction as the ground-truth trajectory. Compared to the local similarity metrics above, this captures whether the model preserves the coarse, scene-level 

motion tendency implied by the setup (e.g., sliding forward vs. backward). 

Formulation: Let p(1)  

> gt

and p(T ) 

> gt

be the ground-truth start and end positions, and p(1) 

> pred

, p(T ) 

> pred

be the predicted ones. We define displacement vectors 

dgt = p(T ) 

> gt

− p(1)  

> gt

, dpred = p(T ) 

> pred

− p(1) 

> pred

. (11) The angle θ between them is 

θ = arccos 

 dgt · dpred 

∥dgt ∥2 ∥dpred ∥2



, (12) where θ is measured in degrees. The directional consistency score is 

Sdir = 180 − θ

180 . (13) Thus Sdir = 1 when the two trajectories share exactly the same overall direction ( θ = 0 ◦), and Sdir = 0 when they move in opposite directions ( θ = 180 ◦). 

## D. Additional Statistical analyses for PhysicsMind 

This appendix complements the main results with a more systematic analysis of model behavior on P HYSICS MIND for both VQA and video generation. We organise the discussion around three research questions: 

RQ1 (Task difficulty). Do the three physics tasks exhibit a consistent and statistically supported difficulty ordering? 

RQ2 (Model families). Are there significant performance differences across model ecosystems and between “chat” and “reasoning” model variants? 

RQ3 (Cross-modality consistency). Do VQA and video-based evaluation expose aligned physical failure modes, or do they stress different aspects of physical competence? Unless otherwise noted, all analyses use the same data splits, metrics, and experimental protocol as in the main paper. 

D.1. VQA performance across tasks and model families (RQ1, RQ2) 

We analyse the performance of 22 models on the three PhysicsMind VQA tasks using descriptive statistics, one-way and two-way ANOVA, Pearson correlations, and two-sample t-tests. The focus is on task difficulty, category effects, and the extent to which the tasks capture complementary abilities. 

D.1.1. Task-level statistics and difficulty ordering 

Aggregating accuracies of all models on the three VQA tasks (Newton’s First Law, Lever Equilibrium, Center of Mass) yields the following patterns: • Newton’s First Law has the highest mean accuracy at roughly 60 .8% .• Lever Equilibrium has a mean accuracy around 48 .0% .• Center of Mass is the hardest, with a mean accuracy around 39 .8% .• All three tasks have standard deviations between 12 and 13 percentage points. Thus, the main differences lie in the mean level rather than in dispersion across models: the spread of accuracies is similar, but the average difficulty follows Newton’s First Law > Lever Equilibrium > Center of Mass .

This matches the intended complexity ladder: from relatively direct inertia judgements, to lever balance with explicit geo-metric reasoning, to Center of Mass problems that are more sensitive to geometric configuration and mass distribution. 

D.1.2. Model category effects: one-way ANOVA 

Figure 6 shows boxplots of overall VQA accuracy for four model categories (closed-source / open-source × chat / reasoning): • Closed-source reasoning models have the highest median accuracy. • Closed-source chat models have the lowest median. • The height of the boxes and whiskers is comparable across categories, indicating similar within-category variability. Closed−source Chat 

> Closed−source Reasoning
> Open−source Chat
> Open−source Reasoning
> 20
> 30
> 40
> 50
> 60
> 70
> 80
> Accuracy (%)
> Center of Mass
> Lever Equilibrium
> Newton's First Law
> 20
> 30
> 40
> 50
> 60
> 70
> 80
> Accuracy (%)

Figure 6. Overall PhysicsMind VQA accuracy by model category. 

We run a one-way ANOVA with Category as the independent variable and overall VQA accuracy as the dependent variable: 

Table 11. One-way ANOVA of overall VQA accuracy by model category. 

Source df SS MS F p

Category 3 1279 426.4 1.963 0.1288 Residuals 62 13468 217.2 – –The ANOVA yields 

F (3 , 62) = 1 .963 , p = 0 .1288 > 0.05 ,

so we cannot reject the null hypothesis that the four categories have the same mean accuracy. In Tukey HSD post-hoc tests, the largest contrast (closed-source reasoning vs. closed-source chat) has a mean difference of about 11 .9 percentage points, but the multiple-comparisons-adjusted p-value is ≈ 0.09 and not statistically significant. Under the current sample size and variance, there is no robust evidence that any single category enjoys a stable advantage on PhysicsMind VQA. This is consistent with the main paper’s observation that basic physics remains challenging across ecosystems. 

D.1.3. Task main effects and Category–Task interaction: two-way ANOVA 

We next fit a two-way ANOVA with Category and Task as factors (no repeated-measures structure): 

Table 12. Two-way ANOVA of VQA accuracy with Category and Task. 

Source df SS MS F p

Category 3 1279 426.4 3.095 0.034 Task 2 4922 2460.9 17.867 < 0.001 ∗∗∗ 

Category:Task 6 1108 184.7 1.341 0.255 Residuals 54 7438 137.7 – –The main effect of Task is highly significant ( p < 0.001 ), formally confirming the difficulty ordering discussed above. The main effect of Category reaches p = 0 .034 in this specification, but the effect size is small and somewhat unstable when compared with the one-way ANOVA and Tukey tests. Crucially, the Category:Task interaction is not significant (p = 0 .255 > 0.05 ). The interaction is visualised in Figure 7: the performance curves of the four categories over the three tasks are roughly parallel. Every category performs best on Newton’s First Law and worst on Center of Mass, which supports the view that difficulty is driven primarily by the underlying physical structure of each task rather than by ecosystem-specific preferences or training recipes. 29.6        

> 42.8
> 57.5
> 45.4
> 59.9 60.4
> 43.5
> 40.5
> 63.0
> 38.6
> 44.1
> 62.5
> 0
> 20
> 40
> 60
> 80
> Center of Mass Lever Equilibrium Newton's First Law
> Mean Accuracy (%)
> Category Closed−source Chat Closed−source Reasoning Open−source Chat Open−source Reasoning

Figure 7. Mean VQA accuracy of four model categories on the three tasks. D.1.4. Task correlations: complementary rather than redundant 

Figure 8 and Table 13 show the Pearson correlation matrix of accuracies across the three VQA tasks. 1.00 

## 0.34 

## 0.26 

## 0.34 

## 1.00 

## 0.15 

## 0.26 

## 0.15 

## 1.00 

> −1
> −0.8
> −0.6
> −0.4
> −0.2
> 0
> 0.2
> 0.4
> 0.6
> 0.8
> 1
> CenterOfMass
> LeverEquilibrium
> NewtonFirstLaw

CenterOfMass 

LeverEquilibrium 

NewtonFirstLaw 

> Figure 8. Pearson correlation matrix among the three PhysicsMind VQA tasks. Table 13. Pearson correlation coefficients among VQA tasks.

Center of Mass Lever Equilibrium Newton’s First Law 

Center of Mass 1.000 0.337 0.265 Lever Equilibrium 0.337 1.000 0.151 Newton’s First Law 0.265 0.151 1.000 All off-diagonal correlations lie between 0.15 and 0.34 , i.e., weak to moderate positive correlations. At the 0.05 significance level, none of them are statistically significant: the corresponding p-values are approximately 0.12 , 0.23 , and 0.50 .In practice, high accuracy on one task does not strongly predict performance on the others. From the task design: • Center of Mass emphasizes spatial/geometric reasoning and mass distribution. • Lever Equilibrium focuses on torque balance and lever-arm comparison. • Newton’s First Law targets understanding of motion states, inertia, and the presence or absence of external forces. The three tasks therefore behave as complementary probes rather than redundant measurements of a single latent “physics score”. This empirically supports the main paper’s aim of capturing multi-dimensional physical competence. 

D.1.5. Ecosystem and paradigm comparisons: two-sample t-tests 

We finally report two standard splits via independent two-sample t-tests, used in the main paper to support the claim that physics is a common failure mode across ecosystems. 

> Table 14. Two-sample t-test of overall VQA accuracy: closed-source vs. open-source models.

Test statistic t(64) = 0 .420 

p-value 0.676 Mean difference 1.55% (closed-source higher) 95% CI [-5.81%, 8.91%] Closed-source mean 50.24% Open-source mean 48.69% 

Closed-source vs. open-source models. The p-value is far above 0.05 and the confidence interval crosses zero, indicating no statistically significant difference in mean performance between closed- and open-source models on PhysicsMind VQA. The “closed vs. open” label alone does not predict physics reasoning ability. 

> Table 15. Two-sample t-test of overall VQA accuracy: reasoning vs. chat models.

Test statistic t(62 .38) = −1.695 

p-value 0.095 Mean difference -6.21% (reasoning higher) 95% CI [-13.53%, 1.11%] Reasoning mean 52.36% Chat mean 46.15% 

Reasoning vs. chat models. Reasoning models have higher mean accuracy, and the p-value ( ≈ 0.095 ) is close to conven-tional significance thresholds. However, the effect is not statistically significant under a strict 0.05 level. This aligns with the main paper: generic chain-of-thought or instruction tuning helps on physics questions, but does not remove systematic physical errors. 

Relation to the research questions. Taken together, the VQA analyses above (i) establish a clear and statistically supported difficulty ordering across the three physics tasks, directly answering RQ1 , and (ii) show that ecosystem- and paradigm-level differences are modest and often not statistically robust, informing RQ2 by indicating that physics remains a shared challenge across model families. 

D.2. Video-based physical evaluation and cross-modality consistency (RQ1, RQ3) 

We now discuss the video evaluation results of the seven video generation models on the three PhysicsMind scenarios, based on the metrics reported in the main paper. Unlike VQA, video evaluation involves multiple real-valued physics metrics, some “higher is better”, some “lower is better”, and some potentially negative. We therefore focus on qualitative patterns rather than constructing a single aggregate score. D.2.1. Task-level patterns Center of Mass (video). In this scenario, we measure whether generated videos obey center-of-mass constraints using the mask IoU and the error of the object’s center-of-mass position. Overall: • All models achieve IoU values very close to zero, far below typical “usable” segmentation levels. • Center-of-mass position errors are large in pixel space (mean around ∼ 180 pixels). Although there are differences across models, all lie in a regime of substantial deviation. This suggests that when the generator needs to balance appearance with physical consistency, current models have almost no reliable control over the Center of Mass. They tend to place objects in a visually plausible way rather than aligning them to a physically consistent mass distribution. 

Lever Equilibrium (video). For the lever scenario, we only consider whether the final balanced state is correct. According to the main paper results, even the strongest model achieves only about 45 –50% final-state accuracy. The overall mean is around 35% , noticeably below the corresponding VQA mean of roughly 48% .In other words, even when a model can answer “which side is heavier” in a static question, driving the full generation process towards the correct equilibrium state is considerably harder. There is a disconnect between linguistic reasoning and physical trajectory generation. 

Newton’s First Law (video). In the inertia scenario, we use trajectory RMSE, final position error, velocity and acceleration similarity, and directional consistency to assess compliance with inertial motion: • Trajectory RMSE lies in a relatively narrow range across models ( ∼ 0.35 –0.41 ), indicating comparable error magnitude in the global shape of the trajectory. • Final position error and directional consistency vary more: some models with reasonable RMSE still exhibit large terminal offsets or obvious “turn-back” motion. • Velocity and acceleration similarity span near-zero and strongly negative values, reflecting cases where the motion looks smooth but the direction of change in velocity is physically inverted. Among the three video scenarios, Newton’s First Law is relatively better handled, yet there remains a noticeable gap between the generated motion and physically plausible trajectories. 

D.2.2. Model differences and metric heterogeneity 

Unlike VQA, where accuracy has a single direction, video evaluation metrics differ in both scale and direction: smaller is better for position error and RMSE, larger is better for IoU and directional consistency, and similarity scores can be negative. Rather than forcing a global score, several stable patterns emerge: • Most models excel on only a few specific metrics; for instance, some achieve higher final-state accuracy in the lever scenario but do not reduce center-of-mass error. • No model is best on all physical metrics simultaneously: a model that is strong on global trajectory shape (RMSE) may still be weak on terminal position or directional consistency. • Each model shows strong task dependence: a model that is relatively stable on inertia can still perform poorly on center-of-mass scenarios. Taken together, current video generators look more like collections of local strengths than coherent physical world models. The heterogeneity across metrics reinforces the need for multi-metric, scenario-wise evaluation. 

D.2.3. Comparing VQA and video: a gap between understanding and generation 

Comparing Section D.1 with the video analysis above yields several observations that support the main paper: • Video generation is consistently harder than VQA under the same physics setup. For example, in the lever scenario, final-state accuracy in generation is lower than VQA accuracy, indicating a systematic gap between answering correctly and producing a physically correct process. • Center of Mass scenarios are the hardest on both sides. Center of Mass questions have the lowest VQA accuracies, and the corresponding video metrics (IoU and position error) are also the worst. Spatial geometry and mass distribution emerge as joint bottlenecks for understanding and generation. • The difficulty ordering is consistent across understanding and generation. In both settings, we observe Center of Mass > Lever Equilibrium > Newton’s First Law From hardest to easiest, reinforcing the interpretation of the three scenarios as distinct levels of physical complexity. Overall, the VQA and video analyses are qualitatively aligned: the three PhysicsMind tasks form complementary axes for static understanding and reveal analogous failure modes in dynamic generation. Together, they provide a unified and fine-grained lens on the physical consistency of large models. 

Relation to the research questions. The video-based analyses primarily address RQ3 : VQA and video evaluation ex-pose strongly aligned difficulty patterns and shared bottlenecks, while also revealing a persistent gap between “knowing” and “generating”. At the same time, they refine RQ1 by showing that the difficulty ordering observed for static VQA per-sists when models are required to produce full physical rollouts, highlighting the need for architectures that couple visible dynamics with coherent internal world models. E. Prompt Ablations for Physics-Aware VQA and Video Generation 

E.1. Prompt Design and Ablation for VQA 

We investigate how different prompting strategies influence GPT-5’s performance on our multiple-choice Physics VQA benchmarks. All experiments use identical images, questions, and evaluation settings; only the prompt template is var-ied. Decoding hyperparameters are fixed across conditions (temperature 0.3, maximum output length 8k tokens, and up to five retries on API failure).                                                                         

> Table 16. Prompt ablation on the Physics VQA datasets for two representative models (GPT-5 and GPT-4o). We report subtype-level and overall accuracies (%) across the three prompting strategies: Direct Answering (DA), Chain-of-Physics (CoP), and Law-Conditioned (LC).
> Model & Prompt Center of Mass (VQA) Lever Equilibrium (VQA) Newton’s First Law (VQA) Position Rotation Overall Equilibrium Balance Adj. Overall Obj. Pos. Obj. Stability Overall Acc (%) Acc (%) Acc (%) Acc (%) Acc (%) Acc (%) Acc (%) Acc (%) Acc (%) GPT-5 (DA) 60.0 80.0 70.0 85.7 66.7 76.2 60.0 95.0 77.5 GPT-5 (CoP) 69.2 64.0 65.8 81.0 61.9 71.4 70.0 95.0 82.5 GPT-5 (LC) 25.0 75.0 50.0 81.0 66.7 73.8 65.0 85.0 75.0 GPT-4o (DA) 50.0 70.0 60.0 28.6 61.9 45.2 45.0 75.0 60.0 GPT-4o (CoP) 38.5 85.7 67.6 42.9 42.9 42.9 40.0 89.5 64.1 GPT-4o (LC) 25.0 50.0 37.5 42.9 61.9 52.4 40.0 90.0 65.0

E.2. VQA: Effect of Prompt Design on Physical Question Answering 

Table 16 summarizes the effects of the three prompting strategies across the three Physics VQA domains: Lever Equilibrium (LE), hanging Center of Mass questions (CoM), and the Newton’s first law (NFL) paper pulling setup. For GPT-5, the DA prompt already yields strong performance across all domains (overall: 76 .2% on LE, 70 .0% on CoM, and 77 .5% on NFL). The CoP prompt provides the largest gains on the NFL dataset, which requires intensive reasoning, increasing overall accuracy to 82 .5% primarily by improving the object position subtype (from 60 .0% to 70 .0% ) while maintaining high object stability accuracy ( 95 .0% ). On Lever Equilibrium, CoP and LC remain close to DA, with LC giving a small improvement to 73 .8% overall, likely because the torque relationships are visually explicit. For CoM, DA remains the strongest overall. CoP improves position understanding but reduces rotation accuracy, while LC underperforms substantially on the position subtype, resulting in a lower overall score ( 50 .0% ). In contrast, GPT-4o exhibits lower absolute performance but still shows informative trends that depend on the prompt. Under DA, GPT-4o underperforms GPT-5 across all domains, especially on Lever Equilibrium (overall 45 .2% ) where questions about equilibrium state are particularly challenging. Prompts that incorporate physics awareness partially mitigate these weaknesses: on NFL, both CoP and LC raise overall accuracy (to 64 .1% and 65 .0% ), mainly via substantially improved object stability performance (approximately 90% ). On CoM, CoP achieves GPT-4o’s best overall accuracy ( 67 .6% ), driven by a considerable improvement in rotation predictions. On LE, LC provides a moderate improvement over DA, although questions about equilibrium state remain difficult across all prompts. Taken together, these results demonstrate two key findings. First, GPT-5 is consistently more robust than GPT-4o across all prompting strategies. Second, prompting that is guided by physics principles, particularly the CoP template, is most beneficial in settings that require genuine multistep physical reasoning (NFL and, to a lesser extent, CoM rotation), whereas in visually explicit tasks involving torque balancing, the simple DA prompt already captures most of the relevant information. E.3. Detailed Prompt Templates 

Below we present the complete prompt templates used in our ablation study. All three prompts share the same output format requirement but differ in the level of physical guidance provided to the model. 

Direct Answering (DA). The DA baseline instructs the model to inspect the image and select the correct option without requiring any intermediate reasoning: 

Prompt for DA 

Please answer the following multiple choice question based on the image. Question: {question }

Instructions: - Look carefully at the image - Choose the correct answer from the options provided - Only return the letter of your answer (A, B, C, or D) - Format: Just output “Answer: X” where X is A, B, C, or D 

Law-Conditioned (LC). The LC prompt identifies the relevant physical law (e.g., torque balance, Newton’s first law, center-of-mass rule) but does not require explicit reasoning steps. Instead, the model is instructed to use the named law as guidance when selecting the answer. 

Prompt for LC (Law-Cued) 

You are a physics expert. This question is mainly about {law }.

Question: {question }

Instructions: 

• Carefully inspect the associated image. • Use the named physical law(s) as the main guideline when choosing the answer. • You do not need to write detailed step-by-step reasoning. • At the end, on a new line, output only “ Answer: X” where X is one of A, B, C, or D. 

Law Mapping by Question Category: 

equilibrium state → the torque balance law for levers balance adjustment → the torque balance law for levers object position → Newton’s first law of motion (inertia of translational motion) object stability → Newton’s first law and rotational stability of rigid bodies arrow direction → the rule that a suspended body rotates until its Center of Mass lies directly below the suspension point trajectory → the same center-of-mass principle for suspended bodies 

Note: For each question, the placeholder {law } is automatically filled according to the question category using the mapping above. 

Chain-of-Physics (CoP). The CoP prompt explicitly requests short, structured physical reasoning before producing an answer. The instructions adapt to the question category: lever questions ask the model to enumerate masses and lever arms and compare torques; Newton’s-first-law (NFL) questions compare inertia and friction under different pulling speeds; center-of-mass (CoM) questions remind the model that suspended objects rotate until the Center of Mass lies vertically beneath the suspension point. 

Prompt Template for CoP 

You are a physics tutor. The question below comes from a mechanics experiment. 

Question: {question }

Instructions: Choose the appropriate reasoning approach based on the question type. 

Output Format: 

1. Begin with step-by-step reasoning 2. On a new line, output only “ Answer: X” where X is one of A, B, C, or D Prompt for CoP for Lever Equilibrium Questions 

• Think like a physics teacher about Lever Equilibrium. • Step 1: List all objects on the left and right of the fulcrum; note their masses and distances. • Step 2: For each side, compute total torque = mass × distance. • Step 3: Compare the total torque on the left and right. • Step 4: Decide which side rotates downward or whether torques stay equal. 

Example: “On the left: 2 kg at 1 unit → torque = 2; on the right: 1 kg at 3 units → torque = 3. Right side has larger torque and will go down.” 

Prompt for CoP for Inertia Questions 

• Think using Newton’s first law (inertia). • Step 1: Identify whether the pull is fast or slow, and whether the object is light or heavy. • Step 2: Recall that a fast pull means the paper accelerates quickly while the object tends to stay at rest; a slow pull lets friction drag the object along. • Step 3: Decide whether the object stays in place, moves with the paper, or shifts slightly; assess tipping risk based on shape and support. 

Example: “Because the paper is pulled very quickly and friction acts for a short time, the cylinder almost stays where it is due to inertia, so it remains at its original position.” 

Prompt for CoP for center-of-Mass Questions 

• Use the idea that a suspended object rotates until its Center of Mass lies directly below the suspension point. • Step 1: From the image, decide where the Center of Mass is relative to the suspension point (left / right / above / below). • Step 2: Recall that gravity produces a torque that moves the Center of Mass to lie vertically under the suspension point. • Step 3: Infer the direction of rotation (clockwise or anticlockwise) or whether there is no obvious net torque. 

Example: “The heavy part is to the left of the suspension point, so gravity makes the object rotate anticlockwise until that side hangs lower.” E.4. Video Generation: Effect of Prompt Design on Physical Rollout 

For video generation, we select two representative models and evaluate their performance under the same three prompt templates. For each scenario, the initial frame is kept fixed while the textual description is modified according to the prompt type. We evaluate the generated videos on one representative physics-aware metric per task: 

Table 17. Prompt ablation for PhysicsMind video generation. Two representative video models (Sora2 and LTX-Video) evaluated on the three physics scenarios under three prompt templates. Columns and metric definitions follow Table 4 in the main paper. Values are reported as mean over n runs. Model Prompt Center of Mass (Video) Lever Equilibrium (Video) Newton’s First Law (Video) Seg. Mask IoU Seg. Mask Center Final State Acc. (%) Trajectory RMSE Final Position Error Speed Similarity Acceleration Similarity Directional Consistency Sora2 Scene-only 0.1002 459.74 30 0.384 0.087 -0.018 0.004 0.4765 Scene + Action 0.167 121.42 40 0.380 0.199 -0.042 0.017 0.5494 Scene + Action + Law 0.1341 239.77 40 0.402 0.243 0.052 0.021 0.5934 LTX-Video Scene-only 0.0982 92.72 8 0.301 0.203 0.009 -0.023 0.2115 Scene + Action 0.005 76.37 4.76 0.406 0.213 -0.011 -0.056 0.5594 Scene + Action + Law 0.006 520.97 12 0.421 0.192 0.021 0.034 0.5479 

Empirically, the effects of prompt design on video generation are more heterogeneous than on VQA. For Sora2, moving from a Scene-only description to the canonical Scene+Action prompt consistently improves center-of-mass tracking (IoU increases from 0.1002 to 0.167 and the mask-center error drops from 459 .7 to 121 .4 pixels), final lever accuracy (from 30% 

to 40% ), and directional consistency in the Newton’s-first-law videos (from 0.4765 to 0.5494 ). Adding an explicit law hint further sharpens local kinematic behavior: both speed and acceleration similarity become positive (up to 0.052 and 0.021 ), and directional consistency reaches 0.5934 , but at the cost of slightly worse global trajectory RMSE and final-position error. This suggests that Sora2 can partially exploit law-conditioned prompts to align short-horizon motion trends with the intended physics, while still struggling to globally match the ground-truth path. For LTX-Video, the trade-offs are even more pronounced. Under Scene-only prompts, the model already attains the lowest trajectory RMSE on Newton’s-first-law videos ( 0.301 ), but with very poor directional consistency ( 0.2115 ) and low lever accuracy ( 8% ). The Scene+Action prompt dramatically increases directional consistency (to 0.5594 ) at the expense of higher trajectory error and only marginal lever improvements. Law-conditioned prompts further improve lever accuracy (to 12% )and reduce final-position error, but also produce highly unstable center-of-mass behavior (mask-center error > 500 pixels). Overall, these results indicate that current video models can be steered by richer physics-aware prompts toward more coher-ent local motion and lever outcomes, but this steering does not reliably translate into globally accurate, physically faithful trajectories, and may even destabilize other aspects of the scene. This analysis complements the main results by disentangling two factors: (i) how much current video generators rely on textual action descriptions versus visual setup; and (ii) whether explicit mention of the underlying physical law helps to enforce physically consistent motion, beyond purely semantic or aesthetic guidance. F. Case Studies 

The following case studies highlight how different classes of multimodal models handle physically grounded reasoning tasks. By examining both static VQA scenarios and dynamic video-based experiments, we identify characteristic failure modes such as misestimation of geometric cues, over-reliance on physics priors, and weakness in maintaining temporal or structural consistency. These examples illustrate not only the performance gaps across model families but also the broader challenge of integrating visual perception with physically plausible inference. 

F.1. VQA 

Our VQA case studies focus on situations where the underlying mechanics are simple but the visual evidence and textual priors can easily come into conflict. For each scenario, we show the input image, the multiple–choice question, the ground-truth label, and the free-form rationales produced by several representative VLMs. The lever-equilibrium example illustrates how different models apply torque reasoning: some attempt to estimate distances from the fulcrum and derive a numerical comparison, others assume symmetry or rely mainly on the mass labels, and they often disagree both with each other and with the benchmark annotation. This exposes a systematic difficulty in jointly using pixel-level geometry and symbolic physics when answering even very short mechanical questions. The center-of-mass example highlights a complementary failure mode: several models verbally recall the correct rule that a suspended object rotates until its Center of Mass lies vertically below the pivot, yet still misidentify the direction of rotation because they mis-locate the mass distribution in the image. In both cases, the detailed per-model thoughts and answers are printed inside the figure panels for readability, while the surrounding text distils the main qualitative patterns: current VLMs can often recite relevant physical laws but struggle to ground them in precise visual measurements, leading to inconsistent or self-contradictory predictions. Lever-Equilibrium VQA Case Study 

Image 

Input image for the lever-equilibrium VQA task. 

Question 

In the image, a lever has a fulcrum at the center with objects suspended on both sides. Observe the mass labels on each object and their distances from the fulcrum. After releasing the lever, what state will the lever be in? • A) Remain horizontally balanced • B) Left end sinks, right end rises • C) Right end sinks, left end rises • D) Cannot be determined 

Instructions: Look carefully at the image. Choose the correct answer from the options provided. Only return the letter of your answer (A, B, C, or D). Format: Your Thoughts: <thoughts > Answer: X 

Ground-truth answer: A

Lever-Equilibrium VQA Case Study - Model Responses 

Model Responses o4 mini Your Thoughts: The lever is balanced by torques . The 20 kg mass is about 1 unit from the fulcrum (torque ≈ 20 ), the 5 kg mass is about 8 units away (torque ≈ 40 ), so the right-hand torque wins and the right end sinks . Answer: C

grok-4 Your Thoughts: The image depicts a seesaw (lever) with a fulcrum at the center . There is a 20 kg mass on the left side and a 5 kg mass on the right. Both masses appear to be placed at equal distances from the fulcrum. Left torque = 20 × d. Right 

torque = 5 × d. Since 20 d > 5d, the left side sinks . Answer: B

Gemini 2.5 Pro Your Thoughts: 1. Left mass = 20 kg at approx. 2 units. 2. Right mass = 5 kg at approx. 4 units. Torque L = 40 ,Torque R = 20 . Left torque is greater → left end sinks . Answer: B

Analysis. Although the physics implies that the lever should not remain balanced, the benchmark’s ground-truth label is A. This case highlights a key challenge in physical VQA: some models (e.g., o4 mini) apply torque reasoning that contradicts the annotation, while others mis-estimate distances or assume symmetry. These discrepancies reveal differences in whether models rely on visual measurement, symbolic physics, or textual priors when interpreting mechanical scenes. Center-of-Mass VQA Case Study 

Image 

Input image for the center-of-mass VQA task. 

Question 

In the image, where is the hand-held suspension point located relative to the object’s center of gravity? • A) To the left of the center • B) To the right of the center • C) Above the center • D) At the center / No clear offset 

Instructions: Look carefully at the image. Choose the correct answer from the options provided. Only return the letter of your answer (A, B, C, or D). Format: Your Thoughts: <thoughts > Answer: X 

Ground-truth answer: A

Center-of-Mass VQA Case Study - Model Responses 

Model Responses 

o4 mini Your Thoughts: The red string is threaded through a hole at one corner of the triangular piece, which is clearly offset toward the left side relative to the object’s centroid. Answer: A

grok-4 Your Thoughts: The object hangs freely under gravity, so its Center of Mass must lie vertically below the suspension point . Because the string is vertical, the suspension point is above the center of gravity . Answer: C

Gemini 2.5 Pro Your Thoughts: A freely hanging object aligns such that the center of gravity lies directly below the suspension point. Therefore the suspension point is above the center . Answer: C

Analysis. This case exposes a divergence between geometric observation and physical inference . The benchmark label (A) is based on the visible fact that the string passes through a hole located on the left side of the cardboard piece. However, two models (grok-4 and Gemini 2.5 Pro) apply the physical rule that a hanging object’s Center of Mass must lie directly below 

the suspension point, leading them to answer C. These responses reveal that some models prioritize implicit physics priors 

over geometric cues, making this an effective diagnostic example for distinguishing visual versus physical reasoning modes in foundation models. F.2. Video Generation 

To further assess the strengths and limitations of current video generation models, we conduct detailed qualitative case studies under physically grounded scenarios. These experiments are designed to probe aspects of motion realism, short-and long-term temporal consistency, and object-to-object interaction fidelity, which go beyond conventional aesthetic or smoothness metrics. 

Case 1: Hanging-Method (Center of Mass Estimation). 

Figure 9 visualizes generation results for the hanging-method sequence, where a small wooden arrow is attached to a thread and released to swing freely until reaching equilibrium. The setup captures low-frequency, gravity-driven oscillations and offers a clear cue for assessing spatial stability and coupled motion between the thread and the suspended object. We observe marked variation across methods. Image-to-video transformers such as Sora 2 and Veo 3.1 generally preserve the rigidity and relative geometry of the object–string system while sustaining smooth damping over time. Diffusion-based models (Wan 2.2, LTX) exhibit locally plausible motion but sometimes fail to maintain proper tension at the joint, result-ing in spatial jitter or partial disappearance of the string. Smaller autoregressive models (CogVideo X, Cosmos Predict 2, Pyramid Flow) show pronounced frame-to-frame inconsistencies and degraded shape coherence, where the hanging arrow deforms or blends with the background. These artifacts indicate that while recent video models learn coarse dynamic pri-ors, they still struggle with enforcing near-rigid constraints and globally consistent motion patterns over extended temporal ranges. In particular, the inability to explicitly reason about physical tethering leads to motion drift or visual ”snapping” when the string’s anchor interacts with the moving body. 

Case 2: Rapid Paper-Pull Experiment. 

Figure 10 presents a contrasting fast-motion scenario in which a sheet of white paper is swiftly pulled from beneath a metallic spoon. Unlike the previous low-frequency oscillation task, this sequence emphasizes high-acceleration, high-contact dynamics with abrupt state transitions. The ground-truth motion reveals fine-grained frictional effects which the spoon remains nearly at rest as the sheet slips away. Across generative models, we find that temporal desynchronization is the dominant failure mode: the paper’s motion either lags behind or advances ahead of the contact event, producing physically implausible interactions. For several diffusion-based variants, the spoon is occasionally displaced or dragged due to blurred motion conditioning, suggesting that temporal diffu-sion sampling may propagate appearance priors faster than scene dynamics. Some transformer-based models show improved temporal sharpness but still exhibit under-damped temporal blending (ghosting) or inconsistent texture tracking on the pa-per surface. The best outputs, although qualitatively convincing at a glance, lack the precise sub-frame contact behavior observed in the reference recording. These cases expose the absence of explicit physical constraints within the generative objective—such as conservation of momentum or mass-center stability—which are crucial for accurate modeling of sudden impulsive interactions. 

Insight and Discussion. Together, these two representative cases illustrate complementary dimensions of the video genera-tion challenge. The hanging-method experiment emphasizes the need for long-range temporal stability and physically con-sistent trajectories under slow, continuous motion; the rapid paper-pull experiment stresses high-frequency responsiveness and contact-aware motion reasoning. While large foundation-scale models have made significant progress in reproducing globally coherent motion and rendering detail, they continue to exhibit a mismatch between visual realism and physical re-alism. These observations point toward integrating explicit physics priors or differentiable simulation modules into future generative architectures to bridge this gap. Incorporating such constraints could enhance both the temporal consistency and the causal fidelity of next-generation video generators. Figure 9. Qualitative comparison on the hanging-method sequence, where an arrow is suspended by a string and allowed to hang freely. The task assesses temporal stability and physical realism in model-generated videos. Figure 10. Qualitative comparison on the rapid paper-pull experiment. A white sheet is quickly pulled from beneath a spoon to evaluate temporal consistency, object-surface contact, and motion realism. G. Future work 

While P HYSICS MIND offers a first step toward systematically probing physical reasoning in modern vision–language and video generation models, it also exposes several limitations that suggest natural directions for future work. First, our benchmark currently focuses on three canonical laws in relatively controlled tabletop scenes. Extending the suite to cover a broader range of phenomena, such as friction, elastic collisions, conservation of energy, or multi-body interactions, would allow a more complete assessment of physical competence. Increasing scene complexity (e.g., clutter, occlusions, and 3D camera motion) and lengthening the temporal horizon are also important for stress-testing whether models can maintain physically consistent behavior under more realistic conditions. Second, our evaluation metrics are deliberately designed around a small set of interpretable quantities (final state, Center of Mass, trajectory, and kinematics). There is room to develop richer metrics that better align with human judgements of physical plausibility, or that decompose errors into complementary categories (e.g., geometric vs. dynamical mistakes). Another direction is to more tightly couple video metrics with downstream decision-making tasks, such as predicting the outcome of interventions or selecting actions that satisfy physical constraints. Third, we have used P HYSICS MIND purely as a diagnostic benchmark. An open question is how to use such structured physics data during training: for example, as a curriculum for targeted finetuning, as an auxiliary self-supervision signal for world models, or as a regularizer that encourages consistency between verbal answers and visual rollouts. Studying whether improvements on P HYSICS MIND transfer to broader embodied or robotics benchmarks would also help clarify the practical value of stronger physical priors. Finally, our analysis is limited to a snapshot of current model families. As multimodal foundation models and video genera-tors continue to evolve, it will be important to update the benchmark with new architectures and training paradigms, and to refine the tasks based on community feedback. We hope that P HYSICS MIND can serve as a compact, extensible testbed that supports this iterative process rather than a definitive verdict on physical reasoning in large models.