Title: Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating

URL Source: https://arxiv.org/pdf/2601.16097v1

Published Time: Fri, 23 Jan 2026 01:59:41 GMT

Number of Pages: 9

Markdown Content:
# Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating 

## Makbule Gulcin Ozsoy 

## Neo4j / London, UK 

## makbule.ozsoy@neo4j.com 

## Abstract 

Large Language Models enable users to access database using natural language interfaces us-ing tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multi-lingual support. This work investigates a scal-able multilingual Text2Cypher, aiming to sup-port new languages without re-running full fine-tuning, avoiding manual hyperparameter tun-ing, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dy-namic gating. Experimental results show that the fusion MLP recovers around 75% of the ac-curacy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incre-mental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expen-sive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task. 

## 1 Introduction 

Database query languages, such as SQL (for re-lational databases), SPARQL (for RDF graphs), and Cypher (for graph databases), enable effi-cient data access. Recent advances in large lan-guage models (LLMs) have enabled natural lan-guage interfaces,like Text2SQL, Text2SPARQL, and Text2Cypher, that convert user questions into database queries. While these systems enhance database accessibility, most research focuses on English,leaving multilingual capabilities underex-plored (Jannuzzi et al., 2024; Geng et al., 2024). This work focuses on scalable multilingual Text2Cypher, incrementally supporting new lan-guages without full retraining (Figure 1). The figure shows language expansion over time: ini-tially (T1) only English is supported. Then Spanish (T2) and Turkish (T3) support are included. At each stage, users query the same graph database in supported languages, expecting identical Cypher output: MATCH (a:Article)-[:MENTIONS]->(c:Company {name: "Acme Inc."}) RETURN a.title .Prior work (Ozsoy and Tai, 2025) established a benchmark across these languages, showing base models favor high-resource English over medium-or low-resource Spanish and Turkish. While joint multilingual fine-tuning improves overall perfor-mance and narrows performance gap across lan-guages,it requires full retraining when a new lan-guage is included, which is computationally expen-sive. In this work, we address three practical con-straints for scalable Text2Cypher: (i) incremental language expansion without full retraining, (ii) au-tomated hyperparameter selection, and (iii) perfor-mance approaching joint multilingual fine-tuning. For this purpose, we train language-specific LoRA adapters for each language, then combine them via uniform linear merging or a learned fusion MLP with dynamic gating. Our main contributions are: • We explore LoRA adapter merging for multi-lingual Text2Cypher, comparing uniform lin-ear merging and fusion MLP gating. The eval-uation results showed that fusion MLP with dynamic adapter gating outperforms uniform linear merging across all three languages. • Fusion MLP recovers 75% of joint fine-tuning gains using a subset of the training data (i.e., 20% in our experiments). It delivers balanced performance across high- and low-resourced languages. 

> arXiv:2601.16097v1 [cs.CL] 22 Jan 2026 Figure 1: Incremental language expansion in multilingual Text2Cypher. At T1, only English is supported. Spanish (T2) and Turkish (T3) are added via new LoRA adapters + MLP retraining, without touching existing adapters. All speakers receive identical Cypher output.

• Fusion MLP enables incremental scaling, where adding a new language requires only one LoRA adapter and lightweight MLP re-training. • We provide a practical framework for multi-lingual Text2Cypher, balancing performance and computational cost. The paper is organized as follows: Section 2 re-views related work. Section 3 describes the method-ology. Section 4 presents evaluation results. Sec-tion 5 concludes the paper. 

## 2 Related Work 

This section reviews related work on large language models (LLMs) for non-English content, adapter merging and fusion approaches and multilingual approaches for database query generation tasks. 

2.1 LLMs and Non-English Content 

Recent advances in LLMs have improved multi-lingual capabilities (Lai et al., 2024). Multilin-gual LLMs learn cross-lingual representations from multi-language training data, transferring word as-sociations and grammatical patterns from high-resource languages like English to lower-resource ones. However, English-dominant corpora cause models to internalize English-specific linguistic pat-terns and assumptions (Nicholas and Bhatia, 2023; Zhao et al., 2024a), leading to significantly better performance on high-resource and linguistically similar languages (Nicholas and Bhatia, 2023). Mishra et al. (Mishra et al., 2025) found that larger models show more consistent behavior across lan-guages with similar structure, though performance still varies by domain and resource level. Several approaches have been proposed to im-prove multilingual abilities of LLMs. Some works continue pretraining on multilingual parallel data (Yang et al., 2023; Zhu et al., 2023). Oth-ers fine-tune models on multilingual instruction data (Üstün et al., 2024; Luo et al., 2023; Li et al., 2023; Lai et al., 2023). Another line of work uses cross-lingual prompting at inference time (Huang et al., 2023; Etxaniz et al., 2023), or studies the internal mechanisms of these models (Zhao et al., 2024a; Kargaran et al., 2024; Zhong et al., 2024; Schut et al., 2025; Bandarkar et al., 2024). 

2.2 Adapter Merging and Fusion Methods 

Adapter merging and fusion methods have also gained attention for multilingual and multi-task adaptation. Techniques such as Task Arithmetic (Il-harco et al., 2022), TIES (Yadav et al., 2023) and DARE (Yu et al., 2024) combine adapters by com-puting and scaling their parameter differences, of-ten followed by a post-processing like trimming or rescaling. Furthermore, LoRA Soups (Prab-hakar et al., 2025) and LoRA-LEGO (Zhao et al., 2024b) presented ways for merging adapters us-ing weighted or modular combinations. Adapter-Fusion (Pfeiffer et al., 2021) and UniPELT (Mao et al., 2022) use gating mechanisms to dynami-cally combine multiple adapters. In terms of mul-tilingual support, AdaMergeX (Zhao et al., 2025) separates task and language ability into different adapters and merges them using structure-adaptive merging. MLM (Lee et al., 2025) trains a task and a language adapter and combines them through parameter-space interpolation followed by a light post-merging step. While these methods have shown promise in multilingual and cross-task trans-fer, their applicability to structured query genera-tion tasks like Text2Cypher remains unexplored. 

2.3 Multilingual Database Query Generation 

Most work on multilingual database query gener-ation from natural language has focused on the Text2SQL task (Dou et al., 2023; José and Cozman, 2021; Huang et al., 2025; Pham et al., 2025). Sev-eral studies translate the English Spider dataset (Yu et al., 2018) into other languages, such as Chinese in CSpider (Min et al., 2019), Turkish in TUR-Spider (Kanburoglu and Tek, 2024), Arabic in Ar-Spider (Almohaimeed et al., 2024), and mul-tiple languages in MultiSpider (Dou et al., 2023) and MultiSpider 2.0 (Pham et al., 2025). Other work has created new multilingual datasets, such as StatBot.Swiss (Nooralahzadeh et al., 2024) in English and German. Beyond datasets, several stud-ies evaluate LLM performance across languages for Text2SQL, for example in Portuguese (Jannuzzi et al., 2024) and in Russian (Bakshandaeva et al., 2022). Overall, these works show that performance of LLMs on multilingual Text2SQL is still uneven, with clear performance gaps between languages. These findings motivate the need for methods that reduce this gap in a practical and scalable way. For the Text2SPARQL task, there are fewer mul-tilingual datasets. Some datasets and methods are designed for multilingual question answering (Cui et al., 2022; Srivastava et al., 2024; Perevalov et al., 2024). Recently, the Text2SPARQL Chal-lenge (Committee, 2025) released a dataset with questions in English and Spanish. Perevalov et al. (Perevalov and Both, 2024) studied the reverse task, translating SPARQL to natural language, and evaluated English, German, and Russian. For the Text2Cypher task, Ozsoy et al. (Ozsoy and Tai, 2025) created a multilingual dataset in 

> Table 1: Instructions used for Text2Cypher task

Type Instruction prompt 

System Instruct. Task: Generate Cypher statement to query a graph database. Instructions: Use only the provided relationship types and properties in the schema. Do not use any other relationship types or properties that are not provided in the schema. Do not include any explana-tions or apologies in your responses. Do not respond to any questions that might ask anything else than for you to construct a Cypher statement. Do not include any text except the generated Cypher statement. User Instruct. Generate Cypher statement to query a graph database. Use only the provided relationship types and properties in the schema. Schema: {schema} 

Question: {question} 

Cypher output: English, Spanish, and Turkish, and evaluated both base and fine-tuned models. Their results show that base model performance varies widely across languages, while joint multilingual fine-tuning re-duces these disparities. However, full retraining remains computationally expensive for incremen-tal language expansion. We address this gap by exploring parameter-efficient methods that enable new languages without repeated full fine-tuning, while maintaining performance close to joint mul-tilingual training. 

## 3 Methodology 

We address multilingual Text2Cypher deployment challenges by training language-specific LoRA adapters for English (EN), Spanish (ES), and Turk-ish (TR) on the benchmark dataset (Ozsoy and Tai, 2025), then combining them via (i) uniform linear merging or (ii) fusion MLP with dynamic gating. Prior joint multilingual fine-tuning (Ozsoy and Tai, 2025) achieves strong performance but requires full retraining for new languages. Figure 2 compares these approaches: 1) Joint fine-tuning requires com-plete retraining per language expansion. (2) Uni-form linear merging applies static weights across adapters. (3) Fusion MLP dynamically routes each input to the optimal language adapter. Figure 2: Three approaches for multilingual Text2Cypher. (1) Joint fine-tuning requires complete retraining per language expansion. (2) Uniform linear merging applies static weights across adapters. (3) Fusion MLP dynamically routes each input to the optimal language adapter. 

3.1 Per-Language Adapters 

For each target language, we train a separate LoRA adapter on the multilingual Text2Cypher dataset (Ozsoy and Tai, 2025). This dataset con-tains around 12,000 parallel training samples per language, where identical questions are expressed in different languages but map to the same Cypher query output. LoRA (Hu et al., 2022) decomposes updated weight matrix into a frozen base component W0 ∈

Rd×k and low-rank trainable component ∆W =

BA , where A ∈ Rr×k and B ∈ Rd×r . The matri-ces A and B are trainable and rank r ≪ min( d, k ).

h = W0x + ∆ W x = W0x + BAx 

In our setup, each adapter per language uses the identical LoRA hyper-parameters (e.g., rank r = 8 ,

α = 16 ). These adapters capture both Text2Cypher task knowledge and language-specific patterns. 

3.2 Uniform Linear Merging 

Using the per-language LoRA adapters from the previous subsection, we first apply uniform linear merging based on Task Arithmetic (Ilharco et al., 2022). The adapters are combined as a weighted sum (Mangrulkar and Paul, 2024) with fixed uni-form weights ( wi = 1 /3) without hyperparameter search: For example, three adapters (A1, B 1), (A2, B 2)

and (A3, B 3) are merged using the weights 

[w1, w 2, w 3] by the following equations: 

Amerged = w1 · A1 + w2 · A2 + w3 · A3

Bmerged = w1 · B1 + w2 · B2 + w3 · B3

This produces a single merged adapter suitable for all languages. Linear merging applies identical weights, serving as our static baseline for compari-son. 

3.3 Fusion MLP with Dynamic Gating 

Unlike static merging, the fusion MLP dynami-cally routes inputs across adapters. Since base embeddings are language-agnostic, we use adapter previews with logits from the last 200 tokens per adapter. These capture language-specific questions located at the end of the prompt (Table 1) and enable effective routing when concatenated with pooled base embeddings hbase .

fipreview = Mean (logits i(input_ids [: , −200 :])) 

fpreview = [ f1preview , f2preview , f3preview ]

w(x) = softmax  MLP ([ Mean (hbase ), fpreview ]) 

The fused logits combine each adapter’s full-sequence output: logits fused =

> n=3

X

> i=1

wi(x) · logits i(input_ids )

Here w(x) contains per-input weights for each language adapter (e.g., [0 .75 , 0.15 , 0.10] ). Fusion training uses a subset of data with frozen adapters, enabling incremental scaling where new languages require only one LoRA adapter and MLP retrain-ing. Table 2: ROUGE-L score on multilingual Text2Cypher test set. Superscript values show absolute improvements over base model. 

Method English Spanish Turkish Avg. Base 0.65 0.60 0.55 0.60 

EN-Only FT 0.86 +0 .21 0.80 +0 .20 0.71 +0 .16 0.79 +0 .19 

Joint Multilang FT 0.86 +0 .21 0.85 +0 .25 0.83 +0 .28 0.85 +0 .25 

Linear Merge 0.79 +0 .14 0.76 +0 .16 0.71 +0 .16 0.75 +0 .15 

Fusion MLP 0.80 +0 .15 0.80 +0 .20 0.78 +0 .23 0.79 +0 .19 

> Table 3: Example dynamic routing weights learned by fusion MLP. Spanish and Turkish questions include English translations in parentheses.

Language Question Weights [EN, ES, TR] English Which conferences have the highest number of papers in Robotics presented by authors from ’Tsinghua University’ [0.83 , 0.06, 0.11] 

Spanish ¿Cuáles son los 5 principales proveedores por número de productos suministrados? 

(Which are the 5 main suppliers by number of products supplied?) 

[0.03, 0.96 , 0.01] 

Turkish Satı¸ sları en yüksek olan ¸ sirketin genel merkezi nerededir? 

(Where is the headquarters of the company with the highest sales?) 

[0.07, 0.15, 0.78 ]

## 4 Experimental Results 

We evaluate how merging per-language LoRA adapters via uniform linear merging and learned fu-sion MLP affects Text2Cypher performance, com-paring against baseline foundational and jointly fine-tuned multilingual models. 

4.1 Experimental Setup 

We conduct experiments using the train and test sets from the multilingual Text2Cypher datasets (Oz-soy and Tai, 2025). The datasets are composed of samples for English (EN), Spanish (ES) and Turk-ish (TR), where each sample contains ’question’, ’schema’, ’Cypher query’ and additional metadata fields. The test set has 4,783 parallel samples per language. The training set contains around 12,000 samples per language (36,000 in total), structured as: around 6,750 questions are shared across all three languages, around 1,500 questions are shared per language pair (EN–ES, EN–TR, ES–TR), and around 3,800 of them are unique per language. We use the ‘Meta-Llama-3.1-8B-Instruct‘ as the baseline model. For the Text2Cypher task, we use the same prompts as prior work (Ozsoy et al., 2025; Ozsoy and Tai, 2025), shown in Table 1. After gen-eration, a post-processing step is used for removing unwanted text, such as the ’cypher:’ suffix. We use the HuggingFace Evaluate library (Hugging-Face, 2024) to compute evaluation metrics. We present ROUGE-L score as our performance met-ric, which compares generated and ground-truth Cypher queries based on their textual content. In addition, we execute both generated and ground-truth Cypher queries and compare their outputs af-ter lexicographic sorting. For this execution-based evaluation, we use Exact-Match score as the per-formance metric. 

4.2 Performance Comparison 

We train individual LoRA adapters per language (∼12K samples each), then merge using (1) uni-form linear merging and (2) fusion MLP with dy-namic gating. For uniform linear merging, we used equal weights ( wi = 0 .333 ) per adapter in order to avoid manual hyperparameter search while still es-tablishing a simple, strong baseline. For the fusion MLP, we sampled 2,500 instances per language (7,500 total) specifically from the shared questions across all languages, using only 20% of the full training data. Baselines include the base model, English-only fine-tuning ( ∼20.5K samples), and joint multilin-gual fine-tuning ( ∼36K samples). We indepen-Table 4: Comparing training costs when incrementally adding new languages in our Text2Cypher setup. 

Languages Joint Multilang FT Fusion MLP 1st lang 12K instances 12K instances 

2nd lang 24K instances 12K instances + (2 ×2.5K instances) = 17K instances 

3rd lang 36K instances 12K instances + (3 ×2.5K instances) = 19.5K instances 

4th lang 48K instances 12K instances + (4 ×2.5K instances) = 22K instances dently replicated the experimental results from prior work (Ozsoy and Tai, 2025) using identical evaluation setup, obtaining matching evaluation metrics. Table 2 shows performance of each method. Joint multilingual fine-tuning achieves the high-est scores across all languages and serves as our upper bound since it learns from all languages at once. Fusion MLP outperforms uniform linear merging in all languages without needing manual hyperparameter tuning and shows balanced per-formance across high- (EN), medium- (ES), and low-resource (TR) languages. It achieves around 75% of joint fine-tuning’s performance gains (0.19 vs 0.25 on avg.), using only a subset of training data, while not slowing down the inference speed. Furthermore, analysis of execution-based compar-isons between generated and ground-truth Cypher query results shows that linear merging improves the Exact-Match score by 0.03 − 0.05 , whereas Fusion MLP yields larger gains of 0.05 − 0.06 .Table 3 presents example routing weights for language-specific inputs. The fusion MLP correctly assigns higher weights to the matching language adapter, explaining its superior performance over static merging. 

4.3 Training Efficiency 

The main advantage of fusion MLP emerges when we incrementally improve model capability by adding new languages. Joint multilingual fine-tuning requires full retraining when adding lan-guages (e.g., 24 K → 36 K → 48 K samples). In contrast, fusion MLP scales incrementally: each new language needs only (1) one LoRA adapter (e.g., ∼12K instances), and (2) MLP update (e.g., 

∼2.5K per language), where previous adapters are kept frozen. Table 4 gives a hypothetical example, compar-ing training costs when incrementally adding new languages in our Text2Cypher setup. In the exam-ple, adding a fourth language requires 48K samples for joint fine-tuning but only 22K for fusion MLP, with a 54% reduction. This modular approach en-ables continuous language expansion without the computational cost of full retraining. 

## 5 Conclusion 

This work addresses practical multilingual Text2Cypher challenges, namely incremental language expansion without full retraining, auto-mated hyperparameter selection, and performance approaching joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish, combining them via uniform linear merging or fusion MLP with dynamic gating. The fusion MLP outperforms linear merging (0.79 vs 0.75 average ROUGE-L score) across all languages. It recovers around 75% of joint multilingual fine-tuning gains using only a subset of training data. This approach delivers balanced performance across high-, medium-, and low-resource languages and enables incremental scaling, where adding a new language requires only one LoRA adapter and a lightweight MLP retraining. These results demonstrate that learned adapter fusion can be used as a practical alternative to full multilingual fine-tuning. The modular frame-work enables incremental language expansion with competitive Text2Cypher accuracy. Future work will extend this approach to additional languages. Furthermore, we will explore alternative fusion or alignment methodologies in order to support multi-lingual Text2Cypher. 

## Limitations 

While fusion MLP enables incremental language expansion with competitive Text2Cypher perfor-mance, several limitations remain. Current evaluation uses a single benchmark (Oz-soy and Tai, 2025) covering only English, Spanish, and Turkish. Generalization across different do-mains, graph schemas, query complexities, and diverse languages requires further validation. We compare against uniform linear merging but exclude other adapter fusion techniques that may yield complementary benefits. We have not ana-lyzed inference time overhead from dynamic rout-ing, which could be critical for production deploy-ment. Heuristics, like the fixed 200-token preview window, could be improved through adaptive meth-ods. 

## Declaration on Generative AI Usage 

During the preparation of this work, the author(s) used Generative AI tools in order to: ’Improve writing style’, ’Paraphrase and reword’, ’Code de-bugging and fixes’. After using these tool(s) or service(s), the author(s) reviewed and edited the content as needed and take(s) full responsibility for the publication’s content. 

## References 

Saleh Almohaimeed, Saad Almohaimeed, Mansour Al Ghanim, and Liqiang Wang. 2024. Ar-spider: Text-to-sql in arabic. In Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing ,pages 1024–1030. Daria Bakshandaeva, Oleg Somov, Ekaterina Dmitrieva, Vera Davydova, and Elena Tutubalina. 2022. Pauq: Text-to-sql in russian. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 2355–2376. Lucas Bandarkar, Benjamin Muller, Pritish Yuvraj, Rui Hou, Nayan Singhal, Hongjiang Lv, and Bing Liu. 2024. Layer swapping for zero-shot cross-lingual transfer in large language models. arXiv preprint arXiv:2410.01335 .TEXT2SPARQL Challenge Organizing Committee. 2025. Text2sparql challenge 2025. Co-located with Text2KG at ESWC 2025. Ruixiang Cui, Rahul Aralikatte, Heather Lent, and Daniel Hershcovich. 2022. Compositional gener-alization in multilingual semantic parsing over wiki-data. Transactions of the Association for Computa-tional Linguistics .Longxu Dou, Yan Gao, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, and Jian-Guang Lou. 2023. Multispider: towards benchmarking multilin-gual text-to-sql semantic parsing. In Proceedings of the AAAI Conference on Artificial Intelligence , vol-ume 37, pages 12745–12753. Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier Lopez de Lacalle, and Mikel Artetxe. 2023. Do multilin-gual language models think better in english? arXiv preprint arXiv:2308.01223 .Xiang Geng, Ming Zhu, Jiahuan Li, Zhejian Lai, Wei Zou, Shuaijie She, Jiaxin Guo, Xiaofeng Zhao, Yinglu Li, Yuang Li, and 1 others. 2024. Why not transform chat large language models to non-english? 

arXiv preprint arXiv:2405.13923 .Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR , 1(2):3. Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei. 2023. Not all languages are created equal in llms: Improving multilingual capability by cross-lingual-thought prompting. arXiv preprint arXiv:2305.07004 .Yiming Huang, Jiyu Guo, Wenxin Mao, Cuiyun Gao, Peiyi Han, Chuanyi Liu, and Qing Ling. 2025. Ex-ploring the landscape of text-to-sql with large lan-guage models: Progresses, challenges and opportuni-ties. arXiv preprint arXiv:2505.23838 .HuggingFace. 2024. Huggingface evaluate. https: //huggingface.co/evaluate-metric .Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-man, Suchin Gururangan, Ludwig Schmidt, Han-naneh Hajishirzi, and Ali Farhadi. 2022. Edit-ing models with task arithmetic. arXiv preprint arXiv:2212.04089 .Marcelo Jannuzzi, Yuriy Perezhohin, Fernando Peres, Mauro Castelli, and Aleš Popoviˇ c. 2024. Zero-shot prompting strategies for table question answering with a low-resource language. Emerging Science Journal , 8(5):2003–2022. Marcelo Archanjo José and Fabio Gagliardi Cozman. 2021. mrat-sql+ gap: a portuguese text-to-sql trans-former. In Intelligent Systems: 10th Brazilian Con-ference, BRACIS 2021, Virtual Event, November 29– December 3, 2021, Proceedings, Part II 10 , pages 511–525. Springer. Ali Bugra Kanburoglu and Faik Boray Tek. 2024. Tur-spider: A turkish text-to-sql dataset and llm-based study. IEEE Access .Amir Hossein Kargaran, Ali Modarressi, Nafiseh Nikeghbal, Jana Diesner, François Yvon, and Hin-rich Schütze. 2024. Mexa: Multilingual evaluation of english-centric llms via cross-lingual alignment. 

arXiv preprint arXiv:2410.05873 .Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A Rossi, and Thien Huu Nguyen. 2023. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. 

arXiv preprint arXiv:2307.16039 .Wen Lai, Mohsen Mesgar, and Alexander Fraser. 2024. Llms beyond english: Scaling the multilingual ca-pability of llms with cross-lingual feedback. arXiv preprint arXiv:2406.01771 .Jung Lee, Taero Kim, and Nikhil Verma. 2025. Mlm: Multi-linguistic lora merging. In NeurIPS 2025 Workshop on Efficient Reasoning .Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. 2023. Bactrian-x: Multilingual replicable instruction-following mod-els with low-rank adaptation. arXiv preprint arXiv:2305.15011 .Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu, Chenyang Zhao, Donglei Zhang, and 1 others. 2023. Yayi 2: Multi-lingual open-source large language models. arXiv preprint arXiv:2312.14862 .Sourab Mangrulkar and Sayak Paul. 2024. Peft welcomes new merging methods. Https://huggingface.co/blog/peft_merging. Yuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-hairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. 2022. Unipelt: A unified framework for parameter-efficient language model tuning. In Pro-ceedings of the 60th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: Long Papers) , pages 6253–6264. Qingkai Min, Yuefeng Shi, and Yue Zhang. 2019. A pilot study for chinese sql semantic parsing. arXiv preprint arXiv:1909.13293 .Debangan Mishra, Arihant Rastogi, Agyeya Singh Negi, Shashwat Goel, and Ponnurangam Kumaraguru. 2025. What if i ask in alia lingua? measuring func-tional similarity across languages. In Proceedings of the 5th Workshop on Multilingual Representation Learning (MRL 2025) , pages 496–506. Gabriel Nicholas and Aliya Bhatia. 2023. Lost in trans-lation: large language models in non-english content analysis. arXiv preprint arXiv:2306.07377 .Farhad Nooralahzadeh, Yi Zhang, Ellery Smith, Sabine Maennel, Cyril Matthey-Doret, Raphaël de Fondville, and Kurt Stockinger. 2024. Statbot. swiss: bilingual open data exploration in natural language. arXiv preprint arXiv:2406.03170 .Makbule Gulcin Ozsoy, Leila Messallem, Jon Besga, and Gianandrea Minneci. 2025. Text2cypher: Bridg-ing natural language and graph databases. In Pro-ceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK) , pages 100–108. Makbule Gulcin Ozsoy and William Tai. 2025. Text2cypher across languages: Evaluating and fine-tuning llms. arXiv preprint arXiv:2506.21445 .Aleksandr Perevalov and Andreas Both. 2024. To-wards llm-driven natural language generation based on sparql queries and rdf knowledge graphs. In 3rd international workshop on knowledge graph genera-tion from text (Text2KG) at ESWC .Aleksandr Perevalov, Andreas Both, and Axel-Cyrille Ngonga Ngomo. 2024. Multilingual question answer-ing systems for knowledge graphs–a survey. Seman-tic Web , 15(5):2089–2124. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2021. Adapterfusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th con-ference of the European chapter of the association for computational linguistics: main volume , pages 487–503. Khanh Trinh Pham, Thu Huong Nguyen, Jun Jo, Quoc Viet Hung Nguyen, and Thanh Tam Nguyen. 2025. Multilingual text-to-sql: Benchmarking the limits of language models with collaborative language agents. 

arXiv preprint arXiv:2509.24405 .Akshara Prabhakar, Yuanzhi Li, Karthik Narasimhan, Sham Kakade, Eran Malach, and Samy Jelassi. 2025. Lora soups: Merging loras for practical skill compo-sition tasks. In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track , pages 644–655. RunPod. 2024. Runpod. https://www.runpod.io/ .Lisa Schut, Yarin Gal, and Sebastian Farquhar. 2025. Do multilingual llms think in english? arXiv preprint arXiv:2502.15603 .Nikit Srivastava, Mengshi Ma, Daniel Vollmers, Hamada Zahera, Diego Moussallem, and Axel-Cyrille Ngonga Ngomo. 2024. Mst5–multilingual question answering over knowledge graphs. arXiv preprint arXiv:2407.06041 .Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, and 1 others. 2024. Aya model: An instruction finetuned open-access multilingual language model. 

arXiv preprint arXiv:2402.07827 .Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. 2023. Ties-merging: Re-solving interference when merging models. Ad-vances in Neural Information Processing Systems ,36:7093–7115. Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. 2023. Bigtranslate: Augmenting large language models with multilingual translation ca-pability over 100 languages. arXiv preprint arXiv:2305.18098 .Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorb-ing abilities from homologous models as a free lunch. In Forty-first International Conference on Machine Learning .Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-ing Yao, Shanelle Roman, and 1 others. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. 

arXiv preprint arXiv:1809.08887 .Jun Zhao, Zhihao Zhang, Luhui Gao, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024a. Llama beyond english: An empirical study on language capability transfer. arXiv preprint arXiv:2401.01055 .Yiran Zhao, Wenxuan Zhang, Huiming Wang, Kenji Kawaguchi, and Lidong Bing. 2025. Adamergex: Cross-lingual transfer with large language models via adaptive adapter merging. In Proceedings of the 2025 Conference of the Nations of the Ameri-cas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , pages 9785–9800. Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Kun Kuang, and Fei Wu. 2024b. Merging loras like playing lego: Pushing the modularity of lora to extremes through rank-wise clustering. arXiv preprint arXiv:2409.16167 .Chengzhi Zhong, Fei Cheng, Qianying Liu, Junfeng Jiang, Zhen Wan, Chenhui Chu, Yugo Murawaki, and Sadao Kurohashi. 2024. Beyond english-centric llms: What language do multilingual language mod-els think in? arXiv preprint arXiv:2408.10811 .Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023. Extrapolating large language models to non-english by aligning languages. arXiv preprint arXiv:2308.04948 .

## A Fusion MLP Architecture 

The fusion MLP architecture with dynamic gating is shown in Figure 3. Notation: B = batch size, 

T = sequence length, H = hidden dimension, V =vocabulary size, 3 = number of adapters (EN, ES, TR). 

## B Fine-tuning parameters 

For fine-tuning, we used a RunPod (RunPod, 2024) GPU environment with a single A40 machine. The parameters used are presented in Table 5. 

Figure 3: Architecture of fusion MLP with dynamic gating. Table 5: Fine-tuning Parameters 

Model & Tokenizer Parameters 

max_seq_length: 2048 dtype: torch.bfloat16 load_in_4bit: False truncation_side: "left" padding_side: "left" 

PEFT Parameters 

r: 8 target_modules: [ "q_proj", "k_proj", "v_proj", "o_proj" ] lora_alpha: 16 lora_dropout: 0 

Training Arguments 

per_device_train_batch_size: 2 gradient_accumulation_steps: 4 warmup_steps: 5 num_train_epochs: 1 learning_rate: 2e-4 optim: "adamw_8bit" weight_decay: 0.01 lr_scheduler_type: "linear"