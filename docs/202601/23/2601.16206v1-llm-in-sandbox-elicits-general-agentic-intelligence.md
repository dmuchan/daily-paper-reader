# LLM-in-Sandbox Elicits General Agentic Intelligence
# LLM-in-Sandbox 激发通用智能体智能

**Authors**: Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.16206v1
<<<<<<< HEAD
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span> <span class="tag-label tag-pink">keyword:ppo</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 9.0
**Evidence**: introduces reinforcement learning for LLM agents aligning with RL and PPO interests and LLM queries
=======
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 8.0
**Evidence**: combines LLM architectures with reinforcement learning for agentic intelligence
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002

---

## Abstract
We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.

## 摘要
<<<<<<< HEAD
我们引入了 LLM-in-Sandbox，使
=======
我们引入了 LLM-in-Sandbox，使大语言
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002

---

## 论文详细总结（自动生成）

<<<<<<< HEAD
这篇论文由微软亚洲研究院、中国人民大学和清华大学的研究团队合作完成，探讨了如何通过为大语言模型（LLM）提供一个“代码沙盒”（虚拟计算机环境）来激发其在非代码领域的通用智能。

以下是对该论文的结构化深入总结：

### 1. 论文的核心问题与整体含义（研究动机和背景）
*   **核心问题**：如何进一步解锁 LLM 的潜力，使其从单纯的文本生成器转变为能够解决复杂现实问题的通用智能体？
*   **背景**：目前的智能体框架（如 Claude Code）主要局限于软件工程（SWE）领域。作者认为，计算机是人类最通用的工具，其具备的**外部资源访问、文件管理、代码执行**三大元能力，可以被 LLM 用来解决数学、科学、长文本理解等非代码任务。
*   **整体含义**：论文提出了 **LLM-in-Sandbox** 范式，证明了强大的模型无需额外训练即可自发利用沙盒解决复杂任务，并提出了一种利用通用数据进行强化学习（RL）的方法，以增强弱模型的智能体能力。

### 2. 论文提出的方法论
*   **核心思想**：将 LLM 置于一个轻量级的 Ubuntu Docker 容器中，赋予其终端访问权限，使其通过多轮交互（ReAct 模式）自主探索并解决任务。
*   **关键技术细节**：
    *   **轻量级通用沙盒**：不同于 SWE 智能体需要复杂的预配置环境，该沙盒仅提供基础 Python 环境，鼓励模型自主安装工具（如 `pip install`）。
    *   **三大核心工具**：`execute_bash`（执行命令）、`str_replace_editor`（文件读写编辑）、`submit`（提交结果）。
    *   **LLM-in-Sandbox-RL**：针对弱模型在沙盒中“乱撞”的问题，提出使用**基于上下文的任务（Context-based tasks）**进行强化学习。关键创新在于将背景文档存放在沙盒文件系统中而非 Prompt 中，强制模型学习“搜索-读取-分析”的探索路径。
*   **算法流程**：模型接收任务 -> 思考并调用工具 -> 沙盒返回观测结果 -> 循环交互 -> 将最终答案写入指定文件（如 `answer.txt`）并提交。

### 3. 实验设计
*   **数据集/场景**：涵盖 6 个非代码领域：
    1.  **数学**：AIME25（奥数级题目）。
    2.  **物理**：UGPhysics（大学物理）。
    3.  **化学**：ChemBench。
    4.  **生物医学**：MedXpertQA。
    5.  **长文本理解**：AA-LCR（平均 100K token 的多文档推理）。
    6.  **指令遵循**：IFBench。
    7.  **软件工程**：SWE-bench Verified（用于验证通用训练是否损害代码能力）。
*   **对比方法**：
    *   **Vanilla LLM**：直接生成答案（不使用沙盒）。
    *   **LLM-in-Sandbox**：使用沙盒进行多轮探索。
    *   **LLM-RL**：在纯文本模式下进行强化学习（对比基准）。
*   **评估模型**：包括 Claude-Sonnet-4.5、GPT-5、DeepSeek-V3.2、Kimi-K2、MiniMax-M2、Qwen3 等顶尖模型。

### 4. 资源与算力
*   **硬件环境**：实验使用了单台 **NVIDIA DGX 节点**，配备 **2TB 系统内存**。
*   **推理框架**：使用了 SGLang 和 vLLM 进行本地模型部署。
*   **训练细节**：使用了 GRPO++ 算法。Qwen3-4B 训练了 150 步，Qwen3-30B 训练了 50 步。
*   **算力开销**：文中指出沙盒本身的内存开销极低（单个容器空闲 50MB，峰值 200MB），512 个并发沙盒仅占用系统 5% 的内存。

### 5. 实验数量与充分性
*   **实验规模**：论文在 7 个不同规模和类型的模型上进行了跨 6 个领域的测试，每项任务包含数百个样本（如 AIME 重复 16 次取平均），实验组数非常多。
*   **消融实验**：针对训练数据来源（数学 vs. SWE vs. 通用数据）和上下文存放位置（Prompt vs. Sandbox）做了详细的消融对比。
*   **充分性评价**：实验设计非常全面，不仅验证了性能提升，还深入分析了模型在沙盒中的行为模式（如外部资源调用频率）、推理成本（Token 消耗）和系统效率，具有很高的客观性和说服力。

### 6. 论文的主要结论与发现
*   **性能飞跃**：强模型在沙盒模式下性能显著提升，例如 Qwen3-Coder 在数学上提升了 **24.2%**，Claude 在指令遵循上提升了 **12.7%**。
*   **自发行为**：模型展现出令人惊讶的自主性，例如在化学任务中自发安装 Java 环境并下载第三方库来解析分子结构。
*   **RL 的泛化性**：通过
=======
这篇论文由微软亚洲研究院、中国人民大学和清华大学的研究团队合作完成，提出了一种名为 **LLM-in-Sandbox** 的新范式。以下是对该论文的深度结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **核心问题**：如何进一步释放大语言模型（LLM）的通用智能，使其超越单纯的文本生成，具备解决复杂、跨领域任务的能力？
*   **研究背景**：LLM 的演进经历了从上下文学习（ICL）到思维链（CoT），再到智能体（Agent）框架的过程。目前的智能体通常局限于特定工具或编程任务。
*   **核心动机**：作者认为“计算机”是人类发明的最通用平台。如果给 LLM 一个**代码沙盒（虚拟计算机）**，让其具备外部资源访问、文件管理和代码执行这三大“元能力”，LLM 就能像人类使用电脑一样，通过自主探索解决非编程领域的通用任务（如数学、科学、长文本处理等）。

### 2. 论文提出的方法论
#### 核心思想
将 LLM 置于一个轻量级的虚拟计算环境（Ubuntu Docker 容器）中，不预设复杂的任务特定工具，而是让模型通过基础的终端接口自主构建解决方案。

#### 关键技术细节
*   **三大元能力**：
    1.  **外部资源访问**：通过 `curl` 或 `pip install` 获取互联网信息或安装领域特定库（如化学库 RDKit）。
    2.  **文件管理**：利用文件系统处理超出上下文窗口的超长文档，进行持久化存储。
    3.  **代码执行**：编写并运行 Python/Bash 脚本进行精确计算、模拟或格式转换。
*   **LLM-in-Sandbox-RL（强化学习训练）**：
    *   **创新点**：使用**非智能体（Non-agentic）**的通用数据来训练模型的智能体能力。
    *   **训练策略**：将任务背景资料（Context）存放在沙盒的文件系统中，而非直接放入 Prompt。这迫使模型必须学会使用 `ls`、`grep`、`read` 等命令来探索环境并获取信息，从而完成任务。
    *   **奖励机制**：采用基于结果的奖励（Outcome-based Reward），通过 GRPO++ 算法进行大规模强化学习。

### 3. 实验设计
*   **数据集/场景**：涵盖了 6 个非编程领域和 1 个编程领域：
    *   **数学** (AIME25)、**物理** (UGPhysics)、**化学** (ChemBench)、**生物医学** (MedXpertQA)。
    *   **长文本理解** (AA-LCR)：处理平均 100K token 的文档。
    *   **指令遵循** (IFBench)：验证复杂约束下的执行力。
    *   **软件工程** (SWE-bench Verified)：验证编程基础能力。
*   **对比方法**：
    *   **Vanilla LLM**：直接生成答案，不使用沙盒。
    *   **LLM-RL**：仅在文本层面进行强化学习训练的基准。
*   **评估模型**：包括 Claude-Sonnet-4.5、GPT-5、DeepSeek-V3.2、Kimi-K2、MiniMax-M2 以及 Qwen3 系列（4B 到 30B）。

### 4. 资源与算力
*   **算力设备**：实验使用了单台 **NVIDIA DGX 节点**进行推理分析。
*   **训练细节**：
    *   Qwen3-4B-Instruct 训练了 150 个 Step。
    *   Qwen3-Coder-30B 训练了 50 个 Step。
    *   推理时设置了 64 的并发查询量。
*   **基础设施**：沙盒环境极其轻量，单个容器闲置仅需 50MB 内存，峰值约 200MB。在 2TB RAM 的服务器上支持 512 个并发沙盒仅占用 5% 的内存。

### 5. 实验数量与充分性
*   **实验规模**：论文在 7 个基准测试上进行了广泛实验，涵盖了从 4B 到顶级闭源模型的多种架构。
*   **消融实验**：
    *   对比了“上下文在 Prompt 中”与“上下文在沙盒中”的效果。
    *   测试了不同训练数据源（数学、编程、通用数据）对泛化能力的影响。
*   **充分性评价**：实验设计非常充分且具有前瞻性。不仅验证了性能提升，还深入分析了模型在沙盒中的行为模式（如调用外部资源的频率），并探讨了推理效率（Token 消耗和速度）。

### 6. 论文的主要结论与发现
1.  **涌现能力**：强模型（如 GPT-5, Claude 4.5）无需额外训练即可自发利用沙盒解决非编程任务，性能显著提升（如数学提升达 24.2%）。
2.  **RL 的奇效**：通过在沙盒中进行强化学习，弱模型（如 Qwen-4B）也能学会高效探索，且这种“智能体能力”能反哺回非沙盒模式，提升纯文本生成的逻辑性
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
