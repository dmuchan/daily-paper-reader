Title: Explainable Deepfake Detection with RL Enhanced Self-Blended Images

URL Source: https://arxiv.org/pdf/2601.15624v1

Published Time: Fri, 23 Jan 2026 01:21:51 GMT

Number of Pages: 6

Markdown Content:
# EXPLAINABLE DEEPFAKE DETECTION WITH RL ENHANCED SELF-BLENDED IMAGES 

# Ning Jiang 1, Dingheng Zeng 2, Yanhong Liu 2, Haiyang Yi 2, Shijie Yu 2,Minghe Weng 2, Haifeng Shen 2, and Ying Li 1* 

> 1

# School of Software & Microelectronics, Peking University, Beijing, China 

> 2

# Mashang Consumer Finance Co., Ltd., Chongqing, China 

ABSTRACT 

Most prior deepfake detection methods lack explainable out-puts. With the growing interest in multimodal large lan-guage models (MLLMs), researchers have started explor-ing their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attri-bution annotations, as textual annotation is both costly and challenging—particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that rein-forcement learning (RL) can substantially enhance perfor-mance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Im-ages, along with an RL-enhanced deepfake detection frame-work. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mech-anism, and feedback-driven synthetic data generation ap-proach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi. 

Index Terms — Deepfake detection, Multimodal large language model, Reinforcement learning 

1. INTRODUCTION 

The rapid advancement of deepfake techniques presents sig-nificant challenges to existing face forgery detection systems, which have primarily focused on improving generalization to unknown forgery types [1, 2, 3, 4]. Recent developments employ MLLMs for both detection and explainability [5, 6], while annotated datasets with authenticity labels and CoT an-

> © 2026 IEEE. Personal use of this material is permitted. Permis-sion from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for ad-vertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.

notations [7, 8] enhance model interpretability and general-ization. However, their detection performance still lags behind specialized models. This gap primarily stems from the in-herent difficulty in accurately describing forgery clues via CoT annotations, particularly for high-quality fabricated me-dia. Current methodologies often rely on labor-intensive processes involving human annotators and GPT-4o-assisted comparisons between real and fake images to localize forgery regions and generate CoT annotations. However, these ap-proaches remain constrained by high data requirements and potential inaccuracies in forgery localization [9]. Building on the demonstrated potential of Group Rela-tive Policy Optimization (GRPO) in Deepseek-R1 [10], R1-inspired reinforcement learning has shown promise in multi-modal vision tasks [11, 12]. However, applying R1 to face forgery detection - fundamentally a binary classification task - remains challenging due to reward signal sparsity. To address this, we propose a novel framework integrat-ing blending techniques [13] with R1, using precise forgery-conditioned keywords as reward signals. This represents the first successful application of R1 in face forgery detection, effectively mitigating reward sparsity while improving both accuracy and generalization to novel forgery methods. Our key contributions are: 1) We propose an automatic framework for precise forgery description generation. Our method first extracts generation conditions from forged images, then maps them to forgery re-gions/clues, which are expanded into CoT data with MLLMs. This approach significantly mitigates hallucination effects while eliminating the need for extensive manual verification required by prior methods. 2) We introduce a keyword-driven reward mechanism specifi-cally tailored for forgery detection. By embedding high-level semantic keywords of forgery clues into CoT data and imple-menting reward functions to constrain model predictions, our method effectively addresses the reward sparsity challenge in GRPO training for binary classification tasks. 3) We develop an adaptive feedback mechanism integrated with GRPO training. This system dynamically adjusts gen-eration strategies based on historical reward values, enabling progressive learning through iterative optimization. 

> arXiv:2601.15624v1 [cs.CV] 22 Jan 2026

Fig. 1 . The pipeline of generating CoT data. First, using a randomly selected mask and perturbation parameters to generate a fake image. Then retrieve key captions from the lookup table based on the mask and parameters. Finally, feed the image pair and key captions into an MLLM to produce the corresponding CoT annotation. 

2. RELATED WORK Deepfake detection. Recent deepfake detection work using MLLMs incorporate specialized detector structures for im-proving performance [14, 9] or leverage external knowledge from the forgery image generation process to generate CoT data [15]. FFTG [16] locates forged regions and generates simple textual descriptions by performing pixel-level com-parisons between real images and their corresponding forged counterparts. However, FFTG requires applying deepfake techniques to generate forged counterparts for real images, which is time-consuming and labor-intensive. The quality of the generated descriptions heavily depends on the diversity of the constructed forgery methods and images. Our approach builds upon SBI [13] and operates solely on real samples, dynamically generating forged data online. This allows us to precisely identify forged regions, obtain segmentation masks, and simultaneously generate both textual descriptions and fully accurate forensic clues about the forgery. 

Reinforcement learning. The R1-like reinforcement learn-ing methodology has been applied to MLLM research [17, 18]. Curr-ReFT [19] introduces curriculum learning ap-proaches that progress from simple to complex tasks. Vision-R1 [11] constructs high-quality mathematical reasoning datasets through human and GPT4o annotations to guide reinforcement training. VLM-R1 [12] further analyzed the critical role of reward function design in enhancing MLLM capabilities. 

3. METHOD 3.1. Construction of Basic Forged Data 

Inspired by SBI [13], we propose synthesizing fake images using the real data from FaceForensics++’s training set [20]. As shown in Figure 1, the process begins by detecting 81-point landmarks on a real face, followed by face parsing to segment the facial regions (full face, left eye, right eye, nose, and mouth). These segmented parts are then randomly combined to generate forgery region masks, which include 15 single-organ combinations and 4 predefined combinations (full face, eyebrows, and forehead). A series of augmentation operations are applied to the in-put real image, with randomly eroded/dilated mask regions. The augmentation considers multiple factors: hue, lighting, clarity, contrast, scaling, and translation, each applied at three intensity levels (mild, moderate, and severe). Finally, the aug-mented image is blended with the original face to produce the fabricated data. Mathematically, the forged image If is gen-erated as: 

If = α · T (Im) ⊙ A (Ir ; ξ) + (1 − α · T (Im)) ⊙ I r

where α is the blending weight, Ir is the real image, T de-notes the mask transformation, A(·; ξ) represents the aug-mentation operation where ξ is the randomly selected param-eters, and Im denotes the forged mask region. 

3.2. Construction of CoT Data 

We define a key caption table that indicates editing re-gions and discrepancy descriptions caused by modifications. Anomaly keywords for the forged image are randomly se-lected from this table, based on the measured difference thresholds between the forged and original images. Using the generated forged images and their synthesis conditions, we employ the open-source multimodal model InternVL3-38B[21] to produce reasoning descriptions in CoT format. Given the actual tampered regions and asso-ciated anomaly descriptions, we input these into the model. Through prompt engineering, we guide the multimodal model to autonomously select the most contextually appropriate de-scription while ensuring semantic distinctiveness. 3.3. Text-Based Forgery Localization Reward 

In addition to the accuracy and format rewards in GRPO’s framework, we introduce a task-specific reward signal for deepfake detection: text-based forgery localization reward. By adopting SBI-like methods with controllable forgery gen-eration, we obtain precise forgery region information that can be infinitely generated online. This deterministic positional description serves as one of the model’s reward signals. During training, we randomly generate position masks and convert them into textual descriptions. From the model’s text output, we extract the content corresponding to the Re-gions field in the key tags. Let REG pred and REG gt denote the sets of regional fields for the model response and ground truth, respectively. The forgery localization reward is then calculated using the Jaccard similarity between these two sets. 

3.4. Model Tuning 

To validate the effectiveness of our proposed CoT data con-struction method and the enhancement of reinforcement learning in deepfake detection tasks, we follow a traditional two-stage training approach. 

3.4.1. Supervised Fine-Tuning 

Using offline-generated CoT data, we conduct Supervised Fine-Tuning (SFT) on the MLLM in a Visual Question An-swering (VQA) format. The objective is to enable the model to follow the required format, extract key anomalies from images, describe forgery clues, and provide final conclusions. In this phase, we adopt a hybrid training strategy combining Faceforensics++ (FF++) data without CoT descriptions and SBI data with CoT descriptions. FF++ data encompasses four common deepfake manipulation types. By converting binary labels into “Real/Fake” textual annotations, the model can learn effective features for distinguishing genuine from forged images. Furthermore, by generating forged data using SBI methods along with corresponding forgery clue descrip-tions, the model establishes associations between forged features and textual descriptions, ensuring outputs adhered to the specified format. Through this training strategy, we eliminated the need for a dedicated forgery detector. The model achieved robust binary classification capabilities and interpretable textual outputs in a single training step. 

3.4.2. Reinforcement Training 

We perform GRPO on the SFT model. Similar to the SFT phase, we use a mix of FF++ data without CoT descriptions, and the SBI data generated online. During GRPO training, we calculate the following re-wards: classification reward Racc (extracting real/fake labels and comparing with ground truth), format reward Rf ormat 

(verifying adherence to the format structure as specified in the prompt), keyword reward Rkey (measuring the ROUGE score between extracted forgery clues in <key >... </key >and 

Fig. 2 . The feedback-based GRPO training. Dynamically ad-just the data generation parameters based on computed histor-ical reward values. ground truth keywords, which includes the forgery localiza-tion reward calculating overlap between forged region de-scriptions and ground truth annotations),length reward Rlen 

(constraining response length within predefined bounds). The total reward score is obtained by the sum of each item: 

Rtotal = Racc + Rf ormat + Rkey + Rlen 

Additionally, we adjust the generation parameters based on the mean/stability of historical reward values, to generate harder fake data with lower rewards and easier fake samples vice versa. Through GRPO reinforcement training, the model learns from the reward function’s feedback, demonstrating improved generalization to unseen forged data compared to the SFT baseline. 

4. EXPERIMENT Dataset . Following works like [22], we used preprocessed data from DeepfakeBench [23] for training and testing. The training set includes the FF++ and fake data (with correspond-ing text annotationss) generated via SBI from FF++’s real samples. The test set comprised of CDF2 [24], DFD [25], DFDC [26], and DFDCP [27] datasets. 

Evaluation Metrics . To align with prior works, we report AUC as the primary metric. Since the model outputs text (unable to compute AUC directly from Answer’s Real/Fake labels), we adopted the method from X2-DFD to derive AUC and EER by calculating answer probabilities. 

Implementation Details . Consistent with X2-DFD and M2F2-Det [28], we employed LLaVA 1.5-7b as the base model and leveraged Video-R1’s framework [17] for SFT and RL training. During SFT training, LoRA was applied to ViT and LLM (Rank=8, Alpha=16, dropout=0.1), with the pro-jector trainable. We set the learning rate, batch size, gradient accumulation steps to 2e − 4, 4 and 2 respectively. The model was trained for 10 epochs (last epoch used for evaluation and RL baseline). We used full fine-tuning for RL training. The learning rate and batch size is set to 1e − 6 and 3 respectively. Following Video-R1, GRPO was implemented via vLLM (8 generations, beta=0.04, 1 iteration). We used 4× NVIDIA A800 GPUs and trained for 1000 steps. Further training incurred instabil-ity due to sharpened action distributions and tightened KL-penalties amplifing gradient noise from dynamic feedback. 

Main Results . We compared SOTA methods in deepfake de-tection and reported frame-level and video-level results in Ta-ble 1 and Table 2, respectively. In the table headers, “DFD” refers to dedicated deepfake detectors, while “MLLM” indi-cates that the method employs a multimodal large language model framework. If both DFD and MLLM are checked, it means the method incorporates a dedicated deepfake de-tector as an auxiliary component alongside the base MLLM framework. Our method uses only the base MLLM frame-work without introducing any additional detector. 

Table 1 . Frame-level cross-dataset evaluation using AUC metric. For each target dataset, the highest performance is shown in bold, the second-best result is underlined. Method DFD MLLM CDF2 DFDC DFDCP DFD X-Ray [1] ✓ 0.679 0.633 0.694 0.766 RECCE [2] ✓ 0.732 0.713 0.734 0.812 SBI [13] ✓ 0.813 - 0.799 0.774 UCF [3] ✓ 0.753 0.719 0.759 0.807 ED [29] ✓ 0.864 0.721 0.851 -LSDA [4] ✓ 0.830 0.736 0.815 0.880 ProDet [30] ✓ 0.842 - 0.774 0.848 Trident [31] ✓ 0.861 0.826 0.845 0.920 F. Adap. [32] ✓ 0.900 0.843 0.890 0.933 

X2-DFD ✓ ✓ 0.903 0.835 0.897 0.925 FFTG [16] ✓ 0.832 - 0.832 0.948 

RLSBI (Ours) ✓ 0.905 0.818 0.817 0.926 Table 1 presents cross-dataset evaluation results. Data for other methods are sourced from the Forensics Adapter, X2-DFD, and FFTG papers. Our method outperforms previous approaches on CDF2, significantly surpassing FFTG, which shares a similar framework. On the large-scale DFD dataset, our method also exceeds most DFD and MLLM-based meth-ods. Table 2 displays video-level AUC results. On CDF2, our method ranks second only to VLF-FFD, which incorporates a dedicated deepfake detector, and performs comparably to SOTA methods on DFD. However, on the DFDC and DFDCP datasets, our method trails behind SOTA approaches. We 

Table 2 . Video-level cross-dataset evaluation using AUC metric, trained with FF++ c23 dataset. Method DFD MLLM CDF2 DFDC DFDCP DFD RECCE [2] ✓ 0.823 0.696 0.734 0.891 SBI [13] ✓ 0.886 0.717 0.848 0.827 UCF [3] ✓ 0.837 0.742 0.770 0.867 LSDA [4] ✓ 0.875 0.701 0.812 0.881 ProDet [30] ✓ 0.926 0.707 0.828 0.901 CDFA [33] ✓ 0.938 0.830 0.881 0.954 F. Adap. [32] ✓ 0.957 0.872 0.929 –Effort [22] ✓ 0.956 0.843 0.909 0.965 

X2-DFD [9] ✓ ✓ 0.955 0.853 0.912 0.957 KFD [6] ✓ ✓ 0.947 0.791 0.918 0.996 

M2F2-det [28] ✓ ✓ 0.951 0.878 – 0.977 VLF-FFD [14] ✓ ✓ 0.973 0.854 0.920 –RLSBI (Ours) ✓ 0.963 0.839 0.849 0.965 attribute this performance gap to the intrinsic mismatch be-tween SBI’s focus on blending artifacts and DFDC’s diverse, non-blending forgery types coupled with severe environmen-tal degradations. 

Ablation Studies . Table 3 presents the ablation studies con-ducted at the frame level, validating the effectiveness of each component in our RL training strategy. The baseline model (SFT), after undergoing simple instruction tuning, already demonstrates certain detection capabilities. The introduction of a reward mechanism based on key evidence verification (+key verify) leads to a modest performance improvement, confirming that our designed reward function effectively guides the model to focus on critical semantic features in forged regions. With the incorporation of the feedback-guided (+feedback) online data synthesis strategy, the model exhibits a significant performance gain on the CDF2 dataset, with the AUC increasing from 0.882 to 0.905 and the EER decreasing from 0.204 to 0.176. 

Table 3 . Ablation study on the RL strategy. Performance is evaluated using frame-level AUC and EER metrics. Phase Method CDF2 DFDCP AUC EER AUC EER SFT FF++/SBI CoT 0.881 0.206 0.804 0.275 RL +key verify 0.882 0.204 0.815 0.264 

+feedback 0.905 0.176 0.817 0.265 

5. CONCLUSION 

In this paper, we propose a method that leverages self-blending image techniques to generate forged images along with accurate textual descriptions of the forgeries, aiming to address the scarcity of high-quality text-annotated data re-quired by current MLLM-based deepfake detection systems. Furthermore, we introduce a curriculum learning training strategy guided by reinforcement feedback to synthesize data, which enhances the model’s cross-domain detection performance to SOTA levels without incorporating additional dedicated deepfake detectors, while also enabling the model to produce precise descriptions of deepfake clues. We also believe that the use of reinforcement feedback to expand data distributions offers promising avenues for further research and exploration. 

6. REFERENCES 

[1] L. Li, J. Bao, and T. Zhang et al., “Face x-ray for more general face forgery detection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2020, pp. 2, 4, 6. [2] J. Cao, C. Ma, and T. Yao et al., “End-to-end reconstruction-classification learning for face forgery detection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,2022, pp. 2, 6. [3] Z. Yan, Y. Zhang, and Y. Fan et al., “Ucf: Uncovering common features for generalizable deepfake detection,” in Proceedings of the IEEE/CVF International Confer-ence on Computer Vision (ICCV) , 2023, pp. 2, 6. [4] Y. Yan, Y. Luo, and S. Lyu et al., “Transcending forgery specificity with latent space augmentation for general-izable deepfake detection,” in CVPR , 2024a, pp. 8984– 8994. [5] X. Guo, X. Song, and Y. Zhang et al., “Rethinking vision-language model in face forensics: Multi-modal interpretable forged face detector,” in CVPR , 2025. [6] P. Yu, J. Fei, and H. Gao et al., “Unlocking the capabil-ities of large vision-language models for generalizable and explainable deepfake detection,” in ICML , 2025. [7] Y. Zhang, B. Colman, and X. Guo et al., “Common sense reasoning for deepfake detection,” in ECCV ,2024. [8] L. Qin, N. Jiang, and Y. Zhang et al., “Towards interac-tive deepfake analysis,” in ICASSP , 2025. [9] Y. Chen, Z. Yan, and G. Cheng et al., “X2-dfd: Aframework for explainable and extendable deepfake de-tection,” arXiv:2410.06126 , 2025. [10] DeepSeek-AI, D. Guo, and D. Yang et al., “Deepseek-r1: Incentivizing reasoning capability in llms via rein-forcement learning,” arXiv:2501.12948 , 2025. [11] W. Huang, B. Jia, and Z. Zhai et al., “Vision-r1: Incen-tivizing reasoning capability in multimodal large lan-guage models,” arXiv:2503.06749 , 2025. [12] H. Shen, P. Liu, and J. Li et al., “Vlm-r1: A stable and generalizable r1-style large vision-language model,” 

arXiv preprint arXiv:2504.07615 , 2025. [13] K. Shiohara and T. Yamasaki, “Detecting deepfakes with self-blended images,” in CVPR , 2022. [14] S. Peng, Z. Wang, and L. Gao et al., “Mllm-enhanced face forgery detection: A vision-language fusion solu-tion,” arXiv:2505.02013 , 2025. [15] X. He, Y. Zhou, and B. Fan et al., “Vlforgery face triad: Detection, localization and attribution via multimodal large language models,” arXiv:2503.06142 , 2025. [16] K. Sun, S. Chen, and T. Yao et al., “Towards general visual-linguistic face forgery detection,” in CVPR , 2025. [17] K. Feng, K. Gong, and B. Li et al., “Video-r1: Rein-forcing video reasoning in mllms,” arXiv:2503.21776 ,2025. [18] Y. Peng, G. Zhang, and M. Zhang et al., “Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl,” arXiv:2503.07536 ,2025. [19] H. Deng, D. Zou, and R. Ma et al., “Boosting the gener-alization and reasoning of vision language models with curriculum reinforcement learning,” arXiv:2503.07065 ,2025. [20] A. R”ossler, D. Cozzolino, and L. Verdoliva et al., “Faceforensics++: Learning to detect manipulated fa-cial images,” in ICCV , 2019. [21] J. Zhu, W. Wang, and Z. Chen et al., “Internvl3: Ex-ploring advanced training and test-time recipes for open-source multimodal models,” arXiv:2504.10479 , 2025. [22] Z. Yan, J. Wang, and P. Jin et al., “Orthogonal subspace decomposition for generalizable ai-generated image de-tection,” arXiv:2411.15633 , 2025. [23] Z. Yan, Y. Zhang, and X. Yuan et al., “Deepfakebench: A comprehensive benchmark of deepfake detection,” in 

NeurIPS , 2023. [24] Y. Li, X. Yang, and P. Sun et al., “Celeb-df: A large-scale challenging dataset for deepfake forensics,” in 

CVPR , 2020. [25] Google AI, “Contributing data to deepfakedetection,” 2019. [26] B. Dolhansky, J. Bitton, and B. Pflaum et al., “The deepfake detection challenge (dfdc) dataset,” 

arXiv:2006.07397 , 2020. [27] B. Dolhansky, R. Howes, and B. Pflaum et al., “The deepfake detection challenge (dfdc) preview dataset,” 

arXiv:1910.08854 , 2019. [28] X. Guo, X. Song, and Y. Zhang et al., “Rethinking vision-language model in face forensics: Multi-modal interpretable forged face detector,” in CVPR , 2025. [29] Z. Ba, Q. Liu, and Z. Liu et al., “Exposing the deception: Uncovering more forgery clues for deepfake detection,” in Proceedings of the AAAI Conference on Artificial In-telligence , 2024, pp. 5, 6. [30] J. Cheng, Z. Yan, and Y. Zhang et al., “Can we leave deepfake data behind in training deepfake detector?,” in 

NeurIPS , 2024. [31] M. H. Kara, A. Dundar, and U. G¨ ud¨ ukbay, “Trident: De-tecting face forgeries with adversarial triplet learning,” 

arXiv:2506.23189 , 2025. [32] X. Cui, Y. Li, and D. Zhu et al., “Forensics adapter: Un-leashing clip for generalizable face forgery detection,” in CVPR , 2025. [33] Y. Lin, W. Song, and B. Li et al., “Fake it till you make it: Curricular dynamic forgery augmentations towards general deepfake detection,” arXiv preprint arXiv:2409.14444 , 2024.