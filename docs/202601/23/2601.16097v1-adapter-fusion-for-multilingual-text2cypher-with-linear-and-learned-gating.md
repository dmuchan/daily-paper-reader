# Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating
# 基于线性与学习型门控的多语言 Text2Cypher 适配器融合

**Authors**: Makbule Gulcin Ozsoy
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.16097v1
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 7.0
**Evidence**: Multilingual Text2Cypher using LoRA adapters and Large Language Models

---

## Abstract
Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.

## 摘要
大语言模型使用户能够通过 Text

---

## 速览摘要（自动生成）

**问题**：针对 Text2Cypher 缺乏多语言支持且全量微调成本高、扩展性差的问题。
**方法