# Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors
# 通过 Fission-GRPO 实现鲁棒的工具使用：学习从执行错误中恢复

**Authors**: Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang, Jiakang Wang, Yao Hu, Shaosheng Cao, Kam-Fai Wong
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15625v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 8.0
**Evidence**: Addresses LLM tool use and error recovery using reinforcement learning

---

## Abstract
Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.

## 摘要
大语言模型（LLMs）能够有效地调用工具，但在多轮执行中仍表现出脆弱性：在工具调用出错后，较小的模型往往会陷入重复的无效重新调用，无法理解错误反馈并进行自我修正。这种脆弱性阻碍了模型在现实世界中的可靠部署，因为在工具交互过程中，执行错误本质上是不可避免的。我们识别出当前方法的一个关键局限：标准强化学习（RL）将错误视为稀疏的负

---

## 论文详细总结（自动生成）

这篇论文由香港中文大学和赤子城（Xiaohongshu）合作完成，提出了一种名为 **Fission-GRPO** 的强化学习框架，旨在提升小参数量语言模型（SLM）在多轮工具调用中的鲁棒性和错误恢复能力。

以下是对该论文的结构化总结：

### 1. 论文的核心问题与整体含义
*   **研究动机**：虽然大模型在工具调用上表现出色，但小模型（SLMs）在面对执行错误（如 API 返回错误、参数无效）时非常脆弱。它们往往会陷入重复无效调用的死循环，或产生幻觉，无法根据错误反馈进行自我修正。
*   **核心挑战**：
    1.  **标准 RL 的局限**：传统的强化学习（如 GRPO）将错误视为稀疏的负奖励，只告诉模型“不要这样做”，却没教模型“该怎么修复”。
    2.  **静态数据集的局限**：预先收集的离线纠错数据集存在“分布偏移”，即数据集里的错误模式与模型在训练过程中实际犯的错误不一致。
*   **研究目标**：让模型从自己实时犯的错误中学习，将错误转化为有指导意义的监督信号。

### 2. 方法论：Fission-GRPO 框架
该框架通过三个阶段的闭环流程运行：

*   **阶段 1：标准探索与更新 (Standard Exploration)**
    *   使用标准的 GRPO 算法进行采样和策略更新，维持模型基础的工具调用能力。
*   **阶段 2：错误识别与纠错样本构建 (Error Identification & Synthesis)**
    *   **过滤**：识别出格式错误或逻辑错误（奖励值低于阈值）的轨迹。
    *   **错误模拟器 (Error Simulator)**：使用一个经过 SFT 微调的 Qwen3-32B 模型作为模拟器，针对模型犯的特定错误生成具体的、类似运行时的诊断反馈（例如：“错误：参数 status 预期值为 OPEN”）。
    *   **构建上下文**：将 [原始对话 + 失败的调用 + 模拟器反馈] 拼接成一个新的“纠错上下文”。
*   **阶段 3：裂变式批量训练 (Fission-based Update)**
    *   **核心机制（裂变）**：针对每一个纠错上下文，重新采样 $G'$ 个并行的恢复尝试（Rollouts）。这就像核裂变一样，从一个错误事件诱发出多个后续反应，从而产生密集的正向引导信号。
    *   **LIFO 缓冲池**：使用后进先出（LIFO）缓冲区存储纠错样本，确保模型优先学习其当前策略下最容易犯的错误。

### 3. 实验设计
*   **数据集/场景**：主要使用 **BFCL v4 Multi-Turn**（伯克利函数调用排行榜 v4 多轮对话版）。该基准测试允许模型在出错后最多重试 20 次，非常适合评估错误恢复能力。
*   **模型规模**：在 Qwen3-1.7B、4B 和 8B 三个尺度上进行了验证。
*   **对比方法**：
    *   基础模型 (Base)
    *   标准 GRPO
    *   DAPO（动态采样约束的 RL）
    *   Dr.GRPO（缓解长度偏见的 GRPO 变体）
    *   专门的工具代理模型（如 ToolACE-8B, BitAgent-8B）

### 4. 资源与算力
*   **硬件**：使用了单节点 **8 × H800 80GB GPU**。
*   **软件框架**：基于 **Verl** 强化学习框架。
*   **训练细节**：学习率 1e-6，Batch Size 为 8，标准探索采样数 $G=8$。最大 Prompt 长度 12.8k，最大生成长度 4k。

### 5. 实验数量与充分性
*   **实验规模**：论文在三个不同参数规模的模型上进行了完整对比，并针对 BFCL 的四个子集（Base, Miss Func, Miss Param, Long Context）分别记录了成绩。
*   **消融实验**：
    *   **反馈质量分析**：对比了“静态通用反馈”与“动态模拟器反馈”的效果，证明了精准诊断的重要性。
    *   **触发频率分析**：研究了纠错更新频率 ($N$) 对性能的影响。
    *   **错误恢复率分解**：将总分拆解为“一次性成功率”和“错误恢复率”，直观展示了性能增量的来源。
*   **评价**：实验设计较为全面，不仅有量化指标，还有详细的 Case Study 轨迹分析，客观地展示了模型行为的改变。

### 6. 主要结论与发现
*   **性能提升**：Fission-GRPO 在所有规模上均达到 SOTA。在 Qwen3-8B 上，比标准 GRPO 总分提升了 **4%**（42.75% → 46.75%），甚至超过了专门微调的工具代理模型。
*   **恢复能力是关键**：8B 模型的错误恢复率绝对提升了 **5.7%**，尤其在长上下文