# Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning
# 采用 Sigmoid 边界熵的离线策略演员-评论家算法用于真实世界机器人学习

**Authors**: Xiefeng Wu, Mingyu Hu, Shu Zhang
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15761v1
<<<<<<< HEAD
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:RL</span>
**Score**: 7.0
**Evidence**: off-policy actor-critic reinforcement learning for real-world robotics
=======
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span>
**Score**: 8.0
**Evidence**: Proposes a new off-policy actor-critic method for reinforcement learning
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002

---

## Abstract
Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

## 摘要
由于样本效率低下

---

<<<<<<< HEAD
## 速览摘要（自动生成）

**问题**：针对现实机器人强化学习样本效率低、依赖大规模数据且在稀疏奖励下难以收敛的问题。

**方法**：提出 SigEnt-SAC，引入 Sigmoid 边界熵项，防止 Q 函数震荡及向分布外动作过度优化，仅
=======
## 论文详细总结（自动生成）

这篇论文提出了一种名为 **SigEnt-SAC** 的强化学习框架，旨在解决机器人真实世界学习中样本效率低、奖励稀疏和观测噪声大的核心痛点。

以下是对该论文的结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **核心问题**：如何在**极少量数据（仅需一次专家演示）**且**无大规模预训练**的情况下，让机器人在真实世界中从零开始高效、稳定地学习复杂任务？
*   **研究背景**：
    *   传统的“离线到在线”（Offline-to-Online）RL 需要海量高质量数据集。
    *   基于大模型（VLA）的 RL 依赖昂贵的预训练和微调。
    *   在真实世界中，Q 函数的震荡和分布外（OOD）动作的探索会导致硬件损耗和训练失败。

### 2. 方法论：核心思想与关键技术
SigEnt-SAC 基于离线策略演员-评论家（Off-Policy Actor-Critic）框架，核心创新包含两点：
*   **Sigmoid 边界熵（Sigmoid-Bounded Entropy）**：
    *   **核心思想**：传统 SAC 使用无界熵，在保守 Q 学习中，负熵项可能驱动策略向 OOD 区域优化，导致 Q 值震荡。
    *   **技术细节**：通过 Sigmoid 函数将每个维度的惊奇度（Surprisal）映射到一个有界的、严格正值的得分区间。这产生了一个“碗状”的正则化景观，将策略改进限制在状态分布的邻域内，增强了数值稳定性。
*   **门控行为克隆（Gated Behavior Cloning, GBC）**：
    *   **核心思想**：不强制策略完全模仿专家，而是设置一个阈值。
    *   **技术细节**：只有当智能体的动作与专家演示的偏差超过设定阈值 $\epsilon$ 时，才激活行为克隆损失函数。这既能利用专家引导减少无效探索，又允许策略在后期超越专家表现。
*   **算法流程**：结合了简化版的 CQL 正则化项来抑制 OOD 动作的高 Q 值，并在单次专家演示的基础上进行在线交互更新。

### 3. 实验设计
*   **数据集/场景**：
    *   **仿真环境**：D4RL 标准基准测试，包括 Adroit（Door, Hammer, Pen）和 Kitchen 任务。
    *   **真实世界**：涵盖四种不同形态的机器人：
        1.  **机械臂 (RM65-B)**：推方块任务。
        2.  **轮式机器人 (LIMO Pro)**：避障推球进洞。
        3.  **四足机器人 (Go2)**：视觉避障绕桩。
        4.  **人形机器人 (G1)**：全身平衡下的避障绕桩。
*   **Benchmark 与对比方法**：
    *   对比了 Cal-QL, CQL, RLPD, AWAC, IQL 等最先进的离线到在线 RL 算法。
    *   在真实世界中对比了行为克隆（BC）和基于 ChatGPT-5.2 的视觉语言模型（VLM）代理。

### 4. 资源与算力
*   **算力说明**：论文**未明确提及**具体的 GPU 型号和数量。
*   **训练效率**：论文给出了训练开销对比表。SigEnt-SAC 的参数量约为 20M，单次更新耗时约 3.13ms，略高于 AWAC 和 IQL，但处于可接受范围内。其优势在于不增加推理延迟，仅在训练阶段引入 GBC 损耗。

### 5. 实验数量与充分性
*   **实验规模**：
    *   在 D4RL 多个任务上进行了 1M 步的训练对比。
    *   针对专家演示的质量（数据丢失、动作噪声、状态噪声）做了鲁棒性实验。
    *   对关键超参数（GBC 权重 $\lambda$ 和阈值 $\epsilon$）进行了消融实验。
    *   真实世界中每个任务进行了 10 次独立评估。
*   **充分性评价**：实验设计较为充分，跨越了从仿真到真实世界、从机械臂到人形机器人的多种形态（Cross-Embodiment），验证了算法的通用性和鲁棒性。

### 6. 主要结论与发现
*   **单次演示高效性**：SigEnt-SAC 在仅有一次演示的情况下，达到 100% 成功率的速度显著快于所有基准方法。
*   **超越专家**：在真实世界任务中，学习到的策略比专家演示更高效，任务完成时间平均缩短了 **40.9%**（例如推球任务从 26 步缩短至 3 步）。
*   **鲁棒性**：即使专家演示包含 50% 的数据丢失或高噪声，该算法仍能保持稳定学习，而对比方法（如 Cal-QL）表现大幅下降。
*   **视觉适应性**：仅依靠单目车载摄像头（单视图灰度图）即可实现稳定控制，证明了其对局部观测和动态环境的适应力。

### 7. 优点
*   **极低数据依赖**
>>>>>>> 213bc5282355b927a5048646063bed8191c1e002
