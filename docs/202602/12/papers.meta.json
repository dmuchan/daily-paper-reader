{
  "label": "2026-02-12",
  "date": "2026-02-12",
  "generated_at": "2026-02-12T19:40:55",
  "count": 5,
  "papers": [
    {
      "paper_id": "202602/12/2602.10576v1-llm-based-scientific-equation-discovery-via-physics-informed-token-regularized-policy-optimization",
      "section": "deep",
      "title_en": "",
      "authors": "",
      "date": "",
      "pdf": "",
      "score": "",
      "evidence": "",
      "tldr": "",
      "tags": "",
      "abstract_en": "Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.\n\n---"
    },
    {
      "paper_id": "202602/12/2602.10480v1-neuro-symbolic-synergy-for-interactive-world-modeling",
      "section": "deep",
      "title_en": "",
      "authors": "",
      "date": "",
      "pdf": "",
      "score": "",
      "evidence": "",
      "tldr": "",
      "tags": "",
      "abstract_en": "Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.\n\n---"
    },
    {
      "paper_id": "202602/12/2602.10191v1-machine-learning-methods-for-stellar-collisions-i-predicting-outcomes-of-sph-simulations",
      "section": "quick",
      "title_en": "",
      "authors": "",
      "date": "",
      "pdf": "",
      "score": "",
      "evidence": "",
      "tldr": "",
      "tags": "",
      "abstract_en": "Stellar collisions can occur frequently in dense cluster environments, and play a crucial role in producing exotic phenomena from blue stragglers in globular clusters to high-energy transients in galactic nuclei. Successive collisions and mergers of massive stars could also lead to the formation of massive black holes, serving as seeds for supermassive black hole in the early universe. While analytic fitting formulae exist for predicting collision outcomes, they do not generalize across different energy scales or stellar evolutionary phases. Smoothed particle hydrodynamics (SPH) simulations are often used to compute the outcomes of stellar collisions, but, even at low resolution, their computational cost makes running on-the-fly calculations during an $N$-body simulation quite challenging. Here we present a new grid of $27,720$ SPH calculations of main-sequence star collisions, spanning a wide range of masses, ages, relative velocities, and impact parameters. Using this grid, we train machine learning models to predict both collision outcomes (merger vs disruption, or flyby) and final remnant masses. We compare the performance of nearest neighbors, support vector machines, and neural networks, achieving classification balanced accuracy of $98.4\\%$, and regression relative errors as low as $0.11\\%$ and $0.15\\%$ for the final stars $1$ and $2$, respectively. We make our trained models publicly available as part of the package collAIder, enabling rapid predictions of stellar collision outcomes in $N$-body models of dense star cluster dynamics."
    },
    {
      "paper_id": "202602/12/2602.10233v1-improvevolve-ask-alphaevolve-to-improve-the-input-solution-and-then-improvise",
      "section": "quick",
      "title_en": "",
      "authors": "",
      "date": "",
      "pdf": "",
      "score": "",
      "evidence": "",
      "tldr": "",
      "tags": "",
      "abstract_en": "Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve, have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. In this article, we present ImprovEvolve, a simple yet effective technique for enhancing LLM-based evolutionary approaches such as AlphaEvolve. Given an optimization problem, the standard approach is to evolve program code that, when executed, produces a solution close to the optimum. We propose an alternative program parameterization that maintains the ability to construct optimal solutions while reducing the cognitive load on the LLM. Specifically, we evolve a program (implementing, e.g., a Python class with a prescribed interface) that provides the following functionality: (1) propose a valid initial solution, (2) improve any given solution in terms of fitness, and (3) perturb a solution with a specified intensity. The optimum can then be approached by iteratively applying improve() and perturb() with a scheduled intensity. We evaluate ImprovEvolve on challenging problems from the AlphaEvolve paper: hexagon packing in a hexagon and the second autocorrelation inequality. For hexagon packing, the evolved program achieves new state-of-the-art results for 11, 12, 15, and 16 hexagons; a lightly human-edited variant further improves results for 14, 17, and 23 hexagons. For the second autocorrelation inequality, the human-edited program achieves a new state-of-the-art lower bound of 0.96258, improving upon AlphaEvolve's 0.96102."
    },
    {
      "paper_id": "202602/12/2602.10598v1-neuro-symbolic-action-masking-for-deep-reinforcement-learning",
      "section": "quick",
      "title_en": "",
      "authors": "",
      "date": "",
      "pdf": "",
      "score": "",
      "evidence": "",
      "tldr": "",
      "tags": "",
      "abstract_en": "Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations."
    }
  ],
  "errors": []
}
