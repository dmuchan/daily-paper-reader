---
title: LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization
title_zh: 基于物理信息告知的 Token 正则化策略优化的 LLM 科学方程发现
authors: "Boxiao Wang, Kai Li, Tianyi Liu, Chen Li, Junzhe Wang, Yifan Zhang, Jian Cheng"
date: 2026-02-11
pdf: "https://arxiv.org/pdf/2602.10576v1"
tags: ["keyword:SR", "query:SR"]
score: 10.0
evidence: 基于大模型、强化学习和物理先验的符号回归
tldr: 本研究针对符号回归任务，提出了一种名为PiT-PO的强化学习框架，旨在解决现有大语言模型在生成科学方程时存在的物理不一致和结构冗余问题。该方法通过物理信息约束和Token级正则化策略优化模型，使其能够生成既符合物理规律又简洁的方程。实验证明，PiT-PO在多个基准测试中达到领先水平，并成功发现新型湍流模型，显著提升了小规模模型在科学发现中的表现。
motivation: 现有的基于大语言模型的符号回归方法多将其视为静态生成器，难以根据搜索反馈优化内部表示，导致生成的方程缺乏物理一致性且结构冗余。
method: 提出PiT-PO框架，利用强化学习结合层级物理有效性约束和细粒度Token级惩罚，将大语言模型演化为自适应的方程生成器。
result: 该方法在标准基准测试中取得最优性能，成功发现复杂的流体力学湍流模型，并使小模型表现超越了闭源大模型。
conclusion: PiT-PO通过对齐物理先验与结构简洁性，为自动化科学方程发现提供了一种高效且普适的强化学习优化方案。
---

## 摘要
符号回归旨在从观测数据中提取数学方程。最近的方法已成功利用大语言模型（LLM）生成方程假设，利用了其庞大的预训练科学先验。然而，现有框架主要将 LLM 视为静态生成器，依赖提示词级别的引导来驱动探索。这种范式无法根据搜索反馈更新模型的内部表示，通常会产生物理上不一致或数学上冗余的表达式。在这项工作中，我们提出了 PiT-PO（物理信息告知的 Token 正则化策略优化），这是一个通过强化学习将 LLM 演化为自适应生成器的统一框架。PiT-PO 的核心是一种双重约束机制，它在严格执行分层物理有效性的同时，应用细粒度的 Token 级惩罚以抑制冗余结构。因此，PiT-PO 使 LLM 能够生成既符合科学一致性又具有结构简洁性的方程。实验表明，PiT-PO 在标准基准测试中达到了最先进的性能，并成功为具有挑战性的流体动力学问题发现了新型湍流模型。我们还证明了 PiT-PO 能够使小规模模型超越闭源巨头模型，从而使高性能科学发现的获取更加民主化。

## Abstract
Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.

---

## 论文详细总结（自动生成）

这篇论文介绍了一种名为 **PiT-PO**（Physics-informed Token-regularized Policy Optimization）的新框架，旨在利用大语言模型（LLM）进行科学方程发现（符号回归）。以下是对该论文的深度结构化总结：

### 1. 核心问题与研究动机
*   **核心问题**：符号回归（SR）旨在从观测数据中提取简洁、可解释的数学方程。虽然现有方法开始利用 LLM 的科学先验知识，但存在两个主要瓶颈：
    1.  **静态生成限制**：现有方法多将 LLM 视为“冻结”的生成器，仅通过提示词（Prompt）引导，无法根据搜索反馈动态更新模型内部参数。
    2.  **物理与结构失真**：生成的方程往往只追求数值拟合，忽略了物理一致性（如单位统一、边界条件），且容易产生数学上的冗余项。
*   **研究动机**：将 LLM 从“静态提议者”转变为“自适应生成器”，通过强化学习（RL）使模型在搜索过程中不断进化，学习特定领域的科学规律。

### 2. 方法论：PiT-PO 框架
PiT-PO 结合了进化搜索与参数化学习，核心技术包括：
*   **核心思想**：利用 **GRPO（群组相对策略优化）** 算法，在搜索过程中通过 LoRA 微调 LLM。它不依赖额外的价值网络，而是通过组内相对奖励来估计优势函数。
*   **双重约束学习信号**：
    1.  **层级物理约束**：分为通用层（单位齐次性、可微性）和领域特定层（如湍流模型中的实现性、边界条件、渐近缩放等）。
    2.  **理论约束（支撑集排除定理）**：提出了一种数学定理，通过分析拟合系数的量级来识别冗余项。
*   **关键技术细节**：
    *   **Token 级正则化**：不同于传统的序列级评分，PiT-PO 将冗余项识别转化为 **Token 级别的惩罚**，精确引导模型抑制冗余结构的生成概率。
    *   **门控物理惩罚**：为了不抑制早期探索，物理约束仅在方程达到一定拟合精度后才激活。
    *   **多岛屿进化策略**：采用多个独立缓冲区（Islands）维持搜索多样性，防止陷入局部最优。

### 3. 实验设计
*   **数据集与场景**：
    1.  **LLM-SR Suite**：包含非线性振荡器、大肠杆菌生长模型、应力-应变模型等 4 个经典科学任务。
    2.  **LLM-SRBench**：包含 239 个任务，分为 LSR-Transform（公式变形）和 LSR-Synth（合成公式）两个子集，涵盖化学、生物、物理和材料科学。
    3.  **真实案例研究**：周期性山丘流（Periodic Hills）的湍流闭合模型建模。
*   **对比方法**：包括传统符号回归（GPlearn, PySR）、神经符号回归（uDSR, RAG-SR）以及最新的 LLM 方法（LLM-SR, LaSR, SGA）。
*   **评估指标**：平均准确率（Acc）、归一化均方误差（NMSE）、符号准确率（SA，衡量数学等价性）。

### 4. 资源与算力
*   **硬件环境**：实验在单张 **NVIDIA RTX 3090** GPU 上完成。
*   **模型配置**：使用了 4-bit 量化的开源模型，包括 **Llama-3.1-8B**、**Llama-3.2-3B** 和 **Llama-3.2-1B**。
*   **训练细节**：每个任务运行 2500 次搜索迭代，使用 LoRA（Rank=16）进行高效微调。

### 5. 实验数量与充分性
*   **实验规模**：论文进行了大规模测试，涵盖了超过 240 个不同的科学方程发现任务。
*   **消融实验**：专门设计了“无物理约束”和“无 Token 正则化”的对比实验，验证了各组件的必要性。
*   **公平性**：所有对比方法在相同的搜索迭代次数和提示词环境下运行，并提供了基于“墙钟时间”（Wall-clock time）的效率分析，证明了 PiT-PO 虽然增加了微调开销，但在相同时间内能获得更优结果。
*   **充分性**：实验不仅覆盖了合成数据，还包含了真实的物理模拟（CFD 验证），并进行了分布外（OOD）泛化测试，实验设计严谨且客观。

### 6. 主要结论与发现
*   **性能领先**：PiT-PO 在所有基准测试中均达到了 SOTA 性能，尤其在符号准确率（SA）上显著优于现有方法。
*   **小模型逆袭**：通过 PiT-PO 优化的 1B/3B/8B 小规模开源模型，其表现能够超越 GPT-4o-mini 等闭源巨头模型。
*   **突破停滞**：相比于静态 LLM 容易在搜索后期陷入平台期，PiT-PO 的自适应更新机制能多次实现误差的阶梯式下降。
*   **物理实用性**：在湍流建模中，PiT-PO 发现的方程在 OpenFOAM 模拟中表现出更好的物理一致性和预测精度。

### 7. 优点与亮点
*   **细粒度反馈**：将数学冗余转化为 Token 级惩罚，解决了 LLM 在符号回归中常见的“公式臃肿”问题。
*   **物理深度融合**：不是简单地将物理规则写进 Prompt，而是将其作为硬约束整合进强化学习的奖励函数中。
*   **高效性**：证明了在消费级显卡上也能进行高性能的科学发现，降低了该领域的研究门槛。

### 8. 不足与局限
*   **计算开销**：虽然使用了 LoRA，但相比于纯推理的 LLM 方法，PiT-PO 在搜索过程中需要频繁进行反向传播，增加了单次迭代的时间。
*   **领域知识依赖**：对于复杂的领域（如湍流），仍需要专家预先定义特定的物理约束（如实现性条件），这限制了其在完全未知领域的“零样本”发现能力。
*   **超参数敏感性**：强化学习中的奖励权重（如物理惩罚与拟合奖励的比例）可能需要针对不同任务进行微调。

（完）
