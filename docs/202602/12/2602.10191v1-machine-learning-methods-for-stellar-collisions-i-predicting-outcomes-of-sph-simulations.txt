Title: Machine Learning Methods for Stellar Collisions. I. Predicting Outcomes of SPH Simulations

URL Source: https://arxiv.org/pdf/2602.10191v1

Published Time: Thu, 12 Feb 2026 01:09:11 GMT

Number of Pages: 23

Markdown Content:
> Draft version February 12, 2026
> Typeset using L ATEX twocolumn style in AASTeX631

Machine Learning Methods for Stellar Collisions. I. Predicting Outcomes of SPH Simulations 

Elena GonzÂ´ alez Prieto, 1, 2, 3 James C. Lombardi, Jr., 4 Sanaea C. Rose, 2, 3 Charles F.A. Gibson, 1, 2, 3 

Christopher E. Oâ€™Connor, 2, 3 Tjitske Starkenburg, 1, 2, 3 Fulya KÄ±roË˜ glu, 2, 3 Kyle Kremer, 5

Tristan C. Parmerlee, 6 and Frederic A. Rasio 1, 2, 3 

> 1Department of Physics & Astronomy, Northwestern University, Evanston, IL 60208, USA
> 2Center for Interdisciplinary Exploration & Research in Astrophysics (CIERA), Northwestern University, Evanston, IL 60201, USA
> 3NSF-Simons AI Institute for the Sky (SkAI), 172 E. Chestnut St., Chicago, IL 60611, USA
> 4Department of Physics, Allegheny College, Meadville, Pennsylvania 16335, USA
> 5Department of Astronomy & Astrophysics, University of California, San Diego; La Jolla, CA 92093, USA
> 6Department of Physics, Loyola University Chicago, Chicago, IL 60660, USA

ABSTRACT Stellar collisions can occur frequently in dense cluster environments, and play a crucial role in producing exotic phenomena from blue stragglers in globular clusters to high-energy transients in galactic nuclei. Successive collisions and mergers of massive stars could also lead to the formation of massive black holes, serving as seeds for supermassive black hole in the early universe. While analytic fitting formulae exist for predicting collision outcomes, they do not generalize across different energy scales or stellar evolutionary phases. Smoothed particle hydrodynamics (SPH) simulations are often used to compute the outcomes of stellar collisions, but, even at low resolution, their computational cost makes running on-the-fly calculations during an N -body simulation quite challenging. Here we present a new grid of 27 , 720 SPH calculations of main-sequence star collisions, spanning a wide range of masses, ages, relative velocities, and impact parameters. Using this grid, we train machine learning models to predict both collision outcomes (merger vs disruption, or flyby) and final remnant masses. We compare the performance of nearest neighbors, support vector machines, and neural networks, achieving classification balanced accuracy of 98.4%, and regression relative errors as low as 0.11% and 0.15% for the final stars 1 and 2, respectively. We make our trained models publicly available as part of the package collAIder , enabling rapid predictions of stellar collision outcomes in N -body models of dense star cluster dynamics. 

1. INTRODUCTION Stellar collisions and mergers are important across a wide range of dense stellar systems, from galactic nuclei to open clusters (Spitzer & Saslaw 1966; Sanders 1970; Lombardi et al. 1996; Hurley et al. 2001; Shara 2002; Hurley et al. 2005; Glebbeek et al. 2008; Dale et al. 2009; Balberg & Yassur 2023; GonzÂ´ alez Prieto et al. 2024). The properties and outcomes of these collisions vary drastically depending on the environment. In galactic nuclei, stars collide at relative velocities of hundreds to thousands of kilometers per second (e.g., Rauch 1999; Genzel et al. 2010; Rose et al. 2023), while in globu-lar clusters, typical velocity dispersions are only tens of kilometers per second (e.g., Pryor & Meylan 1993; Har-ris 2010; Baumgardt & Hilker 2018). Whether a collision results in a merger, mass stripping, or complete disrup-tion depends on this relative velocity, as well as the peri-center distance, and the mass, radius, and evolutionary stage of each star (e.g., Freitag & Benz 2005). Understanding how these dynamical parameters shape collision outcomes has proven crucial to explain exotic stellar objects and phenomena. For example, blue strag-glers, stars that appear on the main sequence (MS) be-yond the turnoff, could be formed through collisions be-tween lower-mass MS stars (e.g., Leonard 1989; Lom-bardi et al. 1996; Sills et al. 2009). Stellar collisions have also been studied for their possible role as gravitational-wave progenitors (e.g., KÄ±roË˜ glu et al. 2025; GonzÂ´ alez Pri-eto et al. 2024; Amaro Seoane 2023; Kremer et al. 2020; GÂ¨ urkan et al. 2006; Portegies Zwart et al. 2004). Tran-sient events are also expected, such as tidal disruption events (TDE) (e.g., Perets et al. 2016; Kremer et al. 2019; Lopez et al. 2019; Wang et al. 2021; Kremer et al. 2022; Ryu et al. 2022; KÄ±roË˜ glu et al. 2023; Xin et al. 2024; Ryu et al. 2024; Rose & Mockler 2025), energetic explosions from collisions between hypervelocity stars in galactic nuclei (Hu & Loeb 2024; Brutman et al. 2024; Balberg et al. 2013), and collision-driven events (van der Merwe et al. 2024, 2025; Peng et al. 2025). Further-more, high-speed collisions between intermediate-mass stars can produce stripped stellar remnants (e.g., Rose et al. 2023; Freitag et al. 2007; Rauch 1999; Lai et al. 1993) with abundance ratios similar to those observed in the TDE ASASSN-14ko (e.g., Payne et al. 2021; Gib-son et al. 2024). Lastly, mergers can also produce exotic 

> arXiv:2602.10191v1 [astro-ph.HE] 10 Feb 2026

2signatures, such as gamma-ray production from a black hole merger with a helium star (e.g., Oâ€™Connor et al. 2025; Neights et al. 2026) or luminous red novae (e.g., Kirilov et al. 2025; Metzger & Pejcha 2017; MacLeod et al. 2017; Ivanova et al. 2013). Currently, most N -body models of dense stellar sys-tems have simplified treatments for stellar collisions, such as the â€œsticky sphereâ€ approximation where no mass is lost during the collision. While this assump-tion is adequate for nearly parabolic collisions in globu-lar clusters, hyperbolic collisions in nuclear star clusters can produce significant shock-driven mass loss. Accu-rately predicting the detailed outcomes of stellar colli-sions, such as remnant masses and structures, therefore requires 3D hydrodynamics simulations that can resolve relevant thermodynamical and hydrodynamic processes, using methods such as smooth particle hydrodynam-ics (SPH) (Rasio 1991; Monaghan 1992; Gaburov et al. 2010) or grid-based/moving-mesh methods (Weinberger et al. 2020). However, these detailed simulations are computationally expensive, sometimes requiring days of wallclock time per collision, making on-the-fly calcula-tions in N -body cluster simulations infeasible. As a re-sult, a practical approach is to precompute large grids of SPH results and develop fitting formulae (e.g., Rose et al. 2025; Lai et al. 1993; Rauch 1999). Machine learning (ML) offers a unique opportunity to overcome these computational limitations. Using tradi-tional ML algorithms, a model can be trained on a large grid of simulations to learn the mapping between inputs and outputs. This approach has been proven effective in the context of planetary collisions. For example, Cam-bioni et al. (2019) compare classification and regression performance across multiple algorithms. This work was extended to predict core mass-fractions (Cambioni et al. 2021) and orbital parameters (Emsenhuber et al. 2020). Recently, Amaro Seoane (2025) and Rose et al. (2025) have applied similar techniques in the context of stellar collisions, with the former using an older grid of stellar collision SPH simulations presented in Freitag & Benz (2005) and the latter using a new grid of equal-mass but higher-resolution collision calculations. For this study, we compute a comprehensive new grid of SPH results for stellar collisions, spanning wide ranges of ages, stellar masses, pericenter distances, and rela-tive velocities. We improve upon the SPH simulations presented in Freitag & Benz (2005) by using accurate 

MESA stellar profiles and exploring the effects of stellar age on collision outcomes. Following the methodology of Cambioni et al. (2019), we compare the performance of multiple ML algorithms on both classification and re-gression tasks. The classification task predicts the num-ber of surviving stars, while the regression task esti-mates their final masses. Furthermore, we compare dif-ferent ML methods, including nearest neighbors (Cover & Hart 1967), support vector machines (Cortes & Vap-nik 1995), and neural networks (e.g., McCulloch & Pitts 1943; Hornik et al. 1989; LeCun et al. 2015). In Section 2, we describe our SPH simulations, includ-ing the MESA stellar models used as initial conditions. In Sections 3 and 4 we present our ML framework for classi-fying stellar collision outcomes and predicting the prop-erties of final remnants. In Section 5 we compare these models against a Mixture of Experts architecture. We then introduce collAIder (in Sec. 6), a new software that leverages the trained ML models to make rapid predictions for stellar outcomes. Finally, in Section 7 we evaluate model performance on interpolated and ex-trapolated data. We summarize our results and outline future applications in Section 8.  

> 2.

METHODS 2.1. Simulation Grid 

Our training set of SPH results is built by sampling across five parameters: the age of the system ( t), the pri-mary mass ( M1), the secondary mass ( M2), the velocity at infinity ( vâˆž), and the distance of closest approach (pericenter distance, rp). Consistent with assumptions made in many star cluster modeling codes, we assume that all stars are coeval. However, we account for the mass-dependent timescale for stars to contract onto the MS, and we collide only MS stars. For example, at an age of 1 Myr, we only perform collisions involving the most massive stars in the grid, since stars below 4 MâŠ™

are still in the pre-MS stage. Furthermore, some of the lower-mass stars will not reach the zero-age MS (ZAMS) before the higher-mass stars evolve off the MS. As a re-sult, we do not compute collisions between the highest and lowest sampled masses. We restrict this initial grid to stars only on the MS because stellar structure varies greatly across the full age range of the sampled stellar masses. Accurately modeling the evolution of a wider range of collision outcomes across time would require a very finely-sampled grid of pre-MS, MS, and giant star encounters, an undertaking which we reserve for future work. For the base grid, ages are sampled logarithmically in Gyr at ( âˆ’3, âˆ’2.5, âˆ’2, âˆ’1.5, âˆ’1, âˆ’0.5, 0, 0.5, 1) and at 13 .7 Gyr (Hubble time). Stellar masses are sampled at ten values from 0 .2 to 64 MâŠ™. Velocities sampled are (10, 100, 250, 500, 1000, 2000, 4000, 8000, 16000) km/s, chosen to span speeds typical of stellar collisions from globular clusters to galactic nuclei. Pericenter distances are sampled based on fractional enclosed mass f : for each collision we choose f (uniformly in steps of 0.1 from 0 to 0 .9, with additional samples at 0 .95, 0 .97, 0 .99, and 1), compute the radii r1(f ) and r2(f ) that enclose this mass fraction in each parent star, and set the pericenter to rp = r1(f ) + r2(f ). For example, f = 0 corresponds to a head-on collision and f = 1 to a grazing encounter at the sum of the two stellar radii. We complement the base grid by sampling the termi-nal age main sequence (TAMS) for all stellar masses, 3defined as the point when the central hydrogen mass fraction drops below 10 âˆ’5. We also compute a grid at the ZAMS age for the 64 , 32 , 16 , 8, and 4 MâŠ™ stars, where ZAMS is defined as the local minimum in luminosity when nuclear burning provides more than 50% of the luminosity. For the ZAMS grid, each star at the ZAMS is collided with all other stars that are on the MS at that time, but we do not simulate collisions among the MS stars at that time. This brings the total grid size to 27 ,720 SPH simulations of stellar collisions. The com-plete grid is shown in Figure 1, where the lower right panel illustrates how each mass and time slice is sam-pled in rp and vâˆž.2.2. Stellar Profiles 

To generate the realistic stellar profiles needed for the stellar collision simulations, we use MESA (v24.08.1) (Paxton et al. 2011, 2013, 2015, 2018, 2019; Jermyn et al. 2023). We scale the initial chemical composition of our stellar models relative to the proto-solar helium abundance YâŠ™ = 0 .2703 and metallicity ZâŠ™ = 0 .0142 of Asplund et al. (2009). We adopt a metallicity of 

Z = 0 .01 ZâŠ™ and calculate the corresponding helium abundance using equation 2 in Choi et al. (2016) (as-suming Yp = 0 .249 from Planck Collaboration et al. (2016)), yielding a value of Y = 0 .249. We treat mi-crophysical processes relevant to MS stars following the 

Posydon model grids (Fragos et al. 2023; Andrews et al. 2025) unless otherwise specified. All MESA inlists used to generate our stellar models are available for download 1.We do not include stellar winds or rotation since we only perform collisions for stars on the MS. At this metallicity and evolutionary stage, mass loss is not ex-pected to be very efficient. Future grids with stars past the MS will incorporate stellar rotation and metallicity-dependent wind prescriptions. 2.3. Smoothed Particle Hydrodynamics 

The stellar collisions are performed using 

StarSmasher (Rasio 1991; Gaburov et al. 2010), aLagrangian SPH code in which each fluid particle is assigned a mass, position, velocity, and specific internal energy. The particles are evolved using a variational SPH formulation, with gravitational forces and energies computed using direct summation on NVIDIA GPUs (Gaburov et al. 2010). We neglect radiative cooling and heating. Each particle has an extended density profile described by a Wendland C4 kernel (Wendland 1995) with a compact support 2 hi (where hi is the smoothing length of the particle). Artificial viscosity is computed as in Hwang et al. (2015). Because all parent stars are on the MS, an analytic equation of state including ideal-gas and radiation pres-sure is employed. Radiation pressure can contribute ap- 

> 1Will be made available at time of publication.

preciably in our high-mass models and may also become more important in shock-heated regions during the colli-sion. For each SPH particle, given its density Ï, specific internal energy u, and mean molecular mass Î¼ (with dimensions of mass), we obtain the temperature T by solving u = 32 kT /Î¼ + aT 4/Ï , where k is Boltzmannâ€™s constant and a is the radiation constant. The result-ing quartic equation in T is solved analytically following Lombardi et al. (2006). We then compute the pressure from P = ÏkT /Î¼ + 13 aT 4.Each star is composed of 10 ,000 particles with âˆ¼ 50 neighbors, where the 1D MESA stellar profiles are con-verted into 3D SPH models by placing particles in a stretchy hexagonal close-packed (HCP) lattice as de-scribed in Appendix A of Gibson et al. (2024). The value of equalmass is set to 0 .5 for most models, yielding unequal mass particles with a number density n âˆ âˆšÏ

to distribute more particles closer to the center of the star and better resolve the core. After constructing the stellar profile, the particles are relaxed into hydrostatic equilibrium as described in Lombardi et al. (2006) and Gaburov et al. (2010). For later post-processing, we store snapshots of the system at an output interval roughly equal to the dy-namical timescale of the more massive star: âˆ†tout â‰¡

tdyn = pR3max /(GM max ) = âˆšMmax in code units (G = MâŠ™ = RâŠ™ = 1), where for the purposes of as-signing a âˆ† tout value, we have adopted the approximate massâ€“radius relation R/R âŠ™ = ( M/M âŠ™)2/3. Here, Rmax 

and Mmax are, respectively, the radius R and mass M of the more massive of the two parent stars. We compute the bound mass of each star at every snapshot using an iterative energy-based procedure similar to that of Lom-bardi et al. (2006). At a given snapshot, we first com-pute the center-of-mass (COM) and preliminary mass of each stellar component. Then, we evaluate each SPH particleâ€™s specific mechanical energy with respect to each starâ€™s COM. This mechanical energy includes only kinetic and grav-itational potential terms, as recommended by Nan-dez et al. (2014). A particle is considered gravita-tionally bound to star j if its mechanical energy rel-ative to that starâ€™s COM is negative. In particu-lar, for particle i relative to component j, we require 

v2 

> ij

/2 âˆ’ G(Mj âˆ’ mi)/d ij < 0, where vij is the particleâ€™s speed in the frame of star jâ€™s COM, dij is its distance from that center, mi is the mass of the particle, and Mj

is the current mass of star j. This approach is equiva-lent to using the Bernoulli equation at late times, since internal energy and enthalpy become negligible in the outflow after sufficient adiabatic expansion. To avoid spuriously identifying poorly resolved clumps as stars, a bound component is considered a stellar remnant only if it contains at least 20 SPH particles. Otherwise, its particles are reassigned to the unbound mass. We iterate this procedure to refine the bound masses self-consistently. Based on its assignment from the pre-4vious snapshot (or the known configuration at t = 0), each particle is tentatively assigned to the component (star 1 or star 2) for which it has negative energy, or to the unbound mass otherwise. If it has a negative me-chanical energy with respect to both stars, it is assigned to whichever star yields the most negative energy. We then update the COM and mass of each star based on these assignments and recompute the energies, repeat-ing until no particle changes its affiliation (typically con-verging within a few iterations). Particles that do not have negative mechanical energy with respect to either star are assigned to the unbound component. The stopping time ( tf ) is dynamically determined by evaluating the state of the system once per output in-terval âˆ† tout . At each interval, StarSmasher identi-fies the current bound components and then computes their orbital elements. If a merger is detected (i.e., the number of bound components decreases), or if a bi-nary remains bound, we extend the run by resetting 

tf = max( tf , t + 30 âˆ† tout ), where t is the current time, ensuring at least 30 additional outputs. Whenever tf

is updated, the total internal energy is stored, and if it later changes by more than 1% the run is again extended by 30 âˆ† tout to avoid stopping during ongoing thermal readjustment. In cases for which fewer than 100 par-ticles remain in a remnant, we enforce a minimum in-tegration time by setting tf = max( tf , 100 âˆ† tout ) to ensure that the remnant is long-lived. The simulation ends when the time reaches t = tf . 

> 3.

CLASSIFICATION OF COLLISION OUTCOMES The first quantity we aim to predict is the qualita-tive outcome of the collision, closely related to the num-ber of stars remaining after the encounter. In terms of survivors, there are three possibilities: 0, 1, or 2 stars. At sufficiently large pericenter distances, collisions are grazing enough that both stars survive, though not com-pletely intact. As the pericenter distance decreases and collisions become more head-on, the outcome depends more strongly on the relative velocity of the objects, their mass ratio, and details of their stellar structures. For any two colliding stars at sufficiently low relative velocities, the collision will result in a merger. However, as the relative velocity increases at small pericenter dis-tances, collisions can become energetic enough to unbind the material from both stars, leaving no remnants. A special scenario arises in the unequal-mass collisions where, at high enough velocities and moderately off-axis trajectories, only one star is destroyed while the other survives. This results in the surviving star being stripped of its envelope, and so we will refer to these cases as the â€œstrippingâ€ scenario. Although the same number of stars remain as in the merger case, we des-ignate this as a separate class because the underlying hydrodynamic evolution and the properties of the final remnant are qualitatively different. To distinguish between merger and stripped-star out-comes, we monitor the relative orbit and proximity of the two stellar components throughout the collision. In the post-processing, a merger occurs when two star com-ponents become gravitationally bound in such a tight encounter that the semimajor axis of their orbit falls below a critical threshold related to their tidal radii and sizes. Specifically, the code estimates each starâ€™s tidal disruption radius as rtidal ,i = Ri (Mj /M i)1/3, with jÌ¸ = i

and with the effective radius of component i estimated as 

Ri = ( Ii/M i)1/2. Here, Ii is the moment of inertia about the rotation axis, Mi is the total bound mass of com-ponent i, and Mj is the total bound mass of the other component (that is, j = 2 if i = 1, while j = 1 if i = 2). If the semimajor axis is smaller than min( rtidal ,i , R j ) for either component, then we consider this a merger and immediately combine the bound particle sets of the two stars. In the case of no mergers and one star remaining, then we classify the remaining star as a stripped star. We therefore define a four-class classification task with labels 0 , 1, 2, 3, where classes 0, 1, and 2 indicate the number of surviving stars, and class 3 represents the case where a stripped star survives (as opposed to a merger). Before training a model on the simulation data, we transform the input parameters to ensure that they all span comparable ranges, facilitating model training. We apply variable-specific transformations to each com-ponent of the five-dimensional input space that account for their dynamics ranges: 

log( t+0 .001) , log( rp+0 .1) , log( vâˆž+10) , ln( M1), ln( M2)

(1) where t is in units of Gyr, rp in solar radii, vâˆž in km/s and the stellar masses in solar masses. To avoid singu-larities near zero, we introduce offsets for T , rp, and 

vâˆž with the same units and scales as the corresponding variables. In the context of stellar collisions, the pericenter dis-tance is often normalized by the stellar radii to obtain a dimensionless parameter. However, this introduces a strong sensitivity to errors in the stellar radius esti-mates, which are known to be significant in some casesâ€” for example, for massive stars in the Single Star Evolu-tion ( SSE ) code (see Figure 8 in Agrawal et al. 2020). To avoid this issue, we train our ML model using pericenter distances in units of solar radii. Consequently, the peri-center values that the model is trained on vary in scale depending on the stellar masses involved. Additionally, all methods presented below normalize the data following 

xâ€² 

> i

= xi âˆ’ Î¼i

Ïƒi

. (2) where Î¼i and Ïƒi are the mean and standard deviation of input parameter i, respectively. The dataset is divided into 70% training, 15% valida-tion, and 15% testing, corresponding to 19404, 4158, and 564 M

0.2 M 0.4 M 0.8 M 1 M

> 3.00 Myr 4.00 Myr

2 M

> 0.80 Myr 1.00 Myr 3.00 Myr 4.00 Myr

4 M

> 0.20 Myr 1.00 Myr 3.00 Myr 4.00 Myr

8 M

> 0.09 Myr 1.00 Myr 3.00 Myr 4.00 Myr

16 M

> 0.04 Myr 1.00 Myr 3.00 Myr 4.00 Myr

32 M

> 0.02 Myr 1.00 Myr 3.00 Myr 4.00 Myr

64 M

32 M

> 3.00 Myr 4.00 Myr 5.60 Myr
> 0.80 Myr 1.00 Myr 3.00 Myr 4.00 Myr 5.60 Myr
> 0.20 Myr 1.00 Myr 3.00 Myr 4.00 Myr 5.60 Myr
> 0.09 Myr 1.00 Myr 3.00 Myr 4.00 Myr 5.60 Myr
> 0.04 Myr 1.00 Myr 3.00 Myr 4.00 Myr 5.60 Myr

16 M 13 Myr 

> 3.00 Myr 4.00 Myr 5.60 Myr 10 Myr 13 Myr
> 0.80 Myr 1.00 Myr 3.00 Myr 4.00 Myr 5.60 Myr 10 Myr 13 Myr
> 0.20 Myr 1.00 Myr 3.00 Myr 4.00 Myr 5.60 Myr 10 Myr 13 Myr
> 0.09 Myr 1.00 Myr 3.00 Myr 4.00 Myr 5.60 Myr 10 Myr 13 Myr

8 M 40 Myr 

> 13 Myr 40 Myr
> 3.00 Myr 4.00 Myr 5.60 Myr 10 Myr 13 Myr 40 Myr
> 0.80 Myr 1.00 Myr 3.00 Myr 4.00 Myr 5.60 Myr 10 Myr 13 Myr 40 Myr
> 0.20 Myr 1.00 Myr 3.00 Myr 4.00 Myr 5.60 Myr 10 Myr 13 Myr 40 Myr
> 100 Myr 127 Myr

4 M

> 100 Myr 127 Myr
> 32 Myr 40 Myr 100 Myr 127 Myr
> 13 Myr 32 Myr 40 Myr 100 Myr 127 Myr
> 3.00 Myr 4.00 Myr 5.60 Myr 10 Myr 13 Myr 32 Myr 40 Myr 100 Myr 127 Myr
> 0.80 Myr 1.00 Myr 3.00 Myr 4.00 Myr 5.60 Myr 10 Myr 13 Myr 32 Myr 40 Myr 100 Myr 127 Myr
> 100 Myr 316 Myr 600 Myr

2 M

> 100 Myr 316 Myr 600 Myr
> 32 Myr 40 Myr 100 Myr 316 Myr 600 Myr
> 13 Myr 32 Myr 40 Myr 100 Myr 316 Myr 600 Myr
> 3.00 Myr 4.00 Myr 5.60 Myr 10 Myr 13 Myr 32 Myr 40 Myr 100 Myr 316 Myr 600 Myr
> 100 Myr 316 Myr 1 Gyr 3.2 Gyr 5 Gyr

1 M

> 100 Myr 316 Myr 1 Gyr 3.2 Gyr 5 Gyr
> 32 Myr 40 Myr 100 Myr 316 Myr 1 Gyr 3.2 Gyr 5 Gyr
> 13 Myr 32 Myr 40 Myr 100 Myr 316 Myr 1 Gyr 3.2 Gyr 5 Gyr
> 100 Myr 316 Myr 1 Gyr 3.2 Gyr 10 Gyr 11 .2 Gyr

0.8 M

> 100 Myr 316 Myr 1 Gyr 3.2 Gyr 10 Gyr 11 .2 Gyr
> 32 Myr 40 Myr 100 Myr 316 Myr 1 Gyr 3.2 Gyr 10 Gyr 11 .2 Gyr
> 100 Myr 316 Myr 1 Gyr 3.2 Gyr 10 Gyr 13 .7 Gyr

0.4 M

> 100 Myr 316 Myr 1 Gyr 3.2 Gyr 10 Gyr 13 .7 Gyr
> 100 Myr 316 Myr 1 Gyr 3.2 Gyr 10 Gyr 13 .7 Gyr

0.2 M 0.0 0.5 1.0 1.5 2.0 2.5 3.0

rp [R ]

10 1

10 2

10 3

10 4

> vâˆž[km /s]

t = 0 .127 Myr , M1 = 4 .0 M , M2 = 0 .4 M

3 (stripped) 01 (merged) 2

> M1

M2

Figure 1. Complete grid of SPH stellar collision simulations. The left and top axes show the primary and secondary masses in the collisions, respectively. Within each square, the corresponding times at which the collisions were sampled are shown, colored to indicate different times. In the lower right corner, we shown an example of how the pericenter distance ( rp) and velocity at infinity ( vâˆž) are sampled at each age, M1, and M2 dimensions. The colors illustrate the number of stellar remnants, where we distinguish cases where the stars merge from those that result in a stripped star. 

4158 samples. Through stratified sampling, we ensure that each split preserves the overall label proportions. There exists a class imbalance for the stripping (label 3) and mutually destructive cases (label 0). For instance, in the training dataset, they each represent only 4 .8% and 8.1% of the samples, respectively. We prioritize models that optimize predictions across all classes, not just the overrepresented ones. Below, we will present three algo-rithms used for the classification task, and discuss how they compare to each other. 3.1. k-Nearest Neighbors 

k-Nearest Neighbors ( k-NN) (Cover & Hart 1967) is a well-established ML algorithm that classifies data points based on the labels of their â€œ kâ€ closest neighbors. The number of neighbors considered, distance metric, and weighting scheme are all hyperparameters that can be tuned to optimize model performance. We perform a grid search over the number of neighbors (between 3 and 25), the distance metric (Euclidean or Manhattan), and weighting scheme (uniform for all points or weighted by inverse distance). Performance is evaluated using 6balanced accuracy, which averages over the accuracies from each class and reduces sensitivity to class imbal-ance. The set of hyperparameters that yields the high-est performance consists of five neighbors, Euclidean distance, and a weighting scheme in which points are weighted inversely proportional to their distance. The model achieves a test balanced accuracy of 91.7%, with both accuracy and balanced accuracy reported in Ta-ble 2, alongside the performance for the ML methods described below. 3.2. Support Vector Machines 

Support Vector Machines (SVMs) (Cortes & Vapnik 1995) are supervised ML algorithms that optimize deci-sion boundaries by finding hyperplanes that best sepa-rate the different classes in kernel space. Through ker-nel transformations, SVMs can classify non-linear data, making them particularly effective for complex datasets and multi-class classification. As in the previous section, we perform a grid search to find the best-performing set of hyperparameters us-ing the Support Vector Classifier (SVC) available in 

scikit-learn (Pedregosa et al. 2011). The relevant parameters to optimize with SVCs and the values we explore include (1) the kernel that transforms the in-put data into a higher-dimensional feature space (Gaus-sian radial basis function ( rbf ) or polynomial), (2) C, a regularizer that controls the complexity of the decision boundaries (0 .1, 1, 10 , 100 , 1000 , 10000), and (3) gamma ,which controls how much influence a single data point has (0 .001 , 0.01 , 0.1, 1). Within the polynomial kernel function, we also search over varying degrees (2 and 3). As expected, we find rbf to be the optimal kernel, as it offers greater flexibility in non-linear decision bound-aries. Furthermore, the best-performing hyperparame-ters are C=10 , 000 and gamma = 0 .1, yielding a test bal-anced accuracy of 97.7%. Since the optimized value of C

is at the edge of the parameter search range, we perform an additional localized search with higher values, which still yields an optimized value of C=10 , 000. 3.3. Neural Network 

Neural Networks (NNs) learn a mapping between in-puts and outputs through a series of weighted transfor-mations and non-linear activation functions (e.g., Mc-Culloch & Pitts 1943; LeCun et al. 2015). Our NN is a multi-layer perceptron (MLP) (Hornik et al. 1989) composed of three hidden layers, with 512 , 256 , and 128 neurons, respectively. Each hidden layer is followed by a Rectified Linear Unit (ReLU) activation function (e.g., Fukushima 1969; Glorot et al. 2011; Maas et al. 2013; Agarap 2018) and layer normalization (Lei Ba et al. 2016). All hyperparameters, including network architecture, batch size, optimizer, initial learning rate, and sched-uler parameters, were optimized through a hyperparam-

Table 1. Classification Hyperparameters                           

> Parameter Optimized Value Search Range Batch Size 64 64 âˆ’256 Epochs 500 500 âˆ’2000 Optimizer AdamW AdamW, sgd
> Scheduler CA-LR CA-LR, RLRP
> Learning Rate 0.0002 10 âˆ’5âˆ’0.1
> patience Â· Â· Â· 10 âˆ’60
> factor Â· Â· Â· 0.3âˆ’0.5
> Note â€”Hyperparameter optimization search for the classification task. Acronyms: stochastic gradient descent ( sgd ), AdamW optimizer (AdamW ), Cosine Annealing Learning Rate scheduler ( CA-LR ), and Reduce Learning Rate on Plateau scheduler ( RLRP ).

eter search using the Weights and Biases platform 2. We performed a Bayesian optimization (Snoek et al. 2012) sweep, which efficiently identifies hyperparameter com-binations that improve model performance based on pre-vious attempts. Table 1 lists the ranges explored for each hyperparameter and its optimized value. The training and validation data are split into batches of size 64. We use the AdamW (Loshchilov & Hut-ter 2017) optimizer with a weighted cross entropy loss function, where class weights account for the imbalanced representation of collision outcomes. The learning rate is dynamically adjusted using a CosineAnnealingLR 

scheduler, a feature available in the PyTorch library (Paszke et al. 2019). The learning rate starts at 0 .0002 and decreases following a cosine function down to a minimum value of 10 âˆ’8. The model is trained for 500 epochs, which we find sufficient for convergence. The patience and factor parameters belong to the 

ReduceLROnPlateau scheduler, where the learning rate decreases by a factor if a target metric (in our case the validation balanced accuracy) does not improve for a certain number of consecutive epochs (determined by the patience parameter). No optimized values for these two hyperparameters are shown, since the best perform-ing model uses a CosineAnnealingLR scheduler. For each training run, the epoch with the highest val-idation balanced accuracy is saved. Among the mod-els in the hyperparameter optimization sweep, the one with the highest validation balanced accuracy was cho-sen. Its stability was then assessed by performing an additional sweep across ten different random seeds. The mean and standard deviation of the test accuracy and balanced accuracy across these ten runs are reported in Table 2. We do not perform this stability test for the other methods because k-NN is a non-parametric  

> 2https://wandb.ai/site/

7

Table 2. Classification Performance              

> Method Accuracy [%] Balanced Accuracy [%]
> k-NN 96.0 91.7 SVC 98.3 97.7 NN 98.5 Â±0.4 (Best: 98.6) 97.9 Â±0.2 (Best: 98.4)
> Note â€”Performance on the classification task across different methods. For the neural network (NN), we report mean and standard deviation across ten runs with optimized hyperparameters and different random seeds. The best performing run (shown in parentheses) is used in subsequent figures.

method and SVC finds a global solution, unlike neural networks where random weight initialization can lead to different model performance. The best-performing model from this sweep is used in all subsequent analy-ses, and achieves a balanced accuracy of 98.4%. 3.4. Algorithm Performance Comparisons 

Table 2 lists the testing accuracies and balanced ac-curacies across the three methods. The SVC and NN achieve comparable performances, both outperforming 

k-NN, our baseline. The SVCâ€™s competitive perfor-mance is expected, as it explicitly maximizes the margin to the decision boundary, which proves effective for our dataset. Figure 2 shows the confusion matrix for each method, summarizing classification performance across the class labels. High values along the diagonal indicate optimal performance, which correspond to correct predictions for each class. Both the SVC and NN outperform k-NN, our baseline, across all classes. k-NN and SVC perform best in the classes with larger number of training samples and struggle with undersampled cases, particularly scenarios where no stars survive (class 0) or where one stripped star remains after a destructive collision at high relative velocity (class 3). For both methods, class 3 exhibits the lowest accuracy. The reduced performance for class 3 can be attributed to a combination of two factors: (1) its limited number of samples and (2) its strong depen-dence on stellar structure. In particular, the location and extent of the region where this outcome occurs vary significantly across evolutionary times and mass ratios. For example, as illustrated later in Figure 3, the region of class 3 outcomes when the 32 MâŠ™ star is at the TAMS stage (rightmost panel) occurs at lower pericenter dis-tances than at an earlier time (middle panel), and is not present in the equal mass collision (leftmost panel). The NN has more balanced performance, in part due to the use of a weighted cross entropy loss function that penalizes misclassifications in class 3 more heavily. As a result, the NN outperforms the SVC on minority classes. For both methods, the most common misclassifications occur when samples from label 2 (a majority class) are classified as label 3 (a minority class). This is also likely due to the chosen optimization strategy, which priori-tizes performance on minority classes and consequently leads to an increased number of misclassifications in the majority classes. To visualize how these methods differ in their classifi-cation strategies, we show example decision boundaries in feature space in Figure 3. Each row showcases the predictions from the different ML algorithms while each column represents different physical scenarios. The first column shows collision outcomes for an equal-mass interaction as a function of normalized peri-center distance and relative velocity at infinity. For vi-sualization purposes, the pericenter distance has been normalized with respect to the sum of the stellar radii; however, the models were trained using the unnormal-ized pericenter values. Model predictions are shown as a shaded background, while the ground-truth labels are indicated by the colors of the data points. The mid-dle column illustrates a collision between unequal-mass stars with a mass ratio of âˆ¼ 0.13. The rightmost column shows collision outcomes when the primary star, in this case a 32 MâŠ™ star, is on the TAMS. Because the pericenter distance is sampled across radii enclosing a fixed mass fraction, the pericenter distances at the TAMS are smaller than at earlier evolutionary stages. This reflects the structural evolution of the 32 MâŠ™ star, which develops a steeper density profile as it approaches the TAMS. Consequently, the third col-umn highlights the sensitivity of the collision outcome to stellar structure, as the stripping class occurs at smaller relative pericenter distances compared to the earlier evo-lutionary stages. Across each column, we show 2D projections of the decision boundaries produced by each algorithm. The 

k-NN method does not predict smooth decision bound-aries, but rather has artificial features due to the intrin-sic distances between the sampled neighbors. In con-trast, the SVC produces smoother decision boundaries by finding hyperplanes that optimize the classification performance in kernel space. However, discontinuities appear at small pericenter distances, which are likely artifacts of projecting a higher-dimensional decision sur-face onto two dimensions and suggest that the SVC may produce overly complex decision surfaces that introduce nonphysical features. Despite the comparable test balanced accuracies for the SVC and NN, the smoothest and most physically-consistent decision boundaries are achieved with the NN. This reflects the NNâ€™s ability to learn complex, non-linear mappings between inputs and outputs, en-abling it to capture the dependencies across features and therefore produce more physically-consistent clas-sification predictions. In contrast, SVCs are optimized to find hyperplanes that separate classes using support 80 1 2 3 

> Predicted
> 0123True
> 87 .3% (288 /330) 1.5% (5 /330) 4.5% (15 /330) 6.7% (22 /330) 0.1% (2 /1743) 98 .0% (1708 /1743) 1.8% (32 /1743) 0.1% (1 /1743) 0.3% (6 /1891) 2.1% (40 /1891) 96 .8% (1831 /1891) 0.7% (14 /1891) 6.7% (13 /194) 3.1% (6 /194) 5.7% (11 /194) 84 .5% (164 /194)

k-NN     

> 0123
> Predicted
> 0123True
> 97 .0% (320 /330) 0.3% (1 /330) 0.0% (0 /330) 2.7% (9 /330) 0.1% (2 /1743) 99 .4% (1732 /1743) 0.5% (9 /1743) 0.0% (0 /1743) 0.3% (6 /1891) 0.9% (17 /1891) 97 .6% (1846 /1891) 1.2% (22 /1891) 0.5% (1 /194) 1.0% (2 /194) 1.5% (3 /194) 96 .9% (188 /194)

SVC     

> 0123
> Predicted
> 0123True
> 97 .9% (323 /330) 0.3% (1 /330) 0.0% (0 /330) 1.8% (6 /330) 0.1% (1 /1743) 99 .4% (1733 /1743) 0.4% (7 /1743) 0.1% (2 /1743) 0.2% (4 /1891) 0.7% (13 /1891) 97 .9% (1852 /1891) 1.2% (22 /1891) 1.0% (2 /194) 0.0% (0 /194) 0.5% (1 /194) 98 .5% (191 /194)

NN 

Figure 2. From left to right, confusion matrices for the testing data using k-Nearest Neighbors algorithm ( k-NN), Support Vector Classifier (SVC), and a Neural Network (NN). The accuracy and total number of samples in each class are shown. 

vectors and therefore struggle in areas where data may be sparse or classes are imbalanced. As expected, most misclassifications occur near the decision boundaries, where small changes in pericenter distance or velocity can yield different outcomes. The boundaries shift with stellar mass and age, highlighting the complexity of the classification task. Nevertheless, despite a high-dimensional dataset that encompasses a wide range in its feature space, both the SVC and NN achieve remarkable accuracies. In future work, we will incorporate active learning to guide the optimal way to expand the dataset and further improve model perfor-mance. 

4. REGRESSION OF COLLISION OUTCOME PROPERTIES An additional set of key quantities to predict are the final properties of stellar collision remnants, particularly their final masses. This is motivated in part by the fact that most N -body codesâ€”especially those model-ing environments with low velocity dispersionsâ€”assume no mass loss during stellar mergers (the â€œsticky sphere approximationâ€). However, this assumption can break down at small pericenter distances and higher velocities. To predict the final masses of the colliding stars (or the merger product, if the stars merge), we again com-pare three methods: k-NN, SVM, and NN. Rather than directly predicting the final masses, we predict three nor-malized mass fractions: 

 M1,f

Mtot ,i

, M2,f

Mtot ,i

, Mu,f

Mtot ,i



(3) where M1,f and M2,f are the final masses of stars 1 and 2, respectively, M tot ,i is the total initial mass of the system, and Mu,f is the final unbound mass. By construction, these three mass fractions always sum to unity due to mass conservation. Formatting the regression targets in this way encourages our models to predict physically consistent mass estimates. To reduce inhomogeneities in the dataset without altering the out-come of the collisions, we assign the final merger prod-uct mass to star 1 in the case of a single remnant. The same input data is used as in Section 3, and results are summarized in Table 4. 4.1. k-Nearest Neighbors 

To perform regression using the k-NN method, we con-duct the same search strategy as in Section 3.1, but score models using the mean absolute error (MAE). Because the regression targets are mass fractions, the MAE is relative to the initial total mass in the system. To eval-uate model performance on the test dataset, we multiply the predicted fractions by M tot ,i and compute the abso-lute errors in the final masses, as well as relative errors for cases in which the star survives. The optimized k-NN model uses 3 neighbors, Euclidean metric, and a inverse-distance weighting. The resulting median abso-lute errors in M1,f and M2,f are 0.021 MâŠ™ and 1e-05 MâŠ™,respectively. 4.2. Support Vector Regression 

To perform regression with an SVM, we use Epsilon-Support Vector Regression (SVR hereafter). The algo-rithm learns a regression hyperplane, where Ïµ defines the margin around the hyperplane. Training points within the margins donâ€™t contribute to the loss, while points outside the margins become support vectors that shape and optimize the hyperplane. Hyperparameter optimization is performed for each regression quantity separately, beginning with a coarse grid search and sub-sequently refining it. Full details of the hyperparameter search are outlined in Appendix A. The optimal con-figuration for all regression targets uses an rbf kernel with C = 1, Ïµ = 0 .00001, and Î³ = 10. The resulting best model achieves median absolute errors in M 1,f and M2,f of 0.028 MâŠ™ and 0.021 MâŠ™, respectively; additional performance metrics are listed in Table 4. 91.01.52.02.53.03.54.0                      

> log 10 (v âˆž [km /s])
> k-NN
> t = 0 .001 Gyr ,M1= 32 .0M,M2= 32 .0M
> Train Val Test
> No stars Merger Two stars Stripped star
> t = 0 .001 Gyr ,M1= 32 .0M,M2= 4 .0M
> Class
> 0123
> t = 0 .006 Gyr ,M1= 32 .0M,M2= 4 .0M
> 1.01.52.02.53.03.54.0
> log 10 (v âˆž [km /s])
> SVC
> 0.11

log 10 (r p/(R 1 + R 2))  

> 1.01.52.02.53.03.54.0
> log 10 (v âˆž [km /s])
> NN
> 0.11

log 10 (r p/(R 1 + R 2))  

> 0.11

log 10 (r p/(R 1 + R 2)) 

Figure 3. Two dimensional examples of decision boundaries for the stellar collision outcome classification task as a function of pericenter distance ( rp) and speed at infinity ( vâˆž) for fixed age and stellar mass values. Each color indicates a different class, where 0 is a mutual destruction case, 1 is a merger, 2 a fly by, and 3 a stripped star. The different marker shapes indicate the training (circles), validation (squares), and testing (triangles) datasets and are colored according to the ground truth. Classification predictions are shown as a background contour. Each column represents a different collision regime, where the first column is an equal mass collision, the middle panel is an unequal mass ratio collision, and the third panel is at the TAMS of the primary star. The x axis is normalized to standardize the scales across columns. 

The median absolute errors for M1,f are comparable between the k-NN and SVR models. However, the same is not true for M2,f , as the SVR predictions exhibit errors approximately three orders of magnitude larger. The superior performance of k-NN indicates that regres-sion quantities in this input space are well-approximated by adjacent neighbors, and parametric methods such as SVRs struggle to capture it. Nevertheless, median rel-ative errors for M2,f (in cases where the star survives) remain below 2% for the SVR. 4.3. Neural Network 

As in Section 3.3, the NN used for the regression task is an MLP composed of three hidden layers, with 512 , 256, and 128 neurons, each followed by a ReLU ac-tivation function and layer normalization. A key archi-tectural feature is the application of a softmax function to the network outputs, which enforces mass conserva-tion by requiring the three predicted mass fractions to be positive and sum to one. The loss is defined as the MAE of the predictions. Because the predicted values correspond to the final 10 

Table 3. Regression Hyperparameters                               

> Parameter Optimized Value Hyperparameter Search Range Epochs 2000 500 âˆ’2000 Batch Size 64 64 âˆ’512 Optimizer AdamW AdamW, sgd
> Scheduler CA-LR CA-LR, RLRP
> Learning Rate 0.0002 10 âˆ’5âˆ’0.1
> patience Â· Â· Â· 10 âˆ’120
> factor Â· Â· Â· 0.2âˆ’0.8
> auxweight 0.31 0.05 âˆ’1.0
> Note â€”Hyperparameter optimization search for the regression task. Acronyms: stochastic gradient descent ( sgd ), AdamW optimizer (AdamW ), Cosine Annealing Learning Rate scheduler ( CA-LR ), and Reduce Learning Rate on Plateau scheduler ( RLRP ).

fractional mass in each stellar component and unbound mass, the loss represents the error in the final predicted masses normalized by the total initial mass in the sys-tem. In addition, the contribution to the loss from the predicted unbound mass term is weighted in order to be less than or equal to the loss from the predicted masses. This is done to encourage the model to pri-oritize minimizing errors in the predicted stellar masses rather than the unbound mass. The value of this weight (auxweight ) is treated as a hyperparameter and opti-mized during tuning. We perform hyperparameter tuning as described in Section 3.3 and list the explored range in each param-eter along with the optimal configuration in Table 3. For each hyperparameter configuration, the epoch with the lowest validation loss is selected to avoid overfit-ting. The optimized epoch value (2000) is at the edge of the explored range, but we confirmed that training converged. The model in the sweep with the lowest validation median absolute error in M1,f is selected as the best-performing model. Median-based metrics are adopted because they more accurately reflect the typical regression error and are less sensitive to outliers arising from misclassified points near decision boundaries. Fur-thermore, the errors in M1,f were chosen as our primary metric to select the best-performing model for two rea-sons: (1) in cases where only a single star survives, the final mass is always assigned to star 1, and (2) minimiz-ing errors in M1,f effectively improves performance for the remaining predicted quantities due to the enforced mass conservation restriction. Following the same pro-cedure as in Section 3.3, we conduct an additional sweep that varies the random seed to test for model stability. This yields the best-performing model with median ab-solute errors of 0 .0047 MâŠ™ for M1,f and 3 .7 Ã— 10 âˆ’7 for 

M2,f . Errors in M2,f are smaller than those in M1,f be-cause M2,f is zero in a significant portion of the dataset (in cases of a stellar merger and mutual disruption), and 

Table 4. Regression Performance Metrics                     

> Method Median Absolute Error [M âŠ™]Median Relative Error
> M1,f M2,f M1,f M2,f
> k-NN 0.021 1e-05 0.0037 0.011 SVR 0.028 0.021 0.0045 0.015 NN [ Ã—10 âˆ’3]4.67 Â±0.23 (best: 4.41) 0.00047 Â±0.00038 (best: 0.00037) 1.09 Â±0.05 (best: 1.05) 1.76 Â±0.15 (best: 1.51)
> Note â€”Regression performance metrics for k-Nearest Neighbors ( k-NN), sup-port vector regression (SVR), and neural network (NN). We report the mean and standard deviation across ten runs with optimized hyperparameters and different random seeds. Fractional relative errors are only computed for cases where the star survives. Note that performance metrics for the NN are scaled by a factor of 10 âˆ’3.

the model accurately predicts these zero values. All re-gression performance metrics are listed in Table 4. Figure 4 shows the predicted final stellar masses as functions of pericenter distance and relative velocity at infinity for a collision between two 32 MâŠ™ stars at 1 Myr. The model predictions are shown as shaded contour maps, while training and validation samples are shown as scatter points colored by their ground-truth values. Testing points are colored according to the absolute er-rors of their predictions. Qualitatively, despite the simplicity, the k-NN model captures the boundaries between different collision out-comes and exhibits relatively smooth predictions within each region. In contrast, the SVR suffers from non-physical features within each region, such as localized increases in mass loss at velocities between 10 and 100 km/s. The NN recovers the transition boundaries accu-rately and, like k-NN, reproduces characteristic features of equal-mass collisions, such as identical final masses when both survive (shown in light gray). Quantitatively, the highest errors occur near transi-tion boundaries, as expected. These errors are in fact misclassifications and are fundamentally different from regression errors for correctly classified data points. For example, errors in data points away from the transition boundaries tend to be smaller. To characterize the error distributions across the en-tire test dataset, Figure 5 shows violin plots of the ab-solute and relative errors for all three methods. For pre-dictions of M1,f , k-NN and SVR exhibit similar absolute and relative error distributions, while the NN achieves a lower mean and median errors. For M2,f , the k-NN and NN error distributions show pronounced tails toward small absolute errors, corre-sponding to cases in which the final mass is correctly predicted to be close to zero (i.e., collisions where either only one star survives or both stars are destroyed). Al-though the SVR exhibits a larger median absolute error for M2,f , its relative-error distribution indicates that, in 11 1.01.52.02.53.03.54.0

> log 10 (v âˆž [km /s]) kNN

M1,final 

> Train Val Test

M2,final  

> 010 20 30 40 50 60 Mfinal  [M ]
> 010 âˆ’3
> 10 âˆ’2
> 10 âˆ’1
> 10 0
> 10 1
> |Absolute Error | [M  ]

1.01.52.02.53.03.54.0 

> log 10 (v âˆž [km /s]) SVR
> 010 20 30 40 50 60 Mfinal  [M ]
> 010 âˆ’3
> 10 âˆ’2
> 10 âˆ’1
> 10 0
> 10 1
> |Absolute Error | [M  ]

0.1 1

log 10 (r p/(R 1 + R 2)) 

1.01.52.02.53.03.54.0

> log 10 (v âˆž [km /s]) NN

0.1 1

log 10 (r p/(R 1 + R 2))  

> 010 20 30 40 50 60 Mfinal  [M ]
> 010 âˆ’3
> 10 âˆ’2
> 10 âˆ’1
> 10 0
> 10 1
> |Absolute Error | [M  ]

t = 0 .001 Gyr , M1 = 32 .0 M , M2 = 32 .0 M

Figure 4. Examples of 2D regression maps for the final stellar masses as a function of pericenter distance (r p) and relative velocity at infinity ( vâˆž). From top to bottom, the rows indicate predictions from k-Nearest Neighbors ( k-NN), Support Vector Regression (SVR), and Neural Networks (NN). The first two columns show predicted final stellar masses in the color gradient, while the rightmost color shows the absolute error in the predictions for the test dataset. The different markers indicate points from the training (circles), validation (squares), and testing (triangles) datasets. The training and validation points are colored according to the ground truth. 

cases where M2,f is non-zero and therefore relevant, the SVR performs comparably to k-NN. The NN achieves median and mean relative errors below 1% for both stellar masses. Overall, the error distributions shown in Figure 5, together with the performance metrics in Table 4, demonstrate that the NN achieves the lowest errors across all regression targets and is therefore the best-performing method. 

5. MIXTURE OF EXPERTS While training a classifier and a regressor separately yields high accuracies, here we investigate whether a shared architecture between the two tasks can further improve model performance. To this end, we draw inspi-ration from the Mixture of Experts (MoE) architecture (Jordan & Jacobs 1993), which is designed to handle complex data by training different â€œexpertsâ€ on separate tasks. A gating mechanism, or router, determines which 12 âˆ’10 

> âˆ’8
> âˆ’6
> âˆ’4
> âˆ’2
> 0
> 2
> 4
> log 10 (Absolute Error M 1)[M  ]
> âˆ’10
> âˆ’8
> âˆ’6
> âˆ’4
> âˆ’2
> 0
> 2
> 4
> log 10 (Absolute Error M 2)[M  ]
> âˆ’10
> âˆ’8
> âˆ’6
> âˆ’4
> âˆ’2
> 0
> 2
> 4
> log 10 (Relative Error M 1)[%]
> âˆ’10
> âˆ’8
> âˆ’6
> âˆ’4
> âˆ’2
> 0
> 2
> 4
> log 10 (Relative Error M 2)[%]

k-NN SVR NN Median Mean 

Figure 5. Absolute (left two panels) and relative (right two panels) error distribution in the predicted final stellar masses. Colors indicate the results from different algorithms. Colored solid (dashed) bars indicate the median (mean) of each distribution. 

data each expert specializes in. Optimizing this gat-ing mechanism is crucial for achieving a well-performing model. In our approach, we leverage the classification pre-dictions as a gating mechanism to route samples to spe-cialized regression experts, each trained exclusively with data from one class. This design offers two key advan-tages. First, it enhances interpretability, as it is explic-itly known which type of data should be routed to each expert, in contrast to an optimized gating mechanism where routing decisions are learned implicitly. Second, this architecture is motivated by the strong dependence of regression properties on collision outcomes. For in-stance, mutual destruction events will always result in both final masses being zero. By training individual experts on separate collision outcomes, we enable each model to specialize in distinct physical regimes instead of requiring one model to capture all regression patterns simultaneously. The MoE architecture is shown in Figure 6. The five-dimensional input data is first passed through a shared backbone consisting of two fully-connected layers with dimensions 512 and 256, each followed by layer normal-ization and ReLU activation. The features are then fed into a classification head composed of one fully-connected layer (dimension 128) with layer normaliza-tion and ReLU activation, followed by a final layer that maps to four-dimensional logits corresponding to the collision outcome classes. The predicted class label is then used as a gating mechanism to route the data to its corresponding regression expert. Each expert has the same one-layer architecture as the classification head, but we apply a final linear layer that outputs the three predicted values followed by a softmax activation to en-sure mass conservation. When training a multitask NN such as our MoE ar-chitecture, the structure of the loss function is crucial to optimize task-specific metrics. As described in Sections 3.3 and 4.3, we use cross-entropy loss for the classifica-

Table 5. MoE Hyperparameters                               

> Parameter Optimized Value Hyperparameter Search Range Epoch 500 500 âˆ’2000 Batch Size 128 64 âˆ’512 Learning Rate 0.0004 10 âˆ’5âˆ’0.1Optimizer AdamW AdamW, sgd
> Scheduler CA-LR CA-LR, RLRP patience Â· Â· Â· 5âˆ’80
> factor Â· Â· Â· 0.2âˆ’0.8
> auxweight 0.77 0.05 âˆ’1.0
> Note â€”Hyperparameter optimization search for the regression task. Acronyms: stochastic gradient descent ( sgd ), AdamW optimizer (AdamW ), Cosine Annealing Learning Rate scheduler ( CA-LR ), and Reduce Learning Rate on Plateau scheduler ( RLRP ).

tion task and MAE loss for the regression task. The regression loss is comprised of two terms, one for the predicted final mass fractions in each star, and another for the fraction of unbound mass. The weight for the unbound mass term is optimized during hyperparame-ter tuning ( auxweight ). To combine the task-specific losses into a joint loss, we use uncertainty-weighted loss (see Eq. 10 in Kendall et al. 2017) with the modification to the regularization term outlined in Liebel & KÂ¨ orner (2018), which weights each task-specific loss by a learn-able parameter that is jointly optimized with the net-work weights. This eliminates the need for deciding the respective weights in the loss function from each task. A Weights and Biases sweep is performed, with the explored hyperparameter ranges and optimized values listed in Table 5. The epoch with the lowest validation loss is chosen, as a low validation loss will correspond to high balanced accuracy and low absolute errors in the predicted mass fractions. We score models using the metric described in the Appendix C, which prioritizes classification performance while rewarding low regres-13 Shared Layers Classification Task Regression Task      

> Class 0 Expert
> Class 3 Expert Class 2 Expert Class 1 Expert
> (5 x 512) (512 x 256)
> (128 x 3)
> (256 x 128)
> Linear + LayerNorm + ReLU Linear (128 x 4) Linear (128 x 3)
> Softmax (256 x 128) (128 x 4)
> Classification Gate

Input 

Figure 6. Diagram of our Mixture of Experts (MoE) Neural Network. The five-dimensional input data is passed through a shared backbone before being sent to a classification head. The classification labels are used as the gating mechanism to send data to its specialized expert according to its collision outcome class. The regression experts share the same NN structure as the classification heads, with an additional soft max activation function to ensure mass conservation. 

sion errors above a defined threshold. As in the previous sections, we perform an additional sweep which includes different random seeds, arriving at a best-performing model with a balanced accuracy in the classification task of 98.5% and median absolute errors of 0 .01 MâŠ™

and 0 .000004 MâŠ™ in M1,f and M2,f , respectively. Addi-tional performance metrics are listed in Table 6. Note that some of the reported errors are smaller than the mass of a single SPH particle. However, they remain informative in this context, as they indicate the errors from the NN predictions. Figure 7 shows the confusion matrix of the best-performing MoE model. The classification balanced ac-curacy, also listed in Table 6, is comparable to that of the SVR and NN models discussed in Sections 3.2 and 3.3, indicating that the gating mechanism is well-trained. This is particularly important because the ac-curacy of the regression predictions relies on the data being routed to the appropriate regression head. Shown in Table 6 are the performance metrics for the regressed quantities, with the metrics from the individ-ually trained NNs in Sections 3.3 and Sections 4.3 in-cluded for convenience. To visualize the differences in the error distributions between the individually trained NN and the MoE architecture, Figure 8 shows the dis-tribution of absolute errors in the final mass predictions for different ground truth class labels. Across all labels and for both masses, the regression NN consistently has lower mean and median errors compared to the MoE. For class 0 outcomes, both methods yield median and mean errors well below the minimum precision at which the data are physically meaningful. For label 1 data, 0 1 2 3

Predicted 

0123 True 

> 98 .5% (325 /330) 0.3% (1 /330) 0.3% (1 /330) 0.9% (3 /330) 0.1% (1 /1743) 99 .6% (1736 /1743) 0.3% (6 /1743) 0.0% (0 /1743) 0.4% (7 /1891) 0.6% (11 /1891) 97 .6% (1846 /1891) 1.4% (27 /1891) 0.5% (1 /194) 0.5% (1 /194) 0.5% (1 /194) 98 .5% (191 /194)

MoE 

Figure 7. Confusion matrix for the testing data using Mixture of Experts (MoE). The accuracy and total number of samples in each class are shown. 

the errors in the final mass of the merger product M1,f

(always assigned to star 1) are comparable between the two methods, as are the errors for M1,f in the label 3 case. For both final masses in label 2 cases, the sep-arately trained NN outperforms the MoE predictions. The effectiveness of the routing gates at reducing high absolute errors outliers can be seen in the error distribu-tions for M1,f in case 0 and M2,f in case 3, where the NN 14 

Table 6. MoE and Separate NN Performance Metrics                                       

> Model Accuracy [%] Balanced Accuracy [%] Median Absolute Error [M âŠ™]Median Relative Error
> M1,f [Ã—10 âˆ’3]M2,f [Ã—10 âˆ’3]M1,f [Ã—10 âˆ’3]M2,f [Ã—10 âˆ’3]MoE 98.5 Â±0.1 (best: 98.6) 98.2 Â±0.2 (best: 98.5) 12.5 Â±1.5 (best: 10.4) 0.01 Â±0.01 (best: 0.004) 3.1 Â±0.3 (best: 2.7) 8.4 Â±1.7 (best: 7.5) Separate NN 98.5 Â±0.4 (best: 98.6) 97.9 Â±0.2 (best: 98.4) 4.67 Â±0.23 (best: 4.41) 0.00047 Â±0.00038 (best: 0.00037) 1.09 Â±0.05 (best: 1.05) 1.76 Â±0.15 (best: 1.51)
> Note â€”Classification and regression performance metrics for Mixture-of-Experts (MoE). We report the mean and standard devi-ation across ten runs with optimized hyperparameters and different random seeds. Relative errors are only computed for cases where the star survives. The metrics for the classification and regression neural networks described in Sections 3.3 and 4.3, respectively, are included for convenience. Note that regression performance metrics are scaled by a factor of 10 âˆ’3.NN MoE
> âˆ’10
> âˆ’8
> âˆ’6
> âˆ’4
> âˆ’2
> 0
> log 10 (Absolute Error [M  ])
> Label 0
> NN MoE
> âˆ’10
> âˆ’8
> âˆ’6
> âˆ’4
> âˆ’2
> 0
> 2
> log 10 (Absolute Error [M  ])
> Label 1 (Merger)
> NN MoE
> âˆ’6
> âˆ’4
> âˆ’2
> 0
> log 10 (Absolute Error [M  ])
> Label 2
> NN MoE
> âˆ’10
> âˆ’8
> âˆ’6
> âˆ’4
> âˆ’2
> 0
> 2
> log 10 (Absolute Error [M  ])
> Label 3 (Stripped)
> M1,f(left) |M2,f(right) Median Mean

Figure 8. Distribution of absolute errors in the final predicted masses using the neural network (NN) described in Section 4.3 and the Mixture of Experts (MoE) described in Section 5. Each panel shows the distribution of absolute errors for the final masses of star 1 on the left and star 2 on the right of the violin plot, for data with a specific ground truth classification label. Small errors are rounded to 10 âˆ’10 . Solid (dashed) lines show median (mean) values of the raw distribution. 

has a tail towards higher errors, while the MoE exhibits a more constrained distribution at lower errors. The observed differences in the error distributions are due to the structure of the loss function, and its ef-fectiveness at balancing the relative importance of each task. In the individually trained regression NN, the loss function includes only terms directly related to the re-gression quantities. In contrast, the MoE loss function combines terms from both the classification and regres-sion tasks adaptively throughout training. Details of the MoE loss function during training are provided in Ap-pendix B. Future studies will explore alternative MoE architectures and loss functions to improve predictive accuracy. 

6. A NEW SOFTWARE TOOL FOR STELLAR COLLISIONS Building on the models developed in the previous sec-tions, we introduce collAIder , a tool designed to pre-dict stellar interaction outcomes and remnant proper-ties. This tool leverages ML models to provide rapid predictions for interactions between two MS stars across a wide range in masses, pericenter distances, and veloci-ties. While prediction accuracy is the highest within the parameter space sampled by the SPH grid, the tool has been designed to generalize beyond the direct physical collision regime, extending into the tidal capture and fly-by limits. Extrapolation beyond the sampled space can yield inaccurate results, which we explore in Section 7. The largest pericenter distance sampled in our grid is the sum of the individual stellar radii. However, stellar interactions at larger radii can yield interesting results as well. For example, close passages between stars can induce stellar oscillations that dissipate orbital energy (e.g., Lee & Ostriker 1986; Press & Teukolsky 1977). If the tidal perturbation is strong enough, on the order of the orbital energy, the two stars can then form a bound system. This is known as â€œtidal capture,â€ first invoked to explain the close X-ray binaries observed in globular clusters (Fabian et al. 1975). At distances larger than the capture radius, the stars will simply pass by each other without forming a bound system or colliding, i.e., a simple fly-by. 15 The pipeline begins by identifying whether the given initial properties of the interaction are in the collision, tidal capture, or fly-by regimes. To do this, for a given stellar interaction, if rp > R 1 + R2, we calculate the energy dissipated by tides in the following way. Press & Teukolsky (1977) and Lee & Ostriker (1986) derived the expression for energy dissipation from tides as âˆ†E1 =

 GM 21

R1

  M2

M1

2 âˆžX

> l=2

 R1

rp

2l+2 

Tl(Î·) (4) where l = 2 is the quadrupole term, l = 3 the octupole term, etc., and Tl(Î·) is a dimensionless function of 

Î· =

 M1

M1 + M2

 12  rp

R1

 32

(5) Most of the orbital energy is transferred through the quadrupole and octupole perturbations, so we limit our approximations to only those terms. For simplicity, and to enable the use of pre-determined fits, we assign each star an effective polytropic index for the purpose of computing Tl(Î·) values: for stars with M < 0.4MâŠ™

we use n = 1 .5 (appropriate for fully convective low-mass MS stars), while for stars with M > 0.8MâŠ™ we use 

n = 3 (representing a more centrally condensed struc-ture). This mapping is adopted for computational con-venience. A more self-consistent approach would com-pute Tl(Î·) for each stellar model from linear oscilla-tion calculations, for example using GYRE (Townsend & Teitler 2013; Sun et al. 2023), but that is beyond the scope of the present work. For stars with masses be-tween 0 .4 and 0 .8MâŠ™, we calculate the energy from tides for both polytropic indices at each given mass, and in-terpolate the final approximate values as 

Eapprox = E3(Mâ‹† âˆ’ 0.4MâŠ™) + E1.5(0 .8MâŠ™ âˆ’ Mâ‹†)0.4MâŠ™

(6) where E3 and E1.5 are the energy losses from tidal dis-sipation for n = 3 and n = 1 .5 for a given mass Mâ‹†,respectively. The values for Tl(Î·) are taken from the fits in Portegies Zwart & Meinen (1993). We do not apply these fits for data with Î· > 10 and instead assume en-ergy dissipation through tidal interactions is negligible (since high Î· values correspond to pericenter distances in the fly-by regime). Finally, to estimate stellar radii for any given mass, we interpolate from the two nearest stellar mass tracks pro-vided by Posydon v2 (Andrews et al. 2025). More specif-ically, our tool includes an HDF5 file containing the single-star MESA hydrogen-MS evolution tracks, which finely sample initial stellar masses from 0 .1 to 300 MâŠ™.For a given stellar mass and age, we interpolate each radius to the target age. Then, we perform a second interpolation between the two radii based on the target mass. This method allows us to accurately estimate stel-lar radii quickly, using MESA models that were generated with almost identical stellar evolution prescriptions to the ones in the SPH grid. These single-star MESA models also enable us to de-termine whether a given star has evolved beyond the TAMS. For each star, we compare the central hydro-gen abundances of the neighboring tracks in mass and age used for interpolation. We define the TAMS as the epoch where the central hydrogen abundance falls be-low 10 âˆ’5. We then take the minimum of the two TAMS ages and raise a ValueError if the user-provided age exceeds the TAMS by more than 10%. This step pre-vents extrapolation into evolutionary phases where the collision results will be very sensitive to stellar structure and which are not part of the current training data. Finally, if the interaction is determined to be in the direct collision regime ( rp â‰¤ R1 + R2), then the two ML models are used to predict the outcome and final masses. If the interaction is in the tidal capture limit, a merger with no mass loss is assumed (e.g., Benz & Hills 1987; Lai et al. 1993). Lastly, in the case of a fly-by both stellar masses are returned unchanged. Aflowchart illustrating the decision process in collAIder 

is shown in Figure 9. Given the marginal outperformance of the individu-ally trained NNs over the MoE, we employ the former as the ML model to predict collision outcomes. As shown in the first panel of Figure 8, the regression model re-turns continuous values and therefore never predicts a final mass exactly equal to zero. Rather than applying a minimum mass cutoff for the remnant objectâ€”which makes assumptions about what is considered a collision productâ€”we adjust the regression predictions based on the classification output to ensure consistency. For example, when the classifier predicts a merger, the regression model may return a small (but nonzero) mass for star 2, alongside a correct prediction for the merger remnant. To address this discrepancy, we set 

M2,f exactly to zero and add its predicted mass to the ejected mass. This approach is valid because the NN was trained with the merger mass always assigned to 

M1,f .Consequently, the performance metrics change com-pared to those listed in Table 2 and Table 4. We report the adjusted performance metrics on the test dataset us-ing collAIder in Table 7 and explore its performance at both interpolation and extrapolation in the section below. For the test dataset, we observe a slight decrease in accuracy and balanced accuracy for the classification task. This occurs because some inputs are classified as tidal captures or fly-bys (instead of direct collisions) due to slight differences between the stellar radii pre-dicted by our MESA models and those from the Posydon 

v2 dataset. Nevertheless, the performance difference is negligible. For the regression task, errors remain compa-16 CollAIder Flowchart     

> collAIder
> rpâ‰¤R1+R2

User Inputs    

> ,,,,

Age [Myr ] M1, i [MâŠ™] M2, i [MâŠ™] rp [RâŠ™] vâˆž [km /s]       

> Estimate Stellar Radii
> Direct physical collision
> Yes
> No
> E=Eorb âˆ’Etidal â‰¤0No
> Yes
> Fly by Tidal capture
> Number of stars : 1

M1, f [MâŠ™] = M1, i + M2, i

M2, f [MâŠ™] = 0

> Number of stars : 2

M1, f [MâŠ™] = M1, i

M2, f [MâŠ™] = M2, i

> Preprocess data
> (rescale and standardize)
> Predict final number
> of stars and their
> masses using a neural
> network
> Class 0
> Number of stars : 0

M1, f [MâŠ™] = 0

M2, f [MâŠ™] = 0

> Class 1 or 3
> Number of stars : 1

M1, f [MâŠ™] = MNN  

> 1, f

[MâŠ™]

M2, f [MâŠ™] = 0

> Number of stars : 2

M1, f [MâŠ™] = MNN  

> 1, f

[MâŠ™]

M2, f [MâŠ™] = MNN  

> 2, f

[MâŠ™]

> Class 2

Figure 9. Flowchart illustrating the decision process in collAIder , described in Section 6. Blue boxes indicate decision points, while green boxes denote the final outputs of the software package. The neural network is only used when the input parameters correspond to a physical collision, and the regression outputs are adjusted based on the classification decisions. Masses with a NN superscript indicate predictions obtained using the neural network. 

rable. A median error of 0 MâŠ™ for M2,f results from the added step of rounding M2,f to zero in cases of mergers or mutual destruction. 

7. STRESS TESTS The full parameter space for initial conditions is vast, and it extends past the values sampled in this study. To test the robustness of our NN models, we have per-formed additional SPH simulations both within the in-terpolation regime as well as for data outside of the sam-pled parameter space. 

â€¢ Interpolation: For the interpolation dataset, we primarily focus on evaluating how the model gen-eralizes for different initial masses. We perform SPH collisions involving 45 and 23 MâŠ™ stars at 2 .5Myr, which is roughly halfway through the main sequence lifetime of the 45 MâŠ™ star. We ran col-lisions at pericenter distances enclosing 0 , 0.4, 0.8, and 1 of the total stellar mass, and at velocities (10 , 500 , 2000 , 8000) km/s. 17 

Table 7. collAIder Performance Metrics                                   

> Dataset Accuracy [%] Balanced Accuracy [%] Median Absolute Error [M âŠ™]Median Relative Error
> M1,f M2,f M1,f M2,f
> Testing 98.2 98.2 0.0043 0.0 0.0011 0.0014 Interpolation 100 100 0.091 0.0 0.0022 0.0036 TAMS 94.9 89.4 0.0086 0.0 0.0041 0.0044 Extrapolation 84.4 78.6 1.53 0.0 0.015 0.033
> Note â€”Performance metrics using the collAIder package on the datasets described in Section 7. The second and third columns show accuracy and balanced accuracy for the classification task. The fourth and fifth columns show the median absolute errors in the final mass predictions for stars 1 and 2, respectively. The sixth and seventh columns show the median relative errors in the final mass predictions for the cases where the respective star survives.

â€¢ TAMS Set: We constructed a grid at 562 Myr, near the TAMS for a 2 MâŠ™ star ( âˆ¼ 600 Myr), fol-lowing the same sampling techniques for the sec-ondary masses, velocities, and pericenter distances as the base TAMS grid. Although this grid is al-most identical to the existing 2 MâŠ™ TAMS grid, we aim to test how well the model adapts to collisions of stars near the TAMS. 

â€¢ Extrapolation Set: Crucially, we wanted to assert how reliable the models are for out-of-distribution data. Because our SPH models rely on MESA profiles, we have limited this testing data set to two stellar masses, 75 and 90 MâŠ™. Note that they are both outside of the range of stellar masses explored in this study. We perform colli-sions between the 75 MâŠ™ and 90 MâŠ™ stars at 0 .07 and 3 .3 Myr, roughly at the ZAMS of the 75 MâŠ™

star and at the TAMS of the 90 MâŠ™. This serves the purpose of testing whether the algorithm has learned the importance of stellar structure varia-tions across time. We perform collisions with the same pericenter and velocities values as in the in-terpolation dataset. The predictions were computed using collAIder to obtain performance metrics representative of what a user would experience. These metrics are summarized in Table 7. For the classification task, the extrapola-tion set shows the largest decrease in performance with a balanced accuracy of 78.6%. This is expected since both the interpolation and TAMS datasets lie within the range of explored masses and ages. For the regres-sion task, median absolute errors remain low for most datasets, with the notable exception of the extrapolation dataset, which has a median absolute error of 1 .53 MâŠ™

for M1,f . However, the corresponding relative error (ex-cluding mutual destruction cases) is only 1.5%, indicat-ing good accuracy given that the initial masses are both at least 75 MâŠ™. Across all datasets, relative errors in the final mass predictions are below 1.5% and 3.3% for M1,f

and M2,f , respectively. In addition, we performed a targeted stress test moti-vated by a case from the forthcoming work of Parmer-lee et al. 2026 in which collAIder predicts a small net mass gain by the more massive star during a grazing encounter. Such outcomes are uncommon but do occur in the SPH dataset. In grazing envelopeâ€“envelope inter-actions, some shocked fluid is brought to low speeds in the center of mass system and can later be re-accreted: if more material originating from star 2 falls back onto star 1 than mass from star 1 becomes unbound, star 1 can experience a small net mass gain. Because our re-gression model predicts the partition of the bound mass among the two survivors while enforcing overall mass conservation, it naturally permits this behavior rather than imposing mass loss for each star. Our specific test involves the collision at age 0 .79 Gyr with ( M1,i , M 2,i ) = (1 .593 , 1.128) MâŠ™, rp = 0 .568 RâŠ™,and vâˆž = 584 km s âˆ’1. The SPH simulation yields a two-star outcome with ( M1,f , M 2,f ) = (1 .599 , 1.104) MâŠ™. In this case star 1 gains 0 .006 MâŠ™ (0 .4%) while the system as a whole loses 0 .017 MâŠ™ (0 .6%) to unbound ejecta. The collAIder prediction for the same initial conditions correctly captures the sign of the effect ( M1,f > M 1,i ), and although it overpredicts the accreted mass, the pre-dicted M1,f = 1 .63 MâŠ™ remains within 2% of the SPH value. This demonstrates that the model can reproduce these rare events at a level consistent with the expected accuracy of the regression task. 

8. DISCUSSION & CONCLUSIONS In this work we have presented a new set of 27 ,720 SPH simulations of stellar collisions spanning a wide range of ages, masses, relative velocities, and pericen-ter distances. This comprehensive dataset is used to train machine learning models to predict not only the outcome of the interaction but also the final masses of the remnant stars. In this regime, there are three pos-sible classification labels, 0 , 1, 2, and 3. The first three intuitively indicate the number of remnant stars: 0 de-notes mutual destruction events, 1 denotes mergers, and 2 mild collisions. Label 3 indicates stripped star cases, where a highly energetic event (at high relative speeds 18 and moderately small pericenter distances) completely destroys one star and partially strips the other one. Al-though one star remains in this case, it is its own class label since the hydrodynamic evolution of the collision is very different. For the regression task, the model is trained to predict the final fractional mass in each star and unbound material. The NN is set up in a way that ensures mass conservation. We compare the classifica-tion and regression performance of three ML algorithms (k-NN, SVM, and NNs), finding: 

â€¢ Classification: the SVM and NN have compa-rable performance, with balanced accuracies of 97.7% and 98.4%, respectively. This is unsurpris-ing, as SVMs are well suited for multi-class clas-sification. Furthermore, most of the misclassified data points lie near the decision boundaries, as expected. 

â€¢ Regression: the NN outperforms all other meth-ods, with a median absolute error in stellar masses of 0 .00441 MâŠ™ for M1,f and 3 .7 Ã— 10 âˆ’7MâŠ™ for 

M2,f . The corresponding relative errors are 0.11% and 0.15%. As is the case for the classification task, most errors lie along areas near the decision boundaries, where small deviations in initial con-ditions produce large changes in the final masses of the stars. We also investigated whether a MoE architecture, which consists of initial shared layers followed by four separate regression experts, each trained on data cor-responding to predicted classification labels, would out-perform the separately-trained NNs. We find that the MoE has comparable balanced accuracy for the clas-sification task and slightly larger errors for regression. Nevertheless, the gating mechanism is well-trained and the performance of the MoE model remains comparable to that of the individually trained NNs. Future studies will investigate different MoE architectures to improve performance further. Finally, we present the package collAIder , which uses the trained ML models to predict stellar collision out-comes and remnant properties. The package, outlined in Section 6, classifies stellar encounters in the direct phys-ical collision, tidal capture, and fly by regime, returning predictions for a wide range of dynamical properties. Its performance is tested on interpolation, extrapolation, and near-TAMS cases. The model struggles the most with the extrapolation dataset, yielding a balanced ac-curacy of 78.6% and median relative errors of 1.5% and 3.3% in the final predicted masses of stars 1 and 2, re-spectively. Although it is already quite extensive, the SPH grid used here to train the ML models encompasses a limited range of sampled stellar models and collision parame-ters. Thus, performance will inevitably decrease for data beyond the sampled ranges. Nevertheless, the extrapo-lation tests outlined in Section 7 already show satisfac-tory performance for collisions involving MS stars with metallicity of 0 .01 ZâŠ™.The collAIder pipeline approximates stellar radii to estimate whether the encounter will be in the direct physical collision regime (in which case the ML model is used for inference), the tidal capture regime (in which case a merger with no mass loss is assumed) or a dis-tant fly by. These stellar radii are interpolated using the 

Posydon v2 MESA tracks (Andrews et al. 2025). While these tracks sample stellar masses and ages very finely, interpolation is still needed to infer radii at arbitrary user inputs. Uncertainties in these interpolated values introduce small errors in stellar radii which can, in edge cases, cause misclassifications in the nature of the en-counter. Furthermore, predictions are limited to MS stars with metallicity Z = 0 .01 ZâŠ™. Future work will expand into other metallicities as well as incorporate collisions in-volving giant stars and compact objects. This work showcases the potential impact and ap-plication of ML algorithms trained on large datasets. Such NN models can dramatically reduce computational costs and deliver physically-consistent results almost im-mediately. A particularly impactful application lies in 

N -body simulations, where no mass loss is often as-sumed for stellar collisions. Incorporating ML models into large-scale N -body frameworks will enable more re-alistic treatment of stellar collisions, with crucial impli-cations for understanding collision remnants and their properties, including blue stragglers, BHs in the upper-mass gap, and the transient signals that might accom-pany these collisions. Forthcoming work (Parmerlee et al. 2026, in prep.) performs initial tests of the NN mod-els presented in this paper by implementing them into semi-analytical models of the Milky Wayâ€™s Galactic cen-ter.  

> 9.

ACKNOWLEDGEMENTS We thank Ugur Demir, Nabeel Rehembtulla, Ved Shah, and Philipp Srivastava for useful discussions. This work was supported by NSF Grant AST-2511543 to F.A.R. and T.S. at Northwestern University. Support for E.G.P. was provided by the NSF Graduate Re-search Fellowship Program under Grant DGE-2234667. S.C.R. is grateful for support from the Lindheimer Fel-lowship. F.K. and C.E.O. acknowledge support from a CIERA Postdoctoral Fellowship. T.C.P. was sup-ported in part by NSF Grants AST-2149425 and AST-2446392. We gratefully acknowledge the support of the NSF-Simons AI-Institute for the Sky (SkAI) via grants NSF AST-2421845 and Simons Foundation MPS-AI-00010513. This work used Bridges-2 at Pittsburgh Su-percomputing Center through allocation PHY-240311 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, 19 which is supported by NSF Grants 2138259, 2138286, 2138307, 2137603, and 2138296. This research was also supported in part through the computational resources and staff contributions provided for the Quest high-performance computing facility at Northwestern Uni-versity, which is jointly supported by the Office of the Provost, the Office for Research, and Northwestern Uni-versity Information Technology. AF and SC also ac-knowledge the support of the Aspen Center for PHYics, funded by the NSF Grant PHY-2210452, where some of this work was initiated. REFERENCES 

Agarap, A. F. 2018, arXiv e-prints, arXiv:1803.08375, doi: 10.48550/arXiv.1803.08375 Agrawal, P., Hurley, J., Stevenson, S., SzÂ´ ecsi, D., & Flynn, C. 2020, MNRAS, 497, 4549, doi: 10.1093/mnras/staa2264 Amaro Seoane, P. 2023, ApJ, 947, 8, doi: 10.3847/1538-4357/acb8b9 â€”. 2025, arXiv e-prints, arXiv:2509.12352, doi: 10.48550/arXiv.2509.12352 Andrews, J. J., Bavera, S. S., Briel, M., et al. 2025, ApJS, 281, 3, doi: 10.3847/1538-4365/adfb78 Asplund, M., Grevesse, N., Sauval, A. J., & Scott, P. 2009, ARA&A, 47, 481, doi: 10.1146/annurev.astro.46.060407.145222 Balberg, S., Sari, R., & Loeb, A. 2013, MNRAS, 434, L26, doi: 10.1093/mnrasl/slt071 Balberg, S., & Yassur, G. 2023, ApJ, 952, 149, doi: 10.3847/1538-4357/acdd73 Baumgardt, H., & Hilker, M. 2018, MNRAS, 478, 1520, doi: 10.1093/mnras/sty1057 Benz, W., & Hills, J. G. 1987, ApJ, 323, 614, doi: 10.1086/165857 Brutman, Y., Steinberg, E., & Balberg, S. 2024, ApJL, 974, L22, doi: 10.3847/2041-8213/ad808f Cambioni, S., Asphaug, E., Emsenhuber, A., et al. 2019, ApJ, 875, 40, doi: 10.3847/1538-4357/ab0e8a Cambioni, S., Jacobson, S. A., Emsenhuber, A., et al. 2021, PSJ, 2, 93, doi: 10.3847/PSJ/abf0ad Choi, J., Dotter, A., Conroy, C., et al. 2016, ApJ, 823, 102, doi: 10.3847/0004-637X/823/2/102 Cortes, C., & Vapnik, V. N. 1995, Machine Learning, 20, 273. https://api.semanticscholar.org/CorpusID:52874011 Cover, T., & Hart, P. 1967, IEEE Transactions on Information Theory, 13, 21, doi: 10.1109/TIT.1967.1053964 Dale, J. E., Davies, M. B., Church, R. P., & Freitag, M. 2009, MNRAS, 393, 1016, doi: 10.1111/j.1365-2966.2008.14254.x Emsenhuber, A., Cambioni, S., Asphaug, E., et al. 2020, ApJ, 891, 6, doi: 10.3847/1538-4357/ab6de5 Fabian, A. C., Pringle, J. E., & Rees, M. J. 1975, MNRAS, 172, 15, doi: 10.1093/mnras/172.1.15P Fragos, T., Andrews, J. J., Bavera, S. S., et al. 2023, ApJS, 264, 45, doi: 10.3847/1538-4365/ac90c1 Freitag, M., & Benz, W. 2005, MNRAS, 358, 1133, doi: 10.1111/j.1365-2966.2005.08770.x Freitag, M., Dale, J. E., Church, R. P., & Davies, M. B. 2007, Proceedings of the International Astronomical Union, 3, 211â€“214, doi: 10.1017/S1743921308017675 Fukushima, K. 1969, IEEE Transactions on Systems Science and Cybernetics, 5, 322, doi: 10.1109/TSSC.1969.300225 Gaburov, E., Lombardi, Jr., J. C., & Portegies Zwart, S. 2010, MNRAS, 402, 105, doi: 10.1111/j.1365-2966.2009.15900.x Genzel, R., Eisenhauer, F., & Gillessen, S. 2010, Reviews of Modern Physics, 82, 3121, doi: 10.1103/RevModPhys.82.3121 Gibson, C., KÄ±roË˜ glu, F., Lombardi, J. C., J., et al. 2024, arXiv e-prints, arXiv:2410.02146, doi: 10.48550/arXiv.2410.02146 Glebbeek, E., Pols, O. R., & Hurley, J. R. 2008, A&A, 488, 1007, doi: 10.1051/0004-6361:200809930 Glorot, X., Bordes, A., & Bengio, Y. 2011, in Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, 315â€“323 GonzÂ´ alez Prieto, E., Weatherford, N. C., Fragione, G., Kremer, K., & Rasio, F. A. 2024, ApJ, 969, 29, doi: 10.3847/1538-4357/ad43d6 GÂ¨ urkan, M. A., Fregeau, J. M., & Rasio, F. A. 2006, ApJL, 640, L39, doi: 10.1086/503295 Harris, W. E. 2010, arXiv e-prints, arXiv:1012.3224, doi: 10.48550/arXiv.1012.3224 Hornik, K., Stinchcombe, M., & White, H. 1989, Neural Networks, 2, 359, doi: 10.1016/0893-6080(89)90020-8 Hu, B. X., & Loeb, A. 2024, A&A, 689, A23, doi: 10.1051/0004-6361/202450308 Hurley, J. R., Pols, O. R., Aarseth, S. J., & Tout, C. A. 2005, MNRAS, 363, 293, doi: 10.1111/j.1365-2966.2005.09448.x Hurley, J. R., Tout, C. A., Aarseth, S. J., & Pols, O. R. 2001, MNRAS, 323, 630, doi: 10.1046/j.1365-8711.2001.04220.x 20 

Hwang, J., Lombardi, Jr., J. C., Rasio, F. A., & Kalogera, V. 2015, ApJ, 806, 135, doi: 10.1088/0004-637X/806/1/135 Ivanova, N., Justham, S., Avendano Nandez, J. L., & Lombardi, J. C. 2013, Science, 339, 433, doi: 10.1126/science.1225540 Jermyn, A. S., Bauer, E. B., Schwab, J., et al. 2023, ApJS, 265, 15, doi: 10.3847/1538-4365/acae8d Jordan, M., & Jacobs, R. 1993, in Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan), Vol. 2, 1339â€“1344 vol.2, doi: 10.1109/IJCNN.1993.716791 Kendall, A., Gal, Y., & Cipolla, R. 2017, arXiv e-prints, arXiv:1705.07115, doi: 10.48550/arXiv.1705.07115 Kirilov, A., CalderÂ´ on, D., Pejcha, O., & Duffell, P. C. 2025, ApJL, 994, L41, doi: 10.3847/2041-8213/ae1ae7 KÄ±roË˜ glu, F., Kremer, K., & Rasio, F. A. 2025, ApJL, 994, L37, doi: 10.3847/2041-8213/ae1eeb KÄ±roË˜ glu, F., Lombardi, J. C., Kremer, K., et al. 2023, ApJ, 948, 89, doi: 10.3847/1538-4357/acc24c Kremer, K., Lombardi, J. C., Lu, W., Piro, A. L., & Rasio, F. A. 2022, ApJ, 933, 203, doi: 10.3847/1538-4357/ac714f Kremer, K., Lu, W., Rodriguez, C. L., Lachat, M., & Rasio, F. A. 2019, ApJ, 881, 75, doi: 10.3847/1538-4357/ab2e0c Kremer, K., Spera, M., Becker, D., et al. 2020, ApJ, 903, 45, doi: 10.3847/1538-4357/abb945 Lai, D., Rasio, F. A., & Shapiro, S. L. 1993, ApJ, 412, 593, doi: 10.1086/172946 LeCun, Y., Bengio, Y., & Hinton, G. 2015, Nature, 521, 436, doi: 10.1038/nature14539 Lee, H. M., & Ostriker, J. P. 1986, ApJ, 310, 176, doi: 10.1086/164674 Lei Ba, J., Kiros, J. R., & Hinton, G. E. 2016, arXiv e-prints, arXiv:1607.06450, doi: 10.48550/arXiv.1607.06450 Leonard, P. J. T. 1989, AJ, 98, 217, doi: 10.1086/115138 Liebel, L., & KÂ¨ orner, M. 2018, arXiv e-prints, arXiv:1805.06334, doi: 10.48550/arXiv.1805.06334 Lombardi, Jr., J. C., Proulx, Z. F., Dooley, K. L., et al. 2006, ApJ, 640, 441, doi: 10.1086/499938 Lombardi, Jr., J. C., Rasio, F. A., & Shapiro, S. L. 1996, ApJ, 468, 797, doi: 10.1086/177736 Lopez, Jr., M., Batta, A., Ramirez-Ruiz, E., Martinez, I., & Samsing, J. 2019, ApJ, 877, 56, doi: 10.3847/1538-4357/ab1842 Loshchilov, I., & Hutter, F. 2017, arXiv e-prints, arXiv:1711.05101, doi: 10.48550/arXiv.1711.05101 Maas, A. L., Hannun, A. Y., & Ng, A. Y. 2013, in Proc. ICML Workshop on Deep Learning for Audio, Speech and Language Processing MacLeod, M., Macias, P., Ramirez-Ruiz, E., et al. 2017, ApJ, 835, 282, doi: 10.3847/1538-4357/835/2/282 McCulloch, W. S., & Pitts, W. 1943, Bull. Math. Biol., 5, 115, doi: 10.1007/BF02478259 Metzger, B. D., & Pejcha, O. 2017, MNRAS, 471, 3200, doi: 10.1093/mnras/stx1768 Monaghan, J. J. 1992, ARA&A, 30, 543, doi: 10.1146/annurev.aa.30.090192.002551 Nandez, J. L. A., Ivanova, N., & Lombardi, Jr., J. C. 2014, ApJ, 786, 39, doi: 10.1088/0004-637X/786/1/39 Neights, E., Burns, E., Fryer, C. L., et al. 2026, MNRAS, 545, staf2019, doi: 10.1093/mnras/staf2019 Oâ€™Connor, B., Gill, R., DeLaunay, J., et al. 2025, ApJL, 994, L17, doi: 10.3847/2041-8213/ae1741 Paszke, A., Gross, S., Massa, F., et al. 2019, arXiv e-prints, arXiv:1912.01703, doi: 10.48550/arXiv.1912.01703 Paxton, B., Bildsten, L., Dotter, A., et al. 2011, ApJS, 192, 3, doi: 10.1088/0067-0049/192/1/3 Paxton, B., Cantiello, M., Arras, P., et al. 2013, ApJS, 208, 4, doi: 10.1088/0067-0049/208/1/4 Paxton, B., Marchant, P., Schwab, J., et al. 2015, ApJS, 220, 15, doi: 10.1088/0067-0049/220/1/15 Paxton, B., Schwab, J., Bauer, E. B., et al. 2018, ApJS, 234, 34, doi: 10.3847/1538-4365/aaa5a8 Paxton, B., Smolec, R., Schwab, J., et al. 2019, ApJS, 243, 10, doi: 10.3847/1538-4365/ab2241 Payne, A. V., Shappee, B. J., Hinkle, J. T., et al. 2021, ApJ, 910, 125, doi: 10.3847/1538-4357/abe38d Pedregosa, F., Varoquaux, G., Gramfort, A., et al. 2011, Journal of Machine Learning Research, 12, 2825 Peng, Z.-k., Gao, H., & Zhang, X.-F. 2025, ApJ, 995, 58, doi: 10.3847/1538-4357/ae1583 Perets, H. B., Li, Z., Lombardi, Jr., J. C., & Milcarek, Jr., S. R. 2016, ApJ, 823, 113, doi: 10.3847/0004-637X/823/2/113 Planck Collaboration, Ade, P. A. R., Aghanim, N., et al. 2016, A&A, 594, A13, doi: 10.1051/0004-6361/201525830 Portegies Zwart, S. F., Baumgardt, H., Hut, P., Makino, J., & McMillan, S. L. W. 2004, Nature, 428, 724, doi: 10.1038/nature02448 Portegies Zwart, S. F., & Meinen, A. T. 1993, A&A, 280, 174 Press, W. H., & Teukolsky, S. A. 1977, ApJ, 213, 183, doi: 10.1086/155143 Pryor, C., & Meylan, G. 1993, in Astronomical Society of the Pacific Conference Series, Vol. 50, Structure and Dynamics of Globular Clusters, ed. S. G. Djorgovski & G. Meylan, 357 Rasio, F. A. 1991, PhD thesis, Cornell University, New York Rauch, K. P. 1999, ApJ, 514, 725, doi: 10.1086/306953 21 

Rose, S. C., Lombardi, Jr., J. C., GonzÂ´ alez Prieto, E., KÄ±roË˜ glu, F., & Rasio, F. A. 2025, arXiv e-prints, arXiv:2511.01811, doi: 10.48550/arXiv.2511.01811 Rose, S. C., & Mockler, B. 2025, ApJL, 985, L40, doi: 10.3847/2041-8213/add266 Rose, S. C., Naoz, S., Sari, R., & Linial, I. 2023, arXiv e-prints, arXiv:2304.10569, doi: 10.48550/arXiv.2304.10569 Ryu, T., Amaro Seoane, P., Taylor, A. M., & Ohlmann, S. T. 2024, MNRAS, 528, 6193, doi: 10.1093/mnras/stae396 Ryu, T., Perna, R., & Wang, Y.-H. 2022, MNRAS, 516, 2204, doi: 10.1093/mnras/stac2316 Sanders, R. H. 1970, ApJ, 162, 791, doi: 10.1086/150711 Shara, M. M. 2002, in Astronomical Society of the Pacific Conference Series, Vol. 263, Stellar Collisions, Mergers and their Consequences, ed. M. M. Shara, 1 Sills, A., Karakas, A., & Lattanzio, J. 2009, ApJ, 692, 1411, doi: 10.1088/0004-637X/692/2/1411 Snoek, J., Larochelle, H., & Adams, R. P. 2012, arXiv e-prints, arXiv:1206.2944, doi: 10.48550/arXiv.1206.2944 Spitzer, Jr., L., & Saslaw, W. C. 1966, ApJ, 143, 400, doi: 10.1086/148523 Sun, M., Townsend, R. H. D., & Guo, Z. 2023, ApJ, 945, 43, doi: 10.3847/1538-4357/acb33a Townsend, R. H. D., & Teitler, S. A. 2013, MNRAS, 435, 3406, doi: 10.1093/mnras/stt1533 van der Merwe, C. J. T., Mohamed, S. S., JosÂ´ e, J., Shara, M., & KamiÂ´ nski, T. 2024, MNRAS, 534, 3637, doi: 10.1093/mnras/stae2329 van der Merwe, C. J. T., Mohamed, S. S., JosÂ´ e, J., Shara, M. M., & KamiÂ´ nski, T. 2025, MNRAS, 538, 1384, doi: 10.1093/mnras/staf328 Wang, Y.-H., Perna, R., & Armitage, P. J. 2021, MNRAS, 503, 6005, doi: 10.1093/mnras/stab802 Weinberger, R., Springel, V., & Pakmor, R. 2020, ApJS, 248, 32, doi: 10.3847/1538-4365/ab908c Wendland, H. 1995, Advances in Computational Mathematics, 4, 389, doi: 10.1007/BF02123482 Xin, C., Haiman, Z., Perna, R., Wang, Y., & Ryu, T. 2024, ApJ, 961, 149, doi: 10.3847/1538-4357/ad11d3 22 APPENDIX 

A. SVR GRIDSEARCH In this section, we detail the grid search performed for the regression task using the support-vector regression (SVR) module in ScikitLearn . The grid search is performed for each regressed quantity separately, as summarized in Table 8. Three separate kernels were explored: radial basis function ( rbf ), polynomial, and sigmoid. A more limited exploration of the polynomial kernel was followed, since a single model can take several hours to train. The hyperparameter search ranges were chosen according to computational resources and to maximize exploration of the parameter space. For all quantities except the degree we searched over logarithmically-spaced values. The C parameter acts as a regularizer that controls the complexity of the model, with large values imposing stronger penalties and leading to more complex models. The parameter gamma determines the influence of individual training points and Ïµ defines the width of the tube within which no penalty is assigned to errors. Following an initial coarse grid search, we conducted a refined search around smaller epsilon values using the optimized values found for the kernel, C, and gamma . The final optimized values are listed in Table 8. 

Table 8. SVR Hyperparameter Grid Search Ranges                                                                                                                                

> Quantity Kernel Cgamma ÏµDegree
> Initial Coarse Grid Search
> q1,f rbf 0.1âˆ’100 0.01 âˆ’10 0.001 âˆ’0.1â€”
> q1,f Polynomial 0.1âˆ’10 0.01 âˆ’10.01 âˆ’0.12âˆ’3
> q1,f Sigmoid 0.1âˆ’1000 0.01 âˆ’10 0.0001 âˆ’0.1â€”
> q2,f rbf 0.1âˆ’100 0.01 âˆ’10 0.001 âˆ’0.1â€”
> q2,f Polynomial 0.1âˆ’10 0.01 âˆ’10.001 âˆ’0.12âˆ’3
> q2,f Sigmoid 0.1âˆ’1000 0.01 âˆ’10 0.0001 âˆ’0.1â€”
> qu,frbf 0.1âˆ’100 0.01 âˆ’10 0.001 âˆ’0.1â€”
> qu,fPolynomial 0.1âˆ’10 0.01 âˆ’10.001 âˆ’0.12âˆ’3
> qu,fSigmoid 0.1âˆ’1000 0.01 âˆ’10 0.0001 âˆ’0.1â€”
> Refined Fine Grid Search
> All rbf 110 ,50 0.00001 âˆ’0.001 â€”
> Optimized Values
> All rbf 110 0.00001 â€”
> Note â€”Support Vector Regression (SVR) hyperparameter search for the final fractional mass in star 1 ( q1,f ), star 2 ( q2,f ), and unbound mass (qu,f). A search was conducted across three kernels, the radial basis function ( rbf ), polynomial, and sigmoid.

B. MOE LOSS FUNCTION The challenge of any multitask architecture is optimizing performance across all tasks. In this work, we have used uncertainty-weighted loss (see Eq. 10 in Kendall et al. 2017) with the modification to the regularization term outlined in Liebel & KÂ¨ orner (2018). This method weights the loss of each task by learnable parameters that are optimized with the network weights. In the left panel of Figure 10 we show the respective loss functions for the classification and regression tasks, alongside the overall loss of the model during the training. The figure shows both training and validation losses, illustrating that the model begins to overfit after approximately epoch 107 (kept as the best model), shown by the increase in the validation loss. The monotonic decrease of both task-specific loss functions demonstrates that the uncertainty-weighted approach is successfully balancing optimization across both tasks. The evolution of the learned weight parameters is shown in the right panel, with the optimal classification and regression uncertainty weights found to be 0 .36 and 0 .26, respectively. 23 10 0 10 1 10 2 10 3

Epoch    

> 10 âˆ’3
> 10 âˆ’2
> 10 âˆ’1
> 10 0
> Loss
> Train Validation Classification Loss Regression Loss Total Loss
> 10 010 110 210 3

Epoch 

> 0.20.30.40.50.60.70.80.91.0
> Learned Weight Parameter
> Classification Weight Regression Weight Best Epoch

Figure 10. Training (Validation) loss functions for the classification and regression tasks shown in solid (dashed) yellow and blue, respectively. The total loss function is shown in green, obtained using uncertainty-weighted loss (Kendall et al. 2017, Eq. 10). The classification loss function is smoothed with a moving average of size 5 for visualization purposes. The dashed line indicates the epoch with the lowest validation loss. C. MODEL SCORING Because multitask NNs optimize multiple tasks simultaneously, a metric tailored to the specific science case is needed to evaluate model performance. We score the MoE models using Score = Î±â€²(1 âˆ’ BA) + Î²â€² MedAE âˆ— 

> 1

+ Î·â€² MedAE âˆ— 

> 2

(C1) where BA is balanced accuracy and MedAE âˆ— 

> i

= (MedAE i âˆ’ MedAE i, min )/(MedAE i, max âˆ’ MedAE i, min ) are the min-max normalized median absolute errors in the predicted masses where i = 1 , 2 indicate stars 1 and 2. The prefactors that weight each component are set to (Î±â€², Î² â€², Î· â€²) = 

ï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³

(Î± + Î² + Î·, 0, 0) if MedAE 1, MedAE 2 < 0.005 (Î±, Î² + Î·, 0) if MedAE 2 < 0.005 (Î±, 0, Î· + Î²) if MedAE 1 < 0.005 (Î±, Î², Î· ) otherwise (C2) where Î± = 0 .8, and Î² = Î· = 0 .1. The value of Î± is large to prioritize classification performance. Furthermore, we include a threshold of 0 .005 MâŠ™ for MedAE, below which we do not reward regression performance. This avoids rewarding improvement past an absolute error of 0 .005 MâŠ™, since that is negligible at the sampled masses. The model with the lowest score is selected as the best performing model. Note that this metric is particularly suited for the application of this tool and other scientific goals might require a different metric to identify the best model.