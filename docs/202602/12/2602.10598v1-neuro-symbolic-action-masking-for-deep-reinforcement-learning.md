---
title: Neuro-symbolic Action Masking for Deep Reinforcement Learning
title_zh: 深度强化学习中的神经符号动作掩码
authors: "Shuai Han, Mehdi Dastani, Shihan Wang"
date: 2026-02-11
pdf: "https://arxiv.org/pdf/2602.10598v1"
tags: ["keyword:SR", "query:SR"]
score: 6.0
evidence: 用于学习符号模型的神经符号框架
tldr: 针对深度强化学习在训练中常探索不可行动作的问题，本文提出了神经符号动作掩码（NSAM）框架。该框架能在极少监督下，从高维状态中自动学习符合领域约束的符号模型，并据此生成动作掩码以排除无效动作。NSAM 实现了符号推理与策略优化的端到端集成，使两者相互促进，从而在提升采样效率的同时显著减少了约束违反。
motivation: 现有的动作约束方法通常依赖手动设计的符号映射函数和掩码规则，难以自动处理高维状态下的复杂约束。
method: 提出一种自动学习符号模型并生成动作掩码的框架，通过端到端集成符号推理与深度策略优化来约束智能体的行为。
result: 实验结果表明，NSAM 在多个受限领域中显著提升了强化学习的采样效率，并大幅降低了训练和执行过程中的约束违反率。
conclusion: 将神经符号学习与动作掩码相结合，能够有效利用领域知识引导强化学习探索，提升智能体在复杂环境中的性能与安全性。
---

## 摘要
深度强化学习 (DRL) 在训练和执行过程中可能会探索不可行的动作。现有方法通常假设存在一个符号接地 (symbol grounding) 函数，将高维状态映射到一致的符号表示，并采用手动指定的动作掩码技术来约束动作。在本文中，我们提出了神经符号动作掩码 (NSAM)，这是一个新颖的框架，它在 DRL 过程中以极小监督的方式自动学习符号模型，这些模型与给定的高维状态领域约束保持一致。基于学习到的状态符号模型，NSAM 学习动作掩码以排除不可行的动作。NSAM 实现了符号推理与深度策略优化的端到端集成，其中符号接地和策略学习的改进相互促进。我们在多个带有约束的领域中评估了 NSAM，实验结果表明，NSAM 在显著提高 DRL 智能体样本效率的同时，大幅减少了约束违反。

## Abstract
Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.